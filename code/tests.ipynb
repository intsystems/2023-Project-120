{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pkbab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<module 'utils' from 'c:\\\\Users\\\\pkbab\\\\Documents\\\\code\\\\2023-Project-120\\\\code\\\\utils.py'>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from importlib import reload\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import datasets\n",
        "from model import CNN\n",
        "\n",
        "import utils\n",
        "reload(utils)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = 1\n",
        "batch_size = 64\n",
        "log_frequency = 10\n",
        "channels = 16\n",
        "unrolled = False\n",
        "visualization = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = \"fashionmnist\"\n",
        "\n",
        "dataset_train, dataset_valid = datasets.get_dataset(dataset)        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Architecture search for a range of $\\lambda$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weight = 100.0, lambd = 2\n",
            "[2023-11-09 15:31:18] \u001b[32mEpoch [1/10] Step [1/469]  acc1 0.109375 (0.109375)  loss 3548.809082 (3548.809082)\u001b[0m\n",
            "[2023-11-09 15:31:19] \u001b[32mEpoch [1/10] Step [11/469]  acc1 0.203125 (0.133523)  loss 3.945093 (1291.753094)\u001b[0m\n",
            "[2023-11-09 15:31:21] \u001b[32mEpoch [1/10] Step [21/469]  acc1 0.156250 (0.138393)  loss 114.139839 (1027.435139)\u001b[0m\n",
            "[2023-11-09 15:31:22] \u001b[32mEpoch [1/10] Step [31/469]  acc1 0.265625 (0.156754)  loss 883.323486 (802.645937)\u001b[0m\n",
            "[2023-11-09 15:31:24] \u001b[32mEpoch [1/10] Step [41/469]  acc1 0.296875 (0.191311)  loss 106.666603 (654.718983)\u001b[0m\n",
            "[2023-11-09 15:31:25] \u001b[32mEpoch [1/10] Step [51/469]  acc1 0.296875 (0.214767)  loss 14.447690 (539.692744)\u001b[0m\n",
            "[2023-11-09 15:31:26] \u001b[32mEpoch [1/10] Step [61/469]  acc1 0.312500 (0.228484)  loss 16.290672 (463.143814)\u001b[0m\n",
            "[2023-11-09 15:31:28] \u001b[32mEpoch [1/10] Step [71/469]  acc1 0.359375 (0.254621)  loss 1.580795 (398.180246)\u001b[0m\n",
            "[2023-11-09 15:31:29] \u001b[32mEpoch [1/10] Step [81/469]  acc1 0.546875 (0.275270)  loss 1.239853 (356.965124)\u001b[0m\n",
            "[2023-11-09 15:31:31] \u001b[32mEpoch [1/10] Step [91/469]  acc1 0.515625 (0.301511)  loss 1.343783 (326.231509)\u001b[0m\n",
            "[2023-11-09 15:31:32] \u001b[32mEpoch [1/10] Step [101/469]  acc1 0.562500 (0.330910)  loss 1.120848 (304.054462)\u001b[0m\n",
            "[2023-11-09 15:31:34] \u001b[32mEpoch [1/10] Step [111/469]  acc1 0.500000 (0.356278)  loss 18.498690 (285.128280)\u001b[0m\n",
            "[2023-11-09 15:31:35] \u001b[32mEpoch [1/10] Step [121/469]  acc1 0.687500 (0.377841)  loss 0.889400 (261.652865)\u001b[0m\n",
            "[2023-11-09 15:31:36] \u001b[32mEpoch [1/10] Step [131/469]  acc1 0.546875 (0.396589)  loss 1.380499 (241.777149)\u001b[0m\n",
            "[2023-11-09 15:31:38] \u001b[32mEpoch [1/10] Step [141/469]  acc1 0.703125 (0.414340)  loss 0.872126 (224.725262)\u001b[0m\n",
            "[2023-11-09 15:31:39] \u001b[32mEpoch [1/10] Step [151/469]  acc1 0.640625 (0.427049)  loss 1.314284 (210.083229)\u001b[0m\n",
            "[2023-11-09 15:31:41] \u001b[32mEpoch [1/10] Step [161/469]  acc1 0.609375 (0.438956)  loss 1.032187 (203.286086)\u001b[0m\n",
            "[2023-11-09 15:31:42] \u001b[32mEpoch [1/10] Step [171/469]  acc1 0.671875 (0.451298)  loss 1.126467 (192.594639)\u001b[0m\n",
            "[2023-11-09 15:31:44] \u001b[32mEpoch [1/10] Step [181/469]  acc1 0.593750 (0.462189)  loss 1.287469 (182.006943)\u001b[0m\n",
            "[2023-11-09 15:31:45] \u001b[32mEpoch [1/10] Step [191/469]  acc1 0.765625 (0.476194)  loss 0.724170 (172.847447)\u001b[0m\n",
            "[2023-11-09 15:31:46] \u001b[32mEpoch [1/10] Step [201/469]  acc1 0.750000 (0.487718)  loss 0.805111 (164.290687)\u001b[0m\n",
            "[2023-11-09 15:31:48] \u001b[32mEpoch [1/10] Step [211/469]  acc1 0.718750 (0.497630)  loss 1.064221 (156.548441)\u001b[0m\n",
            "[2023-11-09 15:31:49] \u001b[32mEpoch [1/10] Step [221/469]  acc1 0.656250 (0.506292)  loss 0.868831 (149.504645)\u001b[0m\n",
            "[2023-11-09 15:31:51] \u001b[32mEpoch [1/10] Step [231/469]  acc1 0.703125 (0.514949)  loss 0.824937 (143.068554)\u001b[0m\n",
            "[2023-11-09 15:31:52] \u001b[32mEpoch [1/10] Step [241/469]  acc1 0.765625 (0.522822)  loss 0.757966 (137.171550)\u001b[0m\n",
            "[2023-11-09 15:31:54] \u001b[32mEpoch [1/10] Step [251/469]  acc1 0.765625 (0.531437)  loss 0.728961 (131.736214)\u001b[0m\n",
            "[2023-11-09 15:31:55] \u001b[32mEpoch [1/10] Step [261/469]  acc1 0.656250 (0.536339)  loss 0.955120 (126.724747)\u001b[0m\n",
            "[2023-11-09 15:31:56] \u001b[32mEpoch [1/10] Step [271/469]  acc1 0.671875 (0.542897)  loss 0.745706 (122.078172)\u001b[0m\n",
            "[2023-11-09 15:31:58] \u001b[32mEpoch [1/10] Step [281/469]  acc1 0.734375 (0.549377)  loss 0.789630 (117.761866)\u001b[0m\n",
            "[2023-11-09 15:31:59] \u001b[32mEpoch [1/10] Step [291/469]  acc1 0.671875 (0.555090)  loss 0.877913 (113.743589)\u001b[0m\n",
            "[2023-11-09 15:32:01] \u001b[32mEpoch [1/10] Step [301/469]  acc1 0.750000 (0.560943)  loss 0.856407 (109.990064)\u001b[0m\n",
            "[2023-11-09 15:32:02] \u001b[32mEpoch [1/10] Step [311/469]  acc1 0.750000 (0.566770)  loss 0.661146 (106.475877)\u001b[0m\n",
            "[2023-11-09 15:32:04] \u001b[32mEpoch [1/10] Step [321/469]  acc1 0.781250 (0.572673)  loss 0.655595 (103.180759)\u001b[0m\n",
            "[2023-11-09 15:32:05] \u001b[32mEpoch [1/10] Step [331/469]  acc1 0.781250 (0.577511)  loss 0.676111 (100.085806)\u001b[0m\n",
            "[2023-11-09 15:32:06] \u001b[32mEpoch [1/10] Step [341/469]  acc1 0.703125 (0.583349)  loss 0.898551 (97.170715)\u001b[0m\n",
            "[2023-11-09 15:32:08] \u001b[32mEpoch [1/10] Step [351/469]  acc1 0.750000 (0.588141)  loss 0.791862 (94.422058)\u001b[0m\n",
            "[2023-11-09 15:32:09] \u001b[32mEpoch [1/10] Step [361/469]  acc1 0.734375 (0.592452)  loss 0.723375 (91.827080)\u001b[0m\n",
            "[2023-11-09 15:32:11] \u001b[32mEpoch [1/10] Step [371/469]  acc1 0.750000 (0.596530)  loss 0.716413 (89.370892)\u001b[0m\n",
            "[2023-11-09 15:32:12] \u001b[32mEpoch [1/10] Step [381/469]  acc1 0.671875 (0.600722)  loss 0.924808 (87.042991)\u001b[0m\n",
            "[2023-11-09 15:32:13] \u001b[32mEpoch [1/10] Step [391/469]  acc1 0.703125 (0.603780)  loss 0.904408 (84.836107)\u001b[0m\n",
            "[2023-11-09 15:32:15] \u001b[32mEpoch [1/10] Step [401/469]  acc1 0.843750 (0.608167)  loss 0.562511 (82.737369)\u001b[0m\n",
            "[2023-11-09 15:32:16] \u001b[32mEpoch [1/10] Step [411/469]  acc1 0.765625 (0.611542)  loss 0.668060 (80.741225)\u001b[0m\n",
            "[2023-11-09 15:32:18] \u001b[32mEpoch [1/10] Step [421/469]  acc1 0.796875 (0.614163)  loss 0.576316 (78.842357)\u001b[0m\n",
            "[2023-11-09 15:32:19] \u001b[32mEpoch [1/10] Step [431/469]  acc1 0.734375 (0.617604)  loss 0.802475 (77.053567)\u001b[0m\n",
            "[2023-11-09 15:32:21] \u001b[32mEpoch [1/10] Step [441/469]  acc1 0.734375 (0.620500)  loss 0.722678 (75.323003)\u001b[0m\n",
            "[2023-11-09 15:32:22] \u001b[32mEpoch [1/10] Step [451/469]  acc1 0.703125 (0.623683)  loss 0.721837 (73.668008)\u001b[0m\n",
            "[2023-11-09 15:32:23] \u001b[32mEpoch [1/10] Step [461/469]  acc1 0.687500 (0.626356)  loss 0.678209 (72.084734)\u001b[0m\n",
            "[2023-11-09 15:32:26] \u001b[32mEpoch [2/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 0.709324 (0.709324)\u001b[0m\n",
            "[2023-11-09 15:32:28] \u001b[32mEpoch [2/10] Step [11/469]  acc1 0.718750 (0.757102)  loss 0.711397 (0.671945)\u001b[0m\n",
            "[2023-11-09 15:32:29] \u001b[32mEpoch [2/10] Step [21/469]  acc1 0.781250 (0.763393)  loss 0.679500 (0.677357)\u001b[0m\n",
            "[2023-11-09 15:32:31] \u001b[32mEpoch [2/10] Step [31/469]  acc1 0.859375 (0.762097)  loss 0.539668 (0.667291)\u001b[0m\n",
            "[2023-11-09 15:32:32] \u001b[32mEpoch [2/10] Step [41/469]  acc1 0.609375 (0.752668)  loss 0.915586 (0.680885)\u001b[0m\n",
            "[2023-11-09 15:32:33] \u001b[32mEpoch [2/10] Step [51/469]  acc1 0.734375 (0.760417)  loss 0.653542 (0.668151)\u001b[0m\n",
            "[2023-11-09 15:32:35] \u001b[32mEpoch [2/10] Step [61/469]  acc1 0.671875 (0.755379)  loss 0.767225 (0.695210)\u001b[0m\n",
            "[2023-11-09 15:32:36] \u001b[32mEpoch [2/10] Step [71/469]  acc1 0.812500 (0.756162)  loss 0.571060 (14.404368)\u001b[0m\n",
            "[2023-11-09 15:32:38] \u001b[32mEpoch [2/10] Step [81/469]  acc1 0.750000 (0.757909)  loss 0.709178 (12.707063)\u001b[0m\n",
            "[2023-11-09 15:32:39] \u001b[32mEpoch [2/10] Step [91/469]  acc1 0.843750 (0.758242)  loss 0.629228 (11.385198)\u001b[0m\n",
            "[2023-11-09 15:32:41] \u001b[32mEpoch [2/10] Step [101/469]  acc1 0.718750 (0.758973)  loss 0.602616 (10.321768)\u001b[0m\n",
            "[2023-11-09 15:32:42] \u001b[32mEpoch [2/10] Step [111/469]  acc1 0.718750 (0.760276)  loss 0.641270 (9.445821)\u001b[0m\n",
            "[2023-11-09 15:32:43] \u001b[32mEpoch [2/10] Step [121/469]  acc1 0.781250 (0.760718)  loss 0.475797 (8.715133)\u001b[0m\n",
            "[2023-11-09 15:32:45] \u001b[32mEpoch [2/10] Step [131/469]  acc1 0.687500 (0.761570)  loss 0.856312 (8.098838)\u001b[0m\n",
            "[2023-11-09 15:32:46] \u001b[32mEpoch [2/10] Step [141/469]  acc1 0.812500 (0.763963)  loss 0.580556 (7.565455)\u001b[0m\n",
            "[2023-11-09 15:32:48] \u001b[32mEpoch [2/10] Step [151/469]  acc1 0.765625 (0.764590)  loss 0.541397 (7.102789)\u001b[0m\n",
            "[2023-11-09 15:32:49] \u001b[32mEpoch [2/10] Step [161/469]  acc1 0.734375 (0.766401)  loss 0.595858 (6.698523)\u001b[0m\n",
            "[2023-11-09 15:32:50] \u001b[32mEpoch [2/10] Step [171/469]  acc1 0.828125 (0.768183)  loss 0.520357 (6.342127)\u001b[0m\n",
            "[2023-11-09 15:32:52] \u001b[32mEpoch [2/10] Step [181/469]  acc1 0.703125 (0.769941)  loss 0.730500 (6.024576)\u001b[0m\n",
            "[2023-11-09 15:32:53] \u001b[32mEpoch [2/10] Step [191/469]  acc1 0.750000 (0.770861)  loss 0.692257 (5.739844)\u001b[0m\n",
            "[2023-11-09 15:32:55] \u001b[32mEpoch [2/10] Step [201/469]  acc1 0.812500 (0.771922)  loss 0.535353 (5.484985)\u001b[0m\n",
            "[2023-11-09 15:32:56] \u001b[32mEpoch [2/10] Step [211/469]  acc1 0.796875 (0.772290)  loss 0.686831 (5.253522)\u001b[0m\n",
            "[2023-11-09 15:32:58] \u001b[32mEpoch [2/10] Step [221/469]  acc1 0.718750 (0.772483)  loss 0.752872 (5.042488)\u001b[0m\n",
            "[2023-11-09 15:32:59] \u001b[32mEpoch [2/10] Step [231/469]  acc1 0.843750 (0.772795)  loss 0.469855 (4.851039)\u001b[0m\n",
            "[2023-11-09 15:33:00] \u001b[32mEpoch [2/10] Step [241/469]  acc1 0.859375 (0.773405)  loss 0.449342 (4.674394)\u001b[0m\n",
            "[2023-11-09 15:33:02] \u001b[32mEpoch [2/10] Step [251/469]  acc1 0.812500 (0.773593)  loss 0.513371 (4.511361)\u001b[0m\n",
            "[2023-11-09 15:33:03] \u001b[32mEpoch [2/10] Step [261/469]  acc1 0.812500 (0.774665)  loss 0.509360 (4.360532)\u001b[0m\n",
            "[2023-11-09 15:33:05] \u001b[32mEpoch [2/10] Step [271/469]  acc1 0.703125 (0.775600)  loss 0.852176 (4.219978)\u001b[0m\n",
            "[2023-11-09 15:33:06] \u001b[32mEpoch [2/10] Step [281/469]  acc1 0.843750 (0.775968)  loss 0.451857 (4.089502)\u001b[0m\n",
            "[2023-11-09 15:33:07] \u001b[32mEpoch [2/10] Step [291/469]  acc1 0.781250 (0.776525)  loss 0.526709 (4.077427)\u001b[0m\n",
            "[2023-11-09 15:33:09] \u001b[32mEpoch [2/10] Step [301/469]  acc1 0.843750 (0.776215)  loss 0.468081 (7.285824)\u001b[0m\n",
            "[2023-11-09 15:33:10] \u001b[32mEpoch [2/10] Step [311/469]  acc1 0.843750 (0.777432)  loss 0.527458 (7.068531)\u001b[0m\n",
            "[2023-11-09 15:33:12] \u001b[32mEpoch [2/10] Step [321/469]  acc1 0.734375 (0.778427)  loss 0.581493 (6.863971)\u001b[0m\n",
            "[2023-11-09 15:33:13] \u001b[32mEpoch [2/10] Step [331/469]  acc1 0.781250 (0.779031)  loss 0.800082 (6.673583)\u001b[0m\n",
            "[2023-11-09 15:33:15] \u001b[32mEpoch [2/10] Step [341/469]  acc1 0.828125 (0.780059)  loss 0.627799 (6.494157)\u001b[0m\n",
            "[2023-11-09 15:33:16] \u001b[32mEpoch [2/10] Step [351/469]  acc1 0.765625 (0.781072)  loss 0.570526 (6.323394)\u001b[0m\n",
            "[2023-11-09 15:33:17] \u001b[32mEpoch [2/10] Step [361/469]  acc1 0.765625 (0.781856)  loss 0.594366 (6.162905)\u001b[0m\n",
            "[2023-11-09 15:33:19] \u001b[32mEpoch [2/10] Step [371/469]  acc1 0.859375 (0.782303)  loss 0.446686 (6.011080)\u001b[0m\n",
            "[2023-11-09 15:33:20] \u001b[32mEpoch [2/10] Step [381/469]  acc1 0.765625 (0.782931)  loss 0.601060 (5.868213)\u001b[0m\n",
            "[2023-11-09 15:33:22] \u001b[32mEpoch [2/10] Step [391/469]  acc1 0.718750 (0.783168)  loss 0.632842 (5.732304)\u001b[0m\n",
            "[2023-11-09 15:33:23] \u001b[32mEpoch [2/10] Step [401/469]  acc1 0.859375 (0.784133)  loss 0.433357 (5.601315)\u001b[0m\n",
            "[2023-11-09 15:33:25] \u001b[32mEpoch [2/10] Step [411/469]  acc1 0.953125 (0.785394)  loss 0.284700 (5.476995)\u001b[0m\n",
            "[2023-11-09 15:33:26] \u001b[32mEpoch [2/10] Step [421/469]  acc1 0.609375 (0.785481)  loss 0.737501 (5.364171)\u001b[0m\n",
            "[2023-11-09 15:33:27] \u001b[32mEpoch [2/10] Step [431/469]  acc1 0.812500 (0.785600)  loss 0.407847 (5.253155)\u001b[0m\n",
            "[2023-11-09 15:33:29] \u001b[32mEpoch [2/10] Step [441/469]  acc1 0.890625 (0.786919)  loss 0.353542 (5.144310)\u001b[0m\n",
            "[2023-11-09 15:33:31] \u001b[32mEpoch [2/10] Step [451/469]  acc1 0.796875 (0.787902)  loss 0.413047 (5.047200)\u001b[0m\n",
            "[2023-11-09 15:33:32] \u001b[32mEpoch [2/10] Step [461/469]  acc1 0.796875 (0.788469)  loss 0.543880 (4.948684)\u001b[0m\n",
            "[2023-11-09 15:33:35] \u001b[32mEpoch [3/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 0.456098 (0.456098)\u001b[0m\n",
            "[2023-11-09 15:33:37] \u001b[32mEpoch [3/10] Step [11/469]  acc1 0.781250 (0.823864)  loss 0.477750 (0.471338)\u001b[0m\n",
            "[2023-11-09 15:33:38] \u001b[32mEpoch [3/10] Step [21/469]  acc1 0.781250 (0.837798)  loss 0.499250 (0.445399)\u001b[0m\n",
            "[2023-11-09 15:33:40] \u001b[32mEpoch [3/10] Step [31/469]  acc1 0.734375 (0.823589)  loss 0.567182 (0.460415)\u001b[0m\n",
            "[2023-11-09 15:33:41] \u001b[32mEpoch [3/10] Step [41/469]  acc1 0.796875 (0.819360)  loss 0.456598 (0.474898)\u001b[0m\n",
            "[2023-11-09 15:33:43] \u001b[32mEpoch [3/10] Step [51/469]  acc1 0.875000 (0.821078)  loss 0.408045 (9.995420)\u001b[0m\n",
            "[2023-11-09 15:33:44] \u001b[32mEpoch [3/10] Step [61/469]  acc1 0.796875 (0.817623)  loss 0.419257 (8.450082)\u001b[0m\n",
            "[2023-11-09 15:33:46] \u001b[32mEpoch [3/10] Step [71/469]  acc1 0.828125 (0.819542)  loss 0.483526 (7.325536)\u001b[0m\n",
            "[2023-11-09 15:33:47] \u001b[32mEpoch [3/10] Step [81/469]  acc1 0.750000 (0.818480)  loss 0.617312 (6.483465)\u001b[0m\n",
            "[2023-11-09 15:33:49] \u001b[32mEpoch [3/10] Step [91/469]  acc1 0.890625 (0.819712)  loss 0.380149 (5.820919)\u001b[0m\n",
            "[2023-11-09 15:33:50] \u001b[32mEpoch [3/10] Step [101/469]  acc1 0.875000 (0.818998)  loss 0.413765 (5.291810)\u001b[0m\n",
            "[2023-11-09 15:33:52] \u001b[32mEpoch [3/10] Step [111/469]  acc1 0.843750 (0.820946)  loss 0.457662 (4.854664)\u001b[0m\n",
            "[2023-11-09 15:33:53] \u001b[32mEpoch [3/10] Step [121/469]  acc1 0.875000 (0.821539)  loss 0.350972 (4.492647)\u001b[0m\n",
            "[2023-11-09 15:33:54] \u001b[32mEpoch [3/10] Step [131/469]  acc1 0.828125 (0.820372)  loss 0.512487 (4.190355)\u001b[0m\n",
            "[2023-11-09 15:33:56] \u001b[32mEpoch [3/10] Step [141/469]  acc1 0.906250 (0.819592)  loss 0.346770 (3.930311)\u001b[0m\n",
            "[2023-11-09 15:33:57] \u001b[32mEpoch [3/10] Step [151/469]  acc1 0.875000 (0.820261)  loss 0.388811 (3.704286)\u001b[0m\n",
            "[2023-11-09 15:33:59] \u001b[32mEpoch [3/10] Step [161/469]  acc1 0.796875 (0.821429)  loss 0.454635 (3.503188)\u001b[0m\n",
            "[2023-11-09 15:34:00] \u001b[32mEpoch [3/10] Step [171/469]  acc1 0.765625 (0.821181)  loss 0.606537 (3.327783)\u001b[0m\n",
            "[2023-11-09 15:34:01] \u001b[32mEpoch [3/10] Step [181/469]  acc1 0.765625 (0.819924)  loss 0.588176 (3.173525)\u001b[0m\n",
            "[2023-11-09 15:34:03] \u001b[32mEpoch [3/10] Step [191/469]  acc1 0.859375 (0.821335)  loss 0.455802 (3.033122)\u001b[0m\n",
            "[2023-11-09 15:34:04] \u001b[32mEpoch [3/10] Step [201/469]  acc1 0.718750 (0.819963)  loss 0.658045 (2.908664)\u001b[0m\n",
            "[2023-11-09 15:34:06] \u001b[32mEpoch [3/10] Step [211/469]  acc1 0.781250 (0.820350)  loss 0.455639 (2.794366)\u001b[0m\n",
            "[2023-11-09 15:34:07] \u001b[32mEpoch [3/10] Step [221/469]  acc1 0.828125 (0.821055)  loss 0.472756 (2.688382)\u001b[0m\n",
            "[2023-11-09 15:34:09] \u001b[32mEpoch [3/10] Step [231/469]  acc1 0.781250 (0.822578)  loss 0.572445 (2.591144)\u001b[0m\n",
            "[2023-11-09 15:34:10] \u001b[32mEpoch [3/10] Step [241/469]  acc1 0.765625 (0.823327)  loss 0.576445 (2.501933)\u001b[0m\n",
            "[2023-11-09 15:34:11] \u001b[32mEpoch [3/10] Step [251/469]  acc1 0.828125 (0.823643)  loss 0.481059 (2.421739)\u001b[0m\n",
            "[2023-11-09 15:34:13] \u001b[32mEpoch [3/10] Step [261/469]  acc1 0.796875 (0.823575)  loss 0.526832 (2.346973)\u001b[0m\n",
            "[2023-11-09 15:34:14] \u001b[32mEpoch [3/10] Step [271/469]  acc1 0.796875 (0.823570)  loss 0.505512 (2.277909)\u001b[0m\n",
            "[2023-11-09 15:34:16] \u001b[32mEpoch [3/10] Step [281/469]  acc1 0.781250 (0.824344)  loss 0.569447 (2.211505)\u001b[0m\n",
            "[2023-11-09 15:34:17] \u001b[32mEpoch [3/10] Step [291/469]  acc1 0.875000 (0.825387)  loss 0.379041 (2.151557)\u001b[0m\n",
            "[2023-11-09 15:34:18] \u001b[32mEpoch [3/10] Step [301/469]  acc1 0.828125 (0.825218)  loss 0.440056 (2.095830)\u001b[0m\n",
            "[2023-11-09 15:34:20] \u001b[32mEpoch [3/10] Step [311/469]  acc1 0.796875 (0.824859)  loss 0.504836 (2.043271)\u001b[0m\n",
            "[2023-11-09 15:34:21] \u001b[32mEpoch [3/10] Step [321/469]  acc1 0.812500 (0.825643)  loss 0.403576 (1.992258)\u001b[0m\n",
            "[2023-11-09 15:34:23] \u001b[32mEpoch [3/10] Step [331/469]  acc1 0.875000 (0.826284)  loss 0.301029 (1.944927)\u001b[0m\n",
            "[2023-11-09 15:34:24] \u001b[32mEpoch [3/10] Step [341/469]  acc1 0.765625 (0.826063)  loss 0.536006 (1.901880)\u001b[0m\n",
            "[2023-11-09 15:34:26] \u001b[32mEpoch [3/10] Step [351/469]  acc1 0.890625 (0.826745)  loss 0.378295 (1.860606)\u001b[0m\n",
            "[2023-11-09 15:34:27] \u001b[32mEpoch [3/10] Step [361/469]  acc1 0.843750 (0.827432)  loss 0.518781 (1.820641)\u001b[0m\n",
            "[2023-11-09 15:34:29] \u001b[32mEpoch [3/10] Step [371/469]  acc1 0.828125 (0.827198)  loss 0.455252 (1.785090)\u001b[0m\n",
            "[2023-11-09 15:34:30] \u001b[32mEpoch [3/10] Step [381/469]  acc1 0.703125 (0.827059)  loss 0.615955 (1.751608)\u001b[0m\n",
            "[2023-11-09 15:34:32] \u001b[32mEpoch [3/10] Step [391/469]  acc1 0.875000 (0.827166)  loss 0.423205 (1.718757)\u001b[0m\n",
            "[2023-11-09 15:34:33] \u001b[32mEpoch [3/10] Step [401/469]  acc1 0.875000 (0.827073)  loss 0.360650 (1.686549)\u001b[0m\n",
            "[2023-11-09 15:34:35] \u001b[32mEpoch [3/10] Step [411/469]  acc1 0.843750 (0.827783)  loss 0.527318 (1.655883)\u001b[0m\n",
            "[2023-11-09 15:34:36] \u001b[32mEpoch [3/10] Step [421/469]  acc1 0.843750 (0.828496)  loss 0.430061 (1.626515)\u001b[0m\n",
            "[2023-11-09 15:34:38] \u001b[32mEpoch [3/10] Step [431/469]  acc1 0.890625 (0.829430)  loss 0.362892 (1.597191)\u001b[0m\n",
            "[2023-11-09 15:34:39] \u001b[32mEpoch [3/10] Step [441/469]  acc1 0.828125 (0.829471)  loss 0.422527 (1.570851)\u001b[0m\n",
            "[2023-11-09 15:34:41] \u001b[32mEpoch [3/10] Step [451/469]  acc1 0.890625 (0.830134)  loss 0.364330 (1.545190)\u001b[0m\n",
            "[2023-11-09 15:34:42] \u001b[32mEpoch [3/10] Step [461/469]  acc1 0.843750 (0.831040)  loss 0.393524 (1.520447)\u001b[0m\n",
            "[2023-11-09 15:34:45] \u001b[32mEpoch [4/10] Step [1/469]  acc1 0.875000 (0.875000)  loss 0.374720 (0.374720)\u001b[0m\n",
            "[2023-11-09 15:34:47] \u001b[32mEpoch [4/10] Step [11/469]  acc1 0.843750 (0.835227)  loss 0.419307 (0.463327)\u001b[0m\n",
            "[2023-11-09 15:34:48] \u001b[32mEpoch [4/10] Step [21/469]  acc1 0.828125 (0.834821)  loss 0.432411 (0.451758)\u001b[0m\n",
            "[2023-11-09 15:34:49] \u001b[32mEpoch [4/10] Step [31/469]  acc1 0.828125 (0.846270)  loss 0.558635 (0.436666)\u001b[0m\n",
            "[2023-11-09 15:34:51] \u001b[32mEpoch [4/10] Step [41/469]  acc1 0.796875 (0.850991)  loss 0.468035 (0.421282)\u001b[0m\n",
            "[2023-11-09 15:34:52] \u001b[32mEpoch [4/10] Step [51/469]  acc1 0.921875 (0.855699)  loss 0.278131 (0.408933)\u001b[0m\n",
            "[2023-11-09 15:34:54] \u001b[32mEpoch [4/10] Step [61/469]  acc1 0.859375 (0.856814)  loss 0.359144 (0.407178)\u001b[0m\n",
            "[2023-11-09 15:34:55] \u001b[32mEpoch [4/10] Step [71/469]  acc1 0.890625 (0.857835)  loss 0.332808 (0.402934)\u001b[0m\n",
            "[2023-11-09 15:34:57] \u001b[32mEpoch [4/10] Step [81/469]  acc1 0.890625 (0.858603)  loss 0.360101 (0.398901)\u001b[0m\n",
            "[2023-11-09 15:34:58] \u001b[32mEpoch [4/10] Step [91/469]  acc1 0.890625 (0.859718)  loss 0.426492 (0.394078)\u001b[0m\n",
            "[2023-11-09 15:34:59] \u001b[32mEpoch [4/10] Step [101/469]  acc1 0.828125 (0.858601)  loss 0.341774 (0.394751)\u001b[0m\n",
            "[2023-11-09 15:35:01] \u001b[32mEpoch [4/10] Step [111/469]  acc1 0.875000 (0.856560)  loss 0.378076 (0.400350)\u001b[0m\n",
            "[2023-11-09 15:35:02] \u001b[32mEpoch [4/10] Step [121/469]  acc1 0.828125 (0.854985)  loss 0.500725 (0.399171)\u001b[0m\n",
            "[2023-11-09 15:35:04] \u001b[32mEpoch [4/10] Step [131/469]  acc1 0.937500 (0.855439)  loss 0.299346 (0.399638)\u001b[0m\n",
            "[2023-11-09 15:35:05] \u001b[32mEpoch [4/10] Step [141/469]  acc1 0.843750 (0.856383)  loss 0.444928 (0.397982)\u001b[0m\n",
            "[2023-11-09 15:35:07] \u001b[32mEpoch [4/10] Step [151/469]  acc1 0.812500 (0.855132)  loss 0.417484 (0.400901)\u001b[0m\n",
            "[2023-11-09 15:35:08] \u001b[32mEpoch [4/10] Step [161/469]  acc1 0.812500 (0.854134)  loss 0.530563 (0.407198)\u001b[0m\n",
            "[2023-11-09 15:35:09] \u001b[32mEpoch [4/10] Step [171/469]  acc1 0.843750 (0.854806)  loss 0.303761 (0.404620)\u001b[0m\n",
            "[2023-11-09 15:35:11] \u001b[32mEpoch [4/10] Step [181/469]  acc1 0.859375 (0.854800)  loss 0.301249 (0.401957)\u001b[0m\n",
            "[2023-11-09 15:35:12] \u001b[32mEpoch [4/10] Step [191/469]  acc1 0.812500 (0.853730)  loss 0.454236 (0.404038)\u001b[0m\n",
            "[2023-11-09 15:35:14] \u001b[32mEpoch [4/10] Step [201/469]  acc1 0.875000 (0.853778)  loss 0.393655 (0.403888)\u001b[0m\n",
            "[2023-11-09 15:35:15] \u001b[32mEpoch [4/10] Step [211/469]  acc1 0.859375 (0.852562)  loss 0.480799 (0.406101)\u001b[0m\n",
            "[2023-11-09 15:35:17] \u001b[32mEpoch [4/10] Step [221/469]  acc1 0.906250 (0.852234)  loss 0.277651 (0.407473)\u001b[0m\n",
            "[2023-11-09 15:35:18] \u001b[32mEpoch [4/10] Step [231/469]  acc1 0.812500 (0.851664)  loss 0.447924 (0.407967)\u001b[0m\n",
            "[2023-11-09 15:35:19] \u001b[32mEpoch [4/10] Step [241/469]  acc1 0.875000 (0.852049)  loss 0.358054 (0.406644)\u001b[0m\n",
            "[2023-11-09 15:35:21] \u001b[32mEpoch [4/10] Step [251/469]  acc1 0.734375 (0.851531)  loss 0.597957 (0.407022)\u001b[0m\n",
            "[2023-11-09 15:35:22] \u001b[32mEpoch [4/10] Step [261/469]  acc1 0.890625 (0.851952)  loss 0.331509 (0.407185)\u001b[0m\n",
            "[2023-11-09 15:35:24] \u001b[32mEpoch [4/10] Step [271/469]  acc1 0.734375 (0.850726)  loss 0.412385 (0.408360)\u001b[0m\n",
            "[2023-11-09 15:35:25] \u001b[32mEpoch [4/10] Step [281/469]  acc1 0.875000 (0.850756)  loss 0.299026 (0.407767)\u001b[0m\n",
            "[2023-11-09 15:35:27] \u001b[32mEpoch [4/10] Step [291/469]  acc1 0.875000 (0.850891)  loss 0.351609 (0.407572)\u001b[0m\n",
            "[2023-11-09 15:35:28] \u001b[32mEpoch [4/10] Step [301/469]  acc1 0.859375 (0.851796)  loss 0.420652 (0.406174)\u001b[0m\n",
            "[2023-11-09 15:35:29] \u001b[32mEpoch [4/10] Step [311/469]  acc1 0.859375 (0.851186)  loss 0.376209 (0.406683)\u001b[0m\n",
            "[2023-11-09 15:35:31] \u001b[32mEpoch [4/10] Step [321/469]  acc1 0.921875 (0.851343)  loss 0.247175 (0.405725)\u001b[0m\n",
            "[2023-11-09 15:35:32] \u001b[32mEpoch [4/10] Step [331/469]  acc1 0.796875 (0.851067)  loss 0.446731 (0.406859)\u001b[0m\n",
            "[2023-11-09 15:35:34] \u001b[32mEpoch [4/10] Step [341/469]  acc1 0.875000 (0.850990)  loss 0.419453 (0.407365)\u001b[0m\n",
            "[2023-11-09 15:35:35] \u001b[32mEpoch [4/10] Step [351/469]  acc1 0.890625 (0.851273)  loss 0.327110 (0.406572)\u001b[0m\n",
            "[2023-11-09 15:35:37] \u001b[32mEpoch [4/10] Step [361/469]  acc1 0.937500 (0.851584)  loss 0.222377 (0.405420)\u001b[0m\n",
            "[2023-11-09 15:35:38] \u001b[32mEpoch [4/10] Step [371/469]  acc1 0.875000 (0.851920)  loss 0.539475 (0.405955)\u001b[0m\n",
            "[2023-11-09 15:35:40] \u001b[32mEpoch [4/10] Step [381/469]  acc1 0.890625 (0.851665)  loss 0.452034 (0.406957)\u001b[0m\n",
            "[2023-11-09 15:35:41] \u001b[32mEpoch [4/10] Step [391/469]  acc1 0.890625 (0.851862)  loss 0.268282 (0.406216)\u001b[0m\n",
            "[2023-11-09 15:35:43] \u001b[32mEpoch [4/10] Step [401/469]  acc1 0.875000 (0.852011)  loss 0.316698 (0.405667)\u001b[0m\n",
            "[2023-11-09 15:35:44] \u001b[32mEpoch [4/10] Step [411/469]  acc1 0.812500 (0.851391)  loss 0.445989 (0.406829)\u001b[0m\n",
            "[2023-11-09 15:35:46] \u001b[32mEpoch [4/10] Step [421/469]  acc1 0.890625 (0.851618)  loss 0.321892 (0.406058)\u001b[0m\n",
            "[2023-11-09 15:35:47] \u001b[32mEpoch [4/10] Step [431/469]  acc1 0.796875 (0.852233)  loss 0.601535 (0.405083)\u001b[0m\n",
            "[2023-11-09 15:35:48] \u001b[32mEpoch [4/10] Step [441/469]  acc1 0.937500 (0.852147)  loss 0.260058 (0.404838)\u001b[0m\n",
            "[2023-11-09 15:35:50] \u001b[32mEpoch [4/10] Step [451/469]  acc1 0.890625 (0.852169)  loss 0.308162 (0.404513)\u001b[0m\n",
            "[2023-11-09 15:35:51] \u001b[32mEpoch [4/10] Step [461/469]  acc1 0.843750 (0.852291)  loss 0.446479 (0.403727)\u001b[0m\n",
            "[2023-11-09 15:35:54] \u001b[32mEpoch [5/10] Step [1/469]  acc1 0.906250 (0.906250)  loss 0.313841 (0.313841)\u001b[0m\n",
            "[2023-11-09 15:35:56] \u001b[32mEpoch [5/10] Step [11/469]  acc1 0.875000 (0.879261)  loss 0.347561 (0.372918)\u001b[0m\n",
            "[2023-11-09 15:35:57] \u001b[32mEpoch [5/10] Step [21/469]  acc1 0.796875 (0.864583)  loss 0.573045 (0.395188)\u001b[0m\n",
            "[2023-11-09 15:35:58] \u001b[32mEpoch [5/10] Step [31/469]  acc1 0.875000 (0.858367)  loss 0.368396 (0.407839)\u001b[0m\n",
            "[2023-11-09 15:36:00] \u001b[32mEpoch [5/10] Step [41/469]  acc1 0.734375 (0.858994)  loss 0.620542 (0.404733)\u001b[0m\n",
            "[2023-11-09 15:36:01] \u001b[32mEpoch [5/10] Step [51/469]  acc1 0.906250 (0.856924)  loss 0.295245 (0.411153)\u001b[0m\n",
            "[2023-11-09 15:36:03] \u001b[32mEpoch [5/10] Step [61/469]  acc1 0.921875 (0.861424)  loss 0.173267 (0.396572)\u001b[0m\n",
            "[2023-11-09 15:36:04] \u001b[32mEpoch [5/10] Step [71/469]  acc1 0.828125 (0.862456)  loss 0.400614 (0.396526)\u001b[0m\n",
            "[2023-11-09 15:36:06] \u001b[32mEpoch [5/10] Step [81/469]  acc1 0.843750 (0.863233)  loss 0.403957 (0.392920)\u001b[0m\n",
            "[2023-11-09 15:36:07] \u001b[32mEpoch [5/10] Step [91/469]  acc1 0.890625 (0.862294)  loss 0.316022 (0.394095)\u001b[0m\n",
            "[2023-11-09 15:36:09] \u001b[32mEpoch [5/10] Step [101/469]  acc1 0.843750 (0.862933)  loss 0.433828 (0.393003)\u001b[0m\n",
            "[2023-11-09 15:36:10] \u001b[32mEpoch [5/10] Step [111/469]  acc1 0.859375 (0.863316)  loss 0.326785 (0.389652)\u001b[0m\n",
            "[2023-11-09 15:36:12] \u001b[32mEpoch [5/10] Step [121/469]  acc1 0.875000 (0.862732)  loss 0.355554 (0.389066)\u001b[0m\n",
            "[2023-11-09 15:36:13] \u001b[32mEpoch [5/10] Step [131/469]  acc1 0.812500 (0.864027)  loss 0.546723 (0.384577)\u001b[0m\n",
            "[2023-11-09 15:36:15] \u001b[32mEpoch [5/10] Step [141/469]  acc1 0.796875 (0.864473)  loss 0.537922 (0.383738)\u001b[0m\n",
            "[2023-11-09 15:36:16] \u001b[32mEpoch [5/10] Step [151/469]  acc1 0.921875 (0.864342)  loss 0.300996 (0.384854)\u001b[0m\n",
            "[2023-11-09 15:36:17] \u001b[32mEpoch [5/10] Step [161/469]  acc1 0.953125 (0.866654)  loss 0.212508 (0.380004)\u001b[0m\n",
            "[2023-11-09 15:36:19] \u001b[32mEpoch [5/10] Step [171/469]  acc1 0.937500 (0.867781)  loss 0.308350 (0.377519)\u001b[0m\n",
            "[2023-11-09 15:36:20] \u001b[32mEpoch [5/10] Step [181/469]  acc1 0.906250 (0.868180)  loss 0.287753 (0.377446)\u001b[0m\n",
            "[2023-11-09 15:36:22] \u001b[32mEpoch [5/10] Step [191/469]  acc1 0.890625 (0.867637)  loss 0.255786 (0.376367)\u001b[0m\n",
            "[2023-11-09 15:36:23] \u001b[32mEpoch [5/10] Step [201/469]  acc1 0.859375 (0.867304)  loss 0.373201 (0.389696)\u001b[0m\n",
            "[2023-11-09 15:36:25] \u001b[32mEpoch [5/10] Step [211/469]  acc1 0.750000 (0.866484)  loss 0.636559 (0.391453)\u001b[0m\n",
            "[2023-11-09 15:36:26] \u001b[32mEpoch [5/10] Step [221/469]  acc1 0.859375 (0.865526)  loss 0.399758 (0.391964)\u001b[0m\n",
            "[2023-11-09 15:36:27] \u001b[32mEpoch [5/10] Step [231/469]  acc1 0.890625 (0.865463)  loss 0.309061 (0.390604)\u001b[0m\n",
            "[2023-11-09 15:36:29] \u001b[32mEpoch [5/10] Step [241/469]  acc1 0.765625 (0.865599)  loss 0.534998 (0.389897)\u001b[0m\n",
            "[2023-11-09 15:36:30] \u001b[32mEpoch [5/10] Step [251/469]  acc1 0.906250 (0.865600)  loss 0.300988 (0.389363)\u001b[0m\n",
            "[2023-11-09 15:36:32] \u001b[32mEpoch [5/10] Step [261/469]  acc1 0.796875 (0.866020)  loss 0.560833 (0.389152)\u001b[0m\n",
            "[2023-11-09 15:36:33] \u001b[32mEpoch [5/10] Step [271/469]  acc1 0.812500 (0.865833)  loss 0.448310 (0.388157)\u001b[0m\n",
            "[2023-11-09 15:36:35] \u001b[32mEpoch [5/10] Step [281/469]  acc1 0.828125 (0.865603)  loss 0.416550 (0.387291)\u001b[0m\n",
            "[2023-11-09 15:36:36] \u001b[32mEpoch [5/10] Step [291/469]  acc1 0.875000 (0.865013)  loss 0.416637 (0.387479)\u001b[0m\n",
            "[2023-11-09 15:36:37] \u001b[32mEpoch [5/10] Step [301/469]  acc1 0.828125 (0.865500)  loss 0.357774 (0.386346)\u001b[0m\n",
            "[2023-11-09 15:36:39] \u001b[32mEpoch [5/10] Step [311/469]  acc1 0.859375 (0.865253)  loss 0.464404 (0.386430)\u001b[0m\n",
            "[2023-11-09 15:36:40] \u001b[32mEpoch [5/10] Step [321/469]  acc1 0.812500 (0.865800)  loss 0.522475 (0.385196)\u001b[0m\n",
            "[2023-11-09 15:36:42] \u001b[32mEpoch [5/10] Step [331/469]  acc1 0.812500 (0.865889)  loss 0.366270 (0.383570)\u001b[0m\n",
            "[2023-11-09 15:36:43] \u001b[32mEpoch [5/10] Step [341/469]  acc1 0.859375 (0.865423)  loss 0.409904 (0.383888)\u001b[0m\n",
            "[2023-11-09 15:36:44] \u001b[32mEpoch [5/10] Step [351/469]  acc1 0.859375 (0.865474)  loss 0.317702 (0.383974)\u001b[0m\n",
            "[2023-11-09 15:36:46] \u001b[32mEpoch [5/10] Step [361/469]  acc1 0.921875 (0.865694)  loss 0.219802 (0.382994)\u001b[0m\n",
            "[2023-11-09 15:36:47] \u001b[32mEpoch [5/10] Step [371/469]  acc1 0.828125 (0.865735)  loss 0.413964 (0.382131)\u001b[0m\n",
            "[2023-11-09 15:36:49] \u001b[32mEpoch [5/10] Step [381/469]  acc1 0.843750 (0.865568)  loss 59.246994 (0.536088)\u001b[0m\n",
            "[2023-11-09 15:36:50] \u001b[32mEpoch [5/10] Step [391/469]  acc1 0.812500 (0.865489)  loss 0.406150 (0.532106)\u001b[0m\n",
            "[2023-11-09 15:36:52] \u001b[32mEpoch [5/10] Step [401/469]  acc1 0.921875 (0.865025)  loss 0.273492 (0.528531)\u001b[0m\n",
            "[2023-11-09 15:36:53] \u001b[32mEpoch [5/10] Step [411/469]  acc1 0.906250 (0.865154)  loss 0.239683 (0.524417)\u001b[0m\n",
            "[2023-11-09 15:36:54] \u001b[32mEpoch [5/10] Step [421/469]  acc1 0.906250 (0.865350)  loss 0.259851 (0.520019)\u001b[0m\n",
            "[2023-11-09 15:36:56] \u001b[32mEpoch [5/10] Step [431/469]  acc1 0.921875 (0.865574)  loss 0.289369 (0.515764)\u001b[0m\n",
            "[2023-11-09 15:36:57] \u001b[32mEpoch [5/10] Step [441/469]  acc1 0.890625 (0.865611)  loss 0.278748 (0.512324)\u001b[0m\n",
            "[2023-11-09 15:36:59] \u001b[32mEpoch [5/10] Step [451/469]  acc1 0.921875 (0.866062)  loss 0.211616 (0.508828)\u001b[0m\n",
            "[2023-11-09 15:37:00] \u001b[32mEpoch [5/10] Step [461/469]  acc1 0.906250 (0.866052)  loss 0.264778 (0.506111)\u001b[0m\n",
            "[2023-11-09 15:37:03] \u001b[32mEpoch [6/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 0.318657 (0.318657)\u001b[0m\n",
            "[2023-11-09 15:37:05] \u001b[32mEpoch [6/10] Step [11/469]  acc1 0.921875 (0.875000)  loss 0.230679 (0.329642)\u001b[0m\n",
            "[2023-11-09 15:37:06] \u001b[32mEpoch [6/10] Step [21/469]  acc1 0.781250 (0.861607)  loss 0.614772 (0.372867)\u001b[0m\n",
            "[2023-11-09 15:37:07] \u001b[32mEpoch [6/10] Step [31/469]  acc1 0.921875 (0.872480)  loss 0.235657 (0.352256)\u001b[0m\n",
            "[2023-11-09 15:37:09] \u001b[32mEpoch [6/10] Step [41/469]  acc1 0.890625 (0.876905)  loss 0.243064 (0.340319)\u001b[0m\n",
            "[2023-11-09 15:37:10] \u001b[32mEpoch [6/10] Step [51/469]  acc1 0.921875 (0.880821)  loss 0.226832 (0.339970)\u001b[0m\n",
            "[2023-11-09 15:37:12] \u001b[32mEpoch [6/10] Step [61/469]  acc1 0.843750 (0.881660)  loss 0.290850 (0.338145)\u001b[0m\n",
            "[2023-11-09 15:37:13] \u001b[32mEpoch [6/10] Step [71/469]  acc1 0.906250 (0.876100)  loss 0.281861 (0.347627)\u001b[0m\n",
            "[2023-11-09 15:37:15] \u001b[32mEpoch [6/10] Step [81/469]  acc1 0.828125 (0.874614)  loss 0.524582 (0.351157)\u001b[0m\n",
            "[2023-11-09 15:37:16] \u001b[32mEpoch [6/10] Step [91/469]  acc1 0.843750 (0.873283)  loss 0.379153 (0.353518)\u001b[0m\n",
            "[2023-11-09 15:37:17] \u001b[32mEpoch [6/10] Step [101/469]  acc1 0.875000 (0.875464)  loss 0.377728 (0.348935)\u001b[0m\n",
            "[2023-11-09 15:37:19] \u001b[32mEpoch [6/10] Step [111/469]  acc1 0.890625 (0.873592)  loss 0.314815 (0.348909)\u001b[0m\n",
            "[2023-11-09 15:37:20] \u001b[32mEpoch [6/10] Step [121/469]  acc1 0.843750 (0.874354)  loss 0.388647 (0.346811)\u001b[0m\n",
            "[2023-11-09 15:37:22] \u001b[32mEpoch [6/10] Step [131/469]  acc1 0.906250 (0.873807)  loss 0.314731 (0.347118)\u001b[0m\n",
            "[2023-11-09 15:37:23] \u001b[32mEpoch [6/10] Step [141/469]  acc1 0.828125 (0.873005)  loss 0.467886 (0.349166)\u001b[0m\n",
            "[2023-11-09 15:37:25] \u001b[32mEpoch [6/10] Step [151/469]  acc1 0.859375 (0.872517)  loss 0.368115 (0.351661)\u001b[0m\n",
            "[2023-11-09 15:37:26] \u001b[32mEpoch [6/10] Step [161/469]  acc1 0.906250 (0.874515)  loss 0.281837 (0.348449)\u001b[0m\n",
            "[2023-11-09 15:37:28] \u001b[32mEpoch [6/10] Step [171/469]  acc1 0.843750 (0.874178)  loss 0.352889 (0.348440)\u001b[0m\n",
            "[2023-11-09 15:37:29] \u001b[32mEpoch [6/10] Step [181/469]  acc1 0.843750 (0.873360)  loss 0.417930 (0.350829)\u001b[0m\n",
            "[2023-11-09 15:37:31] \u001b[32mEpoch [6/10] Step [191/469]  acc1 0.843750 (0.872709)  loss 0.318959 (0.351817)\u001b[0m\n",
            "[2023-11-09 15:37:32] \u001b[32mEpoch [6/10] Step [201/469]  acc1 0.843750 (0.872201)  loss 0.373200 (0.351656)\u001b[0m\n",
            "[2023-11-09 15:37:34] \u001b[32mEpoch [6/10] Step [211/469]  acc1 0.890625 (0.872186)  loss 0.305229 (0.351497)\u001b[0m\n",
            "[2023-11-09 15:37:35] \u001b[32mEpoch [6/10] Step [221/469]  acc1 0.875000 (0.872879)  loss 0.386625 (0.350847)\u001b[0m\n",
            "[2023-11-09 15:37:37] \u001b[32mEpoch [6/10] Step [231/469]  acc1 0.921875 (0.872430)  loss 0.256695 (0.350993)\u001b[0m\n",
            "[2023-11-09 15:37:38] \u001b[32mEpoch [6/10] Step [241/469]  acc1 0.875000 (0.872601)  loss 0.424848 (0.349851)\u001b[0m\n",
            "[2023-11-09 15:37:39] \u001b[32mEpoch [6/10] Step [251/469]  acc1 0.859375 (0.872759)  loss 0.416902 (0.350193)\u001b[0m\n",
            "[2023-11-09 15:37:41] \u001b[32mEpoch [6/10] Step [261/469]  acc1 0.828125 (0.872486)  loss 0.372821 (0.351100)\u001b[0m\n",
            "[2023-11-09 15:37:42] \u001b[32mEpoch [6/10] Step [271/469]  acc1 0.921875 (0.872463)  loss 0.285513 (0.351411)\u001b[0m\n",
            "[2023-11-09 15:37:44] \u001b[32mEpoch [6/10] Step [281/469]  acc1 0.843750 (0.872553)  loss 0.339501 (0.350546)\u001b[0m\n",
            "[2023-11-09 15:37:45] \u001b[32mEpoch [6/10] Step [291/469]  acc1 0.859375 (0.872906)  loss 0.365384 (0.348936)\u001b[0m\n",
            "[2023-11-09 15:37:47] \u001b[32mEpoch [6/10] Step [301/469]  acc1 0.890625 (0.872716)  loss 0.428627 (0.350504)\u001b[0m\n",
            "[2023-11-09 15:37:48] \u001b[32mEpoch [6/10] Step [311/469]  acc1 0.843750 (0.873794)  loss 0.445468 (0.348238)\u001b[0m\n",
            "[2023-11-09 15:37:50] \u001b[32mEpoch [6/10] Step [321/469]  acc1 0.890625 (0.873053)  loss 0.249701 (0.348829)\u001b[0m\n",
            "[2023-11-09 15:37:51] \u001b[32mEpoch [6/10] Step [331/469]  acc1 0.828125 (0.872829)  loss 0.460619 (0.349685)\u001b[0m\n",
            "[2023-11-09 15:37:52] \u001b[32mEpoch [6/10] Step [341/469]  acc1 0.859375 (0.872938)  loss 0.415123 (0.349449)\u001b[0m\n",
            "[2023-11-09 15:37:54] \u001b[32mEpoch [6/10] Step [351/469]  acc1 0.828125 (0.872552)  loss 0.446429 (0.350590)\u001b[0m\n",
            "[2023-11-09 15:37:55] \u001b[32mEpoch [6/10] Step [361/469]  acc1 0.875000 (0.872143)  loss 0.365036 (0.351022)\u001b[0m\n",
            "[2023-11-09 15:37:57] \u001b[32mEpoch [6/10] Step [371/469]  acc1 0.828125 (0.872389)  loss 0.421567 (0.350696)\u001b[0m\n",
            "[2023-11-09 15:37:58] \u001b[32mEpoch [6/10] Step [381/469]  acc1 0.890625 (0.872457)  loss 0.275661 (0.350660)\u001b[0m\n",
            "[2023-11-09 15:38:00] \u001b[32mEpoch [6/10] Step [391/469]  acc1 0.953125 (0.872682)  loss 0.211076 (0.349842)\u001b[0m\n",
            "[2023-11-09 15:38:01] \u001b[32mEpoch [6/10] Step [401/469]  acc1 0.859375 (0.872935)  loss 0.365465 (0.348783)\u001b[0m\n",
            "[2023-11-09 15:38:02] \u001b[32mEpoch [6/10] Step [411/469]  acc1 0.906250 (0.873099)  loss 0.248137 (0.348663)\u001b[0m\n",
            "[2023-11-09 15:38:04] \u001b[32mEpoch [6/10] Step [421/469]  acc1 0.890625 (0.873107)  loss 0.328384 (0.348658)\u001b[0m\n",
            "[2023-11-09 15:38:05] \u001b[32mEpoch [6/10] Step [431/469]  acc1 0.890625 (0.873260)  loss 0.261794 (0.348259)\u001b[0m\n",
            "[2023-11-09 15:38:07] \u001b[32mEpoch [6/10] Step [441/469]  acc1 0.828125 (0.873158)  loss 0.351959 (0.348656)\u001b[0m\n",
            "[2023-11-09 15:38:08] \u001b[32mEpoch [6/10] Step [451/469]  acc1 0.906250 (0.873060)  loss 0.227353 (0.348683)\u001b[0m\n",
            "[2023-11-09 15:38:09] \u001b[32mEpoch [6/10] Step [461/469]  acc1 0.921875 (0.873543)  loss 0.241736 (0.347623)\u001b[0m\n",
            "[2023-11-09 15:38:12] \u001b[32mEpoch [7/10] Step [1/469]  acc1 0.937500 (0.937500)  loss 0.164356 (0.164356)\u001b[0m\n",
            "[2023-11-09 15:38:14] \u001b[32mEpoch [7/10] Step [11/469]  acc1 0.921875 (0.910511)  loss 0.229536 (0.270450)\u001b[0m\n",
            "[2023-11-09 15:38:15] \u001b[32mEpoch [7/10] Step [21/469]  acc1 0.859375 (0.894345)  loss 0.443104 (0.312157)\u001b[0m\n",
            "[2023-11-09 15:38:17] \u001b[32mEpoch [7/10] Step [31/469]  acc1 0.875000 (0.891633)  loss 0.273557 (0.312236)\u001b[0m\n",
            "[2023-11-09 15:38:18] \u001b[32mEpoch [7/10] Step [41/469]  acc1 0.906250 (0.885290)  loss 0.424384 (0.326842)\u001b[0m\n",
            "[2023-11-09 15:38:20] \u001b[32mEpoch [7/10] Step [51/469]  acc1 0.828125 (0.884804)  loss 0.415701 (0.322805)\u001b[0m\n",
            "[2023-11-09 15:38:21] \u001b[32mEpoch [7/10] Step [61/469]  acc1 0.921875 (0.885246)  loss 0.248374 (0.324837)\u001b[0m\n",
            "[2023-11-09 15:38:22] \u001b[32mEpoch [7/10] Step [71/469]  acc1 0.890625 (0.883583)  loss 0.317789 (0.325781)\u001b[0m\n",
            "[2023-11-09 15:38:24] \u001b[32mEpoch [7/10] Step [81/469]  acc1 0.843750 (0.882523)  loss 0.494346 (0.330710)\u001b[0m\n",
            "[2023-11-09 15:38:25] \u001b[32mEpoch [7/10] Step [91/469]  acc1 0.843750 (0.883070)  loss 0.302089 (0.325896)\u001b[0m\n",
            "[2023-11-09 15:38:27] \u001b[32mEpoch [7/10] Step [101/469]  acc1 0.859375 (0.884127)  loss 0.351979 (0.324161)\u001b[0m\n",
            "[2023-11-09 15:38:28] \u001b[32mEpoch [7/10] Step [111/469]  acc1 0.859375 (0.881475)  loss 0.365129 (0.330612)\u001b[0m\n",
            "[2023-11-09 15:38:29] \u001b[32mEpoch [7/10] Step [121/469]  acc1 0.812500 (0.881586)  loss 0.337226 (0.329118)\u001b[0m\n",
            "[2023-11-09 15:38:31] \u001b[32mEpoch [7/10] Step [131/469]  acc1 0.875000 (0.881202)  loss 0.382622 (0.329500)\u001b[0m\n",
            "[2023-11-09 15:38:32] \u001b[32mEpoch [7/10] Step [141/469]  acc1 0.906250 (0.880430)  loss 0.254354 (0.330856)\u001b[0m\n",
            "[2023-11-09 15:38:34] \u001b[32mEpoch [7/10] Step [151/469]  acc1 0.937500 (0.881209)  loss 0.208057 (0.329269)\u001b[0m\n",
            "[2023-11-09 15:38:35] \u001b[32mEpoch [7/10] Step [161/469]  acc1 0.875000 (0.881405)  loss 0.289934 (0.327917)\u001b[0m\n",
            "[2023-11-09 15:38:37] \u001b[32mEpoch [7/10] Step [171/469]  acc1 0.812500 (0.880300)  loss 0.493638 (0.330464)\u001b[0m\n",
            "[2023-11-09 15:38:38] \u001b[32mEpoch [7/10] Step [181/469]  acc1 0.890625 (0.879748)  loss 0.281617 (0.332077)\u001b[0m\n",
            "[2023-11-09 15:38:39] \u001b[32mEpoch [7/10] Step [191/469]  acc1 0.906250 (0.880072)  loss 0.261215 (0.330896)\u001b[0m\n",
            "[2023-11-09 15:38:41] \u001b[32mEpoch [7/10] Step [201/469]  acc1 0.921875 (0.879198)  loss 0.362321 (0.334279)\u001b[0m\n",
            "[2023-11-09 15:38:42] \u001b[32mEpoch [7/10] Step [211/469]  acc1 0.843750 (0.879739)  loss 0.349103 (0.333735)\u001b[0m\n",
            "[2023-11-09 15:38:44] \u001b[32mEpoch [7/10] Step [221/469]  acc1 0.937500 (0.878959)  loss 0.249076 (0.335473)\u001b[0m\n",
            "[2023-11-09 15:38:45] \u001b[32mEpoch [7/10] Step [231/469]  acc1 0.875000 (0.879397)  loss 0.461533 (0.334538)\u001b[0m\n",
            "[2023-11-09 15:38:46] \u001b[32mEpoch [7/10] Step [241/469]  acc1 0.921875 (0.878890)  loss 0.269756 (0.335449)\u001b[0m\n",
            "[2023-11-09 15:38:48] \u001b[32mEpoch [7/10] Step [251/469]  acc1 0.812500 (0.878611)  loss 0.579455 (0.336290)\u001b[0m\n",
            "[2023-11-09 15:38:49] \u001b[32mEpoch [7/10] Step [261/469]  acc1 0.859375 (0.878592)  loss 0.367355 (0.336127)\u001b[0m\n",
            "[2023-11-09 15:38:51] \u001b[32mEpoch [7/10] Step [271/469]  acc1 0.890625 (0.879324)  loss 0.380173 (0.334845)\u001b[0m\n",
            "[2023-11-09 15:38:52] \u001b[32mEpoch [7/10] Step [281/469]  acc1 0.906250 (0.880338)  loss 0.288872 (0.332681)\u001b[0m\n",
            "[2023-11-09 15:38:54] \u001b[32mEpoch [7/10] Step [291/469]  acc1 0.906250 (0.880047)  loss 0.292309 (0.333484)\u001b[0m\n",
            "[2023-11-09 15:38:55] \u001b[32mEpoch [7/10] Step [301/469]  acc1 0.859375 (0.879724)  loss 0.473280 (0.333789)\u001b[0m\n",
            "[2023-11-09 15:38:56] \u001b[32mEpoch [7/10] Step [311/469]  acc1 0.906250 (0.879421)  loss 0.315787 (0.334603)\u001b[0m\n",
            "[2023-11-09 15:38:58] \u001b[32mEpoch [7/10] Step [321/469]  acc1 0.906250 (0.879478)  loss 0.327393 (0.334146)\u001b[0m\n",
            "[2023-11-09 15:38:59] \u001b[32mEpoch [7/10] Step [331/469]  acc1 0.906250 (0.879957)  loss 0.281068 (0.332746)\u001b[0m\n",
            "[2023-11-09 15:39:01] \u001b[32mEpoch [7/10] Step [341/469]  acc1 0.906250 (0.880407)  loss 0.292966 (0.332356)\u001b[0m\n",
            "[2023-11-09 15:39:02] \u001b[32mEpoch [7/10] Step [351/469]  acc1 0.875000 (0.880743)  loss 0.403858 (0.332025)\u001b[0m\n",
            "[2023-11-09 15:39:04] \u001b[32mEpoch [7/10] Step [361/469]  acc1 0.921875 (0.881189)  loss 0.226871 (0.330419)\u001b[0m\n",
            "[2023-11-09 15:39:05] \u001b[32mEpoch [7/10] Step [371/469]  acc1 0.875000 (0.880980)  loss 0.340195 (0.330698)\u001b[0m\n",
            "[2023-11-09 15:39:06] \u001b[32mEpoch [7/10] Step [381/469]  acc1 0.953125 (0.881726)  loss 0.180316 (0.328442)\u001b[0m\n",
            "[2023-11-09 15:39:08] \u001b[32mEpoch [7/10] Step [391/469]  acc1 0.812500 (0.881754)  loss 0.349617 (0.327838)\u001b[0m\n",
            "[2023-11-09 15:39:09] \u001b[32mEpoch [7/10] Step [401/469]  acc1 0.875000 (0.881546)  loss 0.355156 (0.329038)\u001b[0m\n",
            "[2023-11-09 15:39:11] \u001b[32mEpoch [7/10] Step [411/469]  acc1 0.828125 (0.881653)  loss 0.375549 (0.329787)\u001b[0m\n",
            "[2023-11-09 15:39:12] \u001b[32mEpoch [7/10] Step [421/469]  acc1 0.875000 (0.881384)  loss 0.309237 (0.330243)\u001b[0m\n",
            "[2023-11-09 15:39:14] \u001b[32mEpoch [7/10] Step [431/469]  acc1 0.937500 (0.881743)  loss 0.209430 (0.329106)\u001b[0m\n",
            "[2023-11-09 15:39:15] \u001b[32mEpoch [7/10] Step [441/469]  acc1 0.953125 (0.882122)  loss 0.226772 (0.328589)\u001b[0m\n",
            "[2023-11-09 15:39:16] \u001b[32mEpoch [7/10] Step [451/469]  acc1 0.875000 (0.882310)  loss 0.273494 (0.327842)\u001b[0m\n",
            "[2023-11-09 15:39:18] \u001b[32mEpoch [7/10] Step [461/469]  acc1 0.859375 (0.882287)  loss 0.355117 (0.328107)\u001b[0m\n",
            "[2023-11-09 15:39:21] \u001b[32mEpoch [8/10] Step [1/469]  acc1 0.875000 (0.875000)  loss 0.358668 (0.358668)\u001b[0m\n",
            "[2023-11-09 15:39:22] \u001b[32mEpoch [8/10] Step [11/469]  acc1 0.906250 (0.893466)  loss 0.234952 (0.328992)\u001b[0m\n",
            "[2023-11-09 15:39:24] \u001b[32mEpoch [8/10] Step [21/469]  acc1 0.875000 (0.883929)  loss 0.345365 (0.323472)\u001b[0m\n",
            "[2023-11-09 15:39:25] \u001b[32mEpoch [8/10] Step [31/469]  acc1 0.796875 (0.876512)  loss 0.433489 (0.335112)\u001b[0m\n",
            "[2023-11-09 15:39:26] \u001b[32mEpoch [8/10] Step [41/469]  acc1 0.875000 (0.878430)  loss 0.278646 (0.337020)\u001b[0m\n",
            "[2023-11-09 15:39:28] \u001b[32mEpoch [8/10] Step [51/469]  acc1 0.812500 (0.878064)  loss 0.544324 (0.339825)\u001b[0m\n",
            "[2023-11-09 15:39:29] \u001b[32mEpoch [8/10] Step [61/469]  acc1 0.859375 (0.877305)  loss 0.341143 (0.336301)\u001b[0m\n",
            "[2023-11-09 15:39:31] \u001b[32mEpoch [8/10] Step [71/469]  acc1 0.843750 (0.877641)  loss 0.419282 (0.334465)\u001b[0m\n",
            "[2023-11-09 15:39:32] \u001b[32mEpoch [8/10] Step [81/469]  acc1 0.906250 (0.876157)  loss 0.289952 (0.336043)\u001b[0m\n",
            "[2023-11-09 15:39:33] \u001b[32mEpoch [8/10] Step [91/469]  acc1 0.890625 (0.876717)  loss 0.389963 (0.335946)\u001b[0m\n",
            "[2023-11-09 15:39:35] \u001b[32mEpoch [8/10] Step [101/469]  acc1 0.906250 (0.877011)  loss 0.254779 (0.335982)\u001b[0m\n",
            "[2023-11-09 15:39:36] \u001b[32mEpoch [8/10] Step [111/469]  acc1 0.921875 (0.878378)  loss 0.325530 (0.334007)\u001b[0m\n",
            "[2023-11-09 15:39:38] \u001b[32mEpoch [8/10] Step [121/469]  acc1 0.906250 (0.877841)  loss 0.207117 (0.336166)\u001b[0m\n",
            "[2023-11-09 15:39:39] \u001b[32mEpoch [8/10] Step [131/469]  acc1 0.937500 (0.878101)  loss 0.187747 (0.335472)\u001b[0m\n",
            "[2023-11-09 15:39:41] \u001b[32mEpoch [8/10] Step [141/469]  acc1 0.937500 (0.876773)  loss 0.192280 (0.335979)\u001b[0m\n",
            "[2023-11-09 15:39:42] \u001b[32mEpoch [8/10] Step [151/469]  acc1 0.796875 (0.877380)  loss 0.394258 (0.333104)\u001b[0m\n",
            "[2023-11-09 15:39:44] \u001b[32mEpoch [8/10] Step [161/469]  acc1 0.953125 (0.878300)  loss 0.214513 (0.330996)\u001b[0m\n",
            "[2023-11-09 15:39:45] \u001b[32mEpoch [8/10] Step [171/469]  acc1 0.875000 (0.878289)  loss 0.320412 (0.332175)\u001b[0m\n",
            "[2023-11-09 15:39:46] \u001b[32mEpoch [8/10] Step [181/469]  acc1 0.921875 (0.878539)  loss 0.244491 (0.331001)\u001b[0m\n",
            "[2023-11-09 15:39:48] \u001b[32mEpoch [8/10] Step [191/469]  acc1 0.890625 (0.878599)  loss 0.277095 (0.332076)\u001b[0m\n",
            "[2023-11-09 15:39:49] \u001b[32mEpoch [8/10] Step [201/469]  acc1 0.875000 (0.879198)  loss 0.390880 (0.330764)\u001b[0m\n",
            "[2023-11-09 15:39:51] \u001b[32mEpoch [8/10] Step [211/469]  acc1 0.921875 (0.879739)  loss 0.143329 (0.329283)\u001b[0m\n",
            "[2023-11-09 15:39:52] \u001b[32mEpoch [8/10] Step [221/469]  acc1 0.828125 (0.879878)  loss 0.445256 (0.330553)\u001b[0m\n",
            "[2023-11-09 15:39:54] \u001b[32mEpoch [8/10] Step [231/469]  acc1 0.906250 (0.880817)  loss 0.269100 (0.328810)\u001b[0m\n",
            "[2023-11-09 15:39:55] \u001b[32mEpoch [8/10] Step [241/469]  acc1 0.906250 (0.881159)  loss 0.260034 (0.326668)\u001b[0m\n",
            "[2023-11-09 15:39:56] \u001b[32mEpoch [8/10] Step [251/469]  acc1 0.921875 (0.881972)  loss 0.177029 (0.324934)\u001b[0m\n",
            "[2023-11-09 15:39:58] \u001b[32mEpoch [8/10] Step [261/469]  acc1 0.906250 (0.882244)  loss 0.407588 (0.324250)\u001b[0m\n",
            "[2023-11-09 15:39:59] \u001b[32mEpoch [8/10] Step [271/469]  acc1 0.843750 (0.881976)  loss 0.337669 (0.325717)\u001b[0m\n",
            "[2023-11-09 15:40:01] \u001b[32mEpoch [8/10] Step [281/469]  acc1 0.843750 (0.882006)  loss 0.368582 (0.325092)\u001b[0m\n",
            "[2023-11-09 15:40:02] \u001b[32mEpoch [8/10] Step [291/469]  acc1 0.843750 (0.882249)  loss 0.343946 (0.323620)\u001b[0m\n",
            "[2023-11-09 15:40:04] \u001b[32mEpoch [8/10] Step [301/469]  acc1 0.859375 (0.882216)  loss 0.316452 (0.323601)\u001b[0m\n",
            "[2023-11-09 15:40:05] \u001b[32mEpoch [8/10] Step [311/469]  acc1 0.812500 (0.882285)  loss 0.588792 (3.490044)\u001b[0m\n",
            "[2023-11-09 15:40:06] \u001b[32mEpoch [8/10] Step [321/469]  acc1 0.953125 (0.882983)  loss 0.184782 (3.390402)\u001b[0m\n",
            "[2023-11-09 15:40:08] \u001b[32mEpoch [8/10] Step [331/469]  acc1 0.906250 (0.883591)  loss 0.316499 (3.296200)\u001b[0m\n",
            "[2023-11-09 15:40:09] \u001b[32mEpoch [8/10] Step [341/469]  acc1 0.843750 (0.883523)  loss 0.349695 (3.208974)\u001b[0m\n",
            "[2023-11-09 15:40:11] \u001b[32mEpoch [8/10] Step [351/469]  acc1 0.828125 (0.883280)  loss 0.539084 (3.127574)\u001b[0m\n",
            "[2023-11-09 15:40:12] \u001b[32mEpoch [8/10] Step [361/469]  acc1 0.953125 (0.883397)  loss 0.232917 (3.049470)\u001b[0m\n",
            "[2023-11-09 15:40:13] \u001b[32mEpoch [8/10] Step [371/469]  acc1 0.953125 (0.883971)  loss 0.160267 (2.974679)\u001b[0m\n",
            "[2023-11-09 15:40:15] \u001b[32mEpoch [8/10] Step [381/469]  acc1 0.875000 (0.884022)  loss 0.243978 (2.905048)\u001b[0m\n",
            "[2023-11-09 15:40:16] \u001b[32mEpoch [8/10] Step [391/469]  acc1 0.875000 (0.884111)  loss 0.361564 (2.838696)\u001b[0m\n",
            "[2023-11-09 15:40:18] \u001b[32mEpoch [8/10] Step [401/469]  acc1 0.875000 (0.883611)  loss 0.291291 (2.775832)\u001b[0m\n",
            "[2023-11-09 15:40:19] \u001b[32mEpoch [8/10] Step [411/469]  acc1 0.890625 (0.883896)  loss 0.215927 (2.715813)\u001b[0m\n",
            "[2023-11-09 15:40:21] \u001b[32mEpoch [8/10] Step [421/469]  acc1 0.906250 (0.884204)  loss 0.371973 (2.659424)\u001b[0m\n",
            "[2023-11-09 15:40:22] \u001b[32mEpoch [8/10] Step [431/469]  acc1 0.859375 (0.884535)  loss 0.371919 (2.618387)\u001b[0m\n",
            "[2023-11-09 15:40:23] \u001b[32mEpoch [8/10] Step [441/469]  acc1 0.859375 (0.884247)  loss 0.320765 (2.566333)\u001b[0m\n",
            "[2023-11-09 15:40:25] \u001b[32mEpoch [8/10] Step [451/469]  acc1 0.921875 (0.884354)  loss 0.327240 (2.516395)\u001b[0m\n",
            "[2023-11-09 15:40:26] \u001b[32mEpoch [8/10] Step [461/469]  acc1 0.859375 (0.884592)  loss 0.354423 (2.468407)\u001b[0m\n",
            "[2023-11-09 15:40:29] \u001b[32mEpoch [9/10] Step [1/469]  acc1 0.921875 (0.921875)  loss 0.326965 (0.326965)\u001b[0m\n",
            "[2023-11-09 15:40:31] \u001b[32mEpoch [9/10] Step [11/469]  acc1 0.812500 (0.875000)  loss 0.425309 (0.322748)\u001b[0m\n",
            "[2023-11-09 15:40:32] \u001b[32mEpoch [9/10] Step [21/469]  acc1 0.781250 (0.878720)  loss 0.517815 (0.327183)\u001b[0m\n",
            "[2023-11-09 15:40:34] \u001b[32mEpoch [9/10] Step [31/469]  acc1 0.953125 (0.885585)  loss 0.226725 (0.312172)\u001b[0m\n",
            "[2023-11-09 15:40:35] \u001b[32mEpoch [9/10] Step [41/469]  acc1 0.843750 (0.886814)  loss 0.393158 (0.312537)\u001b[0m\n",
            "[2023-11-09 15:40:37] \u001b[32mEpoch [9/10] Step [51/469]  acc1 0.812500 (0.885110)  loss 0.489366 (0.317794)\u001b[0m\n",
            "[2023-11-09 15:40:38] \u001b[32mEpoch [9/10] Step [61/469]  acc1 0.906250 (0.887295)  loss 0.229638 (0.314796)\u001b[0m\n",
            "[2023-11-09 15:40:40] \u001b[32mEpoch [9/10] Step [71/469]  acc1 0.906250 (0.886444)  loss 0.312634 (0.315724)\u001b[0m\n",
            "[2023-11-09 15:40:41] \u001b[32mEpoch [9/10] Step [81/469]  acc1 0.890625 (0.887924)  loss 0.340071 (0.312277)\u001b[0m\n",
            "[2023-11-09 15:40:42] \u001b[32mEpoch [9/10] Step [91/469]  acc1 0.906250 (0.888908)  loss 0.266705 (0.310407)\u001b[0m\n",
            "[2023-11-09 15:40:44] \u001b[32mEpoch [9/10] Step [101/469]  acc1 0.890625 (0.888304)  loss 0.254671 (0.310653)\u001b[0m\n",
            "[2023-11-09 15:40:45] \u001b[32mEpoch [9/10] Step [111/469]  acc1 0.875000 (0.886965)  loss 0.289429 (0.312411)\u001b[0m\n",
            "[2023-11-09 15:40:47] \u001b[32mEpoch [9/10] Step [121/469]  acc1 0.906250 (0.888042)  loss 0.372832 (0.311052)\u001b[0m\n",
            "[2023-11-09 15:40:48] \u001b[32mEpoch [9/10] Step [131/469]  acc1 0.906250 (0.888359)  loss 0.313763 (0.309746)\u001b[0m\n",
            "[2023-11-09 15:40:50] \u001b[32mEpoch [9/10] Step [141/469]  acc1 0.859375 (0.887965)  loss 0.388331 (0.422668)\u001b[0m\n",
            "[2023-11-09 15:40:51] \u001b[32mEpoch [9/10] Step [151/469]  acc1 0.906250 (0.887935)  loss 0.290785 (0.415163)\u001b[0m\n",
            "[2023-11-09 15:40:53] \u001b[32mEpoch [9/10] Step [161/469]  acc1 0.890625 (0.889557)  loss 0.246507 (0.405440)\u001b[0m\n",
            "[2023-11-09 15:40:54] \u001b[32mEpoch [9/10] Step [171/469]  acc1 0.953125 (0.889894)  loss 0.152509 (0.397166)\u001b[0m\n",
            "[2023-11-09 15:40:56] \u001b[32mEpoch [9/10] Step [181/469]  acc1 0.875000 (0.888985)  loss 0.447175 (0.395574)\u001b[0m\n",
            "[2023-11-09 15:40:57] \u001b[32mEpoch [9/10] Step [191/469]  acc1 0.890625 (0.888662)  loss 0.440784 (0.391885)\u001b[0m\n",
            "[2023-11-09 15:40:59] \u001b[32mEpoch [9/10] Step [201/469]  acc1 0.875000 (0.888371)  loss 0.299273 (0.387822)\u001b[0m\n",
            "[2023-11-09 15:41:00] \u001b[32mEpoch [9/10] Step [211/469]  acc1 0.890625 (0.888626)  loss 0.349279 (0.383749)\u001b[0m\n",
            "[2023-11-09 15:41:02] \u001b[32mEpoch [9/10] Step [221/469]  acc1 0.953125 (0.887868)  loss 0.157261 (0.381553)\u001b[0m\n",
            "[2023-11-09 15:41:03] \u001b[32mEpoch [9/10] Step [231/469]  acc1 0.937500 (0.887987)  loss 0.175942 (0.377483)\u001b[0m\n",
            "[2023-11-09 15:41:05] \u001b[32mEpoch [9/10] Step [241/469]  acc1 0.812500 (0.887707)  loss 0.509981 (0.375008)\u001b[0m\n",
            "[2023-11-09 15:41:06] \u001b[32mEpoch [9/10] Step [251/469]  acc1 0.828125 (0.888197)  loss 0.270386 (0.370970)\u001b[0m\n",
            "[2023-11-09 15:41:08] \u001b[32mEpoch [9/10] Step [261/469]  acc1 0.906250 (0.888649)  loss 0.295472 (0.368136)\u001b[0m\n",
            "[2023-11-09 15:41:10] \u001b[32mEpoch [9/10] Step [271/469]  acc1 0.953125 (0.889472)  loss 0.123880 (0.363961)\u001b[0m\n",
            "[2023-11-09 15:41:11] \u001b[32mEpoch [9/10] Step [281/469]  acc1 0.906250 (0.890013)  loss 0.225855 (0.361085)\u001b[0m\n",
            "[2023-11-09 15:41:13] \u001b[32mEpoch [9/10] Step [291/469]  acc1 0.875000 (0.890088)  loss 0.409572 (0.358826)\u001b[0m\n",
            "[2023-11-09 15:41:14] \u001b[32mEpoch [9/10] Step [301/469]  acc1 0.921875 (0.890469)  loss 0.319253 (0.355946)\u001b[0m\n",
            "[2023-11-09 15:41:16] \u001b[32mEpoch [9/10] Step [311/469]  acc1 0.937500 (0.890926)  loss 0.178086 (0.352993)\u001b[0m\n",
            "[2023-11-09 15:41:17] \u001b[32mEpoch [9/10] Step [321/469]  acc1 0.921875 (0.891745)  loss 0.294398 (0.349442)\u001b[0m\n",
            "[2023-11-09 15:41:19] \u001b[32mEpoch [9/10] Step [331/469]  acc1 0.796875 (0.891427)  loss 0.507913 (0.348615)\u001b[0m\n",
            "[2023-11-09 15:41:20] \u001b[32mEpoch [9/10] Step [341/469]  acc1 0.906250 (0.891725)  loss 0.320708 (0.347723)\u001b[0m\n",
            "[2023-11-09 15:41:22] \u001b[32mEpoch [9/10] Step [351/469]  acc1 0.906250 (0.891827)  loss 0.286821 (0.346679)\u001b[0m\n",
            "[2023-11-09 15:41:23] \u001b[32mEpoch [9/10] Step [361/469]  acc1 0.828125 (0.891144)  loss 0.409272 (0.347222)\u001b[0m\n",
            "[2023-11-09 15:41:25] \u001b[32mEpoch [9/10] Step [371/469]  acc1 0.984375 (0.890709)  loss 0.120448 (0.346740)\u001b[0m\n",
            "[2023-11-09 15:41:26] \u001b[32mEpoch [9/10] Step [381/469]  acc1 0.859375 (0.890830)  loss 0.451478 (0.345299)\u001b[0m\n",
            "[2023-11-09 15:41:28] \u001b[32mEpoch [9/10] Step [391/469]  acc1 0.859375 (0.890985)  loss 0.347592 (0.343618)\u001b[0m\n",
            "[2023-11-09 15:41:29] \u001b[32mEpoch [9/10] Step [401/469]  acc1 0.781250 (0.890352)  loss 0.537159 (0.343807)\u001b[0m\n",
            "[2023-11-09 15:41:30] \u001b[32mEpoch [9/10] Step [411/469]  acc1 0.781250 (0.889446)  loss 0.547395 (0.344506)\u001b[0m\n",
            "[2023-11-09 15:41:32] \u001b[32mEpoch [9/10] Step [421/469]  acc1 0.875000 (0.889326)  loss 0.357183 (0.344432)\u001b[0m\n",
            "[2023-11-09 15:41:33] \u001b[32mEpoch [9/10] Step [431/469]  acc1 0.875000 (0.889139)  loss 0.381645 (0.344248)\u001b[0m\n",
            "[2023-11-09 15:41:35] \u001b[32mEpoch [9/10] Step [441/469]  acc1 0.906250 (0.889101)  loss 0.320497 (0.343105)\u001b[0m\n",
            "[2023-11-09 15:41:36] \u001b[32mEpoch [9/10] Step [451/469]  acc1 0.906250 (0.888754)  loss 0.385357 (0.342928)\u001b[0m\n",
            "[2023-11-09 15:41:38] \u001b[32mEpoch [9/10] Step [461/469]  acc1 0.875000 (0.888591)  loss 0.369070 (0.343265)\u001b[0m\n",
            "[2023-11-09 15:41:41] \u001b[32mEpoch [10/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 0.340821 (0.340821)\u001b[0m\n",
            "[2023-11-09 15:41:42] \u001b[32mEpoch [10/10] Step [11/469]  acc1 0.859375 (0.860795)  loss 0.269207 (0.338292)\u001b[0m\n",
            "[2023-11-09 15:41:44] \u001b[32mEpoch [10/10] Step [21/469]  acc1 0.890625 (0.872768)  loss 0.260158 (0.320585)\u001b[0m\n",
            "[2023-11-09 15:41:45] \u001b[32mEpoch [10/10] Step [31/469]  acc1 0.906250 (0.885585)  loss 0.280273 (0.305920)\u001b[0m\n",
            "[2023-11-09 15:41:46] \u001b[32mEpoch [10/10] Step [41/469]  acc1 0.859375 (0.885290)  loss 0.340788 (0.304140)\u001b[0m\n",
            "[2023-11-09 15:41:48] \u001b[32mEpoch [10/10] Step [51/469]  acc1 0.828125 (0.884498)  loss 0.410181 (0.309840)\u001b[0m\n",
            "[2023-11-09 15:41:49] \u001b[32mEpoch [10/10] Step [61/469]  acc1 0.937500 (0.886270)  loss 0.255920 (0.305975)\u001b[0m\n",
            "[2023-11-09 15:41:51] \u001b[32mEpoch [10/10] Step [71/469]  acc1 0.937500 (0.886444)  loss 0.186946 (0.309161)\u001b[0m\n",
            "[2023-11-09 15:41:52] \u001b[32mEpoch [10/10] Step [81/469]  acc1 0.921875 (0.885610)  loss 0.272489 (0.312279)\u001b[0m\n",
            "[2023-11-09 15:41:54] \u001b[32mEpoch [10/10] Step [91/469]  acc1 0.796875 (0.882898)  loss 0.448227 (0.319451)\u001b[0m\n",
            "[2023-11-09 15:41:55] \u001b[32mEpoch [10/10] Step [101/469]  acc1 0.859375 (0.884437)  loss 0.321838 (0.315491)\u001b[0m\n",
            "[2023-11-09 15:41:56] \u001b[32mEpoch [10/10] Step [111/469]  acc1 0.890625 (0.884009)  loss 0.252962 (0.315184)\u001b[0m\n",
            "[2023-11-09 15:41:58] \u001b[32mEpoch [10/10] Step [121/469]  acc1 0.859375 (0.884943)  loss 0.287156 (0.314046)\u001b[0m\n",
            "[2023-11-09 15:42:00] \u001b[32mEpoch [10/10] Step [131/469]  acc1 0.859375 (0.885496)  loss 0.409490 (0.313049)\u001b[0m\n",
            "[2023-11-09 15:42:01] \u001b[32mEpoch [10/10] Step [141/469]  acc1 0.890625 (0.886636)  loss 0.348450 (0.312417)\u001b[0m\n",
            "[2023-11-09 15:42:02] \u001b[32mEpoch [10/10] Step [151/469]  acc1 0.843750 (0.886382)  loss 0.422302 (0.312833)\u001b[0m\n",
            "[2023-11-09 15:42:04] \u001b[32mEpoch [10/10] Step [161/469]  acc1 0.890625 (0.886355)  loss 0.337351 (0.313967)\u001b[0m\n",
            "[2023-11-09 15:42:05] \u001b[32mEpoch [10/10] Step [171/469]  acc1 0.890625 (0.887244)  loss 0.260744 (0.311874)\u001b[0m\n",
            "[2023-11-09 15:42:07] \u001b[32mEpoch [10/10] Step [181/469]  acc1 0.812500 (0.885359)  loss 0.533111 (0.315956)\u001b[0m\n",
            "[2023-11-09 15:42:08] \u001b[32mEpoch [10/10] Step [191/469]  acc1 0.921875 (0.885962)  loss 0.251761 (0.313916)\u001b[0m\n",
            "[2023-11-09 15:42:10] \u001b[32mEpoch [10/10] Step [201/469]  acc1 0.796875 (0.886116)  loss 0.572211 (0.313165)\u001b[0m\n",
            "[2023-11-09 15:42:11] \u001b[32mEpoch [10/10] Step [211/469]  acc1 0.921875 (0.886626)  loss 0.257196 (0.312244)\u001b[0m\n",
            "[2023-11-09 15:42:13] \u001b[32mEpoch [10/10] Step [221/469]  acc1 0.859375 (0.885817)  loss 0.245829 (0.312908)\u001b[0m\n",
            "[2023-11-09 15:42:14] \u001b[32mEpoch [10/10] Step [231/469]  acc1 0.734375 (0.884267)  loss 0.535440 (0.317020)\u001b[0m\n",
            "[2023-11-09 15:42:16] \u001b[32mEpoch [10/10] Step [241/469]  acc1 0.921875 (0.884077)  loss 0.276969 (0.317345)\u001b[0m\n",
            "[2023-11-09 15:42:17] \u001b[32mEpoch [10/10] Step [251/469]  acc1 0.921875 (0.883653)  loss 0.269579 (0.319118)\u001b[0m\n",
            "[2023-11-09 15:42:19] \u001b[32mEpoch [10/10] Step [261/469]  acc1 0.828125 (0.883740)  loss 0.419889 (0.319030)\u001b[0m\n",
            "[2023-11-09 15:42:20] \u001b[32mEpoch [10/10] Step [271/469]  acc1 0.890625 (0.883879)  loss 0.241715 (0.318164)\u001b[0m\n",
            "[2023-11-09 15:42:21] \u001b[32mEpoch [10/10] Step [281/469]  acc1 0.843750 (0.884453)  loss 0.376898 (0.316404)\u001b[0m\n",
            "[2023-11-09 15:42:23] \u001b[32mEpoch [10/10] Step [291/469]  acc1 0.875000 (0.884235)  loss 0.239127 (0.316452)\u001b[0m\n",
            "[2023-11-09 15:42:24] \u001b[32mEpoch [10/10] Step [301/469]  acc1 0.875000 (0.885330)  loss 0.274863 (0.314167)\u001b[0m\n",
            "[2023-11-09 15:42:26] \u001b[32mEpoch [10/10] Step [311/469]  acc1 0.921875 (0.885199)  loss 0.251607 (0.314121)\u001b[0m\n",
            "[2023-11-09 15:42:27] \u001b[32mEpoch [10/10] Step [321/469]  acc1 0.921875 (0.885319)  loss 0.223811 (0.313143)\u001b[0m\n",
            "[2023-11-09 15:42:29] \u001b[32mEpoch [10/10] Step [331/469]  acc1 0.843750 (0.885763)  loss 0.295845 (0.311797)\u001b[0m\n",
            "[2023-11-09 15:42:30] \u001b[32mEpoch [10/10] Step [341/469]  acc1 0.906250 (0.886135)  loss 0.291595 (0.311077)\u001b[0m\n",
            "[2023-11-09 15:42:31] \u001b[32mEpoch [10/10] Step [351/469]  acc1 0.843750 (0.886441)  loss 0.469675 (0.310730)\u001b[0m\n",
            "[2023-11-09 15:42:33] \u001b[32mEpoch [10/10] Step [361/469]  acc1 0.906250 (0.886297)  loss 0.307481 (0.310698)\u001b[0m\n",
            "[2023-11-09 15:42:34] \u001b[32mEpoch [10/10] Step [371/469]  acc1 0.812500 (0.886161)  loss 0.354670 (0.310081)\u001b[0m\n",
            "[2023-11-09 15:42:36] \u001b[32mEpoch [10/10] Step [381/469]  acc1 0.906250 (0.886852)  loss 0.236487 (0.309099)\u001b[0m\n",
            "[2023-11-09 15:42:37] \u001b[32mEpoch [10/10] Step [391/469]  acc1 0.890625 (0.887028)  loss 0.281193 (0.308086)\u001b[0m\n",
            "[2023-11-09 15:42:39] \u001b[32mEpoch [10/10] Step [401/469]  acc1 0.843750 (0.887586)  loss 0.325928 (0.306620)\u001b[0m\n",
            "[2023-11-09 15:42:40] \u001b[32mEpoch [10/10] Step [411/469]  acc1 0.953125 (0.888002)  loss 0.141356 (0.306015)\u001b[0m\n",
            "[2023-11-09 15:42:41] \u001b[32mEpoch [10/10] Step [421/469]  acc1 0.890625 (0.888621)  loss 0.358069 (0.305518)\u001b[0m\n",
            "[2023-11-09 15:42:43] \u001b[32mEpoch [10/10] Step [431/469]  acc1 0.921875 (0.889356)  loss 0.221296 (0.303925)\u001b[0m\n",
            "[2023-11-09 15:42:44] \u001b[32mEpoch [10/10] Step [441/469]  acc1 0.890625 (0.889527)  loss 0.307953 (0.303380)\u001b[0m\n",
            "[2023-11-09 15:42:46] \u001b[32mEpoch [10/10] Step [451/469]  acc1 0.937500 (0.889690)  loss 0.177469 (0.302675)\u001b[0m\n",
            "[2023-11-09 15:42:47] \u001b[32mEpoch [10/10] Step [461/469]  acc1 0.906250 (0.889676)  loss 0.171856 (0.302857)\u001b[0m\n",
            "Final architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv5x5', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'sepconv3x3', 'reduce_n4_p2': 'dilconv5x5', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'avgpool', 'reduce_n5_p1': 'dilconv5x5', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [0], 'reduce_n3_switch': [2], 'reduce_n4_switch': [1], 'reduce_n5_switch': [3]}\n",
            "weight = 100.0, lambd = 3\n",
            "[2023-11-09 15:42:50] \u001b[32mEpoch [1/10] Step [1/469]  acc1 0.109375 (0.109375)  loss 8466.780273 (8466.780273)\u001b[0m\n",
            "[2023-11-09 15:42:51] \u001b[32mEpoch [1/10] Step [11/469]  acc1 0.093750 (0.136364)  loss 22.082504 (2512.827312)\u001b[0m\n",
            "[2023-11-09 15:42:53] \u001b[32mEpoch [1/10] Step [21/469]  acc1 0.171875 (0.156994)  loss 5.558091 (1476.541949)\u001b[0m\n",
            "[2023-11-09 15:42:54] \u001b[32mEpoch [1/10] Step [31/469]  acc1 0.187500 (0.171371)  loss 107.731155 (1118.817736)\u001b[0m\n",
            "[2023-11-09 15:42:56] \u001b[32mEpoch [1/10] Step [41/469]  acc1 0.203125 (0.185213)  loss 359.380890 (930.612426)\u001b[0m\n",
            "[2023-11-09 15:42:57] \u001b[32mEpoch [1/10] Step [51/469]  acc1 0.187500 (0.192708)  loss 1214.178955 (920.480260)\u001b[0m\n",
            "[2023-11-09 15:42:59] \u001b[32mEpoch [1/10] Step [61/469]  acc1 0.046875 (0.185707)  loss 968.680786 (817.200243)\u001b[0m\n",
            "[2023-11-09 15:43:00] \u001b[32mEpoch [1/10] Step [71/469]  acc1 0.265625 (0.193222)  loss 2.683599 (702.854283)\u001b[0m\n",
            "[2023-11-09 15:43:01] \u001b[32mEpoch [1/10] Step [81/469]  acc1 0.281250 (0.203318)  loss 2.259671 (639.720628)\u001b[0m\n",
            "[2023-11-09 15:43:03] \u001b[32mEpoch [1/10] Step [91/469]  acc1 0.343750 (0.219265)  loss 1.686983 (579.627939)\u001b[0m\n",
            "[2023-11-09 15:43:04] \u001b[32mEpoch [1/10] Step [101/469]  acc1 0.421875 (0.241182)  loss 1.495944 (522.398287)\u001b[0m\n",
            "[2023-11-09 15:43:06] \u001b[32mEpoch [1/10] Step [111/469]  acc1 0.484375 (0.261684)  loss 1.372459 (488.537031)\u001b[0m\n",
            "[2023-11-09 15:43:07] \u001b[32mEpoch [1/10] Step [121/469]  acc1 0.703125 (0.284349)  loss 6.082120 (448.312426)\u001b[0m\n",
            "[2023-11-09 15:43:09] \u001b[32mEpoch [1/10] Step [131/469]  acc1 0.687500 (0.309876)  loss 0.979676 (414.175587)\u001b[0m\n",
            "[2023-11-09 15:43:10] \u001b[32mEpoch [1/10] Step [141/469]  acc1 0.765625 (0.334109)  loss 0.786632 (384.965127)\u001b[0m\n",
            "[2023-11-09 15:43:11] \u001b[32mEpoch [1/10] Step [151/469]  acc1 0.593750 (0.353891)  loss 0.964537 (359.705415)\u001b[0m\n",
            "[2023-11-09 15:43:13] \u001b[32mEpoch [1/10] Step [161/469]  acc1 0.640625 (0.371021)  loss 0.960784 (339.528839)\u001b[0m\n",
            "[2023-11-09 15:43:14] \u001b[32mEpoch [1/10] Step [171/469]  acc1 0.687500 (0.387884)  loss 0.840941 (319.730194)\u001b[0m\n",
            "[2023-11-09 15:43:16] \u001b[32mEpoch [1/10] Step [181/469]  acc1 0.718750 (0.404092)  loss 0.818281 (302.164647)\u001b[0m\n",
            "[2023-11-09 15:43:17] \u001b[32mEpoch [1/10] Step [191/469]  acc1 0.656250 (0.417212)  loss 2.379140 (286.430476)\u001b[0m\n",
            "[2023-11-09 15:43:19] \u001b[32mEpoch [1/10] Step [201/469]  acc1 0.703125 (0.429415)  loss 0.756479 (272.226805)\u001b[0m\n",
            "[2023-11-09 15:43:20] \u001b[32mEpoch [1/10] Step [211/469]  acc1 0.625000 (0.441129)  loss 40.208931 (259.553852)\u001b[0m\n",
            "[2023-11-09 15:43:21] \u001b[32mEpoch [1/10] Step [221/469]  acc1 0.734375 (0.452418)  loss 0.825068 (248.337600)\u001b[0m\n",
            "[2023-11-09 15:43:23] \u001b[32mEpoch [1/10] Step [231/469]  acc1 0.687500 (0.463001)  loss 0.737932 (237.621013)\u001b[0m\n",
            "[2023-11-09 15:43:24] \u001b[32mEpoch [1/10] Step [241/469]  acc1 0.671875 (0.472186)  loss 0.892539 (231.908842)\u001b[0m\n",
            "[2023-11-09 15:43:26] \u001b[32mEpoch [1/10] Step [251/469]  acc1 0.765625 (0.481698)  loss 0.723682 (222.972804)\u001b[0m\n",
            "[2023-11-09 15:43:27] \u001b[32mEpoch [1/10] Step [261/469]  acc1 0.671875 (0.489943)  loss 0.785251 (214.461597)\u001b[0m\n",
            "[2023-11-09 15:43:29] \u001b[32mEpoch [1/10] Step [271/469]  acc1 0.718750 (0.497636)  loss 0.710351 (206.577582)\u001b[0m\n",
            "[2023-11-09 15:43:30] \u001b[32mEpoch [1/10] Step [281/469]  acc1 0.718750 (0.506395)  loss 0.749463 (199.255120)\u001b[0m\n",
            "[2023-11-09 15:43:32] \u001b[32mEpoch [1/10] Step [291/469]  acc1 0.703125 (0.513907)  loss 0.731104 (194.930081)\u001b[0m\n",
            "[2023-11-09 15:43:33] \u001b[32mEpoch [1/10] Step [301/469]  acc1 0.687500 (0.520660)  loss 0.870472 (188.507472)\u001b[0m\n",
            "[2023-11-09 15:43:35] \u001b[32mEpoch [1/10] Step [311/469]  acc1 0.843750 (0.527231)  loss 1.318698 (182.472789)\u001b[0m\n",
            "[2023-11-09 15:43:36] \u001b[32mEpoch [1/10] Step [321/469]  acc1 0.734375 (0.533002)  loss 0.733818 (176.813050)\u001b[0m\n",
            "[2023-11-09 15:43:38] \u001b[32mEpoch [1/10] Step [331/469]  acc1 0.593750 (0.539464)  loss 1.118017 (171.493684)\u001b[0m\n",
            "[2023-11-09 15:43:39] \u001b[32mEpoch [1/10] Step [341/469]  acc1 0.656250 (0.544630)  loss 0.887085 (166.487301)\u001b[0m\n",
            "[2023-11-09 15:43:41] \u001b[32mEpoch [1/10] Step [351/469]  acc1 0.687500 (0.550570)  loss 0.836799 (161.764142)\u001b[0m\n",
            "[2023-11-09 15:43:42] \u001b[32mEpoch [1/10] Step [361/469]  acc1 0.750000 (0.555618)  loss 0.597910 (157.301522)\u001b[0m\n",
            "[2023-11-09 15:43:44] \u001b[32mEpoch [1/10] Step [371/469]  acc1 0.781250 (0.560731)  loss 0.643948 (153.080343)\u001b[0m\n",
            "[2023-11-09 15:43:45] \u001b[32mEpoch [1/10] Step [381/469]  acc1 0.734375 (0.565084)  loss 1.215515 (149.082499)\u001b[0m\n",
            "[2023-11-09 15:43:47] \u001b[32mEpoch [1/10] Step [391/469]  acc1 0.796875 (0.570332)  loss 0.647768 (145.287000)\u001b[0m\n",
            "[2023-11-09 15:43:48] \u001b[32mEpoch [1/10] Step [401/469]  acc1 0.765625 (0.574540)  loss 0.875675 (141.681411)\u001b[0m\n",
            "[2023-11-09 15:43:49] \u001b[32mEpoch [1/10] Step [411/469]  acc1 0.640625 (0.577517)  loss 0.830188 (138.253003)\u001b[0m\n",
            "[2023-11-09 15:43:51] \u001b[32mEpoch [1/10] Step [421/469]  acc1 0.734375 (0.581391)  loss 0.571927 (136.251405)\u001b[0m\n",
            "[2023-11-09 15:43:52] \u001b[32mEpoch [1/10] Step [431/469]  acc1 0.750000 (0.585448)  loss 0.762245 (133.106237)\u001b[0m\n",
            "[2023-11-09 15:43:54] \u001b[32mEpoch [1/10] Step [441/469]  acc1 0.734375 (0.589109)  loss 0.586736 (130.102955)\u001b[0m\n",
            "[2023-11-09 15:43:55] \u001b[32mEpoch [1/10] Step [451/469]  acc1 0.703125 (0.592849)  loss 0.802029 (127.233452)\u001b[0m\n",
            "[2023-11-09 15:43:56] \u001b[32mEpoch [1/10] Step [461/469]  acc1 0.734375 (0.596800)  loss 0.773575 (124.487649)\u001b[0m\n",
            "[2023-11-09 15:43:59] \u001b[32mEpoch [2/10] Step [1/469]  acc1 0.875000 (0.875000)  loss 0.342959 (0.342959)\u001b[0m\n",
            "[2023-11-09 15:44:01] \u001b[32mEpoch [2/10] Step [11/469]  acc1 0.750000 (0.778409)  loss 0.719972 (0.595853)\u001b[0m\n",
            "[2023-11-09 15:44:02] \u001b[32mEpoch [2/10] Step [21/469]  acc1 0.750000 (0.770833)  loss 0.642147 (0.613808)\u001b[0m\n",
            "[2023-11-09 15:44:04] \u001b[32mEpoch [2/10] Step [31/469]  acc1 0.765625 (0.765121)  loss 0.586846 (0.618125)\u001b[0m\n",
            "[2023-11-09 15:44:05] \u001b[32mEpoch [2/10] Step [41/469]  acc1 0.703125 (0.769436)  loss 0.756399 (0.610104)\u001b[0m\n",
            "[2023-11-09 15:44:06] \u001b[32mEpoch [2/10] Step [51/469]  acc1 0.625000 (0.769608)  loss 0.858625 (0.611783)\u001b[0m\n",
            "[2023-11-09 15:44:08] \u001b[32mEpoch [2/10] Step [61/469]  acc1 0.781250 (0.775102)  loss 0.638455 (0.608179)\u001b[0m\n",
            "[2023-11-09 15:44:09] \u001b[32mEpoch [2/10] Step [71/469]  acc1 0.718750 (0.774208)  loss 0.717610 (0.605120)\u001b[0m\n",
            "[2023-11-09 15:44:11] \u001b[32mEpoch [2/10] Step [81/469]  acc1 0.687500 (0.769869)  loss 0.720636 (0.610670)\u001b[0m\n",
            "[2023-11-09 15:44:12] \u001b[32mEpoch [2/10] Step [91/469]  acc1 0.812500 (0.772321)  loss 0.591775 (0.607630)\u001b[0m\n",
            "[2023-11-09 15:44:13] \u001b[32mEpoch [2/10] Step [101/469]  acc1 0.796875 (0.773515)  loss 0.689966 (0.609188)\u001b[0m\n",
            "[2023-11-09 15:44:15] \u001b[32mEpoch [2/10] Step [111/469]  acc1 0.875000 (0.774916)  loss 0.483446 (9.613846)\u001b[0m\n",
            "[2023-11-09 15:44:16] \u001b[32mEpoch [2/10] Step [121/469]  acc1 0.843750 (0.776730)  loss 0.490869 (8.868375)\u001b[0m\n",
            "[2023-11-09 15:44:18] \u001b[32mEpoch [2/10] Step [131/469]  acc1 0.781250 (0.778865)  loss 0.703014 (15.551991)\u001b[0m\n",
            "[2023-11-09 15:44:19] \u001b[32mEpoch [2/10] Step [141/469]  acc1 0.765625 (0.780142)  loss 0.630819 (14.491537)\u001b[0m\n",
            "[2023-11-09 15:44:21] \u001b[32mEpoch [2/10] Step [151/469]  acc1 0.671875 (0.779387)  loss 0.833094 (13.572601)\u001b[0m\n",
            "[2023-11-09 15:44:22] \u001b[32mEpoch [2/10] Step [161/469]  acc1 0.828125 (0.780085)  loss 0.463643 (12.764867)\u001b[0m\n",
            "[2023-11-09 15:44:23] \u001b[32mEpoch [2/10] Step [171/469]  acc1 0.796875 (0.780610)  loss 0.555009 (12.051553)\u001b[0m\n",
            "[2023-11-09 15:44:25] \u001b[32mEpoch [2/10] Step [181/469]  acc1 0.796875 (0.780559)  loss 0.638955 (11.418190)\u001b[0m\n",
            "[2023-11-09 15:44:26] \u001b[32mEpoch [2/10] Step [191/469]  acc1 0.828125 (0.782313)  loss 0.455477 (10.847035)\u001b[0m\n",
            "[2023-11-09 15:44:28] \u001b[32mEpoch [2/10] Step [201/469]  acc1 0.765625 (0.781483)  loss 0.631520 (10.338890)\u001b[0m\n",
            "[2023-11-09 15:44:29] \u001b[32mEpoch [2/10] Step [211/469]  acc1 0.890625 (0.782213)  loss 0.474026 (9.875569)\u001b[0m\n",
            "[2023-11-09 15:44:31] \u001b[32mEpoch [2/10] Step [221/469]  acc1 0.812500 (0.783654)  loss 0.520925 (9.452601)\u001b[0m\n",
            "[2023-11-09 15:44:32] \u001b[32mEpoch [2/10] Step [231/469]  acc1 0.765625 (0.784497)  loss 0.592559 (9.068056)\u001b[0m\n",
            "[2023-11-09 15:44:33] \u001b[32mEpoch [2/10] Step [241/469]  acc1 0.718750 (0.784362)  loss 0.677125 (8.722313)\u001b[0m\n",
            "[2023-11-09 15:44:35] \u001b[32mEpoch [2/10] Step [251/469]  acc1 0.781250 (0.784612)  loss 0.603700 (10.306282)\u001b[0m\n",
            "[2023-11-09 15:44:36] \u001b[32mEpoch [2/10] Step [261/469]  acc1 0.781250 (0.784962)  loss 0.723492 (9.933012)\u001b[0m\n",
            "[2023-11-09 15:44:38] \u001b[32mEpoch [2/10] Step [271/469]  acc1 0.734375 (0.785632)  loss 0.654897 (9.587180)\u001b[0m\n",
            "[2023-11-09 15:44:39] \u001b[32mEpoch [2/10] Step [281/469]  acc1 0.718750 (0.785198)  loss 0.701520 (9.268449)\u001b[0m\n",
            "[2023-11-09 15:44:40] \u001b[32mEpoch [2/10] Step [291/469]  acc1 0.859375 (0.786136)  loss 0.469615 (8.967718)\u001b[0m\n",
            "[2023-11-09 15:44:42] \u001b[32mEpoch [2/10] Step [301/469]  acc1 0.828125 (0.786856)  loss 0.494802 (8.687736)\u001b[0m\n",
            "[2023-11-09 15:44:43] \u001b[32mEpoch [2/10] Step [311/469]  acc1 0.796875 (0.787580)  loss 0.612221 (8.425765)\u001b[0m\n",
            "[2023-11-09 15:44:45] \u001b[32mEpoch [2/10] Step [321/469]  acc1 0.750000 (0.787578)  loss 0.691862 (8.181542)\u001b[0m\n",
            "[2023-11-09 15:44:46] \u001b[32mEpoch [2/10] Step [331/469]  acc1 0.703125 (0.787764)  loss 0.734248 (7.950097)\u001b[0m\n",
            "[2023-11-09 15:44:48] \u001b[32mEpoch [2/10] Step [341/469]  acc1 0.812500 (0.787665)  loss 0.510674 (7.734447)\u001b[0m\n",
            "[2023-11-09 15:44:49] \u001b[32mEpoch [2/10] Step [351/469]  acc1 0.812500 (0.788328)  loss 0.461521 (7.528178)\u001b[0m\n",
            "[2023-11-09 15:44:50] \u001b[32mEpoch [2/10] Step [361/469]  acc1 0.812500 (0.789171)  loss 0.468969 (7.334275)\u001b[0m\n",
            "[2023-11-09 15:44:52] \u001b[32mEpoch [2/10] Step [371/469]  acc1 0.750000 (0.789126)  loss 0.580543 (7.151963)\u001b[0m\n",
            "[2023-11-09 15:44:53] \u001b[32mEpoch [2/10] Step [381/469]  acc1 0.812500 (0.790231)  loss 0.535699 (6.977379)\u001b[0m\n",
            "[2023-11-09 15:44:55] \u001b[32mEpoch [2/10] Step [391/469]  acc1 0.921875 (0.791121)  loss 0.284220 (6.810606)\u001b[0m\n",
            "[2023-11-09 15:44:56] \u001b[32mEpoch [2/10] Step [401/469]  acc1 0.796875 (0.791810)  loss 0.539869 (6.653549)\u001b[0m\n",
            "[2023-11-09 15:44:58] \u001b[32mEpoch [2/10] Step [411/469]  acc1 0.796875 (0.792503)  loss 0.624934 (6.503509)\u001b[0m\n",
            "[2023-11-09 15:44:59] \u001b[32mEpoch [2/10] Step [421/469]  acc1 0.734375 (0.792458)  loss 0.686941 (6.362870)\u001b[0m\n",
            "[2023-11-09 15:45:00] \u001b[32mEpoch [2/10] Step [431/469]  acc1 0.843750 (0.792887)  loss 0.426096 (6.227163)\u001b[0m\n",
            "[2023-11-09 15:45:02] \u001b[32mEpoch [2/10] Step [441/469]  acc1 0.828125 (0.793013)  loss 0.479272 (6.098344)\u001b[0m\n",
            "[2023-11-09 15:45:03] \u001b[32mEpoch [2/10] Step [451/469]  acc1 0.859375 (0.792995)  loss 0.459006 (5.975262)\u001b[0m\n",
            "[2023-11-09 15:45:05] \u001b[32mEpoch [2/10] Step [461/469]  acc1 0.796875 (0.793418)  loss 0.673123 (5.857586)\u001b[0m\n",
            "[2023-11-09 15:45:08] \u001b[32mEpoch [3/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 0.629220 (0.629220)\u001b[0m\n",
            "[2023-11-09 15:45:09] \u001b[32mEpoch [3/10] Step [11/469]  acc1 0.781250 (0.802557)  loss 0.568117 (0.508355)\u001b[0m\n",
            "[2023-11-09 15:45:10] \u001b[32mEpoch [3/10] Step [21/469]  acc1 0.828125 (0.817708)  loss 0.453100 (0.491923)\u001b[0m\n",
            "[2023-11-09 15:45:12] \u001b[32mEpoch [3/10] Step [31/469]  acc1 0.843750 (0.815020)  loss 0.418492 (15.492053)\u001b[0m\n",
            "[2023-11-09 15:45:13] \u001b[32mEpoch [3/10] Step [41/469]  acc1 0.812500 (0.813262)  loss 0.490289 (11.837481)\u001b[0m\n",
            "[2023-11-09 15:45:15] \u001b[32mEpoch [3/10] Step [51/469]  acc1 0.812500 (0.808824)  loss 0.572670 (9.741500)\u001b[0m\n",
            "[2023-11-09 15:45:16] \u001b[32mEpoch [3/10] Step [61/469]  acc1 0.812500 (0.809682)  loss 0.527297 (8.230814)\u001b[0m\n",
            "[2023-11-09 15:45:18] \u001b[32mEpoch [3/10] Step [71/469]  acc1 0.796875 (0.811620)  loss 0.494878 (20.697910)\u001b[0m\n",
            "[2023-11-09 15:45:19] \u001b[32mEpoch [3/10] Step [81/469]  acc1 0.843750 (0.811150)  loss 0.585633 (18.205978)\u001b[0m\n",
            "[2023-11-09 15:45:20] \u001b[32mEpoch [3/10] Step [91/469]  acc1 0.890625 (0.811298)  loss 0.341227 (16.259898)\u001b[0m\n",
            "[2023-11-09 15:45:22] \u001b[32mEpoch [3/10] Step [101/469]  acc1 0.781250 (0.813119)  loss 0.527190 (14.697607)\u001b[0m\n",
            "[2023-11-09 15:45:23] \u001b[32mEpoch [3/10] Step [111/469]  acc1 0.859375 (0.813063)  loss 0.429307 (13.418315)\u001b[0m\n",
            "[2023-11-09 15:45:25] \u001b[32mEpoch [3/10] Step [121/469]  acc1 0.812500 (0.811596)  loss 0.457696 (12.355272)\u001b[0m\n",
            "[2023-11-09 15:45:26] \u001b[32mEpoch [3/10] Step [131/469]  acc1 0.890625 (0.814408)  loss 0.360606 (11.444585)\u001b[0m\n",
            "[2023-11-09 15:45:27] \u001b[32mEpoch [3/10] Step [141/469]  acc1 0.828125 (0.815603)  loss 0.487030 (10.667259)\u001b[0m\n",
            "[2023-11-09 15:45:29] \u001b[32mEpoch [3/10] Step [151/469]  acc1 0.875000 (0.817467)  loss 0.384818 (9.990270)\u001b[0m\n",
            "[2023-11-09 15:45:30] \u001b[32mEpoch [3/10] Step [161/469]  acc1 0.781250 (0.817741)  loss 0.626595 (9.401526)\u001b[0m\n",
            "[2023-11-09 15:45:32] \u001b[32mEpoch [3/10] Step [171/469]  acc1 0.937500 (0.819353)  loss 0.312412 (8.877240)\u001b[0m\n",
            "[2023-11-09 15:45:33] \u001b[32mEpoch [3/10] Step [181/469]  acc1 0.812500 (0.820701)  loss 0.494482 (8.409671)\u001b[0m\n",
            "[2023-11-09 15:45:34] \u001b[32mEpoch [3/10] Step [191/469]  acc1 0.828125 (0.820272)  loss 0.402223 (7.995109)\u001b[0m\n",
            "[2023-11-09 15:45:36] \u001b[32mEpoch [3/10] Step [201/469]  acc1 0.906250 (0.820818)  loss 0.328516 (7.620463)\u001b[0m\n",
            "[2023-11-09 15:45:38] \u001b[32mEpoch [3/10] Step [211/469]  acc1 0.890625 (0.822053)  loss 0.544459 (7.281157)\u001b[0m\n",
            "[2023-11-09 15:45:39] \u001b[32mEpoch [3/10] Step [221/469]  acc1 0.843750 (0.823247)  loss 0.519319 (6.972032)\u001b[0m\n",
            "[2023-11-09 15:45:41] \u001b[32mEpoch [3/10] Step [231/469]  acc1 0.921875 (0.822781)  loss 0.364798 (6.980044)\u001b[0m\n",
            "[2023-11-09 15:45:42] \u001b[32mEpoch [3/10] Step [241/469]  acc1 0.812500 (0.822225)  loss 1.534414 (6.715531)\u001b[0m\n",
            "[2023-11-09 15:45:44] \u001b[32mEpoch [3/10] Step [251/469]  acc1 0.796875 (0.822273)  loss 0.576571 (6.467940)\u001b[0m\n",
            "[2023-11-09 15:45:45] \u001b[32mEpoch [3/10] Step [261/469]  acc1 0.812500 (0.822079)  loss 140.591995 (6.776986)\u001b[0m\n",
            "[2023-11-09 15:45:47] \u001b[32mEpoch [3/10] Step [271/469]  acc1 0.812500 (0.821783)  loss 0.466613 (6.564799)\u001b[0m\n",
            "[2023-11-09 15:45:48] \u001b[32mEpoch [3/10] Step [281/469]  acc1 0.953125 (0.823232)  loss 0.221842 (6.344651)\u001b[0m\n",
            "[2023-11-09 15:45:50] \u001b[32mEpoch [3/10] Step [291/469]  acc1 0.765625 (0.823615)  loss 0.660451 (6.142494)\u001b[0m\n",
            "[2023-11-09 15:45:51] \u001b[32mEpoch [3/10] Step [301/469]  acc1 0.781250 (0.823401)  loss 0.655259 (5.955582)\u001b[0m\n",
            "[2023-11-09 15:45:52] \u001b[32mEpoch [3/10] Step [311/469]  acc1 0.859375 (0.823955)  loss 0.405026 (5.779277)\u001b[0m\n",
            "[2023-11-09 15:45:54] \u001b[32mEpoch [3/10] Step [321/469]  acc1 0.781250 (0.824328)  loss 0.542717 (5.612904)\u001b[0m\n",
            "[2023-11-09 15:45:55] \u001b[32mEpoch [3/10] Step [331/469]  acc1 0.875000 (0.825104)  loss 0.421167 (5.456288)\u001b[0m\n",
            "[2023-11-09 15:45:57] \u001b[32mEpoch [3/10] Step [341/469]  acc1 0.843750 (0.825559)  loss 0.497181 (5.310699)\u001b[0m\n",
            "[2023-11-09 15:45:58] \u001b[32mEpoch [3/10] Step [351/469]  acc1 0.828125 (0.825543)  loss 0.381184 (5.172174)\u001b[0m\n",
            "[2023-11-09 15:46:00] \u001b[32mEpoch [3/10] Step [361/469]  acc1 0.734375 (0.826091)  loss 0.741321 (5.041261)\u001b[0m\n",
            "[2023-11-09 15:46:01] \u001b[32mEpoch [3/10] Step [371/469]  acc1 0.843750 (0.826146)  loss 0.421352 (4.922144)\u001b[0m\n",
            "[2023-11-09 15:46:02] \u001b[32mEpoch [3/10] Step [381/469]  acc1 0.875000 (0.826280)  loss 0.464485 (4.805977)\u001b[0m\n",
            "[2023-11-09 15:46:04] \u001b[32mEpoch [3/10] Step [391/469]  acc1 0.843750 (0.826087)  loss 0.451539 (4.694658)\u001b[0m\n",
            "[2023-11-09 15:46:05] \u001b[32mEpoch [3/10] Step [401/469]  acc1 0.750000 (0.825943)  loss 0.539912 (4.589164)\u001b[0m\n",
            "[2023-11-09 15:46:07] \u001b[32mEpoch [3/10] Step [411/469]  acc1 0.796875 (0.826604)  loss 0.484945 (4.488454)\u001b[0m\n",
            "[2023-11-09 15:46:08] \u001b[32mEpoch [3/10] Step [421/469]  acc1 0.718750 (0.825490)  loss 0.740091 (4.395349)\u001b[0m\n",
            "[2023-11-09 15:46:09] \u001b[32mEpoch [3/10] Step [431/469]  acc1 0.843750 (0.825406)  loss 0.386021 (4.304467)\u001b[0m\n",
            "[2023-11-09 15:46:11] \u001b[32mEpoch [3/10] Step [441/469]  acc1 0.734375 (0.825539)  loss 0.629297 (4.216762)\u001b[0m\n",
            "[2023-11-09 15:46:12] \u001b[32mEpoch [3/10] Step [451/469]  acc1 0.890625 (0.826012)  loss 0.341296 (4.132427)\u001b[0m\n",
            "[2023-11-09 15:46:14] \u001b[32mEpoch [3/10] Step [461/469]  acc1 0.921875 (0.826600)  loss 0.347313 (4.051400)\u001b[0m\n",
            "[2023-11-09 15:46:17] \u001b[32mEpoch [4/10] Step [1/469]  acc1 0.843750 (0.843750)  loss 0.450248 (0.450248)\u001b[0m\n",
            "[2023-11-09 15:46:18] \u001b[32mEpoch [4/10] Step [11/469]  acc1 0.828125 (0.839489)  loss 0.482257 (0.437943)\u001b[0m\n",
            "[2023-11-09 15:46:19] \u001b[32mEpoch [4/10] Step [21/469]  acc1 0.812500 (0.826637)  loss 0.596267 (0.461084)\u001b[0m\n",
            "[2023-11-09 15:46:21] \u001b[32mEpoch [4/10] Step [31/469]  acc1 0.843750 (0.831149)  loss 0.478545 (0.457837)\u001b[0m\n",
            "[2023-11-09 15:46:22] \u001b[32mEpoch [4/10] Step [41/469]  acc1 0.953125 (0.835747)  loss 0.284707 (0.446840)\u001b[0m\n",
            "[2023-11-09 15:46:24] \u001b[32mEpoch [4/10] Step [51/469]  acc1 0.828125 (0.836397)  loss 0.560206 (0.443423)\u001b[0m\n",
            "[2023-11-09 15:46:25] \u001b[32mEpoch [4/10] Step [61/469]  acc1 0.906250 (0.839652)  loss 0.412405 (0.439168)\u001b[0m\n",
            "[2023-11-09 15:46:27] \u001b[32mEpoch [4/10] Step [71/469]  acc1 0.890625 (0.842210)  loss 0.397279 (1.006979)\u001b[0m\n",
            "[2023-11-09 15:46:28] \u001b[32mEpoch [4/10] Step [81/469]  acc1 0.750000 (0.841049)  loss 0.725260 (1.255903)\u001b[0m\n",
            "[2023-11-09 15:46:29] \u001b[32mEpoch [4/10] Step [91/469]  acc1 0.859375 (0.839801)  loss 0.457068 (1.164522)\u001b[0m\n",
            "[2023-11-09 15:46:31] \u001b[32mEpoch [4/10] Step [101/469]  acc1 0.828125 (0.836634)  loss 0.403218 (10.997570)\u001b[0m\n",
            "[2023-11-09 15:46:32] \u001b[32mEpoch [4/10] Step [111/469]  acc1 0.812500 (0.836712)  loss 0.409050 (10.045667)\u001b[0m\n",
            "[2023-11-09 15:46:34] \u001b[32mEpoch [4/10] Step [121/469]  acc1 0.937500 (0.836131)  loss 0.258057 (9.255041)\u001b[0m\n",
            "[2023-11-09 15:46:35] \u001b[32mEpoch [4/10] Step [131/469]  acc1 0.859375 (0.836355)  loss 0.379990 (8.582203)\u001b[0m\n",
            "[2023-11-09 15:46:37] \u001b[32mEpoch [4/10] Step [141/469]  acc1 0.781250 (0.835660)  loss 0.606547 (8.007974)\u001b[0m\n",
            "[2023-11-09 15:46:39] \u001b[32mEpoch [4/10] Step [151/469]  acc1 0.875000 (0.836403)  loss 0.303153 (7.504131)\u001b[0m\n",
            "[2023-11-09 15:46:40] \u001b[32mEpoch [4/10] Step [161/469]  acc1 0.875000 (0.837733)  loss 0.316667 (7.062675)\u001b[0m\n",
            "[2023-11-09 15:46:42] \u001b[32mEpoch [4/10] Step [171/469]  acc1 0.968750 (0.837628)  loss 0.243230 (6.675293)\u001b[0m\n",
            "[2023-11-09 15:46:43] \u001b[32mEpoch [4/10] Step [181/469]  acc1 0.843750 (0.839606)  loss 0.400930 (6.327069)\u001b[0m\n",
            "[2023-11-09 15:46:44] \u001b[32mEpoch [4/10] Step [191/469]  acc1 0.765625 (0.837942)  loss 0.573309 (6.020917)\u001b[0m\n",
            "[2023-11-09 15:46:46] \u001b[32mEpoch [4/10] Step [201/469]  acc1 0.890625 (0.839086)  loss 0.325272 (10.498328)\u001b[0m\n",
            "[2023-11-09 15:46:47] \u001b[32mEpoch [4/10] Step [211/469]  acc1 0.765625 (0.840640)  loss 0.576990 (10.017998)\u001b[0m\n",
            "[2023-11-09 15:46:49] \u001b[32mEpoch [4/10] Step [221/469]  acc1 0.781250 (0.841558)  loss 0.432434 (9.584194)\u001b[0m\n",
            "[2023-11-09 15:46:50] \u001b[32mEpoch [4/10] Step [231/469]  acc1 0.906250 (0.841924)  loss 0.300284 (9.187180)\u001b[0m\n",
            "[2023-11-09 15:46:52] \u001b[32mEpoch [4/10] Step [241/469]  acc1 0.828125 (0.840962)  loss 0.371519 (8.823884)\u001b[0m\n",
            "[2023-11-09 15:46:53] \u001b[32mEpoch [4/10] Step [251/469]  acc1 0.796875 (0.840575)  loss 0.458883 (8.491409)\u001b[0m\n",
            "[2023-11-09 15:46:54] \u001b[32mEpoch [4/10] Step [261/469]  acc1 0.765625 (0.840637)  loss 0.707206 (8.182421)\u001b[0m\n",
            "[2023-11-09 15:46:56] \u001b[32mEpoch [4/10] Step [271/469]  acc1 0.843750 (0.841213)  loss 0.463780 (7.894374)\u001b[0m\n",
            "[2023-11-09 15:46:57] \u001b[32mEpoch [4/10] Step [281/469]  acc1 0.921875 (0.841804)  loss 0.302251 (7.627088)\u001b[0m\n",
            "[2023-11-09 15:46:59] \u001b[32mEpoch [4/10] Step [291/469]  acc1 0.875000 (0.842354)  loss 0.322192 (7.529884)\u001b[0m\n",
            "[2023-11-09 15:47:00] \u001b[32mEpoch [4/10] Step [301/469]  acc1 0.859375 (0.842816)  loss 0.532949 (7.294313)\u001b[0m\n",
            "[2023-11-09 15:47:01] \u001b[32mEpoch [4/10] Step [311/469]  acc1 0.859375 (0.843097)  loss 0.439433 (7.073219)\u001b[0m\n",
            "[2023-11-09 15:47:03] \u001b[32mEpoch [4/10] Step [321/469]  acc1 0.828125 (0.843312)  loss 0.437451 (6.866415)\u001b[0m\n",
            "[2023-11-09 15:47:04] \u001b[32mEpoch [4/10] Step [331/469]  acc1 0.812500 (0.842995)  loss 0.619289 (6.672687)\u001b[0m\n",
            "[2023-11-09 15:47:06] \u001b[32mEpoch [4/10] Step [341/469]  acc1 0.812500 (0.842650)  loss 0.507185 (6.490649)\u001b[0m\n",
            "[2023-11-09 15:47:07] \u001b[32mEpoch [4/10] Step [351/469]  acc1 0.828125 (0.842637)  loss 0.424874 (6.317578)\u001b[0m\n",
            "[2023-11-09 15:47:09] \u001b[32mEpoch [4/10] Step [361/469]  acc1 0.812500 (0.842062)  loss 0.403034 (6.154336)\u001b[0m\n",
            "[2023-11-09 15:47:10] \u001b[32mEpoch [4/10] Step [371/469]  acc1 0.828125 (0.841939)  loss 0.440522 (6.000018)\u001b[0m\n",
            "[2023-11-09 15:47:11] \u001b[32mEpoch [4/10] Step [381/469]  acc1 0.921875 (0.843053)  loss 0.245086 (5.852523)\u001b[0m\n",
            "[2023-11-09 15:47:13] \u001b[32mEpoch [4/10] Step [391/469]  acc1 0.859375 (0.843111)  loss 0.388588 (5.713106)\u001b[0m\n",
            "[2023-11-09 15:47:14] \u001b[32mEpoch [4/10] Step [401/469]  acc1 0.906250 (0.843711)  loss 0.369482 (5.580449)\u001b[0m\n",
            "[2023-11-09 15:47:16] \u001b[32mEpoch [4/10] Step [411/469]  acc1 0.812500 (0.844282)  loss 0.484848 (5.453542)\u001b[0m\n",
            "[2023-11-09 15:47:17] \u001b[32mEpoch [4/10] Step [421/469]  acc1 0.781250 (0.844084)  loss 0.649289 (5.335649)\u001b[0m\n",
            "[2023-11-09 15:47:19] \u001b[32mEpoch [4/10] Step [431/469]  acc1 0.906250 (0.844366)  loss 0.295436 (5.220499)\u001b[0m\n",
            "[2023-11-09 15:47:20] \u001b[32mEpoch [4/10] Step [441/469]  acc1 0.875000 (0.844281)  loss 0.307282 (5.112024)\u001b[0m\n",
            "[2023-11-09 15:47:21] \u001b[32mEpoch [4/10] Step [451/469]  acc1 0.859375 (0.844616)  loss 0.406535 (5.006995)\u001b[0m\n",
            "[2023-11-09 15:47:23] \u001b[32mEpoch [4/10] Step [461/469]  acc1 0.906250 (0.844665)  loss 0.283795 (4.907727)\u001b[0m\n",
            "[2023-11-09 15:47:26] \u001b[32mEpoch [5/10] Step [1/469]  acc1 0.906250 (0.906250)  loss 0.298276 (0.298276)\u001b[0m\n",
            "[2023-11-09 15:47:27] \u001b[32mEpoch [5/10] Step [11/469]  acc1 0.906250 (0.855114)  loss 0.310387 (0.376829)\u001b[0m\n",
            "[2023-11-09 15:47:29] \u001b[32mEpoch [5/10] Step [21/469]  acc1 0.843750 (0.845238)  loss 0.448154 (0.407821)\u001b[0m\n",
            "[2023-11-09 15:47:30] \u001b[32mEpoch [5/10] Step [31/469]  acc1 0.875000 (0.855847)  loss 0.379371 (0.399231)\u001b[0m\n",
            "[2023-11-09 15:47:31] \u001b[32mEpoch [5/10] Step [41/469]  acc1 0.859375 (0.850610)  loss 0.398038 (0.409095)\u001b[0m\n",
            "[2023-11-09 15:47:33] \u001b[32mEpoch [5/10] Step [51/469]  acc1 0.890625 (0.850184)  loss 0.290124 (0.407755)\u001b[0m\n",
            "[2023-11-09 15:47:34] \u001b[32mEpoch [5/10] Step [61/469]  acc1 0.828125 (0.848361)  loss 0.415665 (0.405080)\u001b[0m\n",
            "[2023-11-09 15:47:36] \u001b[32mEpoch [5/10] Step [71/469]  acc1 0.828125 (0.851232)  loss 0.353296 (0.397346)\u001b[0m\n",
            "[2023-11-09 15:47:37] \u001b[32mEpoch [5/10] Step [81/469]  acc1 0.796875 (0.849344)  loss 0.511445 (0.405873)\u001b[0m\n",
            "[2023-11-09 15:47:39] \u001b[32mEpoch [5/10] Step [91/469]  acc1 0.843750 (0.849588)  loss 0.396777 (0.406131)\u001b[0m\n",
            "[2023-11-09 15:47:40] \u001b[32mEpoch [5/10] Step [101/469]  acc1 0.828125 (0.848082)  loss 0.398422 (0.407753)\u001b[0m\n",
            "[2023-11-09 15:47:42] \u001b[32mEpoch [5/10] Step [111/469]  acc1 0.796875 (0.846425)  loss 0.433296 (0.409013)\u001b[0m\n",
            "[2023-11-09 15:47:43] \u001b[32mEpoch [5/10] Step [121/469]  acc1 0.890625 (0.849432)  loss 0.345621 (0.405028)\u001b[0m\n",
            "[2023-11-09 15:47:44] \u001b[32mEpoch [5/10] Step [131/469]  acc1 0.859375 (0.850072)  loss 0.410464 (0.404423)\u001b[0m\n",
            "[2023-11-09 15:47:46] \u001b[32mEpoch [5/10] Step [141/469]  acc1 0.921875 (0.850953)  loss 0.260985 (0.400897)\u001b[0m\n",
            "[2023-11-09 15:47:47] \u001b[32mEpoch [5/10] Step [151/469]  acc1 0.828125 (0.852235)  loss 0.449072 (0.397318)\u001b[0m\n",
            "[2023-11-09 15:47:49] \u001b[32mEpoch [5/10] Step [161/469]  acc1 0.921875 (0.853455)  loss 0.240245 (0.395849)\u001b[0m\n",
            "[2023-11-09 15:47:50] \u001b[32mEpoch [5/10] Step [171/469]  acc1 0.796875 (0.852979)  loss 0.632350 (0.399224)\u001b[0m\n",
            "[2023-11-09 15:47:52] \u001b[32mEpoch [5/10] Step [181/469]  acc1 0.875000 (0.853764)  loss 0.398925 (0.399016)\u001b[0m\n",
            "[2023-11-09 15:47:53] \u001b[32mEpoch [5/10] Step [191/469]  acc1 0.890625 (0.853894)  loss 0.368550 (0.400495)\u001b[0m\n",
            "[2023-11-09 15:47:54] \u001b[32mEpoch [5/10] Step [201/469]  acc1 0.921875 (0.854322)  loss 0.383493 (0.399535)\u001b[0m\n",
            "[2023-11-09 15:47:56] \u001b[32mEpoch [5/10] Step [211/469]  acc1 0.890625 (0.854636)  loss 0.352615 (0.399363)\u001b[0m\n",
            "[2023-11-09 15:47:57] \u001b[32mEpoch [5/10] Step [221/469]  acc1 0.859375 (0.855062)  loss 0.308117 (0.397160)\u001b[0m\n",
            "[2023-11-09 15:47:59] \u001b[32mEpoch [5/10] Step [231/469]  acc1 0.890625 (0.855249)  loss 0.271691 (0.396073)\u001b[0m\n",
            "[2023-11-09 15:48:00] \u001b[32mEpoch [5/10] Step [241/469]  acc1 0.843750 (0.854448)  loss 0.453881 (0.398191)\u001b[0m\n",
            "[2023-11-09 15:48:02] \u001b[32mEpoch [5/10] Step [251/469]  acc1 0.875000 (0.854582)  loss 0.376030 (0.398658)\u001b[0m\n",
            "[2023-11-09 15:48:03] \u001b[32mEpoch [5/10] Step [261/469]  acc1 0.890625 (0.854945)  loss 0.306224 (0.397075)\u001b[0m\n",
            "[2023-11-09 15:48:05] \u001b[32mEpoch [5/10] Step [271/469]  acc1 0.843750 (0.854647)  loss 0.401516 (0.397683)\u001b[0m\n",
            "[2023-11-09 15:48:06] \u001b[32mEpoch [5/10] Step [281/469]  acc1 0.859375 (0.854259)  loss 0.475703 (0.398105)\u001b[0m\n",
            "[2023-11-09 15:48:08] \u001b[32mEpoch [5/10] Step [291/469]  acc1 0.828125 (0.854757)  loss 0.404549 (0.396818)\u001b[0m\n",
            "[2023-11-09 15:48:09] \u001b[32mEpoch [5/10] Step [301/469]  acc1 0.859375 (0.854444)  loss 0.405846 (0.396991)\u001b[0m\n",
            "[2023-11-09 15:48:11] \u001b[32mEpoch [5/10] Step [311/469]  acc1 0.859375 (0.854703)  loss 0.350891 (0.397147)\u001b[0m\n",
            "[2023-11-09 15:48:12] \u001b[32mEpoch [5/10] Step [321/469]  acc1 0.812500 (0.854410)  loss 0.669824 (0.397917)\u001b[0m\n",
            "[2023-11-09 15:48:14] \u001b[32mEpoch [5/10] Step [331/469]  acc1 0.875000 (0.854654)  loss 0.453909 (0.397482)\u001b[0m\n",
            "[2023-11-09 15:48:15] \u001b[32mEpoch [5/10] Step [341/469]  acc1 0.875000 (0.855801)  loss 0.326520 (0.395079)\u001b[0m\n",
            "[2023-11-09 15:48:17] \u001b[32mEpoch [5/10] Step [351/469]  acc1 0.828125 (0.855947)  loss 0.548485 (0.395792)\u001b[0m\n",
            "[2023-11-09 15:48:18] \u001b[32mEpoch [5/10] Step [361/469]  acc1 0.875000 (0.856345)  loss 0.442287 (0.396137)\u001b[0m\n",
            "[2023-11-09 15:48:20] \u001b[32mEpoch [5/10] Step [371/469]  acc1 0.843750 (0.855627)  loss 0.501531 (0.397991)\u001b[0m\n",
            "[2023-11-09 15:48:21] \u001b[32mEpoch [5/10] Step [381/469]  acc1 0.875000 (0.855725)  loss 0.306094 (0.397417)\u001b[0m\n",
            "[2023-11-09 15:48:23] \u001b[32mEpoch [5/10] Step [391/469]  acc1 0.843750 (0.855818)  loss 0.396795 (0.397748)\u001b[0m\n",
            "[2023-11-09 15:48:24] \u001b[32mEpoch [5/10] Step [401/469]  acc1 0.875000 (0.855712)  loss 0.432569 (0.397858)\u001b[0m\n",
            "[2023-11-09 15:48:25] \u001b[32mEpoch [5/10] Step [411/469]  acc1 0.921875 (0.855915)  loss 0.291620 (0.397600)\u001b[0m\n",
            "[2023-11-09 15:48:27] \u001b[32mEpoch [5/10] Step [421/469]  acc1 0.828125 (0.855738)  loss 0.447764 (0.397680)\u001b[0m\n",
            "[2023-11-09 15:48:28] \u001b[32mEpoch [5/10] Step [431/469]  acc1 0.843750 (0.855315)  loss 0.390494 (0.587011)\u001b[0m\n",
            "[2023-11-09 15:48:30] \u001b[32mEpoch [5/10] Step [441/469]  acc1 0.843750 (0.855052)  loss 0.407526 (0.583065)\u001b[0m\n",
            "[2023-11-09 15:48:31] \u001b[32mEpoch [5/10] Step [451/469]  acc1 0.859375 (0.855356)  loss 0.297135 (0.577929)\u001b[0m\n",
            "[2023-11-09 15:48:33] \u001b[32mEpoch [5/10] Step [461/469]  acc1 0.906250 (0.855274)  loss 0.255771 (0.574191)\u001b[0m\n",
            "[2023-11-09 15:48:35] \u001b[32mEpoch [6/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 0.493876 (0.493876)\u001b[0m\n",
            "[2023-11-09 15:48:37] \u001b[32mEpoch [6/10] Step [11/469]  acc1 0.828125 (0.873580)  loss 0.442378 (0.372071)\u001b[0m\n",
            "[2023-11-09 15:48:38] \u001b[32mEpoch [6/10] Step [21/469]  acc1 0.796875 (0.870536)  loss 0.529217 (0.377775)\u001b[0m\n",
            "[2023-11-09 15:48:40] \u001b[32mEpoch [6/10] Step [31/469]  acc1 0.875000 (0.872480)  loss 0.339180 (0.365886)\u001b[0m\n",
            "[2023-11-09 15:48:41] \u001b[32mEpoch [6/10] Step [41/469]  acc1 0.937500 (0.873095)  loss 0.195209 (0.362613)\u001b[0m\n",
            "[2023-11-09 15:48:43] \u001b[32mEpoch [6/10] Step [51/469]  acc1 0.937500 (0.873775)  loss 0.209749 (0.359406)\u001b[0m\n",
            "[2023-11-09 15:48:44] \u001b[32mEpoch [6/10] Step [61/469]  acc1 0.750000 (0.868852)  loss 0.598848 (0.369022)\u001b[0m\n",
            "[2023-11-09 15:48:45] \u001b[32mEpoch [6/10] Step [71/469]  acc1 0.937500 (0.869278)  loss 0.208146 (0.368990)\u001b[0m\n",
            "[2023-11-09 15:48:47] \u001b[32mEpoch [6/10] Step [81/469]  acc1 0.718750 (0.866705)  loss 0.545065 (0.373207)\u001b[0m\n",
            "[2023-11-09 15:48:48] \u001b[32mEpoch [6/10] Step [91/469]  acc1 0.875000 (0.866415)  loss 0.385372 (0.373494)\u001b[0m\n",
            "[2023-11-09 15:48:50] \u001b[32mEpoch [6/10] Step [101/469]  acc1 0.890625 (0.864635)  loss 0.266230 (0.373513)\u001b[0m\n",
            "[2023-11-09 15:48:51] \u001b[32mEpoch [6/10] Step [111/469]  acc1 0.906250 (0.867258)  loss 0.374891 (0.415760)\u001b[0m\n",
            "[2023-11-09 15:48:53] \u001b[32mEpoch [6/10] Step [121/469]  acc1 0.890625 (0.868543)  loss 0.399893 (0.410859)\u001b[0m\n",
            "[2023-11-09 15:48:54] \u001b[32mEpoch [6/10] Step [131/469]  acc1 0.812500 (0.867724)  loss 0.417843 (0.409085)\u001b[0m\n",
            "[2023-11-09 15:48:55] \u001b[32mEpoch [6/10] Step [141/469]  acc1 0.828125 (0.865691)  loss 0.469263 (0.408611)\u001b[0m\n",
            "[2023-11-09 15:48:57] \u001b[32mEpoch [6/10] Step [151/469]  acc1 0.796875 (0.865584)  loss 0.538491 (0.406140)\u001b[0m\n",
            "[2023-11-09 15:48:58] \u001b[32mEpoch [6/10] Step [161/469]  acc1 0.859375 (0.864616)  loss 0.323237 (0.410298)\u001b[0m\n",
            "[2023-11-09 15:49:00] \u001b[32mEpoch [6/10] Step [171/469]  acc1 0.843750 (0.863487)  loss 0.367569 (0.409148)\u001b[0m\n",
            "[2023-11-09 15:49:01] \u001b[32mEpoch [6/10] Step [181/469]  acc1 0.843750 (0.864209)  loss 0.467058 (0.413380)\u001b[0m\n",
            "[2023-11-09 15:49:02] \u001b[32mEpoch [6/10] Step [191/469]  acc1 0.921875 (0.863629)  loss 0.258477 (0.411299)\u001b[0m\n",
            "[2023-11-09 15:49:04] \u001b[32mEpoch [6/10] Step [201/469]  acc1 0.843750 (0.863495)  loss 0.399694 (0.408681)\u001b[0m\n",
            "[2023-11-09 15:49:05] \u001b[32mEpoch [6/10] Step [211/469]  acc1 0.765625 (0.862781)  loss 0.625984 (0.407875)\u001b[0m\n",
            "[2023-11-09 15:49:07] \u001b[32mEpoch [6/10] Step [221/469]  acc1 0.921875 (0.863900)  loss 0.310552 (0.404080)\u001b[0m\n",
            "[2023-11-09 15:49:08] \u001b[32mEpoch [6/10] Step [231/469]  acc1 0.921875 (0.864583)  loss 0.274096 (0.400945)\u001b[0m\n",
            "[2023-11-09 15:49:10] \u001b[32mEpoch [6/10] Step [241/469]  acc1 0.812500 (0.864691)  loss 0.383874 (0.399314)\u001b[0m\n",
            "[2023-11-09 15:49:11] \u001b[32mEpoch [6/10] Step [251/469]  acc1 0.875000 (0.865164)  loss 0.364084 (0.396651)\u001b[0m\n",
            "[2023-11-09 15:49:12] \u001b[32mEpoch [6/10] Step [261/469]  acc1 0.859375 (0.865182)  loss 0.397932 (0.396113)\u001b[0m\n",
            "[2023-11-09 15:49:14] \u001b[32mEpoch [6/10] Step [271/469]  acc1 0.843750 (0.863642)  loss 0.330020 (0.397901)\u001b[0m\n",
            "[2023-11-09 15:49:15] \u001b[32mEpoch [6/10] Step [281/469]  acc1 0.906250 (0.863990)  loss 0.277711 (0.395710)\u001b[0m\n",
            "[2023-11-09 15:49:17] \u001b[32mEpoch [6/10] Step [291/469]  acc1 0.875000 (0.863671)  loss 0.428053 (0.396307)\u001b[0m\n",
            "[2023-11-09 15:49:18] \u001b[32mEpoch [6/10] Step [301/469]  acc1 0.843750 (0.863632)  loss 0.615717 (0.395411)\u001b[0m\n",
            "[2023-11-09 15:49:20] \u001b[32mEpoch [6/10] Step [311/469]  acc1 0.921875 (0.863746)  loss 0.320033 (0.395212)\u001b[0m\n",
            "[2023-11-09 15:49:21] \u001b[32mEpoch [6/10] Step [321/469]  acc1 0.906250 (0.864437)  loss 0.271708 (0.393871)\u001b[0m\n",
            "[2023-11-09 15:49:22] \u001b[32mEpoch [6/10] Step [331/469]  acc1 0.796875 (0.863623)  loss 0.512166 (0.394543)\u001b[0m\n",
            "[2023-11-09 15:49:24] \u001b[32mEpoch [6/10] Step [341/469]  acc1 0.859375 (0.863453)  loss 0.352340 (0.394807)\u001b[0m\n",
            "[2023-11-09 15:49:25] \u001b[32mEpoch [6/10] Step [351/469]  acc1 0.828125 (0.863114)  loss 0.435023 (0.394594)\u001b[0m\n",
            "[2023-11-09 15:49:27] \u001b[32mEpoch [6/10] Step [361/469]  acc1 0.796875 (0.862967)  loss 0.507229 (0.394058)\u001b[0m\n",
            "[2023-11-09 15:49:28] \u001b[32mEpoch [6/10] Step [371/469]  acc1 0.812500 (0.862829)  loss 0.447479 (0.394250)\u001b[0m\n",
            "[2023-11-09 15:49:30] \u001b[32mEpoch [6/10] Step [381/469]  acc1 0.906250 (0.862697)  loss 0.307029 (0.393948)\u001b[0m\n",
            "[2023-11-09 15:49:31] \u001b[32mEpoch [6/10] Step [391/469]  acc1 0.890625 (0.862612)  loss 0.288545 (0.394267)\u001b[0m\n",
            "[2023-11-09 15:49:32] \u001b[32mEpoch [6/10] Step [401/469]  acc1 0.906250 (0.862414)  loss 0.344069 (0.394408)\u001b[0m\n",
            "[2023-11-09 15:49:34] \u001b[32mEpoch [6/10] Step [411/469]  acc1 0.796875 (0.862378)  loss 0.558243 (0.393443)\u001b[0m\n",
            "[2023-11-09 15:49:35] \u001b[32mEpoch [6/10] Step [421/469]  acc1 0.906250 (0.862418)  loss 0.251875 (0.391934)\u001b[0m\n",
            "[2023-11-09 15:49:37] \u001b[32mEpoch [6/10] Step [431/469]  acc1 0.828125 (0.862674)  loss 0.486037 (0.391067)\u001b[0m\n",
            "[2023-11-09 15:49:38] \u001b[32mEpoch [6/10] Step [441/469]  acc1 0.828125 (0.863095)  loss 0.328387 (0.389244)\u001b[0m\n",
            "[2023-11-09 15:49:40] \u001b[32mEpoch [6/10] Step [451/469]  acc1 0.796875 (0.863013)  loss 0.450427 (0.388185)\u001b[0m\n",
            "[2023-11-09 15:49:41] \u001b[32mEpoch [6/10] Step [461/469]  acc1 0.906250 (0.863341)  loss 0.274194 (0.386692)\u001b[0m\n",
            "[2023-11-09 15:49:44] \u001b[32mEpoch [7/10] Step [1/469]  acc1 0.937500 (0.937500)  loss 0.162835 (0.162835)\u001b[0m\n",
            "[2023-11-09 15:49:45] \u001b[32mEpoch [7/10] Step [11/469]  acc1 0.859375 (0.870739)  loss 0.362639 (0.362932)\u001b[0m\n",
            "[2023-11-09 15:49:47] \u001b[32mEpoch [7/10] Step [21/469]  acc1 0.890625 (0.882440)  loss 0.360941 (0.341924)\u001b[0m\n",
            "[2023-11-09 15:49:48] \u001b[32mEpoch [7/10] Step [31/469]  acc1 0.875000 (0.884577)  loss 0.368930 (0.337724)\u001b[0m\n",
            "[2023-11-09 15:49:50] \u001b[32mEpoch [7/10] Step [41/469]  acc1 0.890625 (0.882241)  loss 0.289775 (0.342173)\u001b[0m\n",
            "[2023-11-09 15:49:51] \u001b[32mEpoch [7/10] Step [51/469]  acc1 0.937500 (0.883578)  loss 0.244324 (0.331894)\u001b[0m\n",
            "[2023-11-09 15:49:52] \u001b[32mEpoch [7/10] Step [61/469]  acc1 0.890625 (0.881404)  loss 0.309931 (0.333992)\u001b[0m\n",
            "[2023-11-09 15:49:54] \u001b[32mEpoch [7/10] Step [71/469]  acc1 0.890625 (0.880282)  loss 0.232586 (0.333395)\u001b[0m\n",
            "[2023-11-09 15:49:55] \u001b[32mEpoch [7/10] Step [81/469]  acc1 0.875000 (0.881559)  loss 0.330422 (0.327780)\u001b[0m\n",
            "[2023-11-09 15:49:57] \u001b[32mEpoch [7/10] Step [91/469]  acc1 0.890625 (0.882383)  loss 0.303242 (0.327357)\u001b[0m\n",
            "[2023-11-09 15:49:58] \u001b[32mEpoch [7/10] Step [101/469]  acc1 0.812500 (0.879486)  loss 0.364197 (0.329901)\u001b[0m\n",
            "[2023-11-09 15:50:00] \u001b[32mEpoch [7/10] Step [111/469]  acc1 0.828125 (0.878801)  loss 0.350655 (0.329264)\u001b[0m\n",
            "[2023-11-09 15:50:01] \u001b[32mEpoch [7/10] Step [121/469]  acc1 0.890625 (0.877712)  loss 0.307628 (0.331886)\u001b[0m\n",
            "[2023-11-09 15:50:02] \u001b[32mEpoch [7/10] Step [131/469]  acc1 0.765625 (0.874881)  loss 0.639863 (0.336950)\u001b[0m\n",
            "[2023-11-09 15:50:04] \u001b[32mEpoch [7/10] Step [141/469]  acc1 0.921875 (0.874778)  loss 0.313170 (0.337332)\u001b[0m\n",
            "[2023-11-09 15:50:05] \u001b[32mEpoch [7/10] Step [151/469]  acc1 0.890625 (0.875000)  loss 0.328448 (0.338368)\u001b[0m\n",
            "[2023-11-09 15:50:07] \u001b[32mEpoch [7/10] Step [161/469]  acc1 0.875000 (0.874806)  loss 0.273493 (0.337638)\u001b[0m\n",
            "[2023-11-09 15:50:08] \u001b[32mEpoch [7/10] Step [171/469]  acc1 0.812500 (0.873721)  loss 0.424541 (0.339992)\u001b[0m\n",
            "[2023-11-09 15:50:09] \u001b[32mEpoch [7/10] Step [181/469]  acc1 0.843750 (0.872497)  loss 0.339902 (0.342519)\u001b[0m\n",
            "[2023-11-09 15:50:11] \u001b[32mEpoch [7/10] Step [191/469]  acc1 0.796875 (0.872300)  loss 0.532436 (0.345492)\u001b[0m\n",
            "[2023-11-09 15:50:12] \u001b[32mEpoch [7/10] Step [201/469]  acc1 0.906250 (0.872590)  loss 0.294182 (0.344888)\u001b[0m\n",
            "[2023-11-09 15:50:14] \u001b[32mEpoch [7/10] Step [211/469]  acc1 0.921875 (0.872038)  loss 0.244162 (0.346985)\u001b[0m\n",
            "[2023-11-09 15:50:15] \u001b[32mEpoch [7/10] Step [221/469]  acc1 0.906250 (0.873162)  loss 0.265278 (0.345763)\u001b[0m\n",
            "[2023-11-09 15:50:17] \u001b[32mEpoch [7/10] Step [231/469]  acc1 0.921875 (0.873647)  loss 0.294407 (0.346624)\u001b[0m\n",
            "[2023-11-09 15:50:18] \u001b[32mEpoch [7/10] Step [241/469]  acc1 0.906250 (0.873898)  loss 0.376615 (0.347545)\u001b[0m\n",
            "[2023-11-09 15:50:19] \u001b[32mEpoch [7/10] Step [251/469]  acc1 0.781250 (0.873693)  loss 0.499445 (0.349055)\u001b[0m\n",
            "[2023-11-09 15:50:21] \u001b[32mEpoch [7/10] Step [261/469]  acc1 0.859375 (0.872426)  loss 0.420588 (0.351268)\u001b[0m\n",
            "[2023-11-09 15:50:22] \u001b[32mEpoch [7/10] Step [271/469]  acc1 0.843750 (0.872060)  loss 0.272507 (0.351748)\u001b[0m\n",
            "[2023-11-09 15:50:24] \u001b[32mEpoch [7/10] Step [281/469]  acc1 0.937500 (0.871664)  loss 0.234964 (0.352494)\u001b[0m\n",
            "[2023-11-09 15:50:25] \u001b[32mEpoch [7/10] Step [291/469]  acc1 0.875000 (0.871617)  loss 0.312331 (0.351617)\u001b[0m\n",
            "[2023-11-09 15:50:26] \u001b[32mEpoch [7/10] Step [301/469]  acc1 0.859375 (0.870795)  loss 0.415013 (0.353565)\u001b[0m\n",
            "[2023-11-09 15:50:28] \u001b[32mEpoch [7/10] Step [311/469]  acc1 0.906250 (0.870830)  loss 0.368106 (0.354508)\u001b[0m\n",
            "[2023-11-09 15:50:29] \u001b[32mEpoch [7/10] Step [321/469]  acc1 0.906250 (0.871301)  loss 0.229303 (0.353383)\u001b[0m\n",
            "[2023-11-09 15:50:31] \u001b[32mEpoch [7/10] Step [331/469]  acc1 0.859375 (0.870846)  loss 0.324891 (0.353585)\u001b[0m\n",
            "[2023-11-09 15:50:32] \u001b[32mEpoch [7/10] Step [341/469]  acc1 0.921875 (0.870876)  loss 0.234356 (0.353643)\u001b[0m\n",
            "[2023-11-09 15:50:34] \u001b[32mEpoch [7/10] Step [351/469]  acc1 0.875000 (0.870994)  loss 0.410434 (0.354202)\u001b[0m\n",
            "[2023-11-09 15:50:35] \u001b[32mEpoch [7/10] Step [361/469]  acc1 0.890625 (0.871105)  loss 0.289217 (0.354089)\u001b[0m\n",
            "[2023-11-09 15:50:36] \u001b[32mEpoch [7/10] Step [371/469]  acc1 0.953125 (0.871462)  loss 0.283562 (0.354859)\u001b[0m\n",
            "[2023-11-09 15:50:38] \u001b[32mEpoch [7/10] Step [381/469]  acc1 0.843750 (0.871842)  loss 0.431256 (0.354003)\u001b[0m\n",
            "[2023-11-09 15:50:39] \u001b[32mEpoch [7/10] Step [391/469]  acc1 0.921875 (0.872363)  loss 0.204254 (0.353501)\u001b[0m\n",
            "[2023-11-09 15:50:41] \u001b[32mEpoch [7/10] Step [401/469]  acc1 0.906250 (0.873169)  loss 0.237706 (0.351048)\u001b[0m\n",
            "[2023-11-09 15:50:42] \u001b[32mEpoch [7/10] Step [411/469]  acc1 0.890625 (0.873137)  loss 0.345487 (0.351749)\u001b[0m\n",
            "[2023-11-09 15:50:43] \u001b[32mEpoch [7/10] Step [421/469]  acc1 0.781250 (0.872550)  loss 0.545871 (0.352997)\u001b[0m\n",
            "[2023-11-09 15:50:45] \u001b[32mEpoch [7/10] Step [431/469]  acc1 0.859375 (0.872354)  loss 0.377520 (0.353814)\u001b[0m\n",
            "[2023-11-09 15:50:46] \u001b[32mEpoch [7/10] Step [441/469]  acc1 0.859375 (0.872272)  loss 0.337723 (0.353903)\u001b[0m\n",
            "[2023-11-09 15:50:48] \u001b[32mEpoch [7/10] Step [451/469]  acc1 0.843750 (0.872159)  loss 0.400202 (0.354309)\u001b[0m\n",
            "[2023-11-09 15:50:49] \u001b[32mEpoch [7/10] Step [461/469]  acc1 0.875000 (0.872017)  loss 0.267414 (2.523387)\u001b[0m\n",
            "[2023-11-09 15:50:52] \u001b[32mEpoch [8/10] Step [1/469]  acc1 0.921875 (0.921875)  loss 0.242274 (0.242274)\u001b[0m\n",
            "[2023-11-09 15:50:53] \u001b[32mEpoch [8/10] Step [11/469]  acc1 0.875000 (0.893466)  loss 0.291160 (0.323794)\u001b[0m\n",
            "[2023-11-09 15:50:55] \u001b[32mEpoch [8/10] Step [21/469]  acc1 0.921875 (0.880952)  loss 0.247868 (0.330722)\u001b[0m\n",
            "[2023-11-09 15:50:56] \u001b[32mEpoch [8/10] Step [31/469]  acc1 0.937500 (0.873488)  loss 0.246611 (0.338703)\u001b[0m\n",
            "[2023-11-09 15:50:58] \u001b[32mEpoch [8/10] Step [41/469]  acc1 0.890625 (0.869284)  loss 0.286551 (0.347712)\u001b[0m\n",
            "[2023-11-09 15:50:59] \u001b[32mEpoch [8/10] Step [51/469]  acc1 0.843750 (0.867341)  loss 0.427189 (0.353467)\u001b[0m\n",
            "[2023-11-09 15:51:01] \u001b[32mEpoch [8/10] Step [61/469]  acc1 0.875000 (0.865266)  loss 0.387795 (0.359725)\u001b[0m\n",
            "[2023-11-09 15:51:02] \u001b[32mEpoch [8/10] Step [71/469]  acc1 0.875000 (0.868178)  loss 0.300182 (0.356193)\u001b[0m\n",
            "[2023-11-09 15:51:03] \u001b[32mEpoch [8/10] Step [81/469]  acc1 0.890625 (0.868248)  loss 0.323504 (0.360243)\u001b[0m\n",
            "[2023-11-09 15:51:05] \u001b[32mEpoch [8/10] Step [91/469]  acc1 0.906250 (0.869849)  loss 0.293662 (0.354903)\u001b[0m\n",
            "[2023-11-09 15:51:06] \u001b[32mEpoch [8/10] Step [101/469]  acc1 0.906250 (0.870978)  loss 0.363865 (0.354938)\u001b[0m\n",
            "[2023-11-09 15:51:08] \u001b[32mEpoch [8/10] Step [111/469]  acc1 0.843750 (0.871481)  loss 0.536703 (0.353167)\u001b[0m\n",
            "[2023-11-09 15:51:09] \u001b[32mEpoch [8/10] Step [121/469]  acc1 0.859375 (0.870351)  loss 0.449847 (0.353655)\u001b[0m\n",
            "[2023-11-09 15:51:10] \u001b[32mEpoch [8/10] Step [131/469]  acc1 0.875000 (0.870945)  loss 0.386206 (0.353013)\u001b[0m\n",
            "[2023-11-09 15:51:12] \u001b[32mEpoch [8/10] Step [141/469]  acc1 0.906250 (0.872673)  loss 0.278535 (0.348685)\u001b[0m\n",
            "[2023-11-09 15:51:13] \u001b[32mEpoch [8/10] Step [151/469]  acc1 0.875000 (0.873241)  loss 0.311050 (0.346735)\u001b[0m\n",
            "[2023-11-09 15:51:15] \u001b[32mEpoch [8/10] Step [161/469]  acc1 0.843750 (0.872477)  loss 0.462813 (0.348974)\u001b[0m\n",
            "[2023-11-09 15:51:16] \u001b[32mEpoch [8/10] Step [171/469]  acc1 0.953125 (0.872624)  loss 0.168779 (0.348926)\u001b[0m\n",
            "[2023-11-09 15:51:18] \u001b[32mEpoch [8/10] Step [181/469]  acc1 0.921875 (0.872669)  loss 0.196965 (0.348153)\u001b[0m\n",
            "[2023-11-09 15:51:19] \u001b[32mEpoch [8/10] Step [191/469]  acc1 0.828125 (0.871973)  loss 0.406129 (0.350486)\u001b[0m\n",
            "[2023-11-09 15:51:20] \u001b[32mEpoch [8/10] Step [201/469]  acc1 0.890625 (0.872357)  loss 0.280505 (0.348801)\u001b[0m\n",
            "[2023-11-09 15:51:22] \u001b[32mEpoch [8/10] Step [211/469]  acc1 0.781250 (0.872334)  loss 0.655630 (0.348675)\u001b[0m\n",
            "[2023-11-09 15:51:23] \u001b[32mEpoch [8/10] Step [221/469]  acc1 0.875000 (0.873303)  loss 0.305232 (0.346991)\u001b[0m\n",
            "[2023-11-09 15:51:25] \u001b[32mEpoch [8/10] Step [231/469]  acc1 0.921875 (0.873647)  loss 0.258580 (0.345777)\u001b[0m\n",
            "[2023-11-09 15:51:26] \u001b[32mEpoch [8/10] Step [241/469]  acc1 0.875000 (0.873963)  loss 0.365435 (0.344282)\u001b[0m\n",
            "[2023-11-09 15:51:27] \u001b[32mEpoch [8/10] Step [251/469]  acc1 0.859375 (0.874128)  loss 0.422101 (0.344910)\u001b[0m\n",
            "[2023-11-09 15:51:29] \u001b[32mEpoch [8/10] Step [261/469]  acc1 0.828125 (0.873922)  loss 0.543858 (0.345011)\u001b[0m\n",
            "[2023-11-09 15:51:30] \u001b[32mEpoch [8/10] Step [271/469]  acc1 0.859375 (0.874020)  loss 0.431308 (0.345021)\u001b[0m\n",
            "[2023-11-09 15:51:32] \u001b[32mEpoch [8/10] Step [281/469]  acc1 0.828125 (0.873610)  loss 0.486007 (0.345588)\u001b[0m\n",
            "[2023-11-09 15:51:33] \u001b[32mEpoch [8/10] Step [291/469]  acc1 0.828125 (0.873013)  loss 0.381638 (0.345877)\u001b[0m\n",
            "[2023-11-09 15:51:34] \u001b[32mEpoch [8/10] Step [301/469]  acc1 0.890625 (0.872612)  loss 0.243029 (0.346486)\u001b[0m\n",
            "[2023-11-09 15:51:36] \u001b[32mEpoch [8/10] Step [311/469]  acc1 0.906250 (0.873593)  loss 0.358232 (0.344856)\u001b[0m\n",
            "[2023-11-09 15:51:37] \u001b[32mEpoch [8/10] Step [321/469]  acc1 0.906250 (0.873248)  loss 0.225836 (0.345469)\u001b[0m\n",
            "[2023-11-09 15:51:39] \u001b[32mEpoch [8/10] Step [331/469]  acc1 0.859375 (0.873301)  loss 0.549059 (0.345886)\u001b[0m\n",
            "[2023-11-09 15:51:40] \u001b[32mEpoch [8/10] Step [341/469]  acc1 0.921875 (0.873076)  loss 0.344005 (0.346711)\u001b[0m\n",
            "[2023-11-09 15:51:42] \u001b[32mEpoch [8/10] Step [351/469]  acc1 0.906250 (0.873264)  loss 0.210778 (0.346805)\u001b[0m\n",
            "[2023-11-09 15:51:43] \u001b[32mEpoch [8/10] Step [361/469]  acc1 0.843750 (0.872966)  loss 0.524670 (0.346810)\u001b[0m\n",
            "[2023-11-09 15:51:44] \u001b[32mEpoch [8/10] Step [371/469]  acc1 0.890625 (0.873147)  loss 0.189005 (0.345973)\u001b[0m\n",
            "[2023-11-09 15:51:46] \u001b[32mEpoch [8/10] Step [381/469]  acc1 0.953125 (0.872990)  loss 0.210850 (0.346239)\u001b[0m\n",
            "[2023-11-09 15:51:47] \u001b[32mEpoch [8/10] Step [391/469]  acc1 0.859375 (0.873362)  loss 0.341386 (0.345127)\u001b[0m\n",
            "[2023-11-09 15:51:49] \u001b[32mEpoch [8/10] Step [401/469]  acc1 0.890625 (0.873753)  loss 0.287207 (0.344077)\u001b[0m\n",
            "[2023-11-09 15:51:50] \u001b[32mEpoch [8/10] Step [411/469]  acc1 0.921875 (0.873707)  loss 0.255614 (0.344808)\u001b[0m\n",
            "[2023-11-09 15:51:52] \u001b[32mEpoch [8/10] Step [421/469]  acc1 0.828125 (0.873441)  loss 0.682712 (0.346000)\u001b[0m\n",
            "[2023-11-09 15:51:53] \u001b[32mEpoch [8/10] Step [431/469]  acc1 0.921875 (0.873550)  loss 0.268553 (0.345486)\u001b[0m\n",
            "[2023-11-09 15:51:54] \u001b[32mEpoch [8/10] Step [441/469]  acc1 0.953125 (0.873476)  loss 0.235377 (0.345280)\u001b[0m\n",
            "[2023-11-09 15:51:56] \u001b[32mEpoch [8/10] Step [451/469]  acc1 0.921875 (0.873718)  loss 0.295459 (0.344269)\u001b[0m\n",
            "[2023-11-09 15:51:57] \u001b[32mEpoch [8/10] Step [461/469]  acc1 0.906250 (0.873610)  loss 0.336179 (0.344838)\u001b[0m\n",
            "[2023-11-09 15:52:00] \u001b[32mEpoch [9/10] Step [1/469]  acc1 0.890625 (0.890625)  loss 0.267461 (0.267461)\u001b[0m\n",
            "[2023-11-09 15:52:01] \u001b[32mEpoch [9/10] Step [11/469]  acc1 0.828125 (0.867898)  loss 0.357463 (0.328434)\u001b[0m\n",
            "[2023-11-09 15:52:03] \u001b[32mEpoch [9/10] Step [21/469]  acc1 0.906250 (0.878720)  loss 0.360727 (0.331729)\u001b[0m\n",
            "[2023-11-09 15:52:04] \u001b[32mEpoch [9/10] Step [31/469]  acc1 0.921875 (0.884577)  loss 0.306540 (0.325744)\u001b[0m\n",
            "[2023-11-09 15:52:06] \u001b[32mEpoch [9/10] Step [41/469]  acc1 0.859375 (0.886814)  loss 0.322975 (0.315808)\u001b[0m\n",
            "[2023-11-09 15:52:07] \u001b[32mEpoch [9/10] Step [51/469]  acc1 0.859375 (0.886336)  loss 0.320092 (0.317403)\u001b[0m\n",
            "[2023-11-09 15:52:09] \u001b[32mEpoch [9/10] Step [61/469]  acc1 0.890625 (0.883197)  loss 0.299802 (0.323648)\u001b[0m\n",
            "[2023-11-09 15:52:10] \u001b[32mEpoch [9/10] Step [71/469]  acc1 0.859375 (0.884463)  loss 0.365205 (0.324292)\u001b[0m\n",
            "[2023-11-09 15:52:11] \u001b[32mEpoch [9/10] Step [81/469]  acc1 0.734375 (0.883295)  loss 0.665888 (0.324702)\u001b[0m\n",
            "[2023-11-09 15:52:13] \u001b[32mEpoch [9/10] Step [91/469]  acc1 0.796875 (0.880323)  loss 0.443600 (0.330928)\u001b[0m\n",
            "[2023-11-09 15:52:14] \u001b[32mEpoch [9/10] Step [101/469]  acc1 0.906250 (0.882271)  loss 0.287039 (0.328532)\u001b[0m\n",
            "[2023-11-09 15:52:16] \u001b[32mEpoch [9/10] Step [111/469]  acc1 0.843750 (0.882038)  loss 0.378536 (0.326051)\u001b[0m\n",
            "[2023-11-09 15:52:17] \u001b[32mEpoch [9/10] Step [121/469]  acc1 0.890625 (0.880165)  loss 0.393129 (0.330759)\u001b[0m\n",
            "[2023-11-09 15:52:18] \u001b[32mEpoch [9/10] Step [131/469]  acc1 0.875000 (0.880248)  loss 0.248246 (0.328726)\u001b[0m\n",
            "[2023-11-09 15:52:20] \u001b[32mEpoch [9/10] Step [141/469]  acc1 0.843750 (0.879211)  loss 0.428368 (0.328539)\u001b[0m\n",
            "[2023-11-09 15:52:21] \u001b[32mEpoch [9/10] Step [151/469]  acc1 0.843750 (0.878415)  loss 0.403461 (0.329555)\u001b[0m\n",
            "[2023-11-09 15:52:23] \u001b[32mEpoch [9/10] Step [161/469]  acc1 0.812500 (0.877911)  loss 0.413687 (0.329585)\u001b[0m\n",
            "[2023-11-09 15:52:24] \u001b[32mEpoch [9/10] Step [171/469]  acc1 0.906250 (0.878198)  loss 0.216256 (0.328616)\u001b[0m\n",
            "[2023-11-09 15:52:26] \u001b[32mEpoch [9/10] Step [181/469]  acc1 0.906250 (0.878885)  loss 0.233691 (0.327237)\u001b[0m\n",
            "[2023-11-09 15:52:27] \u001b[32mEpoch [9/10] Step [191/469]  acc1 0.890625 (0.879090)  loss 0.244889 (0.324705)\u001b[0m\n",
            "[2023-11-09 15:52:29] \u001b[32mEpoch [9/10] Step [201/469]  acc1 0.875000 (0.878654)  loss 0.328089 (0.326254)\u001b[0m\n",
            "[2023-11-09 15:52:30] \u001b[32mEpoch [9/10] Step [211/469]  acc1 0.921875 (0.879295)  loss 0.182167 (0.324814)\u001b[0m\n",
            "[2023-11-09 15:52:32] \u001b[32mEpoch [9/10] Step [221/469]  acc1 0.875000 (0.879313)  loss 0.278105 (0.324422)\u001b[0m\n",
            "[2023-11-09 15:52:33] \u001b[32mEpoch [9/10] Step [231/469]  acc1 0.828125 (0.878856)  loss 0.385455 (0.326140)\u001b[0m\n",
            "[2023-11-09 15:52:35] \u001b[32mEpoch [9/10] Step [241/469]  acc1 0.796875 (0.878890)  loss 0.454674 (0.326217)\u001b[0m\n",
            "[2023-11-09 15:52:36] \u001b[32mEpoch [9/10] Step [251/469]  acc1 0.921875 (0.879669)  loss 0.217061 (0.325224)\u001b[0m\n",
            "[2023-11-09 15:52:38] \u001b[32mEpoch [9/10] Step [261/469]  acc1 0.875000 (0.879729)  loss 0.344778 (0.325209)\u001b[0m\n",
            "[2023-11-09 15:52:39] \u001b[32mEpoch [9/10] Step [271/469]  acc1 0.890625 (0.879382)  loss 0.258308 (0.326254)\u001b[0m\n",
            "[2023-11-09 15:52:41] \u001b[32mEpoch [9/10] Step [281/469]  acc1 0.921875 (0.879337)  loss 0.232571 (0.326867)\u001b[0m\n",
            "[2023-11-09 15:52:42] \u001b[32mEpoch [9/10] Step [291/469]  acc1 0.875000 (0.879618)  loss 0.283881 (0.326671)\u001b[0m\n",
            "[2023-11-09 15:52:43] \u001b[32mEpoch [9/10] Step [301/469]  acc1 0.906250 (0.879516)  loss 0.237195 (0.326864)\u001b[0m\n",
            "[2023-11-09 15:52:45] \u001b[32mEpoch [9/10] Step [311/469]  acc1 0.859375 (0.879220)  loss 0.329764 (0.327784)\u001b[0m\n",
            "[2023-11-09 15:52:46] \u001b[32mEpoch [9/10] Step [321/469]  acc1 0.906250 (0.879576)  loss 0.416997 (0.327736)\u001b[0m\n",
            "[2023-11-09 15:52:48] \u001b[32mEpoch [9/10] Step [331/469]  acc1 0.921875 (0.879579)  loss 0.352686 (0.327318)\u001b[0m\n",
            "[2023-11-09 15:52:49] \u001b[32mEpoch [9/10] Step [341/469]  acc1 0.906250 (0.879399)  loss 0.197816 (0.328698)\u001b[0m\n",
            "[2023-11-09 15:52:51] \u001b[32mEpoch [9/10] Step [351/469]  acc1 0.843750 (0.879095)  loss 0.453722 (0.330033)\u001b[0m\n",
            "[2023-11-09 15:52:52] \u001b[32mEpoch [9/10] Step [361/469]  acc1 0.890625 (0.879501)  loss 0.251151 (0.329348)\u001b[0m\n",
            "[2023-11-09 15:52:54] \u001b[32mEpoch [9/10] Step [371/469]  acc1 0.828125 (0.879001)  loss 0.407248 (0.330163)\u001b[0m\n",
            "[2023-11-09 15:52:55] \u001b[32mEpoch [9/10] Step [381/469]  acc1 0.843750 (0.878609)  loss 0.325522 (0.330445)\u001b[0m\n",
            "[2023-11-09 15:52:56] \u001b[32mEpoch [9/10] Step [391/469]  acc1 0.906250 (0.878437)  loss 0.292259 (0.331505)\u001b[0m\n",
            "[2023-11-09 15:52:58] \u001b[32mEpoch [9/10] Step [401/469]  acc1 0.906250 (0.878468)  loss 0.261773 (0.331680)\u001b[0m\n",
            "[2023-11-09 15:52:59] \u001b[32mEpoch [9/10] Step [411/469]  acc1 0.937500 (0.878460)  loss 0.239267 (0.331424)\u001b[0m\n",
            "[2023-11-09 15:53:01] \u001b[32mEpoch [9/10] Step [421/469]  acc1 0.875000 (0.878414)  loss 0.460404 (0.332842)\u001b[0m\n",
            "[2023-11-09 15:53:02] \u001b[32mEpoch [9/10] Step [431/469]  acc1 0.921875 (0.877900)  loss 0.233775 (0.333609)\u001b[0m\n",
            "[2023-11-09 15:53:04] \u001b[32mEpoch [9/10] Step [441/469]  acc1 0.906250 (0.878330)  loss 0.287208 (0.333093)\u001b[0m\n",
            "[2023-11-09 15:53:05] \u001b[32mEpoch [9/10] Step [451/469]  acc1 0.890625 (0.878534)  loss 0.290639 (0.332433)\u001b[0m\n",
            "[2023-11-09 15:53:07] \u001b[32mEpoch [9/10] Step [461/469]  acc1 0.875000 (0.878627)  loss 0.259005 (0.337943)\u001b[0m\n",
            "[2023-11-09 15:53:10] \u001b[32mEpoch [10/10] Step [1/469]  acc1 0.812500 (0.812500)  loss 0.489978 (0.489978)\u001b[0m\n",
            "[2023-11-09 15:53:13] \u001b[32mEpoch [10/10] Step [11/469]  acc1 0.859375 (0.869318)  loss 0.369235 (0.359347)\u001b[0m\n",
            "[2023-11-09 15:53:16] \u001b[32mEpoch [10/10] Step [21/469]  acc1 0.875000 (0.868304)  loss 0.294627 (0.364346)\u001b[0m\n",
            "[2023-11-09 15:53:19] \u001b[32mEpoch [10/10] Step [31/469]  acc1 0.875000 (0.875000)  loss 0.256377 (0.340804)\u001b[0m\n",
            "[2023-11-09 15:53:22] \u001b[32mEpoch [10/10] Step [41/469]  acc1 0.890625 (0.877668)  loss 0.344564 (0.335193)\u001b[0m\n",
            "[2023-11-09 15:53:25] \u001b[32mEpoch [10/10] Step [51/469]  acc1 0.890625 (0.879902)  loss 0.275174 (0.328279)\u001b[0m\n",
            "[2023-11-09 15:53:28] \u001b[32mEpoch [10/10] Step [61/469]  acc1 0.796875 (0.876793)  loss 0.407107 (0.333368)\u001b[0m\n",
            "[2023-11-09 15:53:31] \u001b[32mEpoch [10/10] Step [71/469]  acc1 0.812500 (0.876761)  loss 0.499181 (0.333089)\u001b[0m\n",
            "[2023-11-09 15:53:34] \u001b[32mEpoch [10/10] Step [81/469]  acc1 0.921875 (0.878279)  loss 161.330246 (2.319827)\u001b[0m\n",
            "[2023-11-09 15:53:37] \u001b[32mEpoch [10/10] Step [91/469]  acc1 0.812500 (0.876889)  loss 0.502493 (2.104633)\u001b[0m\n",
            "[2023-11-09 15:53:40] \u001b[32mEpoch [10/10] Step [101/469]  acc1 0.828125 (0.877475)  loss 0.375478 (1.925978)\u001b[0m\n",
            "[2023-11-09 15:53:43] \u001b[32mEpoch [10/10] Step [111/469]  acc1 0.859375 (0.878097)  loss 0.404110 (1.784252)\u001b[0m\n",
            "[2023-11-09 15:53:46] \u001b[32mEpoch [10/10] Step [121/469]  acc1 0.906250 (0.879261)  loss 0.285112 (1.661955)\u001b[0m\n",
            "[2023-11-09 15:53:49] \u001b[32mEpoch [10/10] Step [131/469]  acc1 0.921875 (0.878340)  loss 0.201179 (1.561589)\u001b[0m\n",
            "[2023-11-09 15:53:52] \u001b[32mEpoch [10/10] Step [141/469]  acc1 0.890625 (0.878103)  loss 0.263420 (1.473718)\u001b[0m\n",
            "[2023-11-09 15:53:55] \u001b[32mEpoch [10/10] Step [151/469]  acc1 0.906250 (0.879243)  loss 0.286325 (1.396258)\u001b[0m\n",
            "[2023-11-09 15:53:58] \u001b[32mEpoch [10/10] Step [161/469]  acc1 0.875000 (0.879755)  loss 0.463857 (1.328038)\u001b[0m\n",
            "[2023-11-09 15:54:01] \u001b[32mEpoch [10/10] Step [171/469]  acc1 0.921875 (0.880300)  loss 0.329447 (1.267907)\u001b[0m\n",
            "[2023-11-09 15:54:04] \u001b[32mEpoch [10/10] Step [181/469]  acc1 0.875000 (0.880784)  loss 0.274236 (1.214312)\u001b[0m\n",
            "[2023-11-09 15:54:07] \u001b[32mEpoch [10/10] Step [191/469]  acc1 0.890625 (0.880808)  loss 0.286611 (1.168076)\u001b[0m\n",
            "[2023-11-09 15:54:10] \u001b[32mEpoch [10/10] Step [201/469]  acc1 0.921875 (0.880597)  loss 0.374778 (1.126015)\u001b[0m\n",
            "[2023-11-09 15:54:13] \u001b[32mEpoch [10/10] Step [211/469]  acc1 0.906250 (0.880702)  loss 0.245653 (1.088302)\u001b[0m\n",
            "[2023-11-09 15:54:16] \u001b[32mEpoch [10/10] Step [221/469]  acc1 0.875000 (0.880444)  loss 0.272588 (1.053926)\u001b[0m\n",
            "[2023-11-09 15:54:20] \u001b[32mEpoch [10/10] Step [231/469]  acc1 0.875000 (0.880885)  loss 0.256176 (1.021710)\u001b[0m\n",
            "[2023-11-09 15:54:23] \u001b[32mEpoch [10/10] Step [241/469]  acc1 0.875000 (0.881354)  loss 0.386041 (0.992472)\u001b[0m\n",
            "[2023-11-09 15:54:26] \u001b[32mEpoch [10/10] Step [251/469]  acc1 0.890625 (0.881350)  loss 0.351571 (0.965933)\u001b[0m\n",
            "[2023-11-09 15:54:29] \u001b[32mEpoch [10/10] Step [261/469]  acc1 0.890625 (0.881406)  loss 0.246651 (0.942622)\u001b[0m\n",
            "[2023-11-09 15:54:32] \u001b[32mEpoch [10/10] Step [271/469]  acc1 0.875000 (0.881400)  loss 0.359684 (0.920179)\u001b[0m\n",
            "[2023-11-09 15:54:35] \u001b[32mEpoch [10/10] Step [281/469]  acc1 0.765625 (0.880950)  loss 0.570311 (0.899700)\u001b[0m\n",
            "[2023-11-09 15:54:38] \u001b[32mEpoch [10/10] Step [291/469]  acc1 0.890625 (0.881175)  loss 0.298801 (0.879253)\u001b[0m\n",
            "[2023-11-09 15:54:41] \u001b[32mEpoch [10/10] Step [301/469]  acc1 0.843750 (0.881333)  loss 0.427595 (0.860045)\u001b[0m\n",
            "[2023-11-09 15:54:44] \u001b[32mEpoch [10/10] Step [311/469]  acc1 0.875000 (0.880376)  loss 0.287636 (0.844708)\u001b[0m\n",
            "[2023-11-09 15:54:47] \u001b[32mEpoch [10/10] Step [321/469]  acc1 0.828125 (0.880111)  loss 0.471960 (0.829759)\u001b[0m\n",
            "[2023-11-09 15:54:50] \u001b[32mEpoch [10/10] Step [331/469]  acc1 0.890625 (0.880476)  loss 0.220422 (0.813071)\u001b[0m\n",
            "[2023-11-09 15:54:53] \u001b[32mEpoch [10/10] Step [341/469]  acc1 0.937500 (0.880682)  loss 0.250452 (0.798314)\u001b[0m\n",
            "[2023-11-09 15:54:56] \u001b[32mEpoch [10/10] Step [351/469]  acc1 0.843750 (0.880965)  loss 0.419715 (0.784217)\u001b[0m\n",
            "[2023-11-09 15:54:59] \u001b[32mEpoch [10/10] Step [361/469]  acc1 0.859375 (0.880930)  loss 0.414355 (0.770915)\u001b[0m\n",
            "[2023-11-09 15:55:02] \u001b[32mEpoch [10/10] Step [371/469]  acc1 0.968750 (0.881570)  loss 0.170159 (0.758009)\u001b[0m\n",
            "[2023-11-09 15:55:05] \u001b[32mEpoch [10/10] Step [381/469]  acc1 0.875000 (0.881152)  loss 0.361881 (0.747755)\u001b[0m\n",
            "[2023-11-09 15:55:08] \u001b[32mEpoch [10/10] Step [391/469]  acc1 0.843750 (0.881514)  loss 0.399974 (0.736743)\u001b[0m\n",
            "[2023-11-09 15:55:11] \u001b[32mEpoch [10/10] Step [401/469]  acc1 0.875000 (0.881156)  loss 0.306692 (0.727258)\u001b[0m\n",
            "[2023-11-09 15:55:14] \u001b[32mEpoch [10/10] Step [411/469]  acc1 0.828125 (0.881387)  loss 0.423282 (0.717895)\u001b[0m\n",
            "[2023-11-09 15:55:17] \u001b[32mEpoch [10/10] Step [421/469]  acc1 0.843750 (0.881606)  loss 0.283213 (0.708612)\u001b[0m\n",
            "[2023-11-09 15:55:20] \u001b[32mEpoch [10/10] Step [431/469]  acc1 0.843750 (0.881997)  loss 0.412086 (0.699011)\u001b[0m\n",
            "[2023-11-09 15:55:23] \u001b[32mEpoch [10/10] Step [441/469]  acc1 0.906250 (0.882157)  loss 0.309059 (0.690193)\u001b[0m\n",
            "[2023-11-09 15:55:26] \u001b[32mEpoch [10/10] Step [451/469]  acc1 0.890625 (0.882379)  loss 0.221435 (0.681472)\u001b[0m\n",
            "[2023-11-09 15:55:29] \u001b[32mEpoch [10/10] Step [461/469]  acc1 0.875000 (0.882118)  loss 0.329388 (0.674237)\u001b[0m\n",
            "Final architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'avgpool', 'reduce_n3_p0': 'sepconv3x3', 'reduce_n3_p1': 'sepconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'dilconv3x3', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'maxpool', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv5x5', 'reduce_n5_p1': 'dilconv3x3', 'reduce_n5_p2': 'dilconv3x3', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\n",
            "weight = 100.0, lambd = 4\n",
            "[2023-11-09 15:55:34] \u001b[32mEpoch [1/10] Step [1/469]  acc1 0.109375 (0.109375)  loss 15768.869141 (15768.869141)\u001b[0m\n",
            "[2023-11-09 15:55:37] \u001b[32mEpoch [1/10] Step [11/469]  acc1 0.078125 (0.110795)  loss 807.761108 (5999.417641)\u001b[0m\n",
            "[2023-11-09 15:55:40] \u001b[32mEpoch [1/10] Step [21/469]  acc1 0.140625 (0.113095)  loss 24.163918 (3410.669188)\u001b[0m\n",
            "[2023-11-09 15:55:43] \u001b[32mEpoch [1/10] Step [31/469]  acc1 0.109375 (0.114919)  loss 2.269648 (2375.751379)\u001b[0m\n",
            "[2023-11-09 15:55:46] \u001b[32mEpoch [1/10] Step [41/469]  acc1 0.265625 (0.136433)  loss 2.567859 (1847.120588)\u001b[0m\n",
            "[2023-11-09 15:55:49] \u001b[32mEpoch [1/10] Step [51/469]  acc1 0.312500 (0.170956)  loss 244.201950 (1531.810071)\u001b[0m\n",
            "[2023-11-09 15:55:52] \u001b[32mEpoch [1/10] Step [61/469]  acc1 0.359375 (0.198770)  loss 550.853760 (1298.663598)\u001b[0m\n",
            "[2023-11-09 15:55:55] \u001b[32mEpoch [1/10] Step [71/469]  acc1 0.406250 (0.224252)  loss 1.425619 (1123.402014)\u001b[0m\n",
            "[2023-11-09 15:55:58] \u001b[32mEpoch [1/10] Step [81/469]  acc1 0.546875 (0.251929)  loss 1.338671 (985.198676)\u001b[0m\n",
            "[2023-11-09 15:56:01] \u001b[32mEpoch [1/10] Step [91/469]  acc1 0.421875 (0.277644)  loss 18.214159 (886.338822)\u001b[0m\n",
            "[2023-11-09 15:56:04] \u001b[32mEpoch [1/10] Step [101/469]  acc1 0.500000 (0.303063)  loss 1.207340 (821.076908)\u001b[0m\n",
            "[2023-11-09 15:56:07] \u001b[32mEpoch [1/10] Step [111/469]  acc1 0.546875 (0.325591)  loss 1.450054 (756.279292)\u001b[0m\n",
            "[2023-11-09 15:56:10] \u001b[32mEpoch [1/10] Step [121/469]  acc1 0.640625 (0.349044)  loss 1.478087 (702.139947)\u001b[0m\n",
            "[2023-11-09 15:56:13] \u001b[32mEpoch [1/10] Step [131/469]  acc1 0.500000 (0.365577)  loss 1.072670 (649.772694)\u001b[0m\n",
            "[2023-11-09 15:56:16] \u001b[32mEpoch [1/10] Step [141/469]  acc1 0.625000 (0.377992)  loss 0.955677 (606.556767)\u001b[0m\n",
            "[2023-11-09 15:56:19] \u001b[32mEpoch [1/10] Step [151/469]  acc1 0.734375 (0.392074)  loss 0.923139 (568.570129)\u001b[0m\n",
            "[2023-11-09 15:56:22] \u001b[32mEpoch [1/10] Step [161/469]  acc1 0.515625 (0.404115)  loss 100.947990 (538.573783)\u001b[0m\n",
            "[2023-11-09 15:56:25] \u001b[32mEpoch [1/10] Step [171/469]  acc1 0.609375 (0.415479)  loss 0.961959 (507.138371)\u001b[0m\n",
            "[2023-11-09 15:56:28] \u001b[32mEpoch [1/10] Step [181/469]  acc1 0.718750 (0.430335)  loss 0.856659 (480.334422)\u001b[0m\n",
            "[2023-11-09 15:56:31] \u001b[32mEpoch [1/10] Step [191/469]  acc1 0.718750 (0.442245)  loss 0.973975 (455.236701)\u001b[0m\n",
            "[2023-11-09 15:56:34] \u001b[32mEpoch [1/10] Step [201/469]  acc1 0.640625 (0.453203)  loss 0.987608 (433.425647)\u001b[0m\n",
            "[2023-11-09 15:56:38] \u001b[32mEpoch [1/10] Step [211/469]  acc1 0.750000 (0.464455)  loss 0.834977 (412.922351)\u001b[0m\n",
            "[2023-11-09 15:56:41] \u001b[32mEpoch [1/10] Step [221/469]  acc1 0.703125 (0.475608)  loss 0.873559 (394.278209)\u001b[0m\n",
            "[2023-11-09 15:56:44] \u001b[32mEpoch [1/10] Step [231/469]  acc1 0.765625 (0.484240)  loss 0.699954 (377.253040)\u001b[0m\n",
            "[2023-11-09 15:56:47] \u001b[32mEpoch [1/10] Step [241/469]  acc1 0.765625 (0.493192)  loss 0.764693 (361.633132)\u001b[0m\n",
            "[2023-11-09 15:56:50] \u001b[32mEpoch [1/10] Step [251/469]  acc1 0.718750 (0.502303)  loss 0.748365 (347.850420)\u001b[0m\n",
            "[2023-11-09 15:56:53] \u001b[32mEpoch [1/10] Step [261/469]  acc1 0.593750 (0.509459)  loss 0.950612 (334.555166)\u001b[0m\n",
            "[2023-11-09 15:56:56] \u001b[32mEpoch [1/10] Step [271/469]  acc1 0.656250 (0.516490)  loss 1.075954 (323.622400)\u001b[0m\n",
            "[2023-11-09 15:56:59] \u001b[32mEpoch [1/10] Step [281/469]  acc1 0.625000 (0.521908)  loss 1.048236 (312.184539)\u001b[0m\n",
            "[2023-11-09 15:57:02] \u001b[32mEpoch [1/10] Step [291/469]  acc1 0.859375 (0.529263)  loss 0.515693 (302.694097)\u001b[0m\n",
            "[2023-11-09 15:57:05] \u001b[32mEpoch [1/10] Step [301/469]  acc1 0.718750 (0.536130)  loss 0.795546 (292.775146)\u001b[0m\n",
            "[2023-11-09 15:57:08] \u001b[32mEpoch [1/10] Step [311/469]  acc1 0.750000 (0.542203)  loss 0.588698 (283.602872)\u001b[0m\n",
            "[2023-11-09 15:57:11] \u001b[32mEpoch [1/10] Step [321/469]  acc1 0.734375 (0.547751)  loss 0.698740 (275.379614)\u001b[0m\n",
            "[2023-11-09 15:57:14] \u001b[32mEpoch [1/10] Step [331/469]  acc1 0.734375 (0.553295)  loss 921.878723 (269.864406)\u001b[0m\n",
            "[2023-11-09 15:57:17] \u001b[32mEpoch [1/10] Step [341/469]  acc1 0.703125 (0.557505)  loss 0.812220 (261.973533)\u001b[0m\n",
            "[2023-11-09 15:57:20] \u001b[32mEpoch [1/10] Step [351/469]  acc1 0.703125 (0.562188)  loss 0.729409 (254.563953)\u001b[0m\n",
            "[2023-11-09 15:57:23] \u001b[32mEpoch [1/10] Step [361/469]  acc1 0.671875 (0.566482)  loss 0.952975 (247.532655)\u001b[0m\n",
            "[2023-11-09 15:57:26] \u001b[32mEpoch [1/10] Step [371/469]  acc1 0.796875 (0.571471)  loss 0.651750 (240.878575)\u001b[0m\n",
            "[2023-11-09 15:57:29] \u001b[32mEpoch [1/10] Step [381/469]  acc1 0.687500 (0.575992)  loss 0.728041 (234.574122)\u001b[0m\n",
            "[2023-11-09 15:57:32] \u001b[32mEpoch [1/10] Step [391/469]  acc1 0.828125 (0.581122)  loss 0.510867 (228.593024)\u001b[0m\n",
            "[2023-11-09 15:57:35] \u001b[32mEpoch [1/10] Step [401/469]  acc1 0.781250 (0.585450)  loss 0.537613 (222.975785)\u001b[0m\n",
            "[2023-11-09 15:57:38] \u001b[32mEpoch [1/10] Step [411/469]  acc1 0.781250 (0.589910)  loss 0.619768 (217.565732)\u001b[0m\n",
            "[2023-11-09 15:57:41] \u001b[32mEpoch [1/10] Step [421/469]  acc1 0.750000 (0.593230)  loss 0.617989 (212.417709)\u001b[0m\n",
            "[2023-11-09 15:57:44] \u001b[32mEpoch [1/10] Step [431/469]  acc1 0.796875 (0.597085)  loss 0.478221 (207.503530)\u001b[0m\n",
            "[2023-11-09 15:57:47] \u001b[32mEpoch [1/10] Step [441/469]  acc1 0.718750 (0.599844)  loss 0.714014 (204.666819)\u001b[0m\n",
            "[2023-11-09 15:57:50] \u001b[32mEpoch [1/10] Step [451/469]  acc1 0.718750 (0.603347)  loss 0.744795 (200.143264)\u001b[0m\n",
            "[2023-11-09 15:57:53] \u001b[32mEpoch [1/10] Step [461/469]  acc1 0.812500 (0.606901)  loss 0.495268 (196.262716)\u001b[0m\n",
            "[2023-11-09 15:57:58] \u001b[32mEpoch [2/10] Step [1/469]  acc1 0.750000 (0.750000)  loss 0.667268 (0.667268)\u001b[0m\n",
            "[2023-11-09 15:58:01] \u001b[32mEpoch [2/10] Step [11/469]  acc1 0.812500 (0.750000)  loss 0.671574 (87.240734)\u001b[0m\n",
            "[2023-11-09 15:58:04] \u001b[32mEpoch [2/10] Step [21/469]  acc1 0.812500 (0.773065)  loss 0.495252 (56.945113)\u001b[0m\n",
            "[2023-11-09 15:58:07] \u001b[32mEpoch [2/10] Step [31/469]  acc1 0.812500 (0.772681)  loss 0.623404 (38.781521)\u001b[0m\n",
            "[2023-11-09 15:58:10] \u001b[32mEpoch [2/10] Step [41/469]  acc1 0.703125 (0.775534)  loss 988.642456 (57.894342)\u001b[0m\n",
            "[2023-11-09 15:58:13] \u001b[32mEpoch [2/10] Step [51/469]  acc1 0.781250 (0.777880)  loss 0.636067 (46.654830)\u001b[0m\n",
            "[2023-11-09 15:58:16] \u001b[32mEpoch [2/10] Step [61/469]  acc1 0.781250 (0.778689)  loss 0.582864 (39.108472)\u001b[0m\n",
            "[2023-11-09 15:58:19] \u001b[32mEpoch [2/10] Step [71/469]  acc1 0.781250 (0.776849)  loss 0.753002 (33.709146)\u001b[0m\n",
            "[2023-11-09 15:58:22] \u001b[32mEpoch [2/10] Step [81/469]  acc1 0.656250 (0.774691)  loss 0.748741 (29.626827)\u001b[0m\n",
            "[2023-11-09 15:58:25] \u001b[32mEpoch [2/10] Step [91/469]  acc1 0.796875 (0.772493)  loss 0.635254 (26.440940)\u001b[0m\n",
            "[2023-11-09 15:58:28] \u001b[32mEpoch [2/10] Step [101/469]  acc1 0.750000 (0.771040)  loss 0.709905 (24.088445)\u001b[0m\n",
            "[2023-11-09 15:58:31] \u001b[32mEpoch [2/10] Step [111/469]  acc1 0.781250 (0.772804)  loss 0.608957 (21.970611)\u001b[0m\n",
            "[2023-11-09 15:58:34] \u001b[32mEpoch [2/10] Step [121/469]  acc1 0.750000 (0.769370)  loss 0.721948 (20.215585)\u001b[0m\n",
            "[2023-11-09 15:58:37] \u001b[32mEpoch [2/10] Step [131/469]  acc1 0.828125 (0.769442)  loss 0.453429 (18.718016)\u001b[0m\n",
            "[2023-11-09 15:58:40] \u001b[32mEpoch [2/10] Step [141/469]  acc1 0.875000 (0.770833)  loss 0.321257 (17.428984)\u001b[0m\n",
            "[2023-11-09 15:58:43] \u001b[32mEpoch [2/10] Step [151/469]  acc1 0.796875 (0.772661)  loss 0.487125 (16.311106)\u001b[0m\n",
            "[2023-11-09 15:58:46] \u001b[32mEpoch [2/10] Step [161/469]  acc1 0.765625 (0.771836)  loss 0.589379 (15.338146)\u001b[0m\n",
            "[2023-11-09 15:58:49] \u001b[32mEpoch [2/10] Step [171/469]  acc1 0.781250 (0.772478)  loss 0.643743 (14.475265)\u001b[0m\n",
            "[2023-11-09 15:58:52] \u001b[32mEpoch [2/10] Step [181/469]  acc1 0.859375 (0.773394)  loss 0.594546 (13.708778)\u001b[0m\n",
            "[2023-11-09 15:58:55] \u001b[32mEpoch [2/10] Step [191/469]  acc1 0.671875 (0.774133)  loss 0.721360 (13.078268)\u001b[0m\n",
            "[2023-11-09 15:58:58] \u001b[32mEpoch [2/10] Step [201/469]  acc1 0.906250 (0.775498)  loss 0.369367 (12.453063)\u001b[0m\n",
            "[2023-11-09 15:59:01] \u001b[32mEpoch [2/10] Step [211/469]  acc1 0.875000 (0.775770)  loss 0.382983 (11.889943)\u001b[0m\n",
            "[2023-11-09 15:59:04] \u001b[32mEpoch [2/10] Step [221/469]  acc1 0.781250 (0.776230)  loss 0.585042 (15.027431)\u001b[0m\n",
            "[2023-11-09 15:59:07] \u001b[32mEpoch [2/10] Step [231/469]  acc1 0.843750 (0.777327)  loss 0.459466 (15.597118)\u001b[0m\n",
            "[2023-11-09 15:59:10] \u001b[32mEpoch [2/10] Step [241/469]  acc1 0.750000 (0.778203)  loss 0.631448 (14.972945)\u001b[0m\n",
            "[2023-11-09 15:59:13] \u001b[32mEpoch [2/10] Step [251/469]  acc1 0.781250 (0.778822)  loss 0.553707 (14.399050)\u001b[0m\n",
            "[2023-11-09 15:59:16] \u001b[32mEpoch [2/10] Step [261/469]  acc1 0.765625 (0.779813)  loss 0.542088 (13.866155)\u001b[0m\n",
            "[2023-11-09 15:59:19] \u001b[32mEpoch [2/10] Step [271/469]  acc1 0.703125 (0.781077)  loss 0.676129 (13.372795)\u001b[0m\n",
            "[2023-11-09 15:59:22] \u001b[32mEpoch [2/10] Step [281/469]  acc1 0.781250 (0.781306)  loss 0.600400 (12.918885)\u001b[0m\n",
            "[2023-11-09 15:59:25] \u001b[32mEpoch [2/10] Step [291/469]  acc1 0.843750 (0.781572)  loss 0.558477 (12.495412)\u001b[0m\n",
            "[2023-11-09 15:59:28] \u001b[32mEpoch [2/10] Step [301/469]  acc1 0.796875 (0.782548)  loss 0.668582 (12.097847)\u001b[0m\n",
            "[2023-11-09 15:59:31] \u001b[32mEpoch [2/10] Step [311/469]  acc1 0.796875 (0.783410)  loss 0.543851 (11.725870)\u001b[0m\n",
            "[2023-11-09 15:59:34] \u001b[32mEpoch [2/10] Step [321/469]  acc1 0.812500 (0.784122)  loss 2.306807 (11.382880)\u001b[0m\n",
            "[2023-11-09 15:59:37] \u001b[32mEpoch [2/10] Step [331/469]  acc1 0.750000 (0.783894)  loss 0.593702 (12.109334)\u001b[0m\n",
            "[2023-11-09 15:59:40] \u001b[32mEpoch [2/10] Step [341/469]  acc1 0.765625 (0.784916)  loss 0.618183 (11.768163)\u001b[0m\n",
            "[2023-11-09 15:59:43] \u001b[32mEpoch [2/10] Step [351/469]  acc1 0.796875 (0.785657)  loss 0.504185 (11.446673)\u001b[0m\n",
            "[2023-11-09 15:59:46] \u001b[32mEpoch [2/10] Step [361/469]  acc1 0.828125 (0.785795)  loss 0.395408 (11.144447)\u001b[0m\n",
            "[2023-11-09 15:59:49] \u001b[32mEpoch [2/10] Step [371/469]  acc1 0.890625 (0.786472)  loss 0.301488 (10.857855)\u001b[0m\n",
            "[2023-11-09 15:59:53] \u001b[32mEpoch [2/10] Step [381/469]  acc1 0.781250 (0.786868)  loss 0.609155 (13.102504)\u001b[0m\n",
            "[2023-11-09 15:59:56] \u001b[32mEpoch [2/10] Step [391/469]  acc1 0.843750 (0.787324)  loss 0.501462 (12.781359)\u001b[0m\n",
            "[2023-11-09 15:59:59] \u001b[32mEpoch [2/10] Step [401/469]  acc1 0.796875 (0.787329)  loss 0.568519 (12.477396)\u001b[0m\n",
            "[2023-11-09 16:00:02] \u001b[32mEpoch [2/10] Step [411/469]  acc1 0.859375 (0.788093)  loss 0.513906 (13.736122)\u001b[0m\n",
            "[2023-11-09 16:00:05] \u001b[32mEpoch [2/10] Step [421/469]  acc1 0.812500 (0.788339)  loss 0.510978 (13.422991)\u001b[0m\n",
            "[2023-11-09 16:00:08] \u001b[32mEpoch [2/10] Step [431/469]  acc1 0.781250 (0.787703)  loss 0.522874 (13.125480)\u001b[0m\n",
            "[2023-11-09 16:00:11] \u001b[32mEpoch [2/10] Step [441/469]  acc1 0.828125 (0.788478)  loss 0.538306 (12.839273)\u001b[0m\n",
            "[2023-11-09 16:00:14] \u001b[32mEpoch [2/10] Step [451/469]  acc1 0.843750 (0.789703)  loss 0.435099 (12.564940)\u001b[0m\n",
            "[2023-11-09 16:00:17] \u001b[32mEpoch [2/10] Step [461/469]  acc1 0.734375 (0.790401)  loss 0.735472 (12.303017)\u001b[0m\n",
            "[2023-11-09 16:00:21] \u001b[32mEpoch [3/10] Step [1/469]  acc1 0.718750 (0.718750)  loss 0.611597 (0.611597)\u001b[0m\n",
            "[2023-11-09 16:00:25] \u001b[32mEpoch [3/10] Step [11/469]  acc1 0.812500 (0.818182)  loss 0.532553 (0.497161)\u001b[0m\n",
            "[2023-11-09 16:00:27] \u001b[32mEpoch [3/10] Step [21/469]  acc1 0.828125 (0.821429)  loss 0.570119 (0.485108)\u001b[0m\n",
            "[2023-11-09 16:00:28] \u001b[32mEpoch [3/10] Step [31/469]  acc1 0.828125 (0.822581)  loss 0.471074 (0.489850)\u001b[0m\n",
            "[2023-11-09 16:00:30] \u001b[32mEpoch [3/10] Step [41/469]  acc1 0.781250 (0.822409)  loss 0.521321 (0.485880)\u001b[0m\n",
            "[2023-11-09 16:00:31] \u001b[32mEpoch [3/10] Step [51/469]  acc1 0.781250 (0.818015)  loss 0.443859 (0.495118)\u001b[0m\n",
            "[2023-11-09 16:00:33] \u001b[32mEpoch [3/10] Step [61/469]  acc1 0.843750 (0.815061)  loss 0.415043 (0.498192)\u001b[0m\n",
            "[2023-11-09 16:00:34] \u001b[32mEpoch [3/10] Step [71/469]  acc1 0.875000 (0.814701)  loss 0.439316 (0.497106)\u001b[0m\n",
            "[2023-11-09 16:00:36] \u001b[32mEpoch [3/10] Step [81/469]  acc1 0.750000 (0.813079)  loss 0.604519 (0.496348)\u001b[0m\n",
            "[2023-11-09 16:00:37] \u001b[32mEpoch [3/10] Step [91/469]  acc1 0.890625 (0.815247)  loss 0.421687 (0.493564)\u001b[0m\n",
            "[2023-11-09 16:00:38] \u001b[32mEpoch [3/10] Step [101/469]  acc1 0.859375 (0.815439)  loss 0.298120 (0.493746)\u001b[0m\n",
            "[2023-11-09 16:00:40] \u001b[32mEpoch [3/10] Step [111/469]  acc1 0.781250 (0.815597)  loss 0.539965 (0.493077)\u001b[0m\n",
            "[2023-11-09 16:00:41] \u001b[32mEpoch [3/10] Step [121/469]  acc1 0.859375 (0.817149)  loss 0.465104 (0.491213)\u001b[0m\n",
            "[2023-11-09 16:00:43] \u001b[32mEpoch [3/10] Step [131/469]  acc1 0.875000 (0.818106)  loss 0.360264 (0.490444)\u001b[0m\n",
            "[2023-11-09 16:00:44] \u001b[32mEpoch [3/10] Step [141/469]  acc1 0.828125 (0.817598)  loss 0.515728 (0.490094)\u001b[0m\n",
            "[2023-11-09 16:00:45] \u001b[32mEpoch [3/10] Step [151/469]  acc1 0.875000 (0.817777)  loss 0.444695 (0.490577)\u001b[0m\n",
            "[2023-11-09 16:00:47] \u001b[32mEpoch [3/10] Step [161/469]  acc1 0.828125 (0.820070)  loss 0.452502 (0.485993)\u001b[0m\n",
            "[2023-11-09 16:00:48] \u001b[32mEpoch [3/10] Step [171/469]  acc1 0.734375 (0.819810)  loss 0.649273 (0.487183)\u001b[0m\n",
            "[2023-11-09 16:00:50] \u001b[32mEpoch [3/10] Step [181/469]  acc1 0.937500 (0.821996)  loss 0.290835 (0.482063)\u001b[0m\n",
            "[2023-11-09 16:00:51] \u001b[32mEpoch [3/10] Step [191/469]  acc1 0.890625 (0.822235)  loss 0.323137 (0.481450)\u001b[0m\n",
            "[2023-11-09 16:00:53] \u001b[32mEpoch [3/10] Step [201/469]  acc1 0.843750 (0.822373)  loss 0.444011 (0.481169)\u001b[0m\n",
            "[2023-11-09 16:00:54] \u001b[32mEpoch [3/10] Step [211/469]  acc1 0.781250 (0.821757)  loss 0.519682 (0.480837)\u001b[0m\n",
            "[2023-11-09 16:00:55] \u001b[32mEpoch [3/10] Step [221/469]  acc1 0.875000 (0.821408)  loss 0.367218 (0.480613)\u001b[0m\n",
            "[2023-11-09 16:00:57] \u001b[32mEpoch [3/10] Step [231/469]  acc1 0.906250 (0.822714)  loss 0.375640 (0.477596)\u001b[0m\n",
            "[2023-11-09 16:00:58] \u001b[32mEpoch [3/10] Step [241/469]  acc1 0.937500 (0.822809)  loss 0.226487 (0.476849)\u001b[0m\n",
            "[2023-11-09 16:01:00] \u001b[32mEpoch [3/10] Step [251/469]  acc1 0.875000 (0.823020)  loss 0.352365 (0.714358)\u001b[0m\n",
            "[2023-11-09 16:01:01] \u001b[32mEpoch [3/10] Step [261/469]  acc1 0.859375 (0.823096)  loss 0.371611 (0.705708)\u001b[0m\n",
            "[2023-11-09 16:01:03] \u001b[32mEpoch [3/10] Step [271/469]  acc1 0.671875 (0.822417)  loss 0.645875 (0.697395)\u001b[0m\n",
            "[2023-11-09 16:01:04] \u001b[32mEpoch [3/10] Step [281/469]  acc1 0.859375 (0.823287)  loss 0.426533 (0.688874)\u001b[0m\n",
            "[2023-11-09 16:01:05] \u001b[32mEpoch [3/10] Step [291/469]  acc1 0.859375 (0.824152)  loss 0.485899 (0.679188)\u001b[0m\n",
            "[2023-11-09 16:01:07] \u001b[32mEpoch [3/10] Step [301/469]  acc1 0.812500 (0.824543)  loss 0.523975 (3.985492)\u001b[0m\n",
            "[2023-11-09 16:01:08] \u001b[32mEpoch [3/10] Step [311/469]  acc1 0.843750 (0.825010)  loss 0.411351 (3.872091)\u001b[0m\n",
            "[2023-11-09 16:01:10] \u001b[32mEpoch [3/10] Step [321/469]  acc1 0.812500 (0.824426)  loss 0.558573 (4.155431)\u001b[0m\n",
            "[2023-11-09 16:01:11] \u001b[32mEpoch [3/10] Step [331/469]  acc1 0.890625 (0.825293)  loss 0.365367 (4.041808)\u001b[0m\n",
            "[2023-11-09 16:01:13] \u001b[32mEpoch [3/10] Step [341/469]  acc1 0.843750 (0.825376)  loss 0.332048 (4.026677)\u001b[0m\n",
            "[2023-11-09 16:01:14] \u001b[32mEpoch [3/10] Step [351/469]  acc1 0.859375 (0.825855)  loss 0.524122 (3.925465)\u001b[0m\n",
            "[2023-11-09 16:01:15] \u001b[32mEpoch [3/10] Step [361/469]  acc1 0.859375 (0.826004)  loss 0.366129 (3.829014)\u001b[0m\n",
            "[2023-11-09 16:01:17] \u001b[32mEpoch [3/10] Step [371/469]  acc1 0.843750 (0.826525)  loss 0.448995 (3.737782)\u001b[0m\n",
            "[2023-11-09 16:01:18] \u001b[32mEpoch [3/10] Step [381/469]  acc1 0.875000 (0.826198)  loss 0.330363 (3.652127)\u001b[0m\n",
            "[2023-11-09 16:01:20] \u001b[32mEpoch [3/10] Step [391/469]  acc1 0.765625 (0.826527)  loss 0.588530 (3.569306)\u001b[0m\n",
            "[2023-11-09 16:01:21] \u001b[32mEpoch [3/10] Step [401/469]  acc1 0.875000 (0.826644)  loss 0.381727 (3.492038)\u001b[0m\n",
            "[2023-11-09 16:01:22] \u001b[32mEpoch [3/10] Step [411/469]  acc1 0.796875 (0.826642)  loss 0.458976 (3.705778)\u001b[0m\n",
            "[2023-11-09 16:01:24] \u001b[32mEpoch [3/10] Step [421/469]  acc1 0.875000 (0.826715)  loss 0.424248 (3.628698)\u001b[0m\n",
            "[2023-11-09 16:01:25] \u001b[32mEpoch [3/10] Step [431/469]  acc1 0.812500 (0.827110)  loss 0.471693 (3.555070)\u001b[0m\n",
            "[2023-11-09 16:01:27] \u001b[32mEpoch [3/10] Step [441/469]  acc1 0.828125 (0.826885)  loss 0.523986 (3.486057)\u001b[0m\n",
            "[2023-11-09 16:01:28] \u001b[32mEpoch [3/10] Step [451/469]  acc1 0.890625 (0.827605)  loss 0.401243 (3.417661)\u001b[0m\n",
            "[2023-11-09 16:01:30] \u001b[32mEpoch [3/10] Step [461/469]  acc1 0.828125 (0.827820)  loss 0.451553 (5.539141)\u001b[0m\n",
            "[2023-11-09 16:01:33] \u001b[32mEpoch [4/10] Step [1/469]  acc1 0.812500 (0.812500)  loss 0.402646 (0.402646)\u001b[0m\n",
            "[2023-11-09 16:01:34] \u001b[32mEpoch [4/10] Step [11/469]  acc1 0.859375 (0.838068)  loss 0.451884 (0.419501)\u001b[0m\n",
            "[2023-11-09 16:01:35] \u001b[32mEpoch [4/10] Step [21/469]  acc1 0.875000 (0.837798)  loss 0.336622 (0.431399)\u001b[0m\n",
            "[2023-11-09 16:01:37] \u001b[32mEpoch [4/10] Step [31/469]  acc1 0.859375 (0.843750)  loss 0.355351 (25.965555)\u001b[0m\n",
            "[2023-11-09 16:01:38] \u001b[32mEpoch [4/10] Step [41/469]  acc1 0.859375 (0.838796)  loss 0.404912 (19.741722)\u001b[0m\n",
            "[2023-11-09 16:01:40] \u001b[32mEpoch [4/10] Step [51/469]  acc1 0.890625 (0.837316)  loss 0.364626 (15.959186)\u001b[0m\n",
            "[2023-11-09 16:01:41] \u001b[32mEpoch [4/10] Step [61/469]  acc1 0.843750 (0.838115)  loss 0.346412 (13.414478)\u001b[0m\n",
            "[2023-11-09 16:01:42] \u001b[32mEpoch [4/10] Step [71/469]  acc1 0.859375 (0.841769)  loss 0.346511 (11.580821)\u001b[0m\n",
            "[2023-11-09 16:01:44] \u001b[32mEpoch [4/10] Step [81/469]  acc1 0.796875 (0.843943)  loss 0.491989 (10.226798)\u001b[0m\n",
            "[2023-11-09 16:01:45] \u001b[32mEpoch [4/10] Step [91/469]  acc1 0.843750 (0.845124)  loss 0.418777 (9.147537)\u001b[0m\n",
            "[2023-11-09 16:01:47] \u001b[32mEpoch [4/10] Step [101/469]  acc1 0.875000 (0.846689)  loss 0.487562 (8.284611)\u001b[0m\n",
            "[2023-11-09 16:01:48] \u001b[32mEpoch [4/10] Step [111/469]  acc1 0.875000 (0.845158)  loss 0.387266 (7.579568)\u001b[0m\n",
            "[2023-11-09 16:01:50] \u001b[32mEpoch [4/10] Step [121/469]  acc1 0.937500 (0.845687)  loss 0.271260 (6.989797)\u001b[0m\n",
            "[2023-11-09 16:01:51] \u001b[32mEpoch [4/10] Step [131/469]  acc1 0.937500 (0.846732)  loss 0.258787 (7.044401)\u001b[0m\n",
            "[2023-11-09 16:01:52] \u001b[32mEpoch [4/10] Step [141/469]  acc1 0.875000 (0.845412)  loss 0.278833 (6.579672)\u001b[0m\n",
            "[2023-11-09 16:01:54] \u001b[32mEpoch [4/10] Step [151/469]  acc1 0.828125 (0.844681)  loss 0.415989 (6.173408)\u001b[0m\n",
            "[2023-11-09 16:01:55] \u001b[32mEpoch [4/10] Step [161/469]  acc1 0.828125 (0.845206)  loss 0.375482 (5.814643)\u001b[0m\n",
            "[2023-11-09 16:01:57] \u001b[32mEpoch [4/10] Step [171/469]  acc1 0.906250 (0.845212)  loss 0.291588 (5.499957)\u001b[0m\n",
            "[2023-11-09 16:01:58] \u001b[32mEpoch [4/10] Step [181/469]  acc1 0.859375 (0.844786)  loss 0.405151 (5.219252)\u001b[0m\n",
            "[2023-11-09 16:02:00] \u001b[32mEpoch [4/10] Step [191/469]  acc1 0.828125 (0.843750)  loss 0.396438 (4.968450)\u001b[0m\n",
            "[2023-11-09 16:02:01] \u001b[32mEpoch [4/10] Step [201/469]  acc1 0.906250 (0.844372)  loss 0.278032 (4.741152)\u001b[0m\n",
            "[2023-11-09 16:02:03] \u001b[32mEpoch [4/10] Step [211/469]  acc1 0.843750 (0.844491)  loss 0.381740 (4.534710)\u001b[0m\n",
            "[2023-11-09 16:02:04] \u001b[32mEpoch [4/10] Step [221/469]  acc1 0.859375 (0.845093)  loss 0.379115 (4.347645)\u001b[0m\n",
            "[2023-11-09 16:02:05] \u001b[32mEpoch [4/10] Step [231/469]  acc1 0.937500 (0.845644)  loss 0.253789 (4.175674)\u001b[0m\n",
            "[2023-11-09 16:02:07] \u001b[32mEpoch [4/10] Step [241/469]  acc1 0.890625 (0.846019)  loss 0.392578 (4.018222)\u001b[0m\n",
            "[2023-11-09 16:02:08] \u001b[32mEpoch [4/10] Step [251/469]  acc1 0.890625 (0.845742)  loss 0.429500 (3.876811)\u001b[0m\n",
            "[2023-11-09 16:02:10] \u001b[32mEpoch [4/10] Step [261/469]  acc1 0.937500 (0.845007)  loss 0.264179 (3.745186)\u001b[0m\n",
            "[2023-11-09 16:02:11] \u001b[32mEpoch [4/10] Step [271/469]  acc1 0.859375 (0.845999)  loss 0.289115 (3.620235)\u001b[0m\n",
            "[2023-11-09 16:02:13] \u001b[32mEpoch [4/10] Step [281/469]  acc1 0.796875 (0.846308)  loss 0.447959 (3.506463)\u001b[0m\n",
            "[2023-11-09 16:02:14] \u001b[32mEpoch [4/10] Step [291/469]  acc1 0.859375 (0.846703)  loss 0.374606 (3.399682)\u001b[0m\n",
            "[2023-11-09 16:02:15] \u001b[32mEpoch [4/10] Step [301/469]  acc1 0.843750 (0.846865)  loss 0.377103 (3.300673)\u001b[0m\n",
            "[2023-11-09 16:02:17] \u001b[32mEpoch [4/10] Step [311/469]  acc1 0.843750 (0.846463)  loss 0.548694 (3.209285)\u001b[0m\n",
            "[2023-11-09 16:02:18] \u001b[32mEpoch [4/10] Step [321/469]  acc1 0.890625 (0.847060)  loss 0.351866 (3.120250)\u001b[0m\n",
            "[2023-11-09 16:02:20] \u001b[32mEpoch [4/10] Step [331/469]  acc1 0.875000 (0.846205)  loss 0.485968 (3.040474)\u001b[0m\n",
            "[2023-11-09 16:02:21] \u001b[32mEpoch [4/10] Step [341/469]  acc1 0.875000 (0.845583)  loss 0.346198 (2.964236)\u001b[0m\n",
            "[2023-11-09 16:02:23] \u001b[32mEpoch [4/10] Step [351/469]  acc1 0.796875 (0.845709)  loss 0.604355 (2.892906)\u001b[0m\n",
            "[2023-11-09 16:02:24] \u001b[32mEpoch [4/10] Step [361/469]  acc1 0.828125 (0.845828)  loss 0.498487 (2.824544)\u001b[0m\n",
            "[2023-11-09 16:02:25] \u001b[32mEpoch [4/10] Step [371/469]  acc1 0.859375 (0.845856)  loss 0.346173 (2.759284)\u001b[0m\n",
            "[2023-11-09 16:02:27] \u001b[32mEpoch [4/10] Step [381/469]  acc1 0.796875 (0.845801)  loss 0.514675 (2.697363)\u001b[0m\n",
            "[2023-11-09 16:02:28] \u001b[32mEpoch [4/10] Step [391/469]  acc1 0.812500 (0.845908)  loss 0.473142 (2.638308)\u001b[0m\n",
            "[2023-11-09 16:02:30] \u001b[32mEpoch [4/10] Step [401/469]  acc1 0.875000 (0.845737)  loss 0.347457 (2.583296)\u001b[0m\n",
            "[2023-11-09 16:02:31] \u001b[32mEpoch [4/10] Step [411/469]  acc1 0.843750 (0.846031)  loss 0.412056 (2.529605)\u001b[0m\n",
            "[2023-11-09 16:02:32] \u001b[32mEpoch [4/10] Step [421/469]  acc1 0.953125 (0.846385)  loss 0.205181 (2.479773)\u001b[0m\n",
            "[2023-11-09 16:02:34] \u001b[32mEpoch [4/10] Step [431/469]  acc1 0.843750 (0.846324)  loss 0.460605 (2.431902)\u001b[0m\n",
            "[2023-11-09 16:02:35] \u001b[32mEpoch [4/10] Step [441/469]  acc1 0.890625 (0.846195)  loss 0.363349 (2.386485)\u001b[0m\n",
            "[2023-11-09 16:02:37] \u001b[32mEpoch [4/10] Step [451/469]  acc1 0.937500 (0.846418)  loss 0.244628 (2.341735)\u001b[0m\n",
            "[2023-11-09 16:02:38] \u001b[32mEpoch [4/10] Step [461/469]  acc1 0.828125 (0.846529)  loss 0.410025 (2.300005)\u001b[0m\n",
            "[2023-11-09 16:02:41] \u001b[32mEpoch [5/10] Step [1/469]  acc1 0.921875 (0.921875)  loss 0.278697 (0.278697)\u001b[0m\n",
            "[2023-11-09 16:02:42] \u001b[32mEpoch [5/10] Step [11/469]  acc1 0.875000 (0.857955)  loss 0.381088 (0.408223)\u001b[0m\n",
            "[2023-11-09 16:02:44] \u001b[32mEpoch [5/10] Step [21/469]  acc1 0.796875 (0.839286)  loss 0.377698 (0.420502)\u001b[0m\n",
            "[2023-11-09 16:02:45] \u001b[32mEpoch [5/10] Step [31/469]  acc1 0.843750 (0.852823)  loss 0.478936 (0.403114)\u001b[0m\n",
            "[2023-11-09 16:02:47] \u001b[32mEpoch [5/10] Step [41/469]  acc1 0.859375 (0.860137)  loss 0.392460 (0.394134)\u001b[0m\n",
            "[2023-11-09 16:02:48] \u001b[32mEpoch [5/10] Step [51/469]  acc1 0.859375 (0.854779)  loss 0.485476 (0.399433)\u001b[0m\n",
            "[2023-11-09 16:02:50] \u001b[32mEpoch [5/10] Step [61/469]  acc1 0.828125 (0.851434)  loss 0.403336 (0.407170)\u001b[0m\n",
            "[2023-11-09 16:02:51] \u001b[32mEpoch [5/10] Step [71/469]  acc1 0.890625 (0.854313)  loss 0.245929 (0.396930)\u001b[0m\n",
            "[2023-11-09 16:02:52] \u001b[32mEpoch [5/10] Step [81/469]  acc1 0.859375 (0.854167)  loss 0.434055 (0.395056)\u001b[0m\n",
            "[2023-11-09 16:02:54] \u001b[32mEpoch [5/10] Step [91/469]  acc1 0.890625 (0.856456)  loss 0.336537 (0.394180)\u001b[0m\n",
            "[2023-11-09 16:02:55] \u001b[32mEpoch [5/10] Step [101/469]  acc1 0.781250 (0.854889)  loss 0.528586 (0.393478)\u001b[0m\n",
            "[2023-11-09 16:02:57] \u001b[32mEpoch [5/10] Step [111/469]  acc1 0.859375 (0.853463)  loss 0.359953 (0.393728)\u001b[0m\n",
            "[2023-11-09 16:02:58] \u001b[32mEpoch [5/10] Step [121/469]  acc1 0.906250 (0.854468)  loss 0.374525 (0.393262)\u001b[0m\n",
            "[2023-11-09 16:02:59] \u001b[32mEpoch [5/10] Step [131/469]  acc1 0.843750 (0.852576)  loss 0.379846 (0.395867)\u001b[0m\n",
            "[2023-11-09 16:03:01] \u001b[32mEpoch [5/10] Step [141/469]  acc1 0.859375 (0.852504)  loss 0.338235 (0.395523)\u001b[0m\n",
            "[2023-11-09 16:03:02] \u001b[32mEpoch [5/10] Step [151/469]  acc1 0.843750 (0.853270)  loss 0.408682 (0.392403)\u001b[0m\n",
            "[2023-11-09 16:03:04] \u001b[32mEpoch [5/10] Step [161/469]  acc1 0.890625 (0.853455)  loss 0.357418 (0.393109)\u001b[0m\n",
            "[2023-11-09 16:03:05] \u001b[32mEpoch [5/10] Step [171/469]  acc1 0.843750 (0.854989)  loss 0.424825 (0.390148)\u001b[0m\n",
            "[2023-11-09 16:03:07] \u001b[32mEpoch [5/10] Step [181/469]  acc1 0.843750 (0.854886)  loss 0.357282 (0.390278)\u001b[0m\n",
            "[2023-11-09 16:03:08] \u001b[32mEpoch [5/10] Step [191/469]  acc1 0.875000 (0.854712)  loss 0.392874 (0.391460)\u001b[0m\n",
            "[2023-11-09 16:03:09] \u001b[32mEpoch [5/10] Step [201/469]  acc1 0.859375 (0.854633)  loss 0.394791 (0.392875)\u001b[0m\n",
            "[2023-11-09 16:03:11] \u001b[32mEpoch [5/10] Step [211/469]  acc1 0.859375 (0.856339)  loss 0.415035 (0.390280)\u001b[0m\n",
            "[2023-11-09 16:03:12] \u001b[32mEpoch [5/10] Step [221/469]  acc1 0.906250 (0.855699)  loss 0.272185 (0.392729)\u001b[0m\n",
            "[2023-11-09 16:03:14] \u001b[32mEpoch [5/10] Step [231/469]  acc1 0.921875 (0.855384)  loss 0.261159 (0.393684)\u001b[0m\n",
            "[2023-11-09 16:03:15] \u001b[32mEpoch [5/10] Step [241/469]  acc1 0.859375 (0.854577)  loss 0.337785 (0.395586)\u001b[0m\n",
            "[2023-11-09 16:03:17] \u001b[32mEpoch [5/10] Step [251/469]  acc1 0.796875 (0.853523)  loss 0.404388 (0.396303)\u001b[0m\n",
            "[2023-11-09 16:03:18] \u001b[32mEpoch [5/10] Step [261/469]  acc1 0.859375 (0.853448)  loss 0.446078 (0.396110)\u001b[0m\n",
            "[2023-11-09 16:03:19] \u001b[32mEpoch [5/10] Step [271/469]  acc1 0.859375 (0.853321)  loss 0.333378 (0.395137)\u001b[0m\n",
            "[2023-11-09 16:03:21] \u001b[32mEpoch [5/10] Step [281/469]  acc1 0.843750 (0.852814)  loss 0.411482 (0.394966)\u001b[0m\n",
            "[2023-11-09 16:03:22] \u001b[32mEpoch [5/10] Step [291/469]  acc1 0.859375 (0.853200)  loss 0.277233 (0.394333)\u001b[0m\n",
            "[2023-11-09 16:03:24] \u001b[32mEpoch [5/10] Step [301/469]  acc1 0.796875 (0.853821)  loss 0.490665 (0.393162)\u001b[0m\n",
            "[2023-11-09 16:03:25] \u001b[32mEpoch [5/10] Step [311/469]  acc1 0.921875 (0.854200)  loss 0.280082 (0.392267)\u001b[0m\n",
            "[2023-11-09 16:03:26] \u001b[32mEpoch [5/10] Step [321/469]  acc1 0.796875 (0.854653)  loss 0.562507 (0.391915)\u001b[0m\n",
            "[2023-11-09 16:03:28] \u001b[32mEpoch [5/10] Step [331/469]  acc1 0.828125 (0.855032)  loss 0.534828 (0.392305)\u001b[0m\n",
            "[2023-11-09 16:03:29] \u001b[32mEpoch [5/10] Step [341/469]  acc1 0.906250 (0.854885)  loss 0.323340 (0.392053)\u001b[0m\n",
            "[2023-11-09 16:03:31] \u001b[32mEpoch [5/10] Step [351/469]  acc1 0.796875 (0.855012)  loss 0.443816 (0.391166)\u001b[0m\n",
            "[2023-11-09 16:03:32] \u001b[32mEpoch [5/10] Step [361/469]  acc1 0.906250 (0.855393)  loss 0.309913 (0.390848)\u001b[0m\n",
            "[2023-11-09 16:03:34] \u001b[32mEpoch [5/10] Step [371/469]  acc1 0.828125 (0.855542)  loss 0.512785 (0.390416)\u001b[0m\n",
            "[2023-11-09 16:03:35] \u001b[32mEpoch [5/10] Step [381/469]  acc1 0.859375 (0.855684)  loss 0.356691 (0.389919)\u001b[0m\n",
            "[2023-11-09 16:03:36] \u001b[32mEpoch [5/10] Step [391/469]  acc1 0.921875 (0.855499)  loss 0.240132 (0.390095)\u001b[0m\n",
            "[2023-11-09 16:03:38] \u001b[32mEpoch [5/10] Step [401/469]  acc1 0.843750 (0.855595)  loss 0.426199 (0.389338)\u001b[0m\n",
            "[2023-11-09 16:03:39] \u001b[32mEpoch [5/10] Step [411/469]  acc1 0.890625 (0.855003)  loss 0.338515 (0.390013)\u001b[0m\n",
            "[2023-11-09 16:03:41] \u001b[32mEpoch [5/10] Step [421/469]  acc1 0.781250 (0.854290)  loss 0.433467 (0.391073)\u001b[0m\n",
            "[2023-11-09 16:03:42] \u001b[32mEpoch [5/10] Step [431/469]  acc1 0.796875 (0.854227)  loss 0.490916 (0.390945)\u001b[0m\n",
            "[2023-11-09 16:03:44] \u001b[32mEpoch [5/10] Step [441/469]  acc1 0.828125 (0.854202)  loss 0.388037 (0.391096)\u001b[0m\n",
            "[2023-11-09 16:03:45] \u001b[32mEpoch [5/10] Step [451/469]  acc1 0.906250 (0.854594)  loss 0.263445 (0.389987)\u001b[0m\n",
            "[2023-11-09 16:03:47] \u001b[32mEpoch [5/10] Step [461/469]  acc1 0.890625 (0.854935)  loss 0.278382 (0.389789)\u001b[0m\n",
            "[2023-11-09 16:03:49] \u001b[32mEpoch [6/10] Step [1/469]  acc1 0.812500 (0.812500)  loss 0.438683 (0.438683)\u001b[0m\n",
            "[2023-11-09 16:03:51] \u001b[32mEpoch [6/10] Step [11/469]  acc1 0.875000 (0.872159)  loss 0.383293 (0.349024)\u001b[0m\n",
            "[2023-11-09 16:03:52] \u001b[32mEpoch [6/10] Step [21/469]  acc1 0.859375 (0.881696)  loss 0.335014 (0.333173)\u001b[0m\n",
            "[2023-11-09 16:03:54] \u001b[32mEpoch [6/10] Step [31/469]  acc1 0.890625 (0.869456)  loss 0.373856 (0.368697)\u001b[0m\n",
            "[2023-11-09 16:03:55] \u001b[32mEpoch [6/10] Step [41/469]  acc1 0.843750 (0.862805)  loss 0.419866 (0.382324)\u001b[0m\n",
            "[2023-11-09 16:03:56] \u001b[32mEpoch [6/10] Step [51/469]  acc1 0.937500 (0.865196)  loss 0.236362 (0.380976)\u001b[0m\n",
            "[2023-11-09 16:03:58] \u001b[32mEpoch [6/10] Step [61/469]  acc1 0.843750 (0.866035)  loss 0.343772 (0.379171)\u001b[0m\n",
            "[2023-11-09 16:03:59] \u001b[32mEpoch [6/10] Step [71/469]  acc1 0.906250 (0.864437)  loss 0.289296 (0.382281)\u001b[0m\n",
            "[2023-11-09 16:04:01] \u001b[32mEpoch [6/10] Step [81/469]  acc1 0.828125 (0.864776)  loss 0.535392 (0.382099)\u001b[0m\n",
            "[2023-11-09 16:04:02] \u001b[32mEpoch [6/10] Step [91/469]  acc1 0.703125 (0.862809)  loss 0.610158 (0.385348)\u001b[0m\n",
            "[2023-11-09 16:04:04] \u001b[32mEpoch [6/10] Step [101/469]  acc1 0.750000 (0.861077)  loss 0.631535 (0.390359)\u001b[0m\n",
            "[2023-11-09 16:04:05] \u001b[32mEpoch [6/10] Step [111/469]  acc1 0.812500 (0.858671)  loss 0.438989 (0.392626)\u001b[0m\n",
            "[2023-11-09 16:04:06] \u001b[32mEpoch [6/10] Step [121/469]  acc1 0.765625 (0.858988)  loss 0.556819 (1.388002)\u001b[0m\n",
            "[2023-11-09 16:04:08] \u001b[32mEpoch [6/10] Step [131/469]  acc1 0.875000 (0.858302)  loss 0.232719 (1.311773)\u001b[0m\n",
            "[2023-11-09 16:04:09] \u001b[32mEpoch [6/10] Step [141/469]  acc1 0.921875 (0.859375)  loss 0.239212 (1.244338)\u001b[0m\n",
            "[2023-11-09 16:04:11] \u001b[32mEpoch [6/10] Step [151/469]  acc1 0.875000 (0.858547)  loss 0.321302 (1.190071)\u001b[0m\n",
            "[2023-11-09 16:04:12] \u001b[32mEpoch [6/10] Step [161/469]  acc1 0.906250 (0.860054)  loss 2.011130 (1.148247)\u001b[0m\n",
            "[2023-11-09 16:04:14] \u001b[32mEpoch [6/10] Step [171/469]  acc1 0.796875 (0.858827)  loss 0.462235 (1.105598)\u001b[0m\n",
            "[2023-11-09 16:04:15] \u001b[32mEpoch [6/10] Step [181/469]  acc1 0.937500 (0.859202)  loss 0.292167 (1.064589)\u001b[0m\n",
            "[2023-11-09 16:04:16] \u001b[32mEpoch [6/10] Step [191/469]  acc1 0.875000 (0.860520)  loss 0.286577 (1.027317)\u001b[0m\n",
            "[2023-11-09 16:04:18] \u001b[32mEpoch [6/10] Step [201/469]  acc1 0.812500 (0.860075)  loss 0.372529 (0.995667)\u001b[0m\n",
            "[2023-11-09 16:04:19] \u001b[32mEpoch [6/10] Step [211/469]  acc1 0.828125 (0.860264)  loss 0.399605 (0.965847)\u001b[0m\n",
            "[2023-11-09 16:04:21] \u001b[32mEpoch [6/10] Step [221/469]  acc1 0.843750 (0.861284)  loss 0.479433 (0.937305)\u001b[0m\n",
            "[2023-11-09 16:04:22] \u001b[32mEpoch [6/10] Step [231/469]  acc1 0.828125 (0.862216)  loss 0.427398 (0.911174)\u001b[0m\n",
            "[2023-11-09 16:04:24] \u001b[32mEpoch [6/10] Step [241/469]  acc1 0.859375 (0.862163)  loss 0.328721 (0.889529)\u001b[0m\n",
            "[2023-11-09 16:04:25] \u001b[32mEpoch [6/10] Step [251/469]  acc1 0.875000 (0.861990)  loss 0.356857 (0.869589)\u001b[0m\n",
            "[2023-11-09 16:04:26] \u001b[32mEpoch [6/10] Step [261/469]  acc1 0.812500 (0.862548)  loss 0.435503 (0.849119)\u001b[0m\n",
            "[2023-11-09 16:04:28] \u001b[32mEpoch [6/10] Step [271/469]  acc1 0.906250 (0.862892)  loss 0.282359 (0.831165)\u001b[0m\n",
            "[2023-11-09 16:04:29] \u001b[32mEpoch [6/10] Step [281/469]  acc1 0.875000 (0.863267)  loss 0.319969 (0.814088)\u001b[0m\n",
            "[2023-11-09 16:04:31] \u001b[32mEpoch [6/10] Step [291/469]  acc1 0.843750 (0.863241)  loss 0.490825 (0.799881)\u001b[0m\n",
            "[2023-11-09 16:04:32] \u001b[32mEpoch [6/10] Step [301/469]  acc1 0.859375 (0.863943)  loss 0.391892 (0.784365)\u001b[0m\n",
            "[2023-11-09 16:04:34] \u001b[32mEpoch [6/10] Step [311/469]  acc1 0.828125 (0.864248)  loss 0.415343 (0.771137)\u001b[0m\n",
            "[2023-11-09 16:04:35] \u001b[32mEpoch [6/10] Step [321/469]  acc1 0.859375 (0.864243)  loss 0.401951 (0.759127)\u001b[0m\n",
            "[2023-11-09 16:04:37] \u001b[32mEpoch [6/10] Step [331/469]  acc1 0.875000 (0.864520)  loss 0.317328 (0.746894)\u001b[0m\n",
            "[2023-11-09 16:04:38] \u001b[32mEpoch [6/10] Step [341/469]  acc1 0.906250 (0.864553)  loss 0.394395 (0.736106)\u001b[0m\n",
            "[2023-11-09 16:04:40] \u001b[32mEpoch [6/10] Step [351/469]  acc1 0.875000 (0.864939)  loss 0.411912 (0.725111)\u001b[0m\n",
            "[2023-11-09 16:04:41] \u001b[32mEpoch [6/10] Step [361/469]  acc1 0.859375 (0.864872)  loss 0.488846 (0.716002)\u001b[0m\n",
            "[2023-11-09 16:04:43] \u001b[32mEpoch [6/10] Step [371/469]  acc1 0.921875 (0.865061)  loss 0.272650 (0.706330)\u001b[0m\n",
            "[2023-11-09 16:04:44] \u001b[32mEpoch [6/10] Step [381/469]  acc1 0.828125 (0.865075)  loss 0.380830 (0.697488)\u001b[0m\n",
            "[2023-11-09 16:04:46] \u001b[32mEpoch [6/10] Step [391/469]  acc1 0.828125 (0.865090)  loss 0.406925 (0.688636)\u001b[0m\n",
            "[2023-11-09 16:04:47] \u001b[32mEpoch [6/10] Step [401/469]  acc1 0.875000 (0.864635)  loss 0.340458 (0.681163)\u001b[0m\n",
            "[2023-11-09 16:04:49] \u001b[32mEpoch [6/10] Step [411/469]  acc1 0.875000 (0.865002)  loss 0.336135 (0.673291)\u001b[0m\n",
            "[2023-11-09 16:04:50] \u001b[32mEpoch [6/10] Step [421/469]  acc1 0.890625 (0.864831)  loss 0.344498 (0.666464)\u001b[0m\n",
            "[2023-11-09 16:04:51] \u001b[32mEpoch [6/10] Step [431/469]  acc1 0.890625 (0.865067)  loss 0.290843 (0.659159)\u001b[0m\n",
            "[2023-11-09 16:04:53] \u001b[32mEpoch [6/10] Step [441/469]  acc1 0.859375 (0.865150)  loss 0.497836 (0.652498)\u001b[0m\n",
            "[2023-11-09 16:04:54] \u001b[32mEpoch [6/10] Step [451/469]  acc1 0.906250 (0.865022)  loss 0.293599 (0.646097)\u001b[0m\n",
            "[2023-11-09 16:04:56] \u001b[32mEpoch [6/10] Step [461/469]  acc1 0.937500 (0.865069)  loss 0.164485 (0.639589)\u001b[0m\n",
            "[2023-11-09 16:04:59] \u001b[32mEpoch [7/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 0.410365 (0.410365)\u001b[0m\n",
            "[2023-11-09 16:05:00] \u001b[32mEpoch [7/10] Step [11/469]  acc1 0.859375 (0.869318)  loss 0.312385 (0.353470)\u001b[0m\n",
            "[2023-11-09 16:05:01] \u001b[32mEpoch [7/10] Step [21/469]  acc1 0.796875 (0.855655)  loss 0.786171 (0.392158)\u001b[0m\n",
            "[2023-11-09 16:05:03] \u001b[32mEpoch [7/10] Step [31/469]  acc1 0.875000 (0.859375)  loss 0.312835 (0.381998)\u001b[0m\n",
            "[2023-11-09 16:05:04] \u001b[32mEpoch [7/10] Step [41/469]  acc1 0.890625 (0.858994)  loss 0.376958 (0.384060)\u001b[0m\n",
            "[2023-11-09 16:05:06] \u001b[32mEpoch [7/10] Step [51/469]  acc1 0.921875 (0.859069)  loss 0.215685 (0.379097)\u001b[0m\n",
            "[2023-11-09 16:05:07] \u001b[32mEpoch [7/10] Step [61/469]  acc1 0.859375 (0.857326)  loss 0.275244 (0.380666)\u001b[0m\n",
            "[2023-11-09 16:05:09] \u001b[32mEpoch [7/10] Step [71/469]  acc1 0.875000 (0.860695)  loss 0.305468 (0.372892)\u001b[0m\n",
            "[2023-11-09 16:05:10] \u001b[32mEpoch [7/10] Step [81/469]  acc1 0.890625 (0.860340)  loss 0.343207 (0.372710)\u001b[0m\n",
            "[2023-11-09 16:05:11] \u001b[32mEpoch [7/10] Step [91/469]  acc1 0.875000 (0.864354)  loss 0.270387 (0.364964)\u001b[0m\n",
            "[2023-11-09 16:05:13] \u001b[32mEpoch [7/10] Step [101/469]  acc1 0.921875 (0.864944)  loss 0.287919 (0.364669)\u001b[0m\n",
            "[2023-11-09 16:05:14] \u001b[32mEpoch [7/10] Step [111/469]  acc1 0.828125 (0.865709)  loss 0.486240 (0.364935)\u001b[0m\n",
            "[2023-11-09 16:05:16] \u001b[32mEpoch [7/10] Step [121/469]  acc1 0.859375 (0.865057)  loss 0.471738 (0.365445)\u001b[0m\n",
            "[2023-11-09 16:05:17] \u001b[32mEpoch [7/10] Step [131/469]  acc1 0.906250 (0.865219)  loss 0.218490 (0.364754)\u001b[0m\n",
            "[2023-11-09 16:05:18] \u001b[32mEpoch [7/10] Step [141/469]  acc1 0.875000 (0.866356)  loss 0.299098 (0.363276)\u001b[0m\n",
            "[2023-11-09 16:05:20] \u001b[32mEpoch [7/10] Step [151/469]  acc1 0.890625 (0.865273)  loss 0.336240 (0.364223)\u001b[0m\n",
            "[2023-11-09 16:05:21] \u001b[32mEpoch [7/10] Step [161/469]  acc1 0.828125 (0.864227)  loss 0.434284 (0.365979)\u001b[0m\n",
            "[2023-11-09 16:05:23] \u001b[32mEpoch [7/10] Step [171/469]  acc1 0.859375 (0.864035)  loss 0.394324 (0.367299)\u001b[0m\n",
            "[2023-11-09 16:05:24] \u001b[32mEpoch [7/10] Step [181/469]  acc1 0.859375 (0.864296)  loss 0.329916 (0.366546)\u001b[0m\n",
            "[2023-11-09 16:05:26] \u001b[32mEpoch [7/10] Step [191/469]  acc1 0.875000 (0.865101)  loss 0.317181 (0.363907)\u001b[0m\n",
            "[2023-11-09 16:05:27] \u001b[32mEpoch [7/10] Step [201/469]  acc1 0.765625 (0.864739)  loss 0.485772 (0.365025)\u001b[0m\n",
            "[2023-11-09 16:05:28] \u001b[32mEpoch [7/10] Step [211/469]  acc1 0.890625 (0.865151)  loss 0.295406 (0.365420)\u001b[0m\n",
            "[2023-11-09 16:05:30] \u001b[32mEpoch [7/10] Step [221/469]  acc1 0.875000 (0.865102)  loss 0.307715 (0.365367)\u001b[0m\n",
            "[2023-11-09 16:05:31] \u001b[32mEpoch [7/10] Step [231/469]  acc1 0.921875 (0.865936)  loss 0.188871 (0.362829)\u001b[0m\n",
            "[2023-11-09 16:05:33] \u001b[32mEpoch [7/10] Step [241/469]  acc1 0.937500 (0.866053)  loss 0.221683 (0.362599)\u001b[0m\n",
            "[2023-11-09 16:05:34] \u001b[32mEpoch [7/10] Step [251/469]  acc1 0.796875 (0.866098)  loss 0.522221 (0.363421)\u001b[0m\n",
            "[2023-11-09 16:05:36] \u001b[32mEpoch [7/10] Step [261/469]  acc1 0.906250 (0.866798)  loss 0.284104 (0.362210)\u001b[0m\n",
            "[2023-11-09 16:05:37] \u001b[32mEpoch [7/10] Step [271/469]  acc1 0.828125 (0.866294)  loss 0.415739 (0.363850)\u001b[0m\n",
            "[2023-11-09 16:05:39] \u001b[32mEpoch [7/10] Step [281/469]  acc1 0.937500 (0.867215)  loss 0.220229 (0.361672)\u001b[0m\n",
            "[2023-11-09 16:05:40] \u001b[32mEpoch [7/10] Step [291/469]  acc1 0.796875 (0.867429)  loss 0.554939 (0.362242)\u001b[0m\n",
            "[2023-11-09 16:05:42] \u001b[32mEpoch [7/10] Step [301/469]  acc1 0.906250 (0.867213)  loss 0.258580 (0.362731)\u001b[0m\n",
            "[2023-11-09 16:05:43] \u001b[32mEpoch [7/10] Step [311/469]  acc1 0.890625 (0.868016)  loss 0.294511 (0.361146)\u001b[0m\n",
            "[2023-11-09 16:05:45] \u001b[32mEpoch [7/10] Step [321/469]  acc1 0.859375 (0.868088)  loss 0.400500 (0.360677)\u001b[0m\n",
            "[2023-11-09 16:05:46] \u001b[32mEpoch [7/10] Step [331/469]  acc1 0.906250 (0.868816)  loss 0.302255 (0.359297)\u001b[0m\n",
            "[2023-11-09 16:05:47] \u001b[32mEpoch [7/10] Step [341/469]  acc1 0.875000 (0.868539)  loss 0.416372 (0.359981)\u001b[0m\n",
            "[2023-11-09 16:05:49] \u001b[32mEpoch [7/10] Step [351/469]  acc1 0.859375 (0.869347)  loss 0.319406 (0.359877)\u001b[0m\n",
            "[2023-11-09 16:05:50] \u001b[32mEpoch [7/10] Step [361/469]  acc1 0.875000 (0.868940)  loss 0.471282 (0.361407)\u001b[0m\n",
            "[2023-11-09 16:05:52] \u001b[32mEpoch [7/10] Step [371/469]  acc1 0.765625 (0.868219)  loss 0.521227 (0.362731)\u001b[0m\n",
            "[2023-11-09 16:05:53] \u001b[32mEpoch [7/10] Step [381/469]  acc1 0.843750 (0.868479)  loss 0.466135 (0.361845)\u001b[0m\n",
            "[2023-11-09 16:05:55] \u001b[32mEpoch [7/10] Step [391/469]  acc1 0.906250 (0.868246)  loss 0.343824 (0.361972)\u001b[0m\n",
            "[2023-11-09 16:05:56] \u001b[32mEpoch [7/10] Step [401/469]  acc1 0.921875 (0.868415)  loss 0.235160 (0.361376)\u001b[0m\n",
            "[2023-11-09 16:05:58] \u001b[32mEpoch [7/10] Step [411/469]  acc1 0.812500 (0.868157)  loss 0.474167 (0.361953)\u001b[0m\n",
            "[2023-11-09 16:05:59] \u001b[32mEpoch [7/10] Step [421/469]  acc1 0.843750 (0.868394)  loss 0.405570 (0.361781)\u001b[0m\n",
            "[2023-11-09 16:06:00] \u001b[32mEpoch [7/10] Step [431/469]  acc1 0.921875 (0.868257)  loss 0.238932 (0.361654)\u001b[0m\n",
            "[2023-11-09 16:06:02] \u001b[32mEpoch [7/10] Step [441/469]  acc1 0.953125 (0.868233)  loss 0.247043 (0.361140)\u001b[0m\n",
            "[2023-11-09 16:06:03] \u001b[32mEpoch [7/10] Step [451/469]  acc1 0.953125 (0.868487)  loss 0.191613 (0.361041)\u001b[0m\n",
            "[2023-11-09 16:06:05] \u001b[32mEpoch [7/10] Step [461/469]  acc1 0.890625 (0.868560)  loss 0.322529 (0.360529)\u001b[0m\n",
            "[2023-11-09 16:06:08] \u001b[32mEpoch [8/10] Step [1/469]  acc1 0.921875 (0.921875)  loss 0.274152 (0.274152)\u001b[0m\n",
            "[2023-11-09 16:06:09] \u001b[32mEpoch [8/10] Step [11/469]  acc1 0.890625 (0.887784)  loss 0.310808 (0.289197)\u001b[0m\n",
            "[2023-11-09 16:06:11] \u001b[32mEpoch [8/10] Step [21/469]  acc1 0.968750 (0.880208)  loss 0.135674 (0.311758)\u001b[0m\n",
            "[2023-11-09 16:06:12] \u001b[32mEpoch [8/10] Step [31/469]  acc1 0.843750 (0.871472)  loss 0.394405 (0.345357)\u001b[0m\n",
            "[2023-11-09 16:06:13] \u001b[32mEpoch [8/10] Step [41/469]  acc1 0.937500 (0.878430)  loss 0.240330 (0.334125)\u001b[0m\n",
            "[2023-11-09 16:06:15] \u001b[32mEpoch [8/10] Step [51/469]  acc1 0.875000 (0.876532)  loss 0.322996 (0.337163)\u001b[0m\n",
            "[2023-11-09 16:06:16] \u001b[32mEpoch [8/10] Step [61/469]  acc1 0.890625 (0.876281)  loss 0.308174 (0.336666)\u001b[0m\n",
            "[2023-11-09 16:06:18] \u001b[32mEpoch [8/10] Step [71/469]  acc1 0.859375 (0.876540)  loss 0.502002 (0.336575)\u001b[0m\n",
            "[2023-11-09 16:06:19] \u001b[32mEpoch [8/10] Step [81/469]  acc1 0.890625 (0.876736)  loss 0.344148 (0.337933)\u001b[0m\n",
            "[2023-11-09 16:06:20] \u001b[32mEpoch [8/10] Step [91/469]  acc1 0.828125 (0.875343)  loss 0.364420 (0.339989)\u001b[0m\n",
            "[2023-11-09 16:06:22] \u001b[32mEpoch [8/10] Step [101/469]  acc1 0.953125 (0.877321)  loss 0.232749 (0.338192)\u001b[0m\n",
            "[2023-11-09 16:06:23] \u001b[32mEpoch [8/10] Step [111/469]  acc1 0.921875 (0.875845)  loss 0.330717 (0.342997)\u001b[0m\n",
            "[2023-11-09 16:06:25] \u001b[32mEpoch [8/10] Step [121/469]  acc1 0.843750 (0.875129)  loss 0.359838 (0.345054)\u001b[0m\n",
            "[2023-11-09 16:06:26] \u001b[32mEpoch [8/10] Step [131/469]  acc1 0.859375 (0.876193)  loss 0.393628 (0.344436)\u001b[0m\n",
            "[2023-11-09 16:06:28] \u001b[32mEpoch [8/10] Step [141/469]  acc1 0.812500 (0.874003)  loss 0.444307 (0.347483)\u001b[0m\n",
            "[2023-11-09 16:06:29] \u001b[32mEpoch [8/10] Step [151/469]  acc1 0.812500 (0.872827)  loss 0.577406 (0.348426)\u001b[0m\n",
            "[2023-11-09 16:06:30] \u001b[32mEpoch [8/10] Step [161/469]  acc1 0.890625 (0.874127)  loss 0.268809 (0.345839)\u001b[0m\n",
            "[2023-11-09 16:06:32] \u001b[32mEpoch [8/10] Step [171/469]  acc1 0.781250 (0.873447)  loss 0.368824 (0.345568)\u001b[0m\n",
            "[2023-11-09 16:06:33] \u001b[32mEpoch [8/10] Step [181/469]  acc1 0.906250 (0.873360)  loss 0.245167 (0.346032)\u001b[0m\n",
            "[2023-11-09 16:06:35] \u001b[32mEpoch [8/10] Step [191/469]  acc1 0.843750 (0.873364)  loss 0.424584 (0.345554)\u001b[0m\n",
            "[2023-11-09 16:06:36] \u001b[32mEpoch [8/10] Step [201/469]  acc1 0.921875 (0.873368)  loss 0.324958 (0.348082)\u001b[0m\n",
            "[2023-11-09 16:06:38] \u001b[32mEpoch [8/10] Step [211/469]  acc1 0.890625 (0.874334)  loss 0.248806 (0.382556)\u001b[0m\n",
            "[2023-11-09 16:06:39] \u001b[32mEpoch [8/10] Step [221/469]  acc1 0.875000 (0.874788)  loss 0.259720 (0.379890)\u001b[0m\n",
            "[2023-11-09 16:06:40] \u001b[32mEpoch [8/10] Step [231/469]  acc1 0.875000 (0.874391)  loss 0.370892 (0.379002)\u001b[0m\n",
            "[2023-11-09 16:06:42] \u001b[32mEpoch [8/10] Step [241/469]  acc1 0.843750 (0.873444)  loss 0.556131 (4.334853)\u001b[0m\n",
            "[2023-11-09 16:06:43] \u001b[32mEpoch [8/10] Step [251/469]  acc1 0.875000 (0.872572)  loss 0.398270 (4.176863)\u001b[0m\n",
            "[2023-11-09 16:06:45] \u001b[32mEpoch [8/10] Step [261/469]  acc1 0.796875 (0.872306)  loss 0.544952 (4.030305)\u001b[0m\n",
            "[2023-11-09 16:06:46] \u001b[32mEpoch [8/10] Step [271/469]  acc1 0.906250 (0.872867)  loss 0.189806 (3.893354)\u001b[0m\n",
            "[2023-11-09 16:06:47] \u001b[32mEpoch [8/10] Step [281/469]  acc1 0.906250 (0.872053)  loss 0.248219 (3.769078)\u001b[0m\n",
            "[2023-11-09 16:06:49] \u001b[32mEpoch [8/10] Step [291/469]  acc1 0.828125 (0.872101)  loss 0.393943 (3.650803)\u001b[0m\n",
            "[2023-11-09 16:06:50] \u001b[32mEpoch [8/10] Step [301/469]  acc1 0.890625 (0.872612)  loss 0.279232 (3.539904)\u001b[0m\n",
            "[2023-11-09 16:06:52] \u001b[32mEpoch [8/10] Step [311/469]  acc1 0.906250 (0.872890)  loss 0.280939 (3.437348)\u001b[0m\n",
            "[2023-11-09 16:06:53] \u001b[32mEpoch [8/10] Step [321/469]  acc1 0.843750 (0.873394)  loss 0.411404 (3.340713)\u001b[0m\n",
            "[2023-11-09 16:06:55] \u001b[32mEpoch [8/10] Step [331/469]  acc1 0.890625 (0.873914)  loss 0.246735 (3.248063)\u001b[0m\n",
            "[2023-11-09 16:06:56] \u001b[32mEpoch [8/10] Step [341/469]  acc1 0.828125 (0.874221)  loss 0.433188 (3.162605)\u001b[0m\n",
            "[2023-11-09 16:06:57] \u001b[32mEpoch [8/10] Step [351/469]  acc1 0.921875 (0.874332)  loss 0.280781 (3.081309)\u001b[0m\n",
            "[2023-11-09 16:06:59] \u001b[32mEpoch [8/10] Step [361/469]  acc1 0.828125 (0.874610)  loss 0.380976 (3.004552)\u001b[0m\n",
            "[2023-11-09 16:07:00] \u001b[32mEpoch [8/10] Step [371/469]  acc1 0.859375 (0.875000)  loss 0.286308 (2.931303)\u001b[0m\n",
            "[2023-11-09 16:07:02] \u001b[32mEpoch [8/10] Step [381/469]  acc1 0.843750 (0.874877)  loss 0.390660 (2.863885)\u001b[0m\n",
            "[2023-11-09 16:07:03] \u001b[32mEpoch [8/10] Step [391/469]  acc1 0.843750 (0.874720)  loss 0.428325 (2.799751)\u001b[0m\n",
            "[2023-11-09 16:07:05] \u001b[32mEpoch [8/10] Step [401/469]  acc1 0.828125 (0.874260)  loss 0.324207 (2.738938)\u001b[0m\n",
            "[2023-11-09 16:07:06] \u001b[32mEpoch [8/10] Step [411/469]  acc1 0.890625 (0.874772)  loss 0.289975 (2.680034)\u001b[0m\n",
            "[2023-11-09 16:07:07] \u001b[32mEpoch [8/10] Step [421/469]  acc1 0.796875 (0.874703)  loss 0.650079 (2.625232)\u001b[0m\n",
            "[2023-11-09 16:07:09] \u001b[32mEpoch [8/10] Step [431/469]  acc1 0.859375 (0.874529)  loss 0.533243 (2.572852)\u001b[0m\n",
            "[2023-11-09 16:07:10] \u001b[32mEpoch [8/10] Step [441/469]  acc1 0.875000 (0.874823)  loss 0.362672 (2.521570)\u001b[0m\n",
            "[2023-11-09 16:07:12] \u001b[32mEpoch [8/10] Step [451/469]  acc1 0.859375 (0.874965)  loss 0.387005 (2.472826)\u001b[0m\n",
            "[2023-11-09 16:07:13] \u001b[32mEpoch [8/10] Step [461/469]  acc1 0.859375 (0.874864)  loss 0.453712 (2.426886)\u001b[0m\n",
            "[2023-11-09 16:07:16] \u001b[32mEpoch [9/10] Step [1/469]  acc1 0.937500 (0.937500)  loss 0.173538 (0.173538)\u001b[0m\n",
            "[2023-11-09 16:07:17] \u001b[32mEpoch [9/10] Step [11/469]  acc1 0.859375 (0.896307)  loss 0.391956 (0.310891)\u001b[0m\n",
            "[2023-11-09 16:07:19] \u001b[32mEpoch [9/10] Step [21/469]  acc1 0.890625 (0.895089)  loss 1.317949 (0.364953)\u001b[0m\n",
            "[2023-11-09 16:07:20] \u001b[32mEpoch [9/10] Step [31/469]  acc1 0.921875 (0.889617)  loss 0.198156 (0.359905)\u001b[0m\n",
            "[2023-11-09 16:07:22] \u001b[32mEpoch [9/10] Step [41/469]  acc1 0.937500 (0.887576)  loss 0.245846 (0.357649)\u001b[0m\n",
            "[2023-11-09 16:07:23] \u001b[32mEpoch [9/10] Step [51/469]  acc1 0.843750 (0.883885)  loss 0.461122 (0.352239)\u001b[0m\n",
            "[2023-11-09 16:07:25] \u001b[32mEpoch [9/10] Step [61/469]  acc1 0.859375 (0.882428)  loss 0.304304 (0.348774)\u001b[0m\n",
            "[2023-11-09 16:07:26] \u001b[32mEpoch [9/10] Step [71/469]  acc1 0.906250 (0.877861)  loss 0.214806 (0.351702)\u001b[0m\n",
            "[2023-11-09 16:07:27] \u001b[32mEpoch [9/10] Step [81/469]  acc1 0.875000 (0.875772)  loss 0.370451 (0.354037)\u001b[0m\n",
            "[2023-11-09 16:07:29] \u001b[32mEpoch [9/10] Step [91/469]  acc1 0.937500 (0.876717)  loss 0.226371 (0.350318)\u001b[0m\n",
            "[2023-11-09 16:07:30] \u001b[32mEpoch [9/10] Step [101/469]  acc1 0.859375 (0.877011)  loss 0.260449 (0.346807)\u001b[0m\n",
            "[2023-11-09 16:07:32] \u001b[32mEpoch [9/10] Step [111/469]  acc1 0.890625 (0.875422)  loss 0.351119 (0.347956)\u001b[0m\n",
            "[2023-11-09 16:07:33] \u001b[32mEpoch [9/10] Step [121/469]  acc1 0.859375 (0.874742)  loss 0.341069 (0.347858)\u001b[0m\n",
            "[2023-11-09 16:07:35] \u001b[32mEpoch [9/10] Step [131/469]  acc1 0.875000 (0.875358)  loss 0.400048 (0.348273)\u001b[0m\n",
            "[2023-11-09 16:07:36] \u001b[32mEpoch [9/10] Step [141/469]  acc1 0.875000 (0.874335)  loss 0.283346 (0.347643)\u001b[0m\n",
            "[2023-11-09 16:07:37] \u001b[32mEpoch [9/10] Step [151/469]  acc1 0.828125 (0.875621)  loss 0.485197 (0.344545)\u001b[0m\n",
            "[2023-11-09 16:07:39] \u001b[32mEpoch [9/10] Step [161/469]  acc1 0.906250 (0.877135)  loss 0.234383 (0.340680)\u001b[0m\n",
            "[2023-11-09 16:07:40] \u001b[32mEpoch [9/10] Step [171/469]  acc1 0.968750 (0.878381)  loss 0.148924 (0.338188)\u001b[0m\n",
            "[2023-11-09 16:07:42] \u001b[32mEpoch [9/10] Step [181/469]  acc1 0.937500 (0.878280)  loss 0.359476 (0.338550)\u001b[0m\n",
            "[2023-11-09 16:07:43] \u001b[32mEpoch [9/10] Step [191/469]  acc1 0.828125 (0.878845)  loss 0.411339 (0.337207)\u001b[0m\n",
            "[2023-11-09 16:07:45] \u001b[32mEpoch [9/10] Step [201/469]  acc1 0.921875 (0.878809)  loss 0.309472 (0.337418)\u001b[0m\n",
            "[2023-11-09 16:07:46] \u001b[32mEpoch [9/10] Step [211/469]  acc1 0.875000 (0.878925)  loss 0.305830 (0.336035)\u001b[0m\n",
            "[2023-11-09 16:07:47] \u001b[32mEpoch [9/10] Step [221/469]  acc1 0.875000 (0.878323)  loss 0.320122 (0.336041)\u001b[0m\n",
            "[2023-11-09 16:07:49] \u001b[32mEpoch [9/10] Step [231/469]  acc1 0.859375 (0.878314)  loss 0.406566 (0.334991)\u001b[0m\n",
            "[2023-11-09 16:07:50] \u001b[32mEpoch [9/10] Step [241/469]  acc1 0.875000 (0.877464)  loss 0.380036 (0.336887)\u001b[0m\n",
            "[2023-11-09 16:07:52] \u001b[32mEpoch [9/10] Step [251/469]  acc1 0.828125 (0.876930)  loss 0.459879 (0.338416)\u001b[0m\n",
            "[2023-11-09 16:07:53] \u001b[32mEpoch [9/10] Step [261/469]  acc1 0.875000 (0.875958)  loss 0.329681 (0.338915)\u001b[0m\n",
            "[2023-11-09 16:07:54] \u001b[32mEpoch [9/10] Step [271/469]  acc1 0.906250 (0.876211)  loss 0.259212 (0.336990)\u001b[0m\n",
            "[2023-11-09 16:07:56] \u001b[32mEpoch [9/10] Step [281/469]  acc1 0.953125 (0.876501)  loss 0.118498 (0.335284)\u001b[0m\n",
            "[2023-11-09 16:07:57] \u001b[32mEpoch [9/10] Step [291/469]  acc1 0.875000 (0.876826)  loss 0.447588 (0.335049)\u001b[0m\n",
            "[2023-11-09 16:07:59] \u001b[32mEpoch [9/10] Step [301/469]  acc1 0.765625 (0.876350)  loss 0.536823 (0.336054)\u001b[0m\n",
            "[2023-11-09 16:08:00] \u001b[32mEpoch [9/10] Step [311/469]  acc1 0.906250 (0.876608)  loss 0.276038 (0.335127)\u001b[0m\n",
            "[2023-11-09 16:08:02] \u001b[32mEpoch [9/10] Step [321/469]  acc1 0.937500 (0.876558)  loss 0.264234 (0.335651)\u001b[0m\n",
            "[2023-11-09 16:08:03] \u001b[32mEpoch [9/10] Step [331/469]  acc1 0.937500 (0.876841)  loss 0.212585 (0.335793)\u001b[0m\n",
            "[2023-11-09 16:08:05] \u001b[32mEpoch [9/10] Step [341/469]  acc1 0.875000 (0.877016)  loss 0.263909 (0.334978)\u001b[0m\n",
            "[2023-11-09 16:08:06] \u001b[32mEpoch [9/10] Step [351/469]  acc1 0.921875 (0.876514)  loss 0.281547 (0.338672)\u001b[0m\n",
            "[2023-11-09 16:08:07] \u001b[32mEpoch [9/10] Step [361/469]  acc1 0.890625 (0.876385)  loss 0.338888 (0.338485)\u001b[0m\n",
            "[2023-11-09 16:08:09] \u001b[32mEpoch [9/10] Step [371/469]  acc1 0.875000 (0.877274)  loss 0.269378 (0.336319)\u001b[0m\n",
            "[2023-11-09 16:08:10] \u001b[32mEpoch [9/10] Step [381/469]  acc1 0.937500 (0.877871)  loss 0.195892 (0.334966)\u001b[0m\n",
            "[2023-11-09 16:08:12] \u001b[32mEpoch [9/10] Step [391/469]  acc1 0.921875 (0.877997)  loss 0.221155 (0.334278)\u001b[0m\n",
            "[2023-11-09 16:08:13] \u001b[32mEpoch [9/10] Step [401/469]  acc1 0.859375 (0.877650)  loss 0.313207 (0.334433)\u001b[0m\n",
            "[2023-11-09 16:08:15] \u001b[32mEpoch [9/10] Step [411/469]  acc1 0.859375 (0.877015)  loss 0.364523 (0.335868)\u001b[0m\n",
            "[2023-11-09 16:08:16] \u001b[32mEpoch [9/10] Step [421/469]  acc1 0.859375 (0.877190)  loss 0.451368 (0.337003)\u001b[0m\n",
            "[2023-11-09 16:08:17] \u001b[32mEpoch [9/10] Step [431/469]  acc1 0.859375 (0.877030)  loss 0.375238 (0.337006)\u001b[0m\n",
            "[2023-11-09 16:08:19] \u001b[32mEpoch [9/10] Step [441/469]  acc1 0.875000 (0.876842)  loss 0.300418 (0.337432)\u001b[0m\n",
            "[2023-11-09 16:08:20] \u001b[32mEpoch [9/10] Step [451/469]  acc1 0.843750 (0.876628)  loss 0.450402 (0.337918)\u001b[0m\n",
            "[2023-11-09 16:08:22] \u001b[32mEpoch [9/10] Step [461/469]  acc1 0.812500 (0.876051)  loss 0.399452 (0.338981)\u001b[0m\n",
            "[2023-11-09 16:08:24] \u001b[32mEpoch [10/10] Step [1/469]  acc1 0.875000 (0.875000)  loss 0.272992 (0.272992)\u001b[0m\n",
            "[2023-11-09 16:08:26] \u001b[32mEpoch [10/10] Step [11/469]  acc1 0.875000 (0.887784)  loss 0.348232 (0.335659)\u001b[0m\n",
            "[2023-11-09 16:08:27] \u001b[32mEpoch [10/10] Step [21/469]  acc1 0.890625 (0.895833)  loss 0.388750 (0.324111)\u001b[0m\n",
            "[2023-11-09 16:08:29] \u001b[32mEpoch [10/10] Step [31/469]  acc1 0.921875 (0.885081)  loss 0.270479 (0.348151)\u001b[0m\n",
            "[2023-11-09 16:08:30] \u001b[32mEpoch [10/10] Step [41/469]  acc1 0.953125 (0.884909)  loss 0.209608 (0.345518)\u001b[0m\n",
            "[2023-11-09 16:08:31] \u001b[32mEpoch [10/10] Step [51/469]  acc1 0.937500 (0.883885)  loss 0.198273 (0.346254)\u001b[0m\n",
            "[2023-11-09 16:08:33] \u001b[32mEpoch [10/10] Step [61/469]  acc1 0.875000 (0.882428)  loss 0.334720 (0.346135)\u001b[0m\n",
            "[2023-11-09 16:08:34] \u001b[32mEpoch [10/10] Step [71/469]  acc1 0.843750 (0.883363)  loss 0.524356 (0.345062)\u001b[0m\n",
            "[2023-11-09 16:08:36] \u001b[32mEpoch [10/10] Step [81/469]  acc1 0.906250 (0.881366)  loss 0.273978 (0.342878)\u001b[0m\n",
            "[2023-11-09 16:08:37] \u001b[32mEpoch [10/10] Step [91/469]  acc1 0.875000 (0.882212)  loss 0.381342 (0.340011)\u001b[0m\n",
            "[2023-11-09 16:08:39] \u001b[32mEpoch [10/10] Step [101/469]  acc1 0.859375 (0.883045)  loss 0.401334 (0.335599)\u001b[0m\n",
            "[2023-11-09 16:08:40] \u001b[32mEpoch [10/10] Step [111/469]  acc1 0.890625 (0.882320)  loss 0.335455 (0.333837)\u001b[0m\n",
            "[2023-11-09 16:08:41] \u001b[32mEpoch [10/10] Step [121/469]  acc1 0.875000 (0.880682)  loss 0.278182 (0.335060)\u001b[0m\n",
            "[2023-11-09 16:08:43] \u001b[32mEpoch [10/10] Step [131/469]  acc1 0.921875 (0.880367)  loss 0.204406 (0.334180)\u001b[0m\n",
            "[2023-11-09 16:08:44] \u001b[32mEpoch [10/10] Step [141/469]  acc1 0.953125 (0.881649)  loss 0.167455 (0.332117)\u001b[0m\n",
            "[2023-11-09 16:08:46] \u001b[32mEpoch [10/10] Step [151/469]  acc1 0.828125 (0.883485)  loss 0.443103 (0.328044)\u001b[0m\n",
            "[2023-11-09 16:08:47] \u001b[32mEpoch [10/10] Step [161/469]  acc1 0.859375 (0.883443)  loss 0.256253 (0.328326)\u001b[0m\n",
            "[2023-11-09 16:08:49] \u001b[32mEpoch [10/10] Step [171/469]  acc1 0.937500 (0.884046)  loss 0.259212 (0.327866)\u001b[0m\n",
            "[2023-11-09 16:08:50] \u001b[32mEpoch [10/10] Step [181/469]  acc1 0.906250 (0.883460)  loss 0.254483 (0.328122)\u001b[0m\n",
            "[2023-11-09 16:08:51] \u001b[32mEpoch [10/10] Step [191/469]  acc1 0.890625 (0.883671)  loss 0.292045 (0.327736)\u001b[0m\n",
            "[2023-11-09 16:08:53] \u001b[32mEpoch [10/10] Step [201/469]  acc1 0.859375 (0.884017)  loss 0.460324 (0.326851)\u001b[0m\n",
            "[2023-11-09 16:08:54] \u001b[32mEpoch [10/10] Step [211/469]  acc1 0.890625 (0.883442)  loss 0.391793 (0.335169)\u001b[0m\n",
            "[2023-11-09 16:08:56] \u001b[32mEpoch [10/10] Step [221/469]  acc1 0.890625 (0.883201)  loss 0.252287 (0.335286)\u001b[0m\n",
            "[2023-11-09 16:08:57] \u001b[32mEpoch [10/10] Step [231/469]  acc1 0.906250 (0.883252)  loss 0.345084 (0.335999)\u001b[0m\n",
            "[2023-11-09 16:08:59] \u001b[32mEpoch [10/10] Step [241/469]  acc1 0.906250 (0.883558)  loss 0.259698 (0.334988)\u001b[0m\n",
            "[2023-11-09 16:09:00] \u001b[32mEpoch [10/10] Step [251/469]  acc1 0.968750 (0.883777)  loss 0.171658 (0.333610)\u001b[0m\n",
            "[2023-11-09 16:09:01] \u001b[32mEpoch [10/10] Step [261/469]  acc1 0.843750 (0.883800)  loss 0.467817 (0.333809)\u001b[0m\n",
            "[2023-11-09 16:09:03] \u001b[32mEpoch [10/10] Step [271/469]  acc1 0.875000 (0.884052)  loss 0.283328 (0.332137)\u001b[0m\n",
            "[2023-11-09 16:09:04] \u001b[32mEpoch [10/10] Step [281/469]  acc1 0.843750 (0.883341)  loss 0.362119 (0.334149)\u001b[0m\n",
            "[2023-11-09 16:09:06] \u001b[32mEpoch [10/10] Step [291/469]  acc1 0.859375 (0.882678)  loss 0.367198 (0.334535)\u001b[0m\n",
            "[2023-11-09 16:09:07] \u001b[32mEpoch [10/10] Step [301/469]  acc1 0.937500 (0.882631)  loss 0.222667 (0.333715)\u001b[0m\n",
            "[2023-11-09 16:09:09] \u001b[32mEpoch [10/10] Step [311/469]  acc1 0.906250 (0.882988)  loss 0.238834 (3.163784)\u001b[0m\n",
            "[2023-11-09 16:09:10] \u001b[32mEpoch [10/10] Step [321/469]  acc1 0.906250 (0.883470)  loss 0.384700 (3.074244)\u001b[0m\n",
            "[2023-11-09 16:09:11] \u001b[32mEpoch [10/10] Step [331/469]  acc1 0.765625 (0.883308)  loss 0.669055 (2.991436)\u001b[0m\n",
            "[2023-11-09 16:09:13] \u001b[32mEpoch [10/10] Step [341/469]  acc1 0.875000 (0.883202)  loss 0.466489 (2.913761)\u001b[0m\n",
            "[2023-11-09 16:09:14] \u001b[32mEpoch [10/10] Step [351/469]  acc1 0.875000 (0.883636)  loss 0.301325 (2.839565)\u001b[0m\n",
            "[2023-11-09 16:09:16] \u001b[32mEpoch [10/10] Step [361/469]  acc1 0.812500 (0.883051)  loss 0.540086 (2.770594)\u001b[0m\n",
            "[2023-11-09 16:09:17] \u001b[32mEpoch [10/10] Step [371/469]  acc1 0.953125 (0.883592)  loss 0.156620 (2.703114)\u001b[0m\n",
            "[2023-11-09 16:09:19] \u001b[32mEpoch [10/10] Step [381/469]  acc1 0.859375 (0.883530)  loss 0.524348 (2.641118)\u001b[0m\n",
            "[2023-11-09 16:09:20] \u001b[32mEpoch [10/10] Step [391/469]  acc1 0.890625 (0.883552)  loss 0.347165 (2.582172)\u001b[0m\n",
            "[2023-11-09 16:09:21] \u001b[32mEpoch [10/10] Step [401/469]  acc1 0.921875 (0.883767)  loss 0.251763 (2.525679)\u001b[0m\n",
            "[2023-11-09 16:09:23] \u001b[32mEpoch [10/10] Step [411/469]  acc1 0.890625 (0.883972)  loss 0.278669 (2.471230)\u001b[0m\n",
            "[2023-11-09 16:09:24] \u001b[32mEpoch [10/10] Step [421/469]  acc1 0.828125 (0.883536)  loss 0.415563 (2.420660)\u001b[0m\n",
            "[2023-11-09 16:09:26] \u001b[32mEpoch [10/10] Step [431/469]  acc1 0.906250 (0.883773)  loss 0.243321 (2.371518)\u001b[0m\n",
            "[2023-11-09 16:09:27] \u001b[32mEpoch [10/10] Step [441/469]  acc1 0.828125 (0.883751)  loss 0.440544 (2.324983)\u001b[0m\n",
            "[2023-11-09 16:09:29] \u001b[32mEpoch [10/10] Step [451/469]  acc1 0.890625 (0.883627)  loss 0.317573 (2.280998)\u001b[0m\n",
            "[2023-11-09 16:09:31] \u001b[32mEpoch [10/10] Step [461/469]  acc1 0.859375 (0.883338)  loss 0.465370 (2.238944)\u001b[0m\n",
            "Final architecture: {'reduce_n2_p0': 'sepconv3x3', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv3x3', 'reduce_n3_p1': 'dilconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'maxpool', 'reduce_n4_p1': 'dilconv5x5', 'reduce_n4_p2': 'avgpool', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv3x3', 'reduce_n5_p1': 'maxpool', 'reduce_n5_p2': 'avgpool', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv5x5', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "lambd = 1\n",
        "n_chosen = 1\n",
        "\n",
        "for weight in [1e2]:\n",
        "    for lambd in [2, 3, 4]:\n",
        "        print(f\"weight = {weight}, lambd = {lambd}\")\n",
        "        if dataset == \"fashionmnist\":\n",
        "            model = CNN(32, 1, channels, 10, layers, n_chosen=1)\n",
        "        if dataset == \"cifar10\":\n",
        "            model = CNN(32, 3, channels, 10, layers, n_chosen=1)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss() # mycriterion()\n",
        "        optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, epochs, eta_min=0.001)\n",
        "        trainer = utils.MyDartsTrainer( # MyDartsTrainer\n",
        "            model=model,\n",
        "            loss=criterion, # =mycriterion,\n",
        "            metrics=lambda output, target: utils.accuracy(output, target, topk=(1,)),\n",
        "            optimizer=optim,\n",
        "            num_epochs=epochs,\n",
        "            dataset=dataset_train,\n",
        "            batch_size=batch_size,\n",
        "            log_frequency=log_frequency,\n",
        "            unrolled=unrolled,\n",
        "            weight=1e3, # вес регуляризатора\n",
        "            lambd=lambd, # количество общих ребер\n",
        "            train_as_optimal=False,\n",
        "            optimalPath='checkpoints/fashionMNIST/optimal/arc.json',\n",
        "            tau=1.0,\n",
        "            learning_rate=2.5E-3,\n",
        "            arc_learning_rate=3.0E-1,\n",
        "            n_chosen=1,\n",
        "        )\n",
        "        trainer.fit()\n",
        "        final_architecture = trainer.export()\n",
        "        print('Final architecture:', final_architecture)\n",
        "        json.dump(trainer.export(), open(f'checkpoints/fashionMNIST/lambd={lambd}/arc.json', 'w+'))\n",
        "        # json.dump(trainer.export(), open(f\"checkpoints/lambd={lambd}\" + '/arc.json', 'w+'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4, 1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "arcs = []\n",
        "\n",
        "with open(f'checkpoints/fashionMNIST/optimal/arc.json') as f:\n",
        "    arc = json.load(f) # оптимальная архитектура в виде словаря\n",
        "    arcs.append(arc)\n",
        "\n",
        "lambds = [1, 2, 3, 4]\n",
        "for lamb in lambds:\n",
        "    with open(f'checkpoints/fashionMNIST/lambd={lamb}/arc.json') as f:\n",
        "        arc = json.load(f)\n",
        "        arcs.append(arc)\n",
        "\n",
        "all_intersections = []\n",
        "for arc in arcs:\n",
        "    intersections = []\n",
        "    for other_arc in arcs:\n",
        "        same = 0\n",
        "        for n in range(2, 6):\n",
        "            common_parents = set(arc[f\"reduce_n{n}_switch\"]) & set(other_arc[f\"reduce_n{n}_switch\"])\n",
        "            for p in common_parents:\n",
        "                key = f\"reduce_n{n}_p{p}\"\n",
        "                if arc[key] == other_arc[key]:\n",
        "                    same += 1\n",
        "        intersections.append(same)\n",
        "    all_intersections.append(intersections)\n",
        "intersections_with_opt = all_intersections[0]\n",
        "print(intersections_with_opt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'lambda')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "y = intersections_with_opt[1:]\n",
        "plt.plot(lambds, y)\n",
        "plt.ylabel('common edges')\n",
        "plt.xlabel('lambda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = 2\n",
        "batch_size = 96\n",
        "log_frequency = 20\n",
        "channels = 16\n",
        "unrolled = False\n",
        "visualization = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./checkpoints/fashionMNIST/lambd=1/\n",
            "[2023-11-09 16:40:25] \u001b[32mFixed architecture: {'reduce_n2_p0': 'maxpool', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv5x5', 'reduce_n3_p1': 'sepconv5x5', 'reduce_n3_p2': 'skipconnect', 'reduce_n4_p0': 'dilconv5x5', 'reduce_n4_p1': 'dilconv5x5', 'reduce_n4_p2': 'skipconnect', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'skipconnect', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'sepconv5x5', 'reduce_n5_p3': 'avgpool', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [1], 'reduce_n3_switch': [1], 'reduce_n4_switch': [3], 'reduce_n5_switch': [2]}\u001b[0m\n",
            "[2023-11-09 16:40:25] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
            "[2023-11-09 16:40:26] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.261 Prec@(1,5) (9.4%, 50.0%)\u001b[0m\n",
            "[2023-11-09 16:40:26] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.766 Prec@(1,5) (24.3%, 71.1%)\u001b[0m\n",
            "[2023-11-09 16:40:27] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.230 Prec@(1,5) (39.1%, 83.6%)\u001b[0m\n",
            "[2023-11-09 16:40:27] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.970 Prec@(1,5) (46.3%, 88.1%)\u001b[0m\n",
            "[2023-11-09 16:40:27] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.786 Prec@(1,5) (51.3%, 90.7%)\u001b[0m\n",
            "[2023-11-09 16:40:28] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.663 Prec@(1,5) (54.6%, 92.2%)\u001b[0m\n",
            "[2023-11-09 16:40:28] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.566 Prec@(1,5) (57.4%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:40:28] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.488 Prec@(1,5) (59.5%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:40:29] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.420 Prec@(1,5) (61.3%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:40:29] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.369 Prec@(1,5) (62.8%, 95.3%)\u001b[0m\n",
            "[2023-11-09 16:40:29] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.327 Prec@(1,5) (63.9%, 95.6%)\u001b[0m\n",
            "[2023-11-09 16:40:30] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.294 Prec@(1,5) (64.7%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:40:30] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.260 Prec@(1,5) (65.7%, 96.2%)\u001b[0m\n",
            "[2023-11-09 16:40:30] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.229 Prec@(1,5) (66.6%, 96.5%)\u001b[0m\n",
            "[2023-11-09 16:40:31] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.206 Prec@(1,5) (67.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:40:31] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.180 Prec@(1,5) (68.0%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:40:32] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.156 Prec@(1,5) (68.7%, 97.0%)\u001b[0m\n",
            "[2023-11-09 16:40:32] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.136 Prec@(1,5) (69.3%, 97.2%)\u001b[0m\n",
            "[2023-11-09 16:40:32] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.115 Prec@(1,5) (69.8%, 97.3%)\u001b[0m\n",
            "[2023-11-09 16:40:33] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.096 Prec@(1,5) (70.3%, 97.4%)\u001b[0m\n",
            "[2023-11-09 16:40:33] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.079 Prec@(1,5) (70.8%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:40:33] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.064 Prec@(1,5) (71.2%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:40:34] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.049 Prec@(1,5) (71.7%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:40:34] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.033 Prec@(1,5) (72.1%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:40:34] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.019 Prec@(1,5) (72.5%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:40:35] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.004 Prec@(1,5) (72.8%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:40:35] \u001b[32mTrain: [  1/10] Step 520/624 Loss 0.992 Prec@(1,5) (73.2%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:40:35] \u001b[32mTrain: [  1/10] Step 540/624 Loss 0.981 Prec@(1,5) (73.5%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:40:36] \u001b[32mTrain: [  1/10] Step 560/624 Loss 0.971 Prec@(1,5) (73.8%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:40:36] \u001b[32mTrain: [  1/10] Step 580/624 Loss 0.959 Prec@(1,5) (74.1%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:40:37] \u001b[32mTrain: [  1/10] Step 600/624 Loss 0.947 Prec@(1,5) (74.4%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:40:37] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.937 Prec@(1,5) (74.7%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:40:37] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.935 Prec@(1,5) (74.8%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:40:37] \u001b[32mTrain: [  1/10] Final Prec@1 74.8050%\u001b[0m\n",
            "[2023-11-09 16:40:38] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.820 Prec@(1,5) (81.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:40:38] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.666 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:38] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.696 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:38] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.713 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:39] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.680 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:39] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.682 Prec@(1,5) (82.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:39] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.679 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:39] \u001b[32mValid: [  1/10] Final Prec@1 83.0400%\u001b[0m\n",
            "[2023-11-09 16:40:39] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
            "[2023-11-09 16:40:40] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.870 Prec@(1,5) (75.0%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:40:40] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.728 Prec@(1,5) (80.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:40:41] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.722 Prec@(1,5) (81.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:40:41] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.707 Prec@(1,5) (81.5%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:40:41] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.683 Prec@(1,5) (82.1%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:40:42] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.671 Prec@(1,5) (82.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:40:42] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.663 Prec@(1,5) (82.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:40:43] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.658 Prec@(1,5) (82.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:40:43] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.653 Prec@(1,5) (82.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:43] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.652 Prec@(1,5) (82.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:44] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.654 Prec@(1,5) (82.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:44] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.651 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:45] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.650 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:45] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.643 Prec@(1,5) (83.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:45] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.642 Prec@(1,5) (83.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:46] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.643 Prec@(1,5) (83.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:46] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.641 Prec@(1,5) (83.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:46] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.635 Prec@(1,5) (83.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:47] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.634 Prec@(1,5) (83.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:47] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.632 Prec@(1,5) (83.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:48] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.631 Prec@(1,5) (83.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:48] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.629 Prec@(1,5) (83.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:48] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.628 Prec@(1,5) (83.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:49] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.629 Prec@(1,5) (83.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:49] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.626 Prec@(1,5) (83.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:50] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.624 Prec@(1,5) (83.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:50] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.622 Prec@(1,5) (83.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:50] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.620 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:51] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.619 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:51] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.618 Prec@(1,5) (83.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:52] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.617 Prec@(1,5) (83.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:52] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.615 Prec@(1,5) (83.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:52] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.614 Prec@(1,5) (83.8%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:52] \u001b[32mTrain: [  2/10] Final Prec@1 83.7550%\u001b[0m\n",
            "[2023-11-09 16:40:53] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.454 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:40:53] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.417 Prec@(1,5) (87.6%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:40:53] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.456 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:40:54] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.475 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:40:54] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.454 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:40:54] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.464 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:40:54] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.461 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:40:54] \u001b[32mValid: [  2/10] Final Prec@1 86.8500%\u001b[0m\n",
            "[2023-11-09 16:40:54] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
            "[2023-11-09 16:40:55] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.648 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:40:55] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.588 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:56] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.591 Prec@(1,5) (84.2%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:40:56] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.573 Prec@(1,5) (84.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:56] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.572 Prec@(1,5) (84.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:57] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.566 Prec@(1,5) (84.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:57] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.565 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:58] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.563 Prec@(1,5) (85.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:58] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.561 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:58] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.568 Prec@(1,5) (85.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:59] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.570 Prec@(1,5) (85.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:40:59] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.568 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:40:59] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.567 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:00] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.567 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:00] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.565 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:01] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.566 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:01] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.563 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:01] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.562 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:02] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.562 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:02] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.561 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:02] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.559 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:03] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.558 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:03] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.557 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:04] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.556 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:04] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.556 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:04] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.555 Prec@(1,5) (85.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:05] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.553 Prec@(1,5) (85.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:05] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.554 Prec@(1,5) (85.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:05] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.553 Prec@(1,5) (85.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:06] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.550 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:06] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.549 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:07] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.550 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:07] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.550 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:07] \u001b[32mTrain: [  3/10] Final Prec@1 85.4583%\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.589 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.531 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.569 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.579 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.564 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.570 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:08] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.565 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:09] \u001b[32mValid: [  3/10] Final Prec@1 85.1600%\u001b[0m\n",
            "[2023-11-09 16:41:09] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
            "[2023-11-09 16:41:10] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.446 Prec@(1,5) (90.6%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:41:10] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.523 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:10] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.508 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:11] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.495 Prec@(1,5) (87.0%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:11] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.507 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:11] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.509 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:12] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.514 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:12] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.518 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:13] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.520 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:13] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.520 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:13] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.517 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:14] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.517 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:14] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.518 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:14] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.514 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:15] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.516 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:15] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.513 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:16] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.511 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:16] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.513 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:16] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.512 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:17] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.512 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:17] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.512 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:17] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.513 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:18] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.513 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:18] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.511 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:19] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.510 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:20] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.513 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:20] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.514 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:21] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.513 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:22] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.513 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:22] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.513 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:23] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.512 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:24] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.512 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:24] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.511 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:41:24] \u001b[32mTrain: [  4/10] Final Prec@1 86.7267%\u001b[0m\n",
            "[2023-11-09 16:41:25] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.547 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:41:26] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.403 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:41:26] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.437 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:26] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.443 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:26] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.432 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:26] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.432 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:27] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.427 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:41:27] \u001b[32mValid: [  4/10] Final Prec@1 88.3800%\u001b[0m\n",
            "[2023-11-09 16:41:27] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
            "[2023-11-09 16:41:28] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.434 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:41:28] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.501 Prec@(1,5) (87.2%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:41:29] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.517 Prec@(1,5) (87.1%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:41:30] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.499 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:30] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.505 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:31] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.502 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:32] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.503 Prec@(1,5) (87.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:41:32] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.504 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:33] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.505 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:33] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.506 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:34] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.505 Prec@(1,5) (87.2%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:41:35] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.501 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:35] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.499 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:36] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.498 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:37] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.498 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:38] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.493 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:38] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.491 Prec@(1,5) (87.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:39] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.493 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:40] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.494 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:40] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.496 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:41] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.496 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:42] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.496 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:42] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.495 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:43] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.496 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:44] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.493 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:45] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.492 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:45] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.492 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:46] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.492 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:47] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.491 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:47] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.491 Prec@(1,5) (87.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:48] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.491 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:49] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.489 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:49] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.489 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:49] \u001b[32mTrain: [  5/10] Final Prec@1 87.4283%\u001b[0m\n",
            "[2023-11-09 16:41:50] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.458 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:41:51] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.339 Prec@(1,5) (89.8%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:41:51] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.386 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:41:51] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.394 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:41:51] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.381 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:41:52] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.384 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:41:52] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.382 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:41:52] \u001b[32mValid: [  5/10] Final Prec@1 89.4000%\u001b[0m\n",
            "[2023-11-09 16:41:52] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
            "[2023-11-09 16:41:53] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.699 Prec@(1,5) (79.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:41:53] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.451 Prec@(1,5) (88.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:54] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.463 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:41:55] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.463 Prec@(1,5) (88.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:55] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.456 Prec@(1,5) (88.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:56] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.467 Prec@(1,5) (88.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:57] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.467 Prec@(1,5) (88.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:57] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.472 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:58] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.470 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:58] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.467 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:41:59] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.465 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:00] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.465 Prec@(1,5) (88.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:00] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.468 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:01] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.467 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:02] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.468 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:03] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.469 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:03] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.472 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:04] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.471 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:05] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.470 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:05] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.470 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:06] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.469 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:07] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.468 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:08] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.467 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:08] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.468 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:09] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.469 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:10] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.470 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:10] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.472 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:11] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.471 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:12] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.472 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:13] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.470 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:13] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.470 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:14] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.469 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:14] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.470 Prec@(1,5) (88.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:14] \u001b[32mTrain: [  6/10] Final Prec@1 87.9967%\u001b[0m\n",
            "[2023-11-09 16:42:15] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.415 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:42:16] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.385 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:16] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.409 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:16] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.413 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:42:16] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.401 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:17] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.404 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:17] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.402 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:17] \u001b[32mValid: [  6/10] Final Prec@1 89.4300%\u001b[0m\n",
            "[2023-11-09 16:42:17] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
            "[2023-11-09 16:42:18] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.393 Prec@(1,5) (92.7%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:42:18] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.455 Prec@(1,5) (88.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:19] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.442 Prec@(1,5) (88.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:20] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.440 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:20] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.445 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:21] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.455 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:22] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.456 Prec@(1,5) (88.3%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:22] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.458 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:23] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.456 Prec@(1,5) (88.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:23] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.456 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:24] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.456 Prec@(1,5) (88.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:42:25] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.453 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:25] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.455 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:26] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:27] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.453 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:28] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.455 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:28] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.453 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:29] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.453 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:30] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.453 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:31] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:31] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:32] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.450 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:33] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.450 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:33] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:34] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.452 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:35] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.452 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:36] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.452 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:36] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.452 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:37] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.452 Prec@(1,5) (88.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:38] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:38] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:39] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:39] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.451 Prec@(1,5) (88.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:42:39] \u001b[32mTrain: [  7/10] Final Prec@1 88.4833%\u001b[0m\n",
            "[2023-11-09 16:42:41] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.360 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:42:41] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.341 Prec@(1,5) (90.7%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:42:41] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.374 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:41] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.379 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:41] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.364 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:42] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.366 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:42] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.363 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:42:42] \u001b[32mValid: [  7/10] Final Prec@1 90.1900%\u001b[0m\n",
            "[2023-11-09 16:42:42] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
            "[2023-11-09 16:42:43] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.415 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:42:44] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.466 Prec@(1,5) (88.4%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:44] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.460 Prec@(1,5) (88.5%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:45] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.455 Prec@(1,5) (88.7%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:46] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.449 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:46] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.449 Prec@(1,5) (88.7%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:47] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.447 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:47] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.443 Prec@(1,5) (89.0%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:48] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.443 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:49] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.446 Prec@(1,5) (89.0%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:49] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.445 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:50] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.446 Prec@(1,5) (88.9%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:51] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.445 Prec@(1,5) (89.0%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:51] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.444 Prec@(1,5) (89.0%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:42:52] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.441 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:53] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.442 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:53] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.441 Prec@(1,5) (89.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:54] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.442 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:55] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.445 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:56] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.446 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:56] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.446 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:57] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.447 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:58] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.450 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:58] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.449 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:42:59] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.450 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:00] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.449 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:01] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.449 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:01] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.449 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:02] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.449 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:03] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.447 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:03] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.447 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:04] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.448 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:04] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.447 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:43:04] \u001b[32mTrain: [  8/10] Final Prec@1 88.8000%\u001b[0m\n",
            "[2023-11-09 16:43:05] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.311 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:06] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.331 Prec@(1,5) (91.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:06] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.361 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:06] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.364 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:06] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.351 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:07] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.352 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:07] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.349 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:07] \u001b[32mValid: [  8/10] Final Prec@1 90.8400%\u001b[0m\n",
            "[2023-11-09 16:43:07] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
            "[2023-11-09 16:43:08] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.553 Prec@(1,5) (84.4%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:43:09] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.461 Prec@(1,5) (88.4%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:09] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.473 Prec@(1,5) (88.1%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:10] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.469 Prec@(1,5) (87.9%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:11] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.458 Prec@(1,5) (88.2%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:11] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.456 Prec@(1,5) (88.3%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:12] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.453 Prec@(1,5) (88.4%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:12] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.459 Prec@(1,5) (88.2%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:13] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.459 Prec@(1,5) (88.2%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:43:14] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.460 Prec@(1,5) (88.2%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:14] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.458 Prec@(1,5) (88.3%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:15] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.456 Prec@(1,5) (88.4%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:16] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.453 Prec@(1,5) (88.4%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:16] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.450 Prec@(1,5) (88.5%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:17] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.452 Prec@(1,5) (88.4%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:18] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.451 Prec@(1,5) (88.5%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:18] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.450 Prec@(1,5) (88.5%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:19] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.447 Prec@(1,5) (88.6%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:20] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.448 Prec@(1,5) (88.6%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:21] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.446 Prec@(1,5) (88.7%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:21] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.444 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:22] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.443 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:23] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.443 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:23] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.442 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:24] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.441 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:25] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.441 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:26] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.442 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:26] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.442 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:27] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.440 Prec@(1,5) (88.9%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:28] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.441 Prec@(1,5) (88.9%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:28] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.442 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:29] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.442 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:29] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.442 Prec@(1,5) (88.8%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:43:29] \u001b[32mTrain: [  9/10] Final Prec@1 88.8400%\u001b[0m\n",
            "[2023-11-09 16:43:31] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.369 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:31] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.314 Prec@(1,5) (91.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:31] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.348 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:31] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.351 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:31] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.339 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:32] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.339 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:32] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.336 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:32] \u001b[32mValid: [  9/10] Final Prec@1 90.9500%\u001b[0m\n",
            "[2023-11-09 16:43:32] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
            "[2023-11-09 16:43:33] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.495 Prec@(1,5) (85.4%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:43:34] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.490 Prec@(1,5) (87.8%, 98.8%)\u001b[0m\n",
            "[2023-11-09 16:43:34] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.467 Prec@(1,5) (88.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:43:35] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.465 Prec@(1,5) (88.5%, 98.9%)\u001b[0m\n",
            "[2023-11-09 16:43:36] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.464 Prec@(1,5) (88.5%, 98.9%)\u001b[0m\n",
            "[2023-11-09 16:43:36] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.460 Prec@(1,5) (88.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:43:37] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.456 Prec@(1,5) (88.6%, 98.9%)\u001b[0m\n",
            "[2023-11-09 16:43:37] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.451 Prec@(1,5) (88.7%, 98.9%)\u001b[0m\n",
            "[2023-11-09 16:43:38] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.451 Prec@(1,5) (88.7%, 98.9%)\u001b[0m\n",
            "[2023-11-09 16:43:38] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.453 Prec@(1,5) (88.7%, 98.9%)\u001b[0m\n",
            "[2023-11-09 16:43:39] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.451 Prec@(1,5) (88.7%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:43:39] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.452 Prec@(1,5) (88.6%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:43:39] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.450 Prec@(1,5) (88.7%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:43:40] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.448 Prec@(1,5) (88.7%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:40] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.448 Prec@(1,5) (88.8%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:40] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.450 Prec@(1,5) (88.7%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:41] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.449 Prec@(1,5) (88.7%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:41] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.446 Prec@(1,5) (88.8%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:42] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.443 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:42] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.443 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:42] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:43] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:43] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:43] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:44] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.441 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:44] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.441 Prec@(1,5) (89.0%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:45] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:45] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:45] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.443 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:46] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.443 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:46] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.443 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:46] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:47] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.442 Prec@(1,5) (88.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:43:47] \u001b[32mTrain: [ 10/10] Final Prec@1 88.8750%\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.352 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.320 Prec@(1,5) (91.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.355 Prec@(1,5) (90.9%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.355 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.345 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.346 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.343 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:43:48] \u001b[32mValid: [ 10/10] Final Prec@1 90.8600%\u001b[0m\n",
            "Final best Prec@1 = 90.9500%\n",
            "[0.9095000198364258]\n",
            "./checkpoints/fashionMNIST/lambd=2/\n",
            "[2023-11-09 16:43:48] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv5x5', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'sepconv3x3', 'reduce_n4_p2': 'dilconv5x5', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'avgpool', 'reduce_n5_p1': 'dilconv5x5', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [0], 'reduce_n3_switch': [2], 'reduce_n4_switch': [1], 'reduce_n5_switch': [3]}\u001b[0m\n",
            "[2023-11-09 16:43:49] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
            "[2023-11-09 16:43:50] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.295 Prec@(1,5) (9.4%, 45.8%)\u001b[0m\n",
            "[2023-11-09 16:43:50] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.771 Prec@(1,5) (23.1%, 70.7%)\u001b[0m\n",
            "[2023-11-09 16:43:50] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.277 Prec@(1,5) (37.3%, 83.0%)\u001b[0m\n",
            "[2023-11-09 16:43:51] \u001b[32mTrain: [  1/10] Step 060/624 Loss 2.009 Prec@(1,5) (44.9%, 87.8%)\u001b[0m\n",
            "[2023-11-09 16:43:51] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.827 Prec@(1,5) (49.8%, 90.5%)\u001b[0m\n",
            "[2023-11-09 16:43:52] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.702 Prec@(1,5) (53.4%, 92.1%)\u001b[0m\n",
            "[2023-11-09 16:43:52] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.615 Prec@(1,5) (55.6%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:43:53] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.548 Prec@(1,5) (57.5%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:43:53] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.485 Prec@(1,5) (59.2%, 94.7%)\u001b[0m\n",
            "[2023-11-09 16:43:53] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.433 Prec@(1,5) (60.6%, 95.2%)\u001b[0m\n",
            "[2023-11-09 16:43:54] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.393 Prec@(1,5) (61.7%, 95.6%)\u001b[0m\n",
            "[2023-11-09 16:43:54] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.354 Prec@(1,5) (62.7%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:43:55] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.315 Prec@(1,5) (63.8%, 96.2%)\u001b[0m\n",
            "[2023-11-09 16:43:55] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.283 Prec@(1,5) (64.7%, 96.5%)\u001b[0m\n",
            "[2023-11-09 16:43:55] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.258 Prec@(1,5) (65.4%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:43:56] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.233 Prec@(1,5) (66.1%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:43:56] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.212 Prec@(1,5) (66.7%, 97.0%)\u001b[0m\n",
            "[2023-11-09 16:43:57] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.192 Prec@(1,5) (67.2%, 97.1%)\u001b[0m\n",
            "[2023-11-09 16:43:57] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.171 Prec@(1,5) (67.8%, 97.3%)\u001b[0m\n",
            "[2023-11-09 16:43:58] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.154 Prec@(1,5) (68.3%, 97.4%)\u001b[0m\n",
            "[2023-11-09 16:43:58] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.137 Prec@(1,5) (68.8%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:43:58] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.121 Prec@(1,5) (69.3%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:43:59] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.104 Prec@(1,5) (69.8%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:43:59] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.088 Prec@(1,5) (70.2%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:44:00] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.075 Prec@(1,5) (70.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:44:00] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.062 Prec@(1,5) (70.9%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:44:00] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.050 Prec@(1,5) (71.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:44:01] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.038 Prec@(1,5) (71.6%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:44:01] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.028 Prec@(1,5) (71.9%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:44:02] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.016 Prec@(1,5) (72.2%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:44:02] \u001b[32mTrain: [  1/10] Step 600/624 Loss 1.008 Prec@(1,5) (72.5%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:44:03] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.999 Prec@(1,5) (72.8%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:44:03] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.997 Prec@(1,5) (72.8%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:44:03] \u001b[32mTrain: [  1/10] Final Prec@1 72.8317%\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.816 Prec@(1,5) (78.1%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.582 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.654 Prec@(1,5) (81.6%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.652 Prec@(1,5) (81.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.643 Prec@(1,5) (81.6%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.653 Prec@(1,5) (81.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:04] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.649 Prec@(1,5) (81.5%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:05] \u001b[32mValid: [  1/10] Final Prec@1 81.5200%\u001b[0m\n",
            "[2023-11-09 16:44:05] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
            "[2023-11-09 16:44:06] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.789 Prec@(1,5) (77.1%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:44:06] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.775 Prec@(1,5) (78.8%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:06] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.768 Prec@(1,5) (79.4%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:44:07] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.747 Prec@(1,5) (80.0%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:07] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.750 Prec@(1,5) (79.7%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:08] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.746 Prec@(1,5) (80.0%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:08] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.748 Prec@(1,5) (79.9%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:09] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.744 Prec@(1,5) (80.0%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:09] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.734 Prec@(1,5) (80.1%, 99.1%)\u001b[0m\n",
            "[2023-11-09 16:44:10] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.728 Prec@(1,5) (80.3%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:10] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.727 Prec@(1,5) (80.4%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:10] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.724 Prec@(1,5) (80.6%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:11] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.720 Prec@(1,5) (80.6%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:11] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.714 Prec@(1,5) (80.9%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:12] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.710 Prec@(1,5) (81.0%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:12] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.706 Prec@(1,5) (81.1%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:44:13] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.705 Prec@(1,5) (81.1%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:13] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.703 Prec@(1,5) (81.2%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:14] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.700 Prec@(1,5) (81.2%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:14] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.699 Prec@(1,5) (81.2%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:14] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.695 Prec@(1,5) (81.4%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:15] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.691 Prec@(1,5) (81.5%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:15] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.690 Prec@(1,5) (81.5%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:16] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.689 Prec@(1,5) (81.5%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:44:16] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.687 Prec@(1,5) (81.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:17] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.685 Prec@(1,5) (81.7%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:17] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.682 Prec@(1,5) (81.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:18] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.679 Prec@(1,5) (81.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:18] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.676 Prec@(1,5) (81.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:18] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.674 Prec@(1,5) (82.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:19] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.671 Prec@(1,5) (82.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:19] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.669 Prec@(1,5) (82.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:19] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.668 Prec@(1,5) (82.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:20] \u001b[32mTrain: [  2/10] Final Prec@1 82.1267%\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.546 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.474 Prec@(1,5) (85.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.554 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.552 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.535 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.547 Prec@(1,5) (85.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.542 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mValid: [  2/10] Final Prec@1 85.4700%\u001b[0m\n",
            "[2023-11-09 16:44:21] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
            "[2023-11-09 16:44:22] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.649 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:44:23] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.715 Prec@(1,5) (81.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:23] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.670 Prec@(1,5) (82.4%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:44:24] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.638 Prec@(1,5) (83.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:24] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.638 Prec@(1,5) (83.4%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:25] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.621 Prec@(1,5) (83.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:25] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.620 Prec@(1,5) (84.1%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:25] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.621 Prec@(1,5) (83.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:26] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.613 Prec@(1,5) (84.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:26] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.608 Prec@(1,5) (84.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:27] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.610 Prec@(1,5) (84.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:27] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.610 Prec@(1,5) (84.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:28] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.610 Prec@(1,5) (84.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:28] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.613 Prec@(1,5) (84.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:29] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.611 Prec@(1,5) (84.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:29] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.609 Prec@(1,5) (84.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:29] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.610 Prec@(1,5) (84.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:30] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.607 Prec@(1,5) (84.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:30] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.605 Prec@(1,5) (84.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:31] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.604 Prec@(1,5) (84.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:31] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.601 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:32] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.602 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:32] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.603 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:32] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.603 Prec@(1,5) (84.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:33] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.601 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:33] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.599 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:34] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.598 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:34] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.596 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:35] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.594 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:35] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.593 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:36] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.593 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:36] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.593 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:36] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.593 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:36] \u001b[32mTrain: [  3/10] Final Prec@1 84.4200%\u001b[0m\n",
            "[2023-11-09 16:44:37] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.552 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:44:37] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.428 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.473 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.474 Prec@(1,5) (87.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.458 Prec@(1,5) (87.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.467 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.462 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mValid: [  3/10] Final Prec@1 87.2800%\u001b[0m\n",
            "[2023-11-09 16:44:38] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
            "[2023-11-09 16:44:39] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.577 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:44:40] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.563 Prec@(1,5) (84.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:44:40] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.560 Prec@(1,5) (84.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:40] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.569 Prec@(1,5) (84.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:41] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.563 Prec@(1,5) (84.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:41] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:42] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.566 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:42] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.562 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:43] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.562 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:43] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.564 Prec@(1,5) (85.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:43] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.561 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:44] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.559 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:44] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.564 Prec@(1,5) (85.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:45] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.564 Prec@(1,5) (85.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:45] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.563 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:46] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.561 Prec@(1,5) (85.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:46] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.560 Prec@(1,5) (85.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:47] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.559 Prec@(1,5) (85.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:47] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.558 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:47] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.559 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:48] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.559 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:48] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.558 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:49] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.557 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:49] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.555 Prec@(1,5) (85.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:50] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.552 Prec@(1,5) (85.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:50] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.551 Prec@(1,5) (85.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:44:51] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.550 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:51] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.551 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:52] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.551 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:52] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.550 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:52] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.549 Prec@(1,5) (85.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:53] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.549 Prec@(1,5) (85.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:53] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.548 Prec@(1,5) (85.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:53] \u001b[32mTrain: [  4/10] Final Prec@1 85.5800%\u001b[0m\n",
            "[2023-11-09 16:44:54] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.464 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:44:54] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.404 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:44:54] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.448 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:54] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.454 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:55] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.438 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:55] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.446 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:55] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.444 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:55] \u001b[32mValid: [  4/10] Final Prec@1 87.5700%\u001b[0m\n",
            "[2023-11-09 16:44:55] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
            "[2023-11-09 16:44:56] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.468 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:44:56] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.508 Prec@(1,5) (86.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:44:57] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.527 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:57] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.536 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:44:58] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.555 Prec@(1,5) (85.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:58] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.549 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:59] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.546 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:59] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.544 Prec@(1,5) (86.0%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:44:59] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.539 Prec@(1,5) (86.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:00] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.538 Prec@(1,5) (86.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:00] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.534 Prec@(1,5) (86.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:01] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.535 Prec@(1,5) (86.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:01] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.532 Prec@(1,5) (86.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:02] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.530 Prec@(1,5) (86.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:02] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.530 Prec@(1,5) (86.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:03] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.529 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:03] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.531 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:04] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.533 Prec@(1,5) (86.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:04] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.531 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:04] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.532 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:05] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.531 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:05] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.531 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:06] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.528 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:06] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.528 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:07] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.527 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:07] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.527 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:08] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.526 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:08] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.525 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:08] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.523 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:09] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.522 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:09] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.522 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:10] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.522 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:10] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.522 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:10] \u001b[32mTrain: [  5/10] Final Prec@1 86.5100%\u001b[0m\n",
            "[2023-11-09 16:45:11] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.526 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:45:11] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.371 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:11] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.406 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:11] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.414 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:12] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.393 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:12] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.401 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:12] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.398 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:12] \u001b[32mValid: [  5/10] Final Prec@1 89.4800%\u001b[0m\n",
            "[2023-11-09 16:45:12] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
            "[2023-11-09 16:45:13] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.464 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:45:13] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.515 Prec@(1,5) (86.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:14] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.496 Prec@(1,5) (87.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:14] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.497 Prec@(1,5) (87.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:15] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.502 Prec@(1,5) (87.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:15] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.494 Prec@(1,5) (87.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:16] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.502 Prec@(1,5) (87.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:16] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.503 Prec@(1,5) (87.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:17] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.501 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:18] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.502 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:18] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.504 Prec@(1,5) (87.0%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:19] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.506 Prec@(1,5) (86.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:20] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.503 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:21] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.503 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:21] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.498 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:22] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.497 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:23] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.497 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:24] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.499 Prec@(1,5) (87.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:25] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.501 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:26] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.501 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:26] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.500 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:27] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.500 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:28] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.499 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:29] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.498 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:29] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.498 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:30] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.498 Prec@(1,5) (87.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:31] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.499 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:32] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.500 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:33] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.500 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:33] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.500 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:34] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.500 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:35] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.500 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:35] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.500 Prec@(1,5) (87.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:35] \u001b[32mTrain: [  6/10] Final Prec@1 87.0250%\u001b[0m\n",
            "[2023-11-09 16:45:37] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.400 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:45:37] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.326 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:45:37] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.376 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:37] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.372 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:37] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.354 Prec@(1,5) (90.4%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:38] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.366 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:38] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.363 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:38] \u001b[32mValid: [  6/10] Final Prec@1 90.2400%\u001b[0m\n",
            "[2023-11-09 16:45:38] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
            "[2023-11-09 16:45:39] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.438 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:45:40] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.480 Prec@(1,5) (87.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:45:41] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.509 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:41] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.507 Prec@(1,5) (87.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:42] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.510 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:45:43] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.499 Prec@(1,5) (87.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:43] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.497 Prec@(1,5) (87.4%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:44] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.495 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:45] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.497 Prec@(1,5) (87.4%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:45] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.492 Prec@(1,5) (87.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:46] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.492 Prec@(1,5) (87.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:47] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.491 Prec@(1,5) (87.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:48] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.494 Prec@(1,5) (87.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:49] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.490 Prec@(1,5) (87.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:49] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.488 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:50] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.490 Prec@(1,5) (87.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:51] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.486 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:52] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.485 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:53] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.485 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:53] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.487 Prec@(1,5) (87.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:54] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.485 Prec@(1,5) (87.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:55] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.484 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:56] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.484 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:57] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.484 Prec@(1,5) (87.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:57] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.486 Prec@(1,5) (87.6%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:45:58] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.485 Prec@(1,5) (87.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:45:59] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.484 Prec@(1,5) (87.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:00] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.481 Prec@(1,5) (87.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:01] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.481 Prec@(1,5) (87.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:01] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.480 Prec@(1,5) (87.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:02] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.482 Prec@(1,5) (87.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:03] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.483 Prec@(1,5) (87.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:03] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.483 Prec@(1,5) (87.6%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:03] \u001b[32mTrain: [  7/10] Final Prec@1 87.6317%\u001b[0m\n",
            "[2023-11-09 16:46:04] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.424 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:46:05] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.322 Prec@(1,5) (91.1%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:46:05] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.373 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:46:05] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.372 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:46:06] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.356 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:46:06] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.367 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:46:06] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.364 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:46:06] \u001b[32mValid: [  7/10] Final Prec@1 90.1900%\u001b[0m\n",
            "[2023-11-09 16:46:06] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
            "[2023-11-09 16:46:07] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.499 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:46:08] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.502 Prec@(1,5) (87.1%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:46:09] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.473 Prec@(1,5) (87.6%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:09] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.475 Prec@(1,5) (87.7%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:10] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.474 Prec@(1,5) (87.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:11] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.480 Prec@(1,5) (87.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:11] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.479 Prec@(1,5) (87.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:12] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.472 Prec@(1,5) (88.1%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:13] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.477 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:14] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.477 Prec@(1,5) (87.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:14] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.473 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:15] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.473 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:16] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.471 Prec@(1,5) (87.9%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:17] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.470 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:18] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.470 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:18] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.468 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:19] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.468 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:20] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.467 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:21] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.469 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:22] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.471 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:22] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.472 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:23] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.472 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:24] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.473 Prec@(1,5) (87.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:25] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.473 Prec@(1,5) (87.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:26] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.471 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:27] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.471 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:27] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.470 Prec@(1,5) (87.9%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:28] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.470 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:29] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.469 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:30] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.470 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:31] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.470 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:31] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.470 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:32] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.470 Prec@(1,5) (88.0%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:32] \u001b[32mTrain: [  8/10] Final Prec@1 88.0200%\u001b[0m\n",
            "[2023-11-09 16:46:33] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.471 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:46:33] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.334 Prec@(1,5) (90.9%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:46:33] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.396 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:46:34] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.399 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:46:34] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.381 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:46:34] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:46:34] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.384 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:46:34] \u001b[32mValid: [  8/10] Final Prec@1 90.0400%\u001b[0m\n",
            "[2023-11-09 16:46:34] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
            "[2023-11-09 16:46:35] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.558 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:46:36] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.471 Prec@(1,5) (88.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:46:37] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.461 Prec@(1,5) (88.1%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:46:38] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.463 Prec@(1,5) (88.1%, 99.2%)\u001b[0m\n",
            "[2023-11-09 16:46:38] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.472 Prec@(1,5) (88.1%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:46:39] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.475 Prec@(1,5) (87.9%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:46:40] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.480 Prec@(1,5) (87.8%, 99.3%)\u001b[0m\n",
            "[2023-11-09 16:46:41] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.481 Prec@(1,5) (87.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:41] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.475 Prec@(1,5) (87.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:42] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.471 Prec@(1,5) (88.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:43] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.471 Prec@(1,5) (88.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:44] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.467 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:44] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.469 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:45] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.466 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:46] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.463 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:47] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.462 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:48] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.463 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:48] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.461 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:49] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.460 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:50] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.462 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:51] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.462 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:51] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.463 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:52] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.463 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:53] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.463 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:54] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.464 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:55] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.464 Prec@(1,5) (88.3%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:56] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.464 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:56] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.463 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:57] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.463 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:58] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.462 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:46:59] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.462 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:00] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.462 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:00] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.463 Prec@(1,5) (88.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:00] \u001b[32mTrain: [  9/10] Final Prec@1 88.1967%\u001b[0m\n",
            "[2023-11-09 16:47:01] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.432 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:47:01] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.314 Prec@(1,5) (91.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.378 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.358 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.364 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.361 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mValid: [  9/10] Final Prec@1 90.4200%\u001b[0m\n",
            "[2023-11-09 16:47:02] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
            "[2023-11-09 16:47:04] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.547 Prec@(1,5) (85.4%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:47:04] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.448 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:05] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.442 Prec@(1,5) (88.8%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:47:06] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.454 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:06] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.447 Prec@(1,5) (88.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:07] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.444 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:08] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.438 Prec@(1,5) (89.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:09] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.442 Prec@(1,5) (89.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:09] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.442 Prec@(1,5) (89.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:10] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.446 Prec@(1,5) (89.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:11] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.443 Prec@(1,5) (89.2%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:12] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.448 Prec@(1,5) (89.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:12] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.447 Prec@(1,5) (89.1%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:13] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.447 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:14] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.448 Prec@(1,5) (89.0%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:15] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.452 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:16] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.454 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:16] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.455 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:17] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.452 Prec@(1,5) (88.9%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:18] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.451 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:19] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.451 Prec@(1,5) (88.8%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:20] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.452 Prec@(1,5) (88.7%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:20] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.453 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:21] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.453 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:22] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.453 Prec@(1,5) (88.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:23] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.454 Prec@(1,5) (88.6%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:47:24] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.454 Prec@(1,5) (88.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:24] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.455 Prec@(1,5) (88.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:25] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.454 Prec@(1,5) (88.7%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:26] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.454 Prec@(1,5) (88.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:27] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.454 Prec@(1,5) (88.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:27] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.455 Prec@(1,5) (88.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:28] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.455 Prec@(1,5) (88.6%, 99.4%)\u001b[0m\n",
            "[2023-11-09 16:47:28] \u001b[32mTrain: [ 10/10] Final Prec@1 88.6133%\u001b[0m\n",
            "[2023-11-09 16:47:29] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.385 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:47:29] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.296 Prec@(1,5) (92.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:47:29] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.353 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:30] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.353 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:30] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.335 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:30] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.342 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:30] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.339 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:47:30] \u001b[32mValid: [ 10/10] Final Prec@1 91.1600%\u001b[0m\n",
            "Final best Prec@1 = 91.1600%\n",
            "[0.9095000198364258, 0.9116000217437744]\n",
            "./checkpoints/fashionMNIST/lambd=3/\n",
            "[2023-11-09 16:47:30] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'avgpool', 'reduce_n3_p0': 'sepconv3x3', 'reduce_n3_p1': 'sepconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'dilconv3x3', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'maxpool', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv5x5', 'reduce_n5_p1': 'dilconv3x3', 'reduce_n5_p2': 'dilconv3x3', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\u001b[0m\n",
            "[2023-11-09 16:47:31] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
            "[2023-11-09 16:47:32] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.264 Prec@(1,5) (11.5%, 50.0%)\u001b[0m\n",
            "[2023-11-09 16:47:32] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.423 Prec@(1,5) (34.0%, 82.4%)\u001b[0m\n",
            "[2023-11-09 16:47:33] \u001b[32mTrain: [  1/10] Step 040/624 Loss 1.980 Prec@(1,5) (45.4%, 89.8%)\u001b[0m\n",
            "[2023-11-09 16:47:33] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.746 Prec@(1,5) (52.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:47:34] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.612 Prec@(1,5) (55.6%, 93.9%)\u001b[0m\n",
            "[2023-11-09 16:47:35] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.513 Prec@(1,5) (58.5%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:47:35] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.450 Prec@(1,5) (60.1%, 95.6%)\u001b[0m\n",
            "[2023-11-09 16:47:36] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.376 Prec@(1,5) (62.1%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:47:36] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.333 Prec@(1,5) (63.3%, 96.4%)\u001b[0m\n",
            "[2023-11-09 16:47:37] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.289 Prec@(1,5) (64.6%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:47:38] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.248 Prec@(1,5) (65.8%, 97.0%)\u001b[0m\n",
            "[2023-11-09 16:47:38] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.214 Prec@(1,5) (66.7%, 97.2%)\u001b[0m\n",
            "[2023-11-09 16:47:39] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.187 Prec@(1,5) (67.5%, 97.4%)\u001b[0m\n",
            "[2023-11-09 16:47:40] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.158 Prec@(1,5) (68.4%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:47:40] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.132 Prec@(1,5) (69.0%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:47:41] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.111 Prec@(1,5) (69.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:47:42] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.091 Prec@(1,5) (70.2%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:47:42] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.072 Prec@(1,5) (70.8%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:47:43] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.057 Prec@(1,5) (71.2%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:47:44] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.040 Prec@(1,5) (71.6%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:47:44] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.024 Prec@(1,5) (72.1%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:47:45] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.008 Prec@(1,5) (72.5%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:47:46] \u001b[32mTrain: [  1/10] Step 440/624 Loss 0.995 Prec@(1,5) (72.9%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:47:46] \u001b[32mTrain: [  1/10] Step 460/624 Loss 0.980 Prec@(1,5) (73.3%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:47:47] \u001b[32mTrain: [  1/10] Step 480/624 Loss 0.966 Prec@(1,5) (73.7%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:47:48] \u001b[32mTrain: [  1/10] Step 500/624 Loss 0.957 Prec@(1,5) (74.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:47:49] \u001b[32mTrain: [  1/10] Step 520/624 Loss 0.947 Prec@(1,5) (74.3%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:47:49] \u001b[32mTrain: [  1/10] Step 540/624 Loss 0.937 Prec@(1,5) (74.6%, 98.6%)\u001b[0m\n",
            "[2023-11-09 16:47:50] \u001b[32mTrain: [  1/10] Step 560/624 Loss 0.926 Prec@(1,5) (74.9%, 98.6%)\u001b[0m\n",
            "[2023-11-09 16:47:51] \u001b[32mTrain: [  1/10] Step 580/624 Loss 0.915 Prec@(1,5) (75.2%, 98.6%)\u001b[0m\n",
            "[2023-11-09 16:47:51] \u001b[32mTrain: [  1/10] Step 600/624 Loss 0.909 Prec@(1,5) (75.4%, 98.7%)\u001b[0m\n",
            "[2023-11-09 16:47:52] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.900 Prec@(1,5) (75.7%, 98.7%)\u001b[0m\n",
            "[2023-11-09 16:47:52] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.899 Prec@(1,5) (75.7%, 98.7%)\u001b[0m\n",
            "[2023-11-09 16:47:52] \u001b[32mTrain: [  1/10] Final Prec@1 75.7450%\u001b[0m\n",
            "[2023-11-09 16:47:53] \u001b[32mValid: [  1/10] Step 000/104 Loss 1.063 Prec@(1,5) (78.1%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:47:54] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.726 Prec@(1,5) (79.7%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:47:54] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.807 Prec@(1,5) (79.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:47:54] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.813 Prec@(1,5) (79.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:47:55] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.796 Prec@(1,5) (80.0%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:47:55] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.798 Prec@(1,5) (80.1%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:47:55] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.793 Prec@(1,5) (80.2%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:47:55] \u001b[32mValid: [  1/10] Final Prec@1 80.1700%\u001b[0m\n",
            "[2023-11-09 16:47:55] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
            "[2023-11-09 16:47:56] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.851 Prec@(1,5) (78.1%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:47:57] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.772 Prec@(1,5) (80.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:47:58] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.763 Prec@(1,5) (79.9%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:47:58] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.768 Prec@(1,5) (79.5%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:47:59] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.750 Prec@(1,5) (80.2%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:48:00] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.749 Prec@(1,5) (80.1%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:48:00] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.744 Prec@(1,5) (80.2%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:48:01] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.743 Prec@(1,5) (80.2%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:48:02] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.744 Prec@(1,5) (80.2%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:48:02] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.738 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:48:03] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.731 Prec@(1,5) (80.5%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:48:04] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.723 Prec@(1,5) (80.7%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:48:04] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.721 Prec@(1,5) (80.7%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:48:05] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.715 Prec@(1,5) (80.8%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:48:06] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.716 Prec@(1,5) (80.9%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:07] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.716 Prec@(1,5) (80.9%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:48:07] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.712 Prec@(1,5) (81.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:08] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.709 Prec@(1,5) (81.2%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:09] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.703 Prec@(1,5) (81.3%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:10] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.701 Prec@(1,5) (81.4%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:10] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.697 Prec@(1,5) (81.6%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:11] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.691 Prec@(1,5) (81.7%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:12] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.688 Prec@(1,5) (81.8%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:13] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.685 Prec@(1,5) (81.9%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:13] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.684 Prec@(1,5) (82.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:14] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.683 Prec@(1,5) (82.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:15] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.684 Prec@(1,5) (81.9%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:15] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.680 Prec@(1,5) (82.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:16] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.680 Prec@(1,5) (82.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:17] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.679 Prec@(1,5) (82.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:18] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.678 Prec@(1,5) (82.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:18] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.677 Prec@(1,5) (82.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:19] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.677 Prec@(1,5) (82.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:48:19] \u001b[32mTrain: [  2/10] Final Prec@1 82.1183%\u001b[0m\n",
            "[2023-11-09 16:48:20] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.487 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:48:20] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.446 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:48:20] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.488 Prec@(1,5) (86.9%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:21] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.495 Prec@(1,5) (86.8%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:21] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.475 Prec@(1,5) (87.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:21] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.476 Prec@(1,5) (87.0%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:21] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.473 Prec@(1,5) (87.0%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:22] \u001b[32mValid: [  2/10] Final Prec@1 87.0400%\u001b[0m\n",
            "[2023-11-09 16:48:22] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
            "[2023-11-09 16:48:23] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.939 Prec@(1,5) (77.1%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:48:23] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.736 Prec@(1,5) (80.8%, 97.2%)\u001b[0m\n",
            "[2023-11-09 16:48:24] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.716 Prec@(1,5) (81.3%, 97.3%)\u001b[0m\n",
            "[2023-11-09 16:48:25] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.709 Prec@(1,5) (81.5%, 97.3%)\u001b[0m\n",
            "[2023-11-09 16:48:25] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.712 Prec@(1,5) (81.7%, 97.4%)\u001b[0m\n",
            "[2023-11-09 16:48:26] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.705 Prec@(1,5) (81.9%, 97.4%)\u001b[0m\n",
            "[2023-11-09 16:48:27] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.699 Prec@(1,5) (82.0%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:48:27] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.694 Prec@(1,5) (82.1%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:48:28] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.696 Prec@(1,5) (82.1%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:48:29] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.693 Prec@(1,5) (82.1%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:29] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.692 Prec@(1,5) (82.2%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:30] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.694 Prec@(1,5) (82.1%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:31] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.697 Prec@(1,5) (81.9%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:48:32] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.694 Prec@(1,5) (82.0%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:33] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.692 Prec@(1,5) (82.1%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:33] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.689 Prec@(1,5) (82.1%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:34] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.688 Prec@(1,5) (82.2%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:35] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.686 Prec@(1,5) (82.2%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:36] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.685 Prec@(1,5) (82.2%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:36] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.682 Prec@(1,5) (82.3%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:37] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.682 Prec@(1,5) (82.3%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:38] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.682 Prec@(1,5) (82.3%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:39] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.680 Prec@(1,5) (82.4%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:39] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.679 Prec@(1,5) (82.4%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:48:40] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.676 Prec@(1,5) (82.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:41] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.676 Prec@(1,5) (82.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:42] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.675 Prec@(1,5) (82.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:42] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.674 Prec@(1,5) (82.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:43] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.672 Prec@(1,5) (82.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:44] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.672 Prec@(1,5) (82.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:45] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.673 Prec@(1,5) (82.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:45] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.674 Prec@(1,5) (82.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:46] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.674 Prec@(1,5) (82.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:48:46] \u001b[32mTrain: [  3/10] Final Prec@1 82.4400%\u001b[0m\n",
            "[2023-11-09 16:48:47] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.726 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:48:47] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.528 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:48:47] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.605 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:48] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.603 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:48] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.592 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:48:48] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.589 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:48:48] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.585 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:48:49] \u001b[32mValid: [  3/10] Final Prec@1 85.2300%\u001b[0m\n",
            "[2023-11-09 16:48:49] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
            "[2023-11-09 16:48:50] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.536 Prec@(1,5) (88.5%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:48:50] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.717 Prec@(1,5) (81.1%, 96.6%)\u001b[0m\n",
            "[2023-11-09 16:48:51] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.714 Prec@(1,5) (81.1%, 97.0%)\u001b[0m\n",
            "[2023-11-09 16:48:52] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.707 Prec@(1,5) (81.7%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:48:52] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.705 Prec@(1,5) (81.7%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:53] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.702 Prec@(1,5) (81.6%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:48:54] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.707 Prec@(1,5) (81.5%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:48:54] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.706 Prec@(1,5) (81.5%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:55] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.708 Prec@(1,5) (81.4%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:56] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.712 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:56] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.710 Prec@(1,5) (81.4%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:57] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.712 Prec@(1,5) (81.4%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:58] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.712 Prec@(1,5) (81.4%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:48:59] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.710 Prec@(1,5) (81.4%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:48:59] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.706 Prec@(1,5) (81.5%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:49:00] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.706 Prec@(1,5) (81.5%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:49:01] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.706 Prec@(1,5) (81.5%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:49:02] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.704 Prec@(1,5) (81.6%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:49:02] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.701 Prec@(1,5) (81.6%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:49:03] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.701 Prec@(1,5) (81.6%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:49:04] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.700 Prec@(1,5) (81.7%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:05] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.700 Prec@(1,5) (81.7%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:05] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.699 Prec@(1,5) (81.7%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:06] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.698 Prec@(1,5) (81.7%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:07] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.698 Prec@(1,5) (81.7%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:08] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.695 Prec@(1,5) (81.8%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:08] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.695 Prec@(1,5) (81.8%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:09] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.695 Prec@(1,5) (81.8%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:10] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.695 Prec@(1,5) (81.8%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:11] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.694 Prec@(1,5) (81.8%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:11] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.693 Prec@(1,5) (81.9%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:12] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.693 Prec@(1,5) (81.9%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:12] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.693 Prec@(1,5) (81.9%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:49:12] \u001b[32mTrain: [  4/10] Final Prec@1 81.8650%\u001b[0m\n",
            "[2023-11-09 16:49:14] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.734 Prec@(1,5) (81.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:49:14] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.479 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:49:14] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.548 Prec@(1,5) (85.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:14] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.554 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:15] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.534 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:15] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.536 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:15] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.535 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:15] \u001b[32mValid: [  4/10] Final Prec@1 85.6500%\u001b[0m\n",
            "[2023-11-09 16:49:15] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
            "[2023-11-09 16:49:16] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.610 Prec@(1,5) (87.5%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:49:17] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.718 Prec@(1,5) (81.5%, 96.6%)\u001b[0m\n",
            "[2023-11-09 16:49:18] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.724 Prec@(1,5) (81.3%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:49:18] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.724 Prec@(1,5) (81.6%, 96.2%)\u001b[0m\n",
            "[2023-11-09 16:49:19] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.731 Prec@(1,5) (81.2%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:49:20] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.729 Prec@(1,5) (81.2%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:20] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.733 Prec@(1,5) (80.9%, 95.8%)\u001b[0m\n",
            "[2023-11-09 16:49:21] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.733 Prec@(1,5) (80.8%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:49:22] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.734 Prec@(1,5) (80.8%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:49:22] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.734 Prec@(1,5) (80.8%, 95.8%)\u001b[0m\n",
            "[2023-11-09 16:49:23] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.734 Prec@(1,5) (80.8%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:49:23] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.737 Prec@(1,5) (80.7%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:49:24] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.730 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:24] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.729 Prec@(1,5) (80.9%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:49:25] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.724 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:25] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.724 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:25] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.725 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:26] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.723 Prec@(1,5) (81.1%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:26] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.724 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:26] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.725 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:27] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.724 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:27] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.722 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:27] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.721 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:28] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.721 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:28] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.721 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:28] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.723 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:29] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.723 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:29] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.723 Prec@(1,5) (80.9%, 95.9%)\u001b[0m\n",
            "[2023-11-09 16:49:30] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.723 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:30] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.721 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:30] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.721 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:31] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.722 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:31] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.722 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:49:31] \u001b[32mTrain: [  5/10] Final Prec@1 80.9183%\u001b[0m\n",
            "[2023-11-09 16:49:32] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.517 Prec@(1,5) (83.3%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:49:32] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.374 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:32] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.411 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:32] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.409 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:32] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.397 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:33] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.400 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:33] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.397 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:49:33] \u001b[32mValid: [  5/10] Final Prec@1 88.6800%\u001b[0m\n",
            "[2023-11-09 16:49:33] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
            "[2023-11-09 16:49:34] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.881 Prec@(1,5) (74.0%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:49:34] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.766 Prec@(1,5) (78.7%, 95.3%)\u001b[0m\n",
            "[2023-11-09 16:49:34] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.751 Prec@(1,5) (79.9%, 95.1%)\u001b[0m\n",
            "[2023-11-09 16:49:35] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.746 Prec@(1,5) (80.1%, 95.2%)\u001b[0m\n",
            "[2023-11-09 16:49:35] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.757 Prec@(1,5) (80.0%, 95.2%)\u001b[0m\n",
            "[2023-11-09 16:49:36] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.757 Prec@(1,5) (79.8%, 95.1%)\u001b[0m\n",
            "[2023-11-09 16:49:36] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.759 Prec@(1,5) (79.8%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:36] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.762 Prec@(1,5) (79.9%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:37] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.769 Prec@(1,5) (79.7%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:49:37] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.767 Prec@(1,5) (79.8%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:37] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.772 Prec@(1,5) (79.6%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:49:38] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.770 Prec@(1,5) (79.7%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:49:38] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.763 Prec@(1,5) (79.9%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:38] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.759 Prec@(1,5) (80.0%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:39] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.761 Prec@(1,5) (79.9%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:39] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.761 Prec@(1,5) (79.9%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:39] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.760 Prec@(1,5) (80.0%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:40] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.759 Prec@(1,5) (80.0%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:40] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.759 Prec@(1,5) (80.0%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:40] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.757 Prec@(1,5) (80.1%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:41] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.756 Prec@(1,5) (80.1%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:41] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.753 Prec@(1,5) (80.1%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:42] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.753 Prec@(1,5) (80.2%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:42] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.752 Prec@(1,5) (80.3%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:42] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.750 Prec@(1,5) (80.3%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:43] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.751 Prec@(1,5) (80.3%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:49:43] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.751 Prec@(1,5) (80.2%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:43] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.750 Prec@(1,5) (80.3%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:44] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.748 Prec@(1,5) (80.3%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:44] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.748 Prec@(1,5) (80.3%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:44] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.750 Prec@(1,5) (80.3%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:45] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.749 Prec@(1,5) (80.2%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:45] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.749 Prec@(1,5) (80.2%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:49:45] \u001b[32mTrain: [  6/10] Final Prec@1 80.2417%\u001b[0m\n",
            "[2023-11-09 16:49:46] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.409 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:49:46] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.310 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:49:46] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.345 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:46] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.344 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:47] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.332 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:47] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.335 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:47] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.333 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:49:47] \u001b[32mValid: [  6/10] Final Prec@1 89.9500%\u001b[0m\n",
            "[2023-11-09 16:49:47] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
            "[2023-11-09 16:49:48] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.812 Prec@(1,5) (77.1%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:49:48] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.781 Prec@(1,5) (79.2%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:49] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.772 Prec@(1,5) (79.6%, 94.3%)\u001b[0m\n",
            "[2023-11-09 16:49:49] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.765 Prec@(1,5) (79.7%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:49:49] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.771 Prec@(1,5) (79.5%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:50] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.776 Prec@(1,5) (79.2%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:50] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.779 Prec@(1,5) (79.2%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:50] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.778 Prec@(1,5) (79.3%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:51] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.779 Prec@(1,5) (79.3%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:51] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.776 Prec@(1,5) (79.4%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:52] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.772 Prec@(1,5) (79.5%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:52] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.773 Prec@(1,5) (79.4%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:52] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.777 Prec@(1,5) (79.3%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:53] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.776 Prec@(1,5) (79.3%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:53] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.776 Prec@(1,5) (79.3%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:53] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.779 Prec@(1,5) (79.1%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:54] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.781 Prec@(1,5) (79.1%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:54] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.782 Prec@(1,5) (79.1%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:54] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.781 Prec@(1,5) (79.1%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:55] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.782 Prec@(1,5) (79.0%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:55] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.783 Prec@(1,5) (79.1%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:55] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.783 Prec@(1,5) (79.0%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:56] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.782 Prec@(1,5) (79.1%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:56] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.783 Prec@(1,5) (79.0%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:56] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.783 Prec@(1,5) (79.0%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:49:57] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.785 Prec@(1,5) (79.0%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:57] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.786 Prec@(1,5) (78.9%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:58] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.786 Prec@(1,5) (78.9%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:58] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.786 Prec@(1,5) (78.9%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:58] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.786 Prec@(1,5) (78.9%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:59] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.789 Prec@(1,5) (78.9%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:59] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.789 Prec@(1,5) (78.8%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:59] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.789 Prec@(1,5) (78.8%, 94.0%)\u001b[0m\n",
            "[2023-11-09 16:49:59] \u001b[32mTrain: [  7/10] Final Prec@1 78.8400%\u001b[0m\n",
            "[2023-11-09 16:50:00] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.348 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:50:00] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.283 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:00] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.344 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:01] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.343 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:01] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.329 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:01] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.332 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:01] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.330 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:01] \u001b[32mValid: [  7/10] Final Prec@1 90.2500%\u001b[0m\n",
            "[2023-11-09 16:50:01] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
            "[2023-11-09 16:50:02] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.902 Prec@(1,5) (78.1%, 90.6%)\u001b[0m\n",
            "[2023-11-09 16:50:02] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.786 Prec@(1,5) (79.0%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:03] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.789 Prec@(1,5) (79.2%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:03] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.808 Prec@(1,5) (78.7%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:03] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.807 Prec@(1,5) (78.6%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:04] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.812 Prec@(1,5) (78.4%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:04] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.813 Prec@(1,5) (78.3%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:04] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.814 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:05] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.814 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:05] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.811 Prec@(1,5) (78.6%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:06] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.809 Prec@(1,5) (78.6%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:06] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.805 Prec@(1,5) (78.8%, 93.5%)\u001b[0m\n",
            "[2023-11-09 16:50:06] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.807 Prec@(1,5) (78.7%, 93.5%)\u001b[0m\n",
            "[2023-11-09 16:50:07] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.808 Prec@(1,5) (78.6%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:07] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.809 Prec@(1,5) (78.6%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:07] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.806 Prec@(1,5) (78.7%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:08] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.809 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:08] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.809 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:08] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.809 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:09] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.811 Prec@(1,5) (78.4%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:09] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.809 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:09] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.808 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:10] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.806 Prec@(1,5) (78.6%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:50:10] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.806 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:10] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.809 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:11] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.809 Prec@(1,5) (78.5%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:50:11] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.809 Prec@(1,5) (78.4%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:11] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.812 Prec@(1,5) (78.3%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:12] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.811 Prec@(1,5) (78.4%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:12] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.812 Prec@(1,5) (78.3%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:50:13] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.812 Prec@(1,5) (78.3%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:13] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.811 Prec@(1,5) (78.4%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:13] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.811 Prec@(1,5) (78.4%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:50:13] \u001b[32mTrain: [  8/10] Final Prec@1 78.3933%\u001b[0m\n",
            "[2023-11-09 16:50:14] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.408 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:50:14] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.305 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:14] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.340 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:15] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.337 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:15] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.324 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:15] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.327 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:15] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.325 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:15] \u001b[32mValid: [  8/10] Final Prec@1 90.3900%\u001b[0m\n",
            "[2023-11-09 16:50:15] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
            "[2023-11-09 16:50:16] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.721 Prec@(1,5) (79.2%, 92.7%)\u001b[0m\n",
            "[2023-11-09 16:50:16] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.836 Prec@(1,5) (77.6%, 92.7%)\u001b[0m\n",
            "[2023-11-09 16:50:17] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.830 Prec@(1,5) (77.6%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:50:17] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.838 Prec@(1,5) (77.7%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:17] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.846 Prec@(1,5) (77.4%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:50:18] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.840 Prec@(1,5) (77.6%, 92.7%)\u001b[0m\n",
            "[2023-11-09 16:50:18] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.849 Prec@(1,5) (77.4%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:50:18] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.852 Prec@(1,5) (77.2%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:19] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.857 Prec@(1,5) (77.0%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:19] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.859 Prec@(1,5) (77.1%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:19] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.859 Prec@(1,5) (77.0%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:20] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.860 Prec@(1,5) (76.9%, 92.2%)\u001b[0m\n",
            "[2023-11-09 16:50:20] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.863 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:21] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.863 Prec@(1,5) (76.9%, 92.2%)\u001b[0m\n",
            "[2023-11-09 16:50:21] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.859 Prec@(1,5) (77.0%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:21] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.859 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:22] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.860 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:22] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.860 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:22] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.862 Prec@(1,5) (76.8%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:23] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.865 Prec@(1,5) (76.8%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:23] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.864 Prec@(1,5) (76.8%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:23] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.865 Prec@(1,5) (76.8%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:24] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.864 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:24] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.863 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:24] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.864 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:25] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.861 Prec@(1,5) (76.9%, 92.3%)\u001b[0m\n",
            "[2023-11-09 16:50:25] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:25] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:26] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.860 Prec@(1,5) (76.9%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:26] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:26] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:27] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:27] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.858 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:50:27] \u001b[32mTrain: [  9/10] Final Prec@1 76.9900%\u001b[0m\n",
            "[2023-11-09 16:50:28] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.404 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:50:28] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.271 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:28] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.314 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:29] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.311 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:29] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.300 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:29] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.301 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:29] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.299 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:29] \u001b[32mValid: [  9/10] Final Prec@1 90.7900%\u001b[0m\n",
            "[2023-11-09 16:50:29] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
            "[2023-11-09 16:50:30] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.967 Prec@(1,5) (72.9%, 92.7%)\u001b[0m\n",
            "[2023-11-09 16:50:30] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.886 Prec@(1,5) (75.7%, 92.1%)\u001b[0m\n",
            "[2023-11-09 16:50:31] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.900 Prec@(1,5) (75.7%, 91.9%)\u001b[0m\n",
            "[2023-11-09 16:50:31] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.920 Prec@(1,5) (75.3%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:32] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.908 Prec@(1,5) (75.6%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:32] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.911 Prec@(1,5) (75.5%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:50:32] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.907 Prec@(1,5) (75.7%, 91.5%)\u001b[0m\n",
            "[2023-11-09 16:50:33] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.909 Prec@(1,5) (75.7%, 91.6%)\u001b[0m\n",
            "[2023-11-09 16:50:33] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.913 Prec@(1,5) (75.5%, 91.5%)\u001b[0m\n",
            "[2023-11-09 16:50:33] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.919 Prec@(1,5) (75.3%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:34] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.915 Prec@(1,5) (75.4%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:34] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.915 Prec@(1,5) (75.4%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:34] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.918 Prec@(1,5) (75.4%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:35] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.918 Prec@(1,5) (75.4%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:35] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.918 Prec@(1,5) (75.5%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:35] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.917 Prec@(1,5) (75.4%, 91.5%)\u001b[0m\n",
            "[2023-11-09 16:50:36] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.914 Prec@(1,5) (75.5%, 91.5%)\u001b[0m\n",
            "[2023-11-09 16:50:36] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.915 Prec@(1,5) (75.4%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:36] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.913 Prec@(1,5) (75.5%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:37] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.913 Prec@(1,5) (75.5%, 91.5%)\u001b[0m\n",
            "[2023-11-09 16:50:37] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.911 Prec@(1,5) (75.6%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:37] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.911 Prec@(1,5) (75.5%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:38] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.911 Prec@(1,5) (75.5%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:38] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.912 Prec@(1,5) (75.5%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:50:39] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.913 Prec@(1,5) (75.5%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:50:39] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.911 Prec@(1,5) (75.5%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:39] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.910 Prec@(1,5) (75.6%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:40] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.910 Prec@(1,5) (75.6%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:40] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.912 Prec@(1,5) (75.5%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:40] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.910 Prec@(1,5) (75.6%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:41] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.908 Prec@(1,5) (75.7%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:41] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.907 Prec@(1,5) (75.7%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:41] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.907 Prec@(1,5) (75.7%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:50:41] \u001b[32mTrain: [ 10/10] Final Prec@1 75.6717%\u001b[0m\n",
            "[2023-11-09 16:50:42] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.341 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.259 Prec@(1,5) (92.0%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.300 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.295 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.284 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.285 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.284 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mValid: [ 10/10] Final Prec@1 91.2600%\u001b[0m\n",
            "Final best Prec@1 = 91.2600%\n",
            "[0.9095000198364258, 0.9116000217437744, 0.9126000207901]\n",
            "./checkpoints/fashionMNIST/lambd=4/\n",
            "[2023-11-09 16:50:43] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv3x3', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv3x3', 'reduce_n3_p1': 'dilconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'maxpool', 'reduce_n4_p1': 'dilconv5x5', 'reduce_n4_p2': 'avgpool', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv3x3', 'reduce_n5_p1': 'maxpool', 'reduce_n5_p2': 'avgpool', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv5x5', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\u001b[0m\n",
            "[2023-11-09 16:50:43] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
            "[2023-11-09 16:50:44] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.314 Prec@(1,5) (7.3%, 43.8%)\u001b[0m\n",
            "[2023-11-09 16:50:45] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.870 Prec@(1,5) (19.2%, 70.1%)\u001b[0m\n",
            "[2023-11-09 16:50:45] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.385 Prec@(1,5) (31.4%, 82.5%)\u001b[0m\n",
            "[2023-11-09 16:50:45] \u001b[32mTrain: [  1/10] Step 060/624 Loss 2.090 Prec@(1,5) (40.5%, 87.5%)\u001b[0m\n",
            "[2023-11-09 16:50:46] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.914 Prec@(1,5) (45.6%, 90.1%)\u001b[0m\n",
            "[2023-11-09 16:50:46] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.797 Prec@(1,5) (49.1%, 91.8%)\u001b[0m\n",
            "[2023-11-09 16:50:46] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.691 Prec@(1,5) (52.4%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:50:47] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.615 Prec@(1,5) (54.9%, 93.8%)\u001b[0m\n",
            "[2023-11-09 16:50:47] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.555 Prec@(1,5) (56.6%, 94.5%)\u001b[0m\n",
            "[2023-11-09 16:50:47] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.501 Prec@(1,5) (58.1%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:50:48] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.450 Prec@(1,5) (59.7%, 95.4%)\u001b[0m\n",
            "[2023-11-09 16:50:48] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.403 Prec@(1,5) (61.1%, 95.7%)\u001b[0m\n",
            "[2023-11-09 16:50:48] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.361 Prec@(1,5) (62.4%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:50:49] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.325 Prec@(1,5) (63.4%, 96.3%)\u001b[0m\n",
            "[2023-11-09 16:50:49] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.292 Prec@(1,5) (64.4%, 96.5%)\u001b[0m\n",
            "[2023-11-09 16:50:49] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.264 Prec@(1,5) (65.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:50:50] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.240 Prec@(1,5) (66.0%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:50:50] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.214 Prec@(1,5) (66.7%, 97.0%)\u001b[0m\n",
            "[2023-11-09 16:50:50] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.188 Prec@(1,5) (67.4%, 97.1%)\u001b[0m\n",
            "[2023-11-09 16:50:51] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.166 Prec@(1,5) (68.1%, 97.3%)\u001b[0m\n",
            "[2023-11-09 16:50:51] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.148 Prec@(1,5) (68.6%, 97.4%)\u001b[0m\n",
            "[2023-11-09 16:50:51] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.132 Prec@(1,5) (69.0%, 97.5%)\u001b[0m\n",
            "[2023-11-09 16:50:52] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.115 Prec@(1,5) (69.5%, 97.6%)\u001b[0m\n",
            "[2023-11-09 16:50:52] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.101 Prec@(1,5) (69.9%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:50:52] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.086 Prec@(1,5) (70.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:50:53] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.071 Prec@(1,5) (70.9%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:50:53] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.056 Prec@(1,5) (71.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:50:53] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.042 Prec@(1,5) (71.7%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:50:54] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.030 Prec@(1,5) (72.0%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:50:54] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.019 Prec@(1,5) (72.3%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:50:54] \u001b[32mTrain: [  1/10] Step 600/624 Loss 1.006 Prec@(1,5) (72.6%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:50:55] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.996 Prec@(1,5) (72.9%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:50:55] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.995 Prec@(1,5) (73.0%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:50:55] \u001b[32mTrain: [  1/10] Final Prec@1 72.9933%\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.777 Prec@(1,5) (79.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.652 Prec@(1,5) (82.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.701 Prec@(1,5) (81.8%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.711 Prec@(1,5) (81.6%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.700 Prec@(1,5) (81.7%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.708 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
            "[2023-11-09 16:50:56] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.704 Prec@(1,5) (81.6%, 99.5%)\u001b[0m\n",
            "[2023-11-09 16:50:57] \u001b[32mValid: [  1/10] Final Prec@1 81.5900%\u001b[0m\n",
            "[2023-11-09 16:50:57] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
            "[2023-11-09 16:50:58] \u001b[32mTrain: [  2/10] Step 000/624 Loss 1.068 Prec@(1,5) (71.9%, 99.0%)\u001b[0m\n",
            "[2023-11-09 16:50:58] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.815 Prec@(1,5) (78.7%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:50:58] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.792 Prec@(1,5) (79.5%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:50:59] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.782 Prec@(1,5) (80.0%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:50:59] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.786 Prec@(1,5) (79.8%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:50:59] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.779 Prec@(1,5) (80.0%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:51:00] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.771 Prec@(1,5) (80.0%, 98.0%)\u001b[0m\n",
            "[2023-11-09 16:51:00] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.761 Prec@(1,5) (80.2%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:51:01] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.763 Prec@(1,5) (80.1%, 98.1%)\u001b[0m\n",
            "[2023-11-09 16:51:01] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.762 Prec@(1,5) (80.0%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:51:01] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.757 Prec@(1,5) (80.1%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:51:02] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.754 Prec@(1,5) (80.1%, 98.2%)\u001b[0m\n",
            "[2023-11-09 16:51:02] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.746 Prec@(1,5) (80.3%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:02] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.741 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:03] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.739 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:03] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.741 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:03] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.741 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:04] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.737 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:04] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.737 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:04] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.736 Prec@(1,5) (80.4%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:05] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.732 Prec@(1,5) (80.5%, 98.3%)\u001b[0m\n",
            "[2023-11-09 16:51:05] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.729 Prec@(1,5) (80.6%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:06] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.727 Prec@(1,5) (80.7%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:06] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.729 Prec@(1,5) (80.6%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:06] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.728 Prec@(1,5) (80.7%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:07] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.725 Prec@(1,5) (80.8%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:07] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.724 Prec@(1,5) (80.8%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:07] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.721 Prec@(1,5) (80.9%, 98.4%)\u001b[0m\n",
            "[2023-11-09 16:51:08] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.718 Prec@(1,5) (81.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:51:08] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.717 Prec@(1,5) (81.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:51:08] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.715 Prec@(1,5) (81.0%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:51:09] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.714 Prec@(1,5) (81.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:51:09] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.714 Prec@(1,5) (81.1%, 98.5%)\u001b[0m\n",
            "[2023-11-09 16:51:09] \u001b[32mTrain: [  2/10] Final Prec@1 81.0550%\u001b[0m\n",
            "[2023-11-09 16:51:10] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.661 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:51:10] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.524 Prec@(1,5) (85.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:10] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.563 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:51:10] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.576 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:51:10] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.548 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:51:11] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.556 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:51:11] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.554 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
            "[2023-11-09 16:51:11] \u001b[32mValid: [  2/10] Final Prec@1 84.2800%\u001b[0m\n",
            "[2023-11-09 16:51:11] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
            "[2023-11-09 16:51:12] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.572 Prec@(1,5) (87.5%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:12] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.721 Prec@(1,5) (80.1%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:13] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.717 Prec@(1,5) (80.4%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:13] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.717 Prec@(1,5) (80.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:13] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.704 Prec@(1,5) (81.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:14] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.708 Prec@(1,5) (81.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:14] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.705 Prec@(1,5) (81.4%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:14] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.704 Prec@(1,5) (81.3%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:15] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.704 Prec@(1,5) (81.3%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:15] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.701 Prec@(1,5) (81.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:15] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.702 Prec@(1,5) (81.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:16] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.705 Prec@(1,5) (81.2%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:16] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.706 Prec@(1,5) (81.2%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:17] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.704 Prec@(1,5) (81.3%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:17] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.705 Prec@(1,5) (81.3%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:17] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.702 Prec@(1,5) (81.4%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:18] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.704 Prec@(1,5) (81.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:18] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.704 Prec@(1,5) (81.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:18] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.701 Prec@(1,5) (81.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:19] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.702 Prec@(1,5) (81.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:19] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.703 Prec@(1,5) (81.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:20] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.704 Prec@(1,5) (81.4%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:20] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.702 Prec@(1,5) (81.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:20] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.701 Prec@(1,5) (81.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:21] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.702 Prec@(1,5) (81.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:21] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.702 Prec@(1,5) (81.5%, 97.7%)\u001b[0m\n",
            "[2023-11-09 16:51:22] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.700 Prec@(1,5) (81.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:22] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.698 Prec@(1,5) (81.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:22] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.699 Prec@(1,5) (81.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:23] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.699 Prec@(1,5) (81.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:23] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.697 Prec@(1,5) (81.6%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:23] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.695 Prec@(1,5) (81.7%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:23] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.695 Prec@(1,5) (81.7%, 97.8%)\u001b[0m\n",
            "[2023-11-09 16:51:24] \u001b[32mTrain: [  3/10] Final Prec@1 81.6583%\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.659 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.536 Prec@(1,5) (84.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.568 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.566 Prec@(1,5) (84.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.553 Prec@(1,5) (84.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.557 Prec@(1,5) (84.2%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.553 Prec@(1,5) (84.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mValid: [  3/10] Final Prec@1 84.3100%\u001b[0m\n",
            "[2023-11-09 16:51:25] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
            "[2023-11-09 16:51:26] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.671 Prec@(1,5) (83.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:27] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.699 Prec@(1,5) (81.6%, 96.5%)\u001b[0m\n",
            "[2023-11-09 16:51:27] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.706 Prec@(1,5) (81.8%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:27] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.698 Prec@(1,5) (82.2%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:28] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.698 Prec@(1,5) (82.0%, 97.0%)\u001b[0m\n",
            "[2023-11-09 16:51:28] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.709 Prec@(1,5) (81.6%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:29] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.717 Prec@(1,5) (81.4%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:29] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.723 Prec@(1,5) (81.2%, 96.6%)\u001b[0m\n",
            "[2023-11-09 16:51:29] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.714 Prec@(1,5) (81.4%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:30] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.718 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:30] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.717 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:30] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.719 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:31] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.718 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:31] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.720 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:32] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.718 Prec@(1,5) (81.3%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:32] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.721 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:32] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.717 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:33] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.718 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:33] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.716 Prec@(1,5) (81.3%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:33] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.715 Prec@(1,5) (81.2%, 96.7%)\u001b[0m\n",
            "[2023-11-09 16:51:34] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.714 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:34] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.716 Prec@(1,5) (81.2%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:34] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.715 Prec@(1,5) (81.2%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:35] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.715 Prec@(1,5) (81.2%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:35] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.714 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:36] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.714 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:36] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.713 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:36] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.713 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:37] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.714 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:37] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.712 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:38] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.710 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:38] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.710 Prec@(1,5) (81.3%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:38] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.710 Prec@(1,5) (81.4%, 96.8%)\u001b[0m\n",
            "[2023-11-09 16:51:38] \u001b[32mTrain: [  4/10] Final Prec@1 81.3650%\u001b[0m\n",
            "[2023-11-09 16:51:39] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.492 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:51:39] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.439 Prec@(1,5) (86.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:39] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.477 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:39] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.479 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:40] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.459 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:40] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.468 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:40] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.466 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:40] \u001b[32mValid: [  4/10] Final Prec@1 86.5500%\u001b[0m\n",
            "[2023-11-09 16:51:40] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
            "[2023-11-09 16:51:41] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.833 Prec@(1,5) (78.1%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:51:41] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.760 Prec@(1,5) (81.5%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:42] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.766 Prec@(1,5) (80.2%, 95.6%)\u001b[0m\n",
            "[2023-11-09 16:51:42] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.732 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:42] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.726 Prec@(1,5) (81.2%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:43] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.736 Prec@(1,5) (81.1%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:43] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.734 Prec@(1,5) (81.1%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:44] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.735 Prec@(1,5) (81.0%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:44] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.739 Prec@(1,5) (80.8%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:44] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.738 Prec@(1,5) (80.6%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:45] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.731 Prec@(1,5) (80.8%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:45] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.728 Prec@(1,5) (80.9%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:45] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.729 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:46] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.729 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:46] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.728 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:46] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.728 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:47] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.726 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:47] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.725 Prec@(1,5) (80.9%, 96.1%)\u001b[0m\n",
            "[2023-11-09 16:51:48] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.727 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:48] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.726 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:48] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.724 Prec@(1,5) (81.0%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:49] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.727 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:49] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.727 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:49] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.726 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:50] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.727 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:50] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.725 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:51] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.725 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:51] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.723 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:51] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.724 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:52] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.724 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:52] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.724 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:52] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.726 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:52] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.725 Prec@(1,5) (80.9%, 96.0%)\u001b[0m\n",
            "[2023-11-09 16:51:53] \u001b[32mTrain: [  5/10] Final Prec@1 80.8850%\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.619 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.457 Prec@(1,5) (86.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.504 Prec@(1,5) (85.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.500 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.484 Prec@(1,5) (86.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.486 Prec@(1,5) (85.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.481 Prec@(1,5) (86.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mValid: [  5/10] Final Prec@1 85.9800%\u001b[0m\n",
            "[2023-11-09 16:51:54] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
            "[2023-11-09 16:51:55] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.836 Prec@(1,5) (79.2%, 96.9%)\u001b[0m\n",
            "[2023-11-09 16:51:56] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.808 Prec@(1,5) (78.5%, 96.2%)\u001b[0m\n",
            "[2023-11-09 16:51:56] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.796 Prec@(1,5) (79.0%, 95.5%)\u001b[0m\n",
            "[2023-11-09 16:51:56] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.783 Prec@(1,5) (79.4%, 95.2%)\u001b[0m\n",
            "[2023-11-09 16:51:57] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.789 Prec@(1,5) (79.3%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:51:57] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.780 Prec@(1,5) (79.6%, 94.8%)\u001b[0m\n",
            "[2023-11-09 16:51:57] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.771 Prec@(1,5) (79.7%, 94.9%)\u001b[0m\n",
            "[2023-11-09 16:51:58] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.768 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:51:58] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.767 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:51:58] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.766 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:51:59] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.772 Prec@(1,5) (79.4%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:51:59] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.768 Prec@(1,5) (79.6%, 95.1%)\u001b[0m\n",
            "[2023-11-09 16:51:59] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.772 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:00] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.769 Prec@(1,5) (79.6%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:00] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.771 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:00] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.772 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:01] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.771 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:01] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.772 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:02] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.771 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:02] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.769 Prec@(1,5) (79.6%, 95.1%)\u001b[0m\n",
            "[2023-11-09 16:52:02] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.771 Prec@(1,5) (79.5%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:03] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.766 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:03] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.766 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:03] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.764 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:04] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.766 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:04] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.765 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:04] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.765 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:05] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.765 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:05] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.763 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:05] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.761 Prec@(1,5) (79.8%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:06] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.761 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:06] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.760 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:06] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.760 Prec@(1,5) (79.7%, 95.0%)\u001b[0m\n",
            "[2023-11-09 16:52:06] \u001b[32mTrain: [  6/10] Final Prec@1 79.7383%\u001b[0m\n",
            "[2023-11-09 16:52:07] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.547 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:52:07] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.381 Prec@(1,5) (88.4%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:52:07] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.421 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:52:08] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.425 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:52:08] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.414 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:08] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.417 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:08] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.415 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:08] \u001b[32mValid: [  6/10] Final Prec@1 87.8400%\u001b[0m\n",
            "[2023-11-09 16:52:08] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
            "[2023-11-09 16:52:09] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.695 Prec@(1,5) (85.4%, 95.8%)\u001b[0m\n",
            "[2023-11-09 16:52:09] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.811 Prec@(1,5) (78.4%, 94.4%)\u001b[0m\n",
            "[2023-11-09 16:52:10] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.811 Prec@(1,5) (78.7%, 94.3%)\u001b[0m\n",
            "[2023-11-09 16:52:10] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.806 Prec@(1,5) (78.7%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:10] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.815 Prec@(1,5) (78.4%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:11] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.800 Prec@(1,5) (78.8%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:11] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.803 Prec@(1,5) (78.8%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:11] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.803 Prec@(1,5) (78.8%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:12] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.808 Prec@(1,5) (78.5%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:12] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.805 Prec@(1,5) (78.6%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:12] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.801 Prec@(1,5) (78.7%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:13] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.799 Prec@(1,5) (78.7%, 94.3%)\u001b[0m\n",
            "[2023-11-09 16:52:13] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.796 Prec@(1,5) (78.8%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:13] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.796 Prec@(1,5) (78.8%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:14] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.795 Prec@(1,5) (78.8%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:14] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.796 Prec@(1,5) (78.9%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:14] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.794 Prec@(1,5) (78.9%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:15] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.793 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:15] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.792 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:15] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.792 Prec@(1,5) (78.9%, 94.1%)\u001b[0m\n",
            "[2023-11-09 16:52:16] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.794 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:16] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.794 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:16] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.794 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:17] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.793 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:17] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.793 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:18] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.793 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:18] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.794 Prec@(1,5) (78.8%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:18] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.791 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:19] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.791 Prec@(1,5) (79.0%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:19] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.791 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:19] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.790 Prec@(1,5) (79.0%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:20] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.791 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:20] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.791 Prec@(1,5) (78.9%, 94.2%)\u001b[0m\n",
            "[2023-11-09 16:52:20] \u001b[32mTrain: [  7/10] Final Prec@1 78.9367%\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.449 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.303 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.331 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.330 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.315 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.318 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:21] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.317 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:22] \u001b[32mValid: [  7/10] Final Prec@1 90.1400%\u001b[0m\n",
            "[2023-11-09 16:52:22] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
            "[2023-11-09 16:52:23] \u001b[32mTrain: [  8/10] Step 000/624 Loss 1.073 Prec@(1,5) (72.9%, 93.8%)\u001b[0m\n",
            "[2023-11-09 16:52:23] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.840 Prec@(1,5) (78.8%, 92.9%)\u001b[0m\n",
            "[2023-11-09 16:52:23] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.836 Prec@(1,5) (78.6%, 93.5%)\u001b[0m\n",
            "[2023-11-09 16:52:24] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.841 Prec@(1,5) (78.3%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:52:24] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.834 Prec@(1,5) (78.3%, 93.4%)\u001b[0m\n",
            "[2023-11-09 16:52:24] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.839 Prec@(1,5) (78.0%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:52:25] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.841 Prec@(1,5) (77.9%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:25] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.841 Prec@(1,5) (77.7%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:52:25] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.846 Prec@(1,5) (77.5%, 92.9%)\u001b[0m\n",
            "[2023-11-09 16:52:26] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.849 Prec@(1,5) (77.4%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:52:26] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.847 Prec@(1,5) (77.4%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:52:27] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.845 Prec@(1,5) (77.5%, 92.9%)\u001b[0m\n",
            "[2023-11-09 16:52:27] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.839 Prec@(1,5) (77.7%, 92.9%)\u001b[0m\n",
            "[2023-11-09 16:52:27] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.833 Prec@(1,5) (77.8%, 92.9%)\u001b[0m\n",
            "[2023-11-09 16:52:28] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.832 Prec@(1,5) (77.7%, 92.9%)\u001b[0m\n",
            "[2023-11-09 16:52:28] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.831 Prec@(1,5) (77.8%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:52:28] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.830 Prec@(1,5) (77.8%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:52:29] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.830 Prec@(1,5) (77.8%, 93.0%)\u001b[0m\n",
            "[2023-11-09 16:52:29] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.831 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:30] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.834 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:30] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.834 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:30] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.834 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:31] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.835 Prec@(1,5) (77.7%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:31] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.833 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:31] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.833 Prec@(1,5) (77.7%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:32] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.832 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:32] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.829 Prec@(1,5) (77.8%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:32] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.829 Prec@(1,5) (77.9%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:33] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.828 Prec@(1,5) (77.9%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:33] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.828 Prec@(1,5) (77.9%, 93.2%)\u001b[0m\n",
            "[2023-11-09 16:52:33] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.827 Prec@(1,5) (77.9%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:34] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.827 Prec@(1,5) (77.9%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:34] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.827 Prec@(1,5) (77.9%, 93.1%)\u001b[0m\n",
            "[2023-11-09 16:52:34] \u001b[32mTrain: [  8/10] Final Prec@1 77.8917%\u001b[0m\n",
            "[2023-11-09 16:52:35] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.432 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:52:35] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.339 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:35] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.367 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:52:35] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.367 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
            "[2023-11-09 16:52:36] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.354 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:36] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.358 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:36] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.356 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:36] \u001b[32mValid: [  8/10] Final Prec@1 89.3700%\u001b[0m\n",
            "[2023-11-09 16:52:36] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
            "[2023-11-09 16:52:37] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.678 Prec@(1,5) (83.3%, 97.9%)\u001b[0m\n",
            "[2023-11-09 16:52:37] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.814 Prec@(1,5) (77.1%, 93.3%)\u001b[0m\n",
            "[2023-11-09 16:52:38] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.843 Prec@(1,5) (76.3%, 92.8%)\u001b[0m\n",
            "[2023-11-09 16:52:38] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.839 Prec@(1,5) (77.1%, 92.7%)\u001b[0m\n",
            "[2023-11-09 16:52:38] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.851 Prec@(1,5) (77.1%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:39] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.847 Prec@(1,5) (77.1%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:52:39] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.848 Prec@(1,5) (77.1%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:52:39] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.849 Prec@(1,5) (77.1%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:52:40] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.852 Prec@(1,5) (77.1%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:52:40] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.846 Prec@(1,5) (77.2%, 92.6%)\u001b[0m\n",
            "[2023-11-09 16:52:40] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.851 Prec@(1,5) (77.1%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:41] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.851 Prec@(1,5) (77.2%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:41] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.854 Prec@(1,5) (77.1%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:41] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.856 Prec@(1,5) (77.1%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:42] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.856 Prec@(1,5) (77.0%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:42] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.854 Prec@(1,5) (77.1%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:43] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.856 Prec@(1,5) (77.0%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:43] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.855 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:43] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.857 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:44] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.857 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:44] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.857 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:44] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.860 Prec@(1,5) (76.9%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:45] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.861 Prec@(1,5) (76.9%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:45] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.859 Prec@(1,5) (76.9%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:46] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.860 Prec@(1,5) (76.9%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:46] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:46] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.858 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:47] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:47] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.857 Prec@(1,5) (77.0%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:47] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.858 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:48] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.859 Prec@(1,5) (77.0%, 92.4%)\u001b[0m\n",
            "[2023-11-09 16:52:48] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.859 Prec@(1,5) (77.0%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:48] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.859 Prec@(1,5) (77.0%, 92.5%)\u001b[0m\n",
            "[2023-11-09 16:52:48] \u001b[32mTrain: [  9/10] Final Prec@1 76.9900%\u001b[0m\n",
            "[2023-11-09 16:52:49] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.405 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:52:49] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.304 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.331 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.330 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.316 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.320 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.318 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mValid: [  9/10] Final Prec@1 90.2000%\u001b[0m\n",
            "[2023-11-09 16:52:50] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
            "[2023-11-09 16:52:51] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.912 Prec@(1,5) (76.0%, 90.6%)\u001b[0m\n",
            "[2023-11-09 16:52:51] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.893 Prec@(1,5) (75.9%, 90.8%)\u001b[0m\n",
            "[2023-11-09 16:52:52] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.901 Prec@(1,5) (76.4%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:52] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.902 Prec@(1,5) (76.1%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:52:53] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.906 Prec@(1,5) (75.9%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:53] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.904 Prec@(1,5) (75.8%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:52:53] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.900 Prec@(1,5) (75.8%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:52:54] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.899 Prec@(1,5) (75.9%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:52:54] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.893 Prec@(1,5) (76.1%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:54] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.897 Prec@(1,5) (76.0%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:55] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.897 Prec@(1,5) (76.0%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:55] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.900 Prec@(1,5) (75.9%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:55] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.901 Prec@(1,5) (75.9%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:56] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.899 Prec@(1,5) (75.9%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:56] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.901 Prec@(1,5) (75.9%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:57] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.898 Prec@(1,5) (76.0%, 91.5%)\u001b[0m\n",
            "[2023-11-09 16:52:57] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.903 Prec@(1,5) (75.8%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:57] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.905 Prec@(1,5) (75.7%, 91.4%)\u001b[0m\n",
            "[2023-11-09 16:52:58] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.906 Prec@(1,5) (75.7%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:52:58] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.909 Prec@(1,5) (75.6%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:52:58] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.911 Prec@(1,5) (75.5%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:52:59] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.911 Prec@(1,5) (75.5%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:52:59] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.909 Prec@(1,5) (75.6%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:52:59] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.909 Prec@(1,5) (75.6%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:53:00] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.906 Prec@(1,5) (75.6%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:53:00] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.908 Prec@(1,5) (75.6%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:53:01] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.907 Prec@(1,5) (75.6%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:53:01] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.906 Prec@(1,5) (75.6%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:53:01] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.905 Prec@(1,5) (75.6%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:53:02] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.906 Prec@(1,5) (75.5%, 91.2%)\u001b[0m\n",
            "[2023-11-09 16:53:02] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.905 Prec@(1,5) (75.6%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:53:02] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.904 Prec@(1,5) (75.6%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:53:02] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.905 Prec@(1,5) (75.6%, 91.3%)\u001b[0m\n",
            "[2023-11-09 16:53:02] \u001b[32mTrain: [ 10/10] Final Prec@1 75.5817%\u001b[0m\n",
            "[2023-11-09 16:53:03] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.433 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.302 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.333 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.332 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.319 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.322 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.320 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:53:04] \u001b[32mValid: [ 10/10] Final Prec@1 90.2900%\u001b[0m\n",
            "Final best Prec@1 = 90.2900%\n",
            "[0.9095000198364258, 0.9116000217437744, 0.9126000207901, 0.9029000175476074]\n"
          ]
        }
      ],
      "source": [
        "from retrain import train, validate, fixed_arch\n",
        "# reload(train)\n",
        "\n",
        "config = {\n",
        "'layers' : layers,\n",
        "'batch_size' : batch_size,\n",
        "'log_frequency' : log_frequency,\n",
        "'epochs' : 10,\n",
        "'aux_weight' : 0.4,\n",
        "'drop_path_prob' : 0.1,\n",
        "'workers' : 4,\n",
        "'grad_clip' : 5.,\n",
        "'save_folder' : \"./checkpoints/fashionMNIST/\",\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset_train, dataset_valid = datasets.get_dataset(\"fashionmnist\", cutout_length=16)\n",
        "\n",
        "best_top1s = []\n",
        "for lambd in [1, 2, 3, 4]:\n",
        "    if lambd == 0:\n",
        "        folder = config['save_folder'] + \"optimal/\"\n",
        "    else:\n",
        "        folder = config['save_folder'] + f\"lambd={lambd}/\"\n",
        "    print(folder)\n",
        "    with fixed_arch(folder + 'arc.json'):\n",
        "    # with fixed_arch(args.save_folder + \"/arc.json\"):\n",
        "        model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "    criterion.to(device)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'], eta_min=1E-6)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset_train,\n",
        "                                            batch_size=config['batch_size'],\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=config['workers'],\n",
        "                                            pin_memory=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset_valid,\n",
        "                                            batch_size=config['batch_size'],\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=config['workers'],\n",
        "                                            pin_memory=True)\n",
        "\n",
        "    best_top1 = 0.\n",
        "    for epoch in range(config['epochs']):\n",
        "        drop_prob = config['drop_path_prob'] * epoch / config['epochs']\n",
        "        model.drop_path_prob(drop_prob)\n",
        "\n",
        "        # training\n",
        "        train(config, train_loader, model, optimizer, criterion, epoch)\n",
        "\n",
        "        # validation\n",
        "        cur_step = (epoch + 1) * len(train_loader)\n",
        "        top1 = validate(config, valid_loader, model, criterion, epoch, cur_step)\n",
        "        best_top1 = max(best_top1, top1)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), folder + \"mod.json\")\n",
        "    # torch.save(model.state_dict(), args.save_folder + \"/mod.json\")\n",
        "    print(\"Final best Prec@1 = {:.4%}\".format(best_top1))\n",
        "    best_top1s.append(best_top1)\n",
        "    print(best_top1s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 2, 3)\n",
            "./checkpoints/fashionMNIST\\lambd=1\n",
            "[2023-11-09 16:55:46] \u001b[32mFixed architecture: {'reduce_n2_p0': 'maxpool', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv5x5', 'reduce_n3_p1': 'sepconv5x5', 'reduce_n3_p2': 'skipconnect', 'reduce_n4_p0': 'dilconv5x5', 'reduce_n4_p1': 'dilconv5x5', 'reduce_n4_p2': 'skipconnect', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'skipconnect', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'sepconv5x5', 'reduce_n5_p3': 'avgpool', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [1], 'reduce_n3_switch': [1], 'reduce_n4_switch': [3], 'reduce_n5_switch': [2]}\u001b[0m\n",
            "./checkpoints/fashionMNIST\\lambd=2\n",
            "[2023-11-09 16:55:47] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv5x5', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'sepconv3x3', 'reduce_n4_p2': 'dilconv5x5', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'avgpool', 'reduce_n5_p1': 'dilconv5x5', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [0], 'reduce_n3_switch': [2], 'reduce_n4_switch': [1], 'reduce_n5_switch': [3]}\u001b[0m\n",
            "./checkpoints/fashionMNIST\\lambd=3\n",
            "[2023-11-09 16:55:47] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'avgpool', 'reduce_n3_p0': 'sepconv3x3', 'reduce_n3_p1': 'sepconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'dilconv3x3', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'maxpool', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv5x5', 'reduce_n5_p1': 'dilconv3x3', 'reduce_n5_p2': 'dilconv3x3', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\u001b[0m\n",
            "Models in ensemble: 3\n",
            "[2023-11-09 16:55:48] \u001b[32mValid: Step 000/104 Loss 1.572 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:55:48] \u001b[32mValid: Step 030/104 Loss 1.553 Prec@(1,5) (92.1%, 100.0%)\u001b[0m\n",
            "[2023-11-09 16:55:49] \u001b[32mValid: Step 060/104 Loss 1.558 Prec@(1,5) (91.7%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:55:49] \u001b[32mValid: Step 090/104 Loss 1.555 Prec@(1,5) (91.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:55:50] \u001b[32mValid: Step 104/104 Loss 1.555 Prec@(1,5) (91.8%, 99.9%)\u001b[0m\n",
            "[2023-11-09 16:55:50] \u001b[32mFinal best Prec@1 = 91.7800%\u001b[0m\n",
            "{(1, 2, 3): 0.9178000204086304}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "from nni.retiarii.oneshot.pytorch.utils import AverageMeter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "logger = logging.getLogger('nni')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "writer = SummaryWriter()\n",
        "\n",
        "config = {\n",
        "'layers' : 2,\n",
        "'batch_size' : 96,\n",
        "'log_frequency' : 30,\n",
        "'epochs' : 10,\n",
        "'aux_weight' : 0.4,\n",
        "'drop_path_prob' : 0.1,\n",
        "'workers' : 4,\n",
        "'grad_clip' : 5.,\n",
        "'save_folder' : \"./checkpoints/fashionMNIST/\",\n",
        "}\n",
        "\n",
        "dataset_train, dataset_valid = datasets.get_dataset(\"fashionmnist\", cutout_length=16)\n",
        "\n",
        "res_dict_accur = {}\n",
        "models = []\n",
        "\n",
        "# chosen_lambdas = np.random.choice(8, size=3, replace=False) # выбранные lambda\n",
        "chosen_lambdas = (1, 2, 3)\n",
        "\n",
        "print(chosen_lambdas)\n",
        "\n",
        "# with fixed_arch(dir + \"/arc.json\"):\n",
        "#     model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True)\n",
        "#     model.to(device)\n",
        "#     model.load_state_dict(torch.load(config['save_folder'] + \"optimal/mod.json\"))\n",
        "#     model.eval()\n",
        "    \n",
        "#     models.append(model)\n",
        "\n",
        "for dir in glob(config['save_folder'] + \"*\"):\n",
        "    if dir.split('\\\\')[-1] != 'optimal' and float(dir.split('\\\\')[-1].split('=')[-1]) in chosen_lambdas:\n",
        "    # if dir == \"./checkpoints\\\\0\":\n",
        "        print(dir)\n",
        "        with fixed_arch(dir + \"/arc.json\"):\n",
        "            model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True, n_chosen=n_chosen)\n",
        "        model.to(device)\n",
        "        model.load_state_dict(torch.load(dir + \"/mod.json\"))\n",
        "        model.eval()\n",
        "        \n",
        "        models.append(model)\n",
        "\n",
        "print(f\"Models in ensemble: {len(models)}\")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset_valid,\n",
        "                                            batch_size=config['batch_size'],\n",
        "                                            shuffle=False,\n",
        "                                            num_workers=config['workers'],\n",
        "                                            pin_memory=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "top1 = AverageMeter(\"top1\")\n",
        "top5 = AverageMeter(\"top5\")\n",
        "losses = AverageMeter(\"losses\")\n",
        "\n",
        "# validation\n",
        "softmax = nn.Softmax(dim=1)\n",
        "for step, (X, y) in enumerate(valid_loader):\n",
        "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        bs = X.size(0)\n",
        "\n",
        "        probabilities = softmax(models[0](X))\n",
        "        for i in range(1, len(models)):\n",
        "            probabilities += softmax(models[i](X))\n",
        "        probabilities = probabilities / len(models)\n",
        "        loss = criterion(probabilities, y)\n",
        "\n",
        "        accuracy = utils.accuracy(probabilities, y, topk=(1, 5))\n",
        "        losses.update(loss.item(), bs)\n",
        "        top1.update(accuracy[\"acc1\"], bs)\n",
        "        top5.update(accuracy[\"acc5\"], bs)\n",
        "\n",
        "        if step % config['log_frequency'] == 0 or step == len(valid_loader) - 1:\n",
        "            logger.info(\n",
        "                \"Valid: Step {:03d}/{:03d} Loss {losses.avg:.3f} \"\n",
        "                \"Prec@(1,5) ({top1.avg:.1%}, {top5.avg:.1%})\".format(\n",
        "                    step, len(valid_loader) - 1, losses=losses,\n",
        "                    top1=top1, top5=top5))\n",
        "\n",
        "logger.info(\"Final best Prec@1 = {:.4%}\".format(top1.avg))\n",
        "\n",
        "res_dict_accur[chosen_lambdas] = top1.avg\n",
        "print(res_dict_accur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6498)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.distributions import RelaxedOneHotCategorical\n",
        "\n",
        "a = torch.tensor([2, 0.5, -0.5])\n",
        "b = RelaxedOneHotCategorical(logits=a, temperature=0.5).rsample().t()\n",
        "b[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
