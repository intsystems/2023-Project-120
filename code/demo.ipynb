{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('b', 2), ('a', 3)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'b' : 2, 'a' : 3}\n",
    "\n",
    "a.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/petr12375/Documents/mipt/m1p/2023-Project-120/code/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import datasets\n",
    "from model import CNN\n",
    "\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 1\n",
    "batch_size = 64\n",
    "log_frequency = 10\n",
    "channels = 16\n",
    "unrolled = False\n",
    "visualization = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fashionmnist\"\n",
    "\n",
    "dataset_train, dataset_valid = datasets.get_dataset(dataset)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search of the optimal architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-31 07:18:51] INFO (nni.retiarii.oneshot.pytorch.darts/MainThread) Epoch [1/10] Step [1/469]  acc1 0.046875 (0.046875)  loss 2.298134 (2.298134)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     12\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optim, epochs, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mMyDartsTrainer( \u001b[38;5;66;03m# MyDartsTrainer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m=\u001b[39mcriterion, \u001b[38;5;66;03m# =mycriterion,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     arc_learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0E-1\u001b[39m,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m final_architecture \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mexport()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal architecture:\u001b[39m\u001b[38;5;124m'\u001b[39m, final_architecture)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nni/retiarii/oneshot/pytorch/darts.py:277\u001b[0m, in \u001b[0;36mDartsTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[0;32m--> 277\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nni/retiarii/oneshot/pytorch/darts.py:171\u001b[0m, in \u001b[0;36mDartsTrainer._train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unrolled_backward(trn_X, trn_y, val_X, val_y)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctrl_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# phase 2: child network step\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nni/retiarii/oneshot/pytorch/darts.py:199\u001b[0m, in \u001b[0;36mDartsTrainer._backward\u001b[0;34m(self, val_X, val_y)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03mSimple backward with gradient descent\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits_and_loss(val_X, val_y)\n\u001b[0;32m--> 199\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_folder = 'checkpoints/0'\n",
    "epochs = 10\n",
    "lambd = 0\n",
    "\n",
    "if dataset == \"fashionmnist\":\n",
    "    model = CNN(32, 1, channels, 10, layers)\n",
    "if dataset == \"cifar10\":\n",
    "    model = CNN(32, 3, channels, 10, layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # mycriterion()\n",
    "optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, epochs, eta_min=0.001)\n",
    "trainer = utils.MyDartsTrainer( # MyDartsTrainer\n",
    "    model=model,\n",
    "    loss=criterion, # =mycriterion,\n",
    "    metrics=lambda output, target: utils.accuracy(output, target, topk=(1,)),\n",
    "    optimizer=optim,\n",
    "    num_epochs=epochs,\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    log_frequency=log_frequency,\n",
    "    unrolled=unrolled,\n",
    "    weight=1e3, # вес регуляризатора\n",
    "    lambd=lambd, # количество общих ребер\n",
    "    train_as_optimal=True,\n",
    "    optimalPath='checkpoints/' + dataset + '/optimal/arc.json',\n",
    "    tau=0.98,\n",
    "    learning_rate=2.5E-3,\n",
    "    arc_learning_rate=3.0E-1,\n",
    ")\n",
    "trainer.fit()\n",
    "final_architecture = trainer.export()\n",
    "print('Final architecture:', final_architecture)\n",
    "if trainer.train_as_optimal:\n",
    "    json.dump(trainer.export(), open(f'checkpoints/' + dataset + '/optimal/arc.json', 'w+'))\n",
    "else:\n",
    "    json.dump(trainer.export(), open(f'checkpoints/' + dataset + '/lambd={lambd}/arc.json', 'w+'))\n",
    "# json.dump(trainer.export(), open(f\"checkpoints/lambd={lambd}\" + '/arc.json', 'w+'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture search for a range of $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight = 1000.0, lambd = 3.5\n",
      "[2023-10-22 11:42:00] \u001b[32mEpoch [1/10] Step [1/469]  acc1 0.109375 (0.109375)  loss 5281.082520 (5281.082520)\u001b[0m\n",
      "[2023-10-22 11:42:01] \u001b[32mEpoch [1/10] Step [11/469]  acc1 0.093750 (0.083807)  loss 186.878647 (1929.453348)\u001b[0m\n",
      "[2023-10-22 11:42:02] \u001b[32mEpoch [1/10] Step [21/469]  acc1 0.062500 (0.074405)  loss 166.476303 (1124.850833)\u001b[0m\n",
      "[2023-10-22 11:42:04] \u001b[32mEpoch [1/10] Step [31/469]  acc1 0.109375 (0.079133)  loss 3124.931885 (1001.904640)\u001b[0m\n",
      "[2023-10-22 11:42:05] \u001b[32mEpoch [1/10] Step [41/469]  acc1 0.062500 (0.082317)  loss 176.438950 (888.207335)\u001b[0m\n",
      "[2023-10-22 11:42:07] \u001b[32mEpoch [1/10] Step [51/469]  acc1 0.109375 (0.090074)  loss 171.233185 (750.694304)\u001b[0m\n",
      "[2023-10-22 11:42:08] \u001b[32mEpoch [1/10] Step [61/469]  acc1 0.125000 (0.093494)  loss 93.085716 (651.725369)\u001b[0m\n",
      "[2023-10-22 11:42:09] \u001b[32mEpoch [1/10] Step [71/469]  acc1 0.140625 (0.104754)  loss 176.104324 (580.948936)\u001b[0m\n",
      "[2023-10-22 11:42:11] \u001b[32mEpoch [1/10] Step [81/469]  acc1 0.218750 (0.114198)  loss 181.038223 (522.578136)\u001b[0m\n",
      "[2023-10-22 11:42:12] \u001b[32mEpoch [1/10] Step [91/469]  acc1 0.296875 (0.128262)  loss 673.243103 (494.938486)\u001b[0m\n",
      "[2023-10-22 11:42:13] \u001b[32mEpoch [1/10] Step [101/469]  acc1 0.328125 (0.148979)  loss 187.188156 (456.144288)\u001b[0m\n",
      "[2023-10-22 11:42:15] \u001b[32mEpoch [1/10] Step [111/469]  acc1 0.343750 (0.163288)  loss 186.886887 (429.727071)\u001b[0m\n",
      "[2023-10-22 11:42:16] \u001b[32mEpoch [1/10] Step [121/469]  acc1 0.375000 (0.177944)  loss 176.947433 (406.090583)\u001b[0m\n",
      "[2023-10-22 11:42:17] \u001b[32mEpoch [1/10] Step [131/469]  acc1 0.468750 (0.194179)  loss 170.771622 (386.441911)\u001b[0m\n",
      "[2023-10-22 11:42:19] \u001b[32mEpoch [1/10] Step [141/469]  acc1 0.328125 (0.204898)  loss 160.653473 (369.244262)\u001b[0m\n",
      "[2023-10-22 11:42:20] \u001b[32mEpoch [1/10] Step [151/469]  acc1 0.375000 (0.216267)  loss 97.367325 (354.740478)\u001b[0m\n",
      "[2023-10-22 11:42:22] \u001b[32mEpoch [1/10] Step [161/469]  acc1 0.390625 (0.225835)  loss 174.270050 (344.400046)\u001b[0m\n",
      "[2023-10-22 11:42:23] \u001b[32mEpoch [1/10] Step [171/469]  acc1 0.328125 (0.235015)  loss 178.089417 (333.675328)\u001b[0m\n",
      "[2023-10-22 11:42:24] \u001b[32mEpoch [1/10] Step [181/469]  acc1 0.390625 (0.243698)  loss 7.937036 (323.120104)\u001b[0m\n",
      "[2023-10-22 11:42:26] \u001b[32mEpoch [1/10] Step [191/469]  acc1 0.468750 (0.254499)  loss 161.257401 (314.093686)\u001b[0m\n",
      "[2023-10-22 11:42:27] \u001b[32mEpoch [1/10] Step [201/469]  acc1 0.468750 (0.264614)  loss 185.703476 (306.743019)\u001b[0m\n",
      "[2023-10-22 11:42:29] \u001b[32mEpoch [1/10] Step [211/469]  acc1 0.625000 (0.274807)  loss 187.426102 (298.751207)\u001b[0m\n",
      "[2023-10-22 11:42:30] \u001b[32mEpoch [1/10] Step [221/469]  acc1 0.484375 (0.284078)  loss 174.762222 (292.663964)\u001b[0m\n",
      "[2023-10-22 11:42:31] \u001b[32mEpoch [1/10] Step [231/469]  acc1 0.515625 (0.293561)  loss 187.263702 (286.694582)\u001b[0m\n",
      "[2023-10-22 11:42:33] \u001b[32mEpoch [1/10] Step [241/469]  acc1 0.671875 (0.305044)  loss 187.093292 (281.267716)\u001b[0m\n",
      "[2023-10-22 11:42:34] \u001b[32mEpoch [1/10] Step [251/469]  acc1 0.515625 (0.316297)  loss 187.258224 (277.485188)\u001b[0m\n",
      "[2023-10-22 11:42:35] \u001b[32mEpoch [1/10] Step [261/469]  acc1 0.625000 (0.326149)  loss 98.229706 (272.559185)\u001b[0m\n",
      "[2023-10-22 11:42:37] \u001b[32mEpoch [1/10] Step [271/469]  acc1 0.515625 (0.335044)  loss 42.586063 (267.475424)\u001b[0m\n",
      "[2023-10-22 11:42:38] \u001b[32mEpoch [1/10] Step [281/469]  acc1 0.625000 (0.344417)  loss 185.770355 (262.078167)\u001b[0m\n",
      "[2023-10-22 11:42:40] \u001b[32mEpoch [1/10] Step [291/469]  acc1 0.625000 (0.354220)  loss 116.652336 (263.302856)\u001b[0m\n",
      "[2023-10-22 11:42:41] \u001b[32mEpoch [1/10] Step [301/469]  acc1 0.515625 (0.361763)  loss 125.957726 (259.762367)\u001b[0m\n",
      "[2023-10-22 11:42:42] \u001b[32mEpoch [1/10] Step [311/469]  acc1 0.546875 (0.367966)  loss 297.993134 (255.686763)\u001b[0m\n",
      "[2023-10-22 11:42:44] \u001b[32mEpoch [1/10] Step [321/469]  acc1 0.703125 (0.375876)  loss 164.421555 (251.895941)\u001b[0m\n",
      "[2023-10-22 11:42:45] \u001b[32mEpoch [1/10] Step [331/469]  acc1 0.578125 (0.382600)  loss 87.727470 (248.665088)\u001b[0m\n",
      "[2023-10-22 11:42:46] \u001b[32mEpoch [1/10] Step [341/469]  acc1 0.562500 (0.388013)  loss 199.877838 (246.022982)\u001b[0m\n",
      "[2023-10-22 11:42:48] \u001b[32mEpoch [1/10] Step [351/469]  acc1 0.671875 (0.394186)  loss 90.072380 (242.891740)\u001b[0m\n",
      "[2023-10-22 11:42:49] \u001b[32mEpoch [1/10] Step [361/469]  acc1 0.671875 (0.399238)  loss 36.102390 (240.712696)\u001b[0m\n",
      "[2023-10-22 11:42:51] \u001b[32mEpoch [1/10] Step [371/469]  acc1 0.609375 (0.403555)  loss 162.970001 (237.847671)\u001b[0m\n",
      "[2023-10-22 11:42:52] \u001b[32mEpoch [1/10] Step [381/469]  acc1 0.687500 (0.408465)  loss 302.156677 (236.935490)\u001b[0m\n",
      "[2023-10-22 11:42:53] \u001b[32mEpoch [1/10] Step [391/469]  acc1 0.625000 (0.414522)  loss 269.964600 (234.999716)\u001b[0m\n",
      "[2023-10-22 11:42:55] \u001b[32mEpoch [1/10] Step [401/469]  acc1 0.640625 (0.419810)  loss 173.347549 (232.194775)\u001b[0m\n",
      "[2023-10-22 11:42:56] \u001b[32mEpoch [1/10] Step [411/469]  acc1 0.718750 (0.425297)  loss 1710.896973 (233.664993)\u001b[0m\n",
      "[2023-10-22 11:42:58] \u001b[32mEpoch [1/10] Step [421/469]  acc1 0.656250 (0.430151)  loss 183.632523 (232.078718)\u001b[0m\n",
      "[2023-10-22 11:42:59] \u001b[32mEpoch [1/10] Step [431/469]  acc1 0.687500 (0.435289)  loss 186.837112 (230.387082)\u001b[0m\n",
      "[2023-10-22 11:43:00] \u001b[32mEpoch [1/10] Step [441/469]  acc1 0.625000 (0.438598)  loss 15.237873 (228.596084)\u001b[0m\n",
      "[2023-10-22 11:43:02] \u001b[32mEpoch [1/10] Step [451/469]  acc1 0.625000 (0.441969)  loss 301.882355 (230.337883)\u001b[0m\n",
      "[2023-10-22 11:43:03] \u001b[32mEpoch [1/10] Step [461/469]  acc1 0.593750 (0.446821)  loss 301.730408 (231.938985)\u001b[0m\n",
      "[2023-10-22 11:43:06] \u001b[32mEpoch [2/10] Step [1/469]  acc1 0.671875 (0.671875)  loss 302.153442 (302.153442)\u001b[0m\n",
      "[2023-10-22 11:43:07] \u001b[32mEpoch [2/10] Step [11/469]  acc1 0.578125 (0.633523)  loss 302.056946 (303.045715)\u001b[0m\n",
      "[2023-10-22 11:43:09] \u001b[32mEpoch [2/10] Step [21/469]  acc1 0.625000 (0.617560)  loss 300.922028 (310.021519)\u001b[0m\n",
      "[2023-10-22 11:43:10] \u001b[32mEpoch [2/10] Step [31/469]  acc1 0.640625 (0.630040)  loss 301.973053 (320.290586)\u001b[0m\n",
      "[2023-10-22 11:43:11] \u001b[32mEpoch [2/10] Step [41/469]  acc1 0.640625 (0.634909)  loss 301.330383 (315.219235)\u001b[0m\n",
      "[2023-10-22 11:43:13] \u001b[32mEpoch [2/10] Step [51/469]  acc1 0.625000 (0.644608)  loss 163.751007 (304.580975)\u001b[0m\n",
      "[2023-10-22 11:43:14] \u001b[32mEpoch [2/10] Step [61/469]  acc1 0.609375 (0.647541)  loss 301.921570 (283.674067)\u001b[0m\n",
      "[2023-10-22 11:43:15] \u001b[32mEpoch [2/10] Step [71/469]  acc1 0.718750 (0.651408)  loss 301.745331 (286.158300)\u001b[0m\n",
      "[2023-10-22 11:43:17] \u001b[32mEpoch [2/10] Step [81/469]  acc1 0.687500 (0.658372)  loss 302.020355 (287.397500)\u001b[0m\n",
      "[2023-10-22 11:43:18] \u001b[32mEpoch [2/10] Step [91/469]  acc1 0.750000 (0.664492)  loss 77.706459 (297.437803)\u001b[0m\n",
      "[2023-10-22 11:43:19] \u001b[32mEpoch [2/10] Step [101/469]  acc1 0.796875 (0.667853)  loss 180.789780 (285.497452)\u001b[0m\n",
      "[2023-10-22 11:43:21] \u001b[32mEpoch [2/10] Step [111/469]  acc1 0.703125 (0.667793)  loss 186.783859 (270.450113)\u001b[0m\n",
      "[2023-10-22 11:43:22] \u001b[32mEpoch [2/10] Step [121/469]  acc1 0.656250 (0.668259)  loss 11.743007 (260.891799)\u001b[0m\n",
      "[2023-10-22 11:43:24] \u001b[32mEpoch [2/10] Step [131/469]  acc1 0.609375 (0.666865)  loss 14.931196 (252.248308)\u001b[0m\n",
      "[2023-10-22 11:43:25] \u001b[32mEpoch [2/10] Step [141/469]  acc1 0.718750 (0.670767)  loss 172.252167 (242.728922)\u001b[0m\n",
      "[2023-10-22 11:43:26] \u001b[32mEpoch [2/10] Step [151/469]  acc1 0.718750 (0.673738)  loss 162.361710 (236.606817)\u001b[0m\n",
      "[2023-10-22 11:43:28] \u001b[32mEpoch [2/10] Step [161/469]  acc1 0.687500 (0.674204)  loss 0.785849 (231.576919)\u001b[0m\n",
      "[2023-10-22 11:43:29] \u001b[32mEpoch [2/10] Step [171/469]  acc1 0.812500 (0.676078)  loss 186.473221 (226.673714)\u001b[0m\n",
      "[2023-10-22 11:43:30] \u001b[32mEpoch [2/10] Step [181/469]  acc1 0.515625 (0.677918)  loss 186.914703 (223.517316)\u001b[0m\n",
      "[2023-10-22 11:43:32] \u001b[32mEpoch [2/10] Step [191/469]  acc1 0.609375 (0.677192)  loss 186.992905 (221.363185)\u001b[0m\n",
      "[2023-10-22 11:43:33] \u001b[32mEpoch [2/10] Step [201/469]  acc1 0.625000 (0.677472)  loss 186.871536 (219.646537)\u001b[0m\n",
      "[2023-10-22 11:43:34] \u001b[32mEpoch [2/10] Step [211/469]  acc1 0.750000 (0.677799)  loss 186.798386 (218.076657)\u001b[0m\n",
      "[2023-10-22 11:43:36] \u001b[32mEpoch [2/10] Step [221/469]  acc1 0.781250 (0.681066)  loss 186.735718 (216.626848)\u001b[0m\n",
      "[2023-10-22 11:43:37] \u001b[32mEpoch [2/10] Step [231/469]  acc1 0.750000 (0.683983)  loss 186.836609 (214.708083)\u001b[0m\n",
      "[2023-10-22 11:43:38] \u001b[32mEpoch [2/10] Step [241/469]  acc1 0.734375 (0.686852)  loss 186.635208 (211.702706)\u001b[0m\n",
      "[2023-10-22 11:43:40] \u001b[32mEpoch [2/10] Step [251/469]  acc1 0.843750 (0.689679)  loss 186.688736 (210.699867)\u001b[0m\n",
      "[2023-10-22 11:43:41] \u001b[32mEpoch [2/10] Step [261/469]  acc1 0.765625 (0.691391)  loss 182.900726 (209.767494)\u001b[0m\n",
      "[2023-10-22 11:43:42] \u001b[32mEpoch [2/10] Step [271/469]  acc1 0.656250 (0.692574)  loss 147.619644 (208.747140)\u001b[0m\n",
      "[2023-10-22 11:43:44] \u001b[32mEpoch [2/10] Step [281/469]  acc1 0.796875 (0.694395)  loss 185.811829 (206.647796)\u001b[0m\n",
      "[2023-10-22 11:43:45] \u001b[32mEpoch [2/10] Step [291/469]  acc1 0.765625 (0.695393)  loss 5.471468 (204.545895)\u001b[0m\n",
      "[2023-10-22 11:43:46] \u001b[32mEpoch [2/10] Step [301/469]  acc1 0.718750 (0.696948)  loss 184.921219 (202.559501)\u001b[0m\n",
      "[2023-10-22 11:43:48] \u001b[32mEpoch [2/10] Step [311/469]  acc1 0.828125 (0.699759)  loss 161.638275 (200.105101)\u001b[0m\n",
      "[2023-10-22 11:43:49] \u001b[32mEpoch [2/10] Step [321/469]  acc1 0.765625 (0.700935)  loss 186.732849 (199.084101)\u001b[0m\n",
      "[2023-10-22 11:43:50] \u001b[32mEpoch [2/10] Step [331/469]  acc1 0.796875 (0.702464)  loss 186.643768 (198.441057)\u001b[0m\n",
      "[2023-10-22 11:43:52] \u001b[32mEpoch [2/10] Step [341/469]  acc1 0.781250 (0.704454)  loss 186.434204 (196.714814)\u001b[0m\n",
      "[2023-10-22 11:43:53] \u001b[32mEpoch [2/10] Step [351/469]  acc1 0.750000 (0.705662)  loss 179.390411 (195.269511)\u001b[0m\n",
      "[2023-10-22 11:43:54] \u001b[32mEpoch [2/10] Step [361/469]  acc1 0.734375 (0.705635)  loss 179.261353 (194.907646)\u001b[0m\n",
      "[2023-10-22 11:43:56] \u001b[32mEpoch [2/10] Step [371/469]  acc1 0.812500 (0.707042)  loss 130.584534 (194.151595)\u001b[0m\n",
      "[2023-10-22 11:43:57] \u001b[32mEpoch [2/10] Step [381/469]  acc1 0.828125 (0.708661)  loss 212.008698 (193.882009)\u001b[0m\n",
      "[2023-10-22 11:43:59] \u001b[32mEpoch [2/10] Step [391/469]  acc1 0.640625 (0.708600)  loss 186.950195 (193.205328)\u001b[0m\n",
      "[2023-10-22 11:44:00] \u001b[32mEpoch [2/10] Step [401/469]  acc1 0.796875 (0.710334)  loss 186.738297 (192.401645)\u001b[0m\n",
      "[2023-10-22 11:44:01] \u001b[32mEpoch [2/10] Step [411/469]  acc1 0.734375 (0.711489)  loss 126.174881 (191.570507)\u001b[0m\n",
      "[2023-10-22 11:44:03] \u001b[32mEpoch [2/10] Step [421/469]  acc1 0.734375 (0.711476)  loss 171.259964 (191.765560)\u001b[0m\n",
      "[2023-10-22 11:44:04] \u001b[32mEpoch [2/10] Step [431/469]  acc1 0.828125 (0.712696)  loss 65.479622 (190.404358)\u001b[0m\n",
      "[2023-10-22 11:44:05] \u001b[32mEpoch [2/10] Step [441/469]  acc1 0.750000 (0.713506)  loss 179.750214 (190.068574)\u001b[0m\n",
      "[2023-10-22 11:44:07] \u001b[32mEpoch [2/10] Step [451/469]  acc1 0.859375 (0.715528)  loss 186.677628 (189.283048)\u001b[0m\n",
      "[2023-10-22 11:44:08] \u001b[32mEpoch [2/10] Step [461/469]  acc1 0.765625 (0.716106)  loss 185.167801 (188.494792)\u001b[0m\n",
      "[2023-10-22 11:44:11] \u001b[32mEpoch [3/10] Step [1/469]  acc1 0.671875 (0.671875)  loss 180.017731 (180.017731)\u001b[0m\n",
      "[2023-10-22 11:44:12] \u001b[32mEpoch [3/10] Step [11/469]  acc1 0.687500 (0.752841)  loss 134.129196 (165.575351)\u001b[0m\n",
      "[2023-10-22 11:44:13] \u001b[32mEpoch [3/10] Step [21/469]  acc1 0.750000 (0.752976)  loss 186.719528 (213.009092)\u001b[0m\n",
      "[2023-10-22 11:44:15] \u001b[32mEpoch [3/10] Step [31/469]  acc1 0.812500 (0.765625)  loss 186.644028 (204.162165)\u001b[0m\n",
      "[2023-10-22 11:44:16] \u001b[32mEpoch [3/10] Step [41/469]  acc1 0.781250 (0.766006)  loss 184.962463 (198.077465)\u001b[0m\n",
      "[2023-10-22 11:44:18] \u001b[32mEpoch [3/10] Step [51/469]  acc1 0.796875 (0.768382)  loss 186.667709 (195.246791)\u001b[0m\n",
      "[2023-10-22 11:44:19] \u001b[32mEpoch [3/10] Step [61/469]  acc1 0.750000 (0.768186)  loss 186.717392 (192.917553)\u001b[0m\n",
      "[2023-10-22 11:44:21] \u001b[32mEpoch [3/10] Step [71/469]  acc1 0.765625 (0.767386)  loss 186.128540 (189.763044)\u001b[0m\n",
      "[2023-10-22 11:44:22] \u001b[32mEpoch [3/10] Step [81/469]  acc1 0.656250 (0.768326)  loss 186.910782 (189.361355)\u001b[0m\n",
      "[2023-10-22 11:44:23] \u001b[32mEpoch [3/10] Step [91/469]  acc1 0.687500 (0.768201)  loss 186.663788 (187.347302)\u001b[0m\n",
      "[2023-10-22 11:44:25] \u001b[32mEpoch [3/10] Step [101/469]  acc1 0.750000 (0.766553)  loss 186.803635 (188.175891)\u001b[0m\n",
      "[2023-10-22 11:44:26] \u001b[32mEpoch [3/10] Step [111/469]  acc1 0.750000 (0.768863)  loss 186.035110 (187.976647)\u001b[0m\n",
      "[2023-10-22 11:44:28] \u001b[32mEpoch [3/10] Step [121/469]  acc1 0.671875 (0.768337)  loss 186.992569 (187.094006)\u001b[0m\n",
      "[2023-10-22 11:44:29] \u001b[32mEpoch [3/10] Step [131/469]  acc1 0.750000 (0.769919)  loss 186.861481 (186.969180)\u001b[0m\n",
      "[2023-10-22 11:44:30] \u001b[32mEpoch [3/10] Step [141/469]  acc1 0.796875 (0.769614)  loss 186.734146 (186.066880)\u001b[0m\n",
      "[2023-10-22 11:44:32] \u001b[32mEpoch [3/10] Step [151/469]  acc1 0.781250 (0.768626)  loss 186.720200 (185.534939)\u001b[0m\n",
      "[2023-10-22 11:44:33] \u001b[32mEpoch [3/10] Step [161/469]  acc1 0.828125 (0.769313)  loss 186.625916 (185.599452)\u001b[0m\n",
      "[2023-10-22 11:44:34] \u001b[32mEpoch [3/10] Step [171/469]  acc1 0.796875 (0.769645)  loss 186.502869 (185.580778)\u001b[0m\n",
      "[2023-10-22 11:44:36] \u001b[32mEpoch [3/10] Step [181/469]  acc1 0.656250 (0.768215)  loss 186.883362 (185.627577)\u001b[0m\n",
      "[2023-10-22 11:44:37] \u001b[32mEpoch [3/10] Step [191/469]  acc1 0.718750 (0.767425)  loss 186.794800 (185.620656)\u001b[0m\n",
      "[2023-10-22 11:44:38] \u001b[32mEpoch [3/10] Step [201/469]  acc1 0.765625 (0.766713)  loss 186.861908 (185.163742)\u001b[0m\n",
      "[2023-10-22 11:44:40] \u001b[32mEpoch [3/10] Step [211/469]  acc1 0.875000 (0.765921)  loss 186.525009 (185.087127)\u001b[0m\n",
      "[2023-10-22 11:44:41] \u001b[32mEpoch [3/10] Step [221/469]  acc1 0.796875 (0.766403)  loss 185.411026 (184.815608)\u001b[0m\n",
      "[2023-10-22 11:44:43] \u001b[32mEpoch [3/10] Step [231/469]  acc1 0.750000 (0.766843)  loss 186.761734 (184.910919)\u001b[0m\n",
      "[2023-10-22 11:44:44] \u001b[32mEpoch [3/10] Step [241/469]  acc1 0.765625 (0.758493)  loss 101.712128 (183.576366)\u001b[0m\n",
      "[2023-10-22 11:44:45] \u001b[32mEpoch [3/10] Step [251/469]  acc1 0.859375 (0.758591)  loss 186.225769 (183.657631)\u001b[0m\n",
      "[2023-10-22 11:44:47] \u001b[32mEpoch [3/10] Step [261/469]  acc1 0.750000 (0.758980)  loss 185.763840 (183.722115)\u001b[0m\n",
      "[2023-10-22 11:44:48] \u001b[32mEpoch [3/10] Step [271/469]  acc1 0.781250 (0.760032)  loss 186.689194 (183.816466)\u001b[0m\n",
      "[2023-10-22 11:44:49] \u001b[32mEpoch [3/10] Step [281/469]  acc1 0.781250 (0.761177)  loss 186.165314 (183.850404)\u001b[0m\n",
      "[2023-10-22 11:44:51] \u001b[32mEpoch [3/10] Step [291/469]  acc1 0.812500 (0.762027)  loss 183.611618 (183.546125)\u001b[0m\n",
      "[2023-10-22 11:44:52] \u001b[32mEpoch [3/10] Step [301/469]  acc1 0.796875 (0.763133)  loss 226.565475 (182.822822)\u001b[0m\n",
      "[2023-10-22 11:44:53] \u001b[32mEpoch [3/10] Step [311/469]  acc1 0.765625 (0.764721)  loss 185.829483 (187.748443)\u001b[0m\n",
      "[2023-10-22 11:44:55] \u001b[32mEpoch [3/10] Step [321/469]  acc1 0.812500 (0.765041)  loss 214.029465 (187.794865)\u001b[0m\n",
      "[2023-10-22 11:44:56] \u001b[32mEpoch [3/10] Step [331/469]  acc1 0.718750 (0.764445)  loss 7.086775 (186.315432)\u001b[0m\n",
      "[2023-10-22 11:44:57] \u001b[32mEpoch [3/10] Step [341/469]  acc1 0.765625 (0.764846)  loss 287.197784 (186.247179)\u001b[0m\n",
      "[2023-10-22 11:44:59] \u001b[32mEpoch [3/10] Step [351/469]  acc1 0.734375 (0.764423)  loss 75.177933 (184.118176)\u001b[0m\n",
      "[2023-10-22 11:45:00] \u001b[32mEpoch [3/10] Step [361/469]  acc1 0.718750 (0.764153)  loss 185.303772 (184.087080)\u001b[0m\n",
      "[2023-10-22 11:45:01] \u001b[32mEpoch [3/10] Step [371/469]  acc1 0.828125 (0.764277)  loss 186.721268 (183.880328)\u001b[0m\n",
      "[2023-10-22 11:45:03] \u001b[32mEpoch [3/10] Step [381/469]  acc1 0.765625 (0.763698)  loss 138.343964 (183.217603)\u001b[0m\n",
      "[2023-10-22 11:45:04] \u001b[32mEpoch [3/10] Step [391/469]  acc1 0.875000 (0.764066)  loss 299.572052 (184.257101)\u001b[0m\n",
      "[2023-10-22 11:45:05] \u001b[32mEpoch [3/10] Step [401/469]  acc1 0.843750 (0.764573)  loss 470.364990 (186.430675)\u001b[0m\n",
      "[2023-10-22 11:45:07] \u001b[32mEpoch [3/10] Step [411/469]  acc1 0.781250 (0.764979)  loss 185.951721 (186.288613)\u001b[0m\n",
      "[2023-10-22 11:45:08] \u001b[32mEpoch [3/10] Step [421/469]  acc1 0.828125 (0.765588)  loss 186.635986 (185.633750)\u001b[0m\n",
      "[2023-10-22 11:45:09] \u001b[32mEpoch [3/10] Step [431/469]  acc1 0.921875 (0.766314)  loss 285.939178 (185.602293)\u001b[0m\n",
      "[2023-10-22 11:45:11] \u001b[32mEpoch [3/10] Step [441/469]  acc1 0.703125 (0.767184)  loss 186.771713 (185.433533)\u001b[0m\n",
      "[2023-10-22 11:45:12] \u001b[32mEpoch [3/10] Step [451/469]  acc1 0.812500 (0.768085)  loss 186.123184 (185.319002)\u001b[0m\n",
      "[2023-10-22 11:45:14] \u001b[32mEpoch [3/10] Step [461/469]  acc1 0.734375 (0.768167)  loss 186.645584 (185.341675)\u001b[0m\n",
      "[2023-10-22 11:45:16] \u001b[32mEpoch [4/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 186.628403 (186.628403)\u001b[0m\n",
      "[2023-10-22 11:45:18] \u001b[32mEpoch [4/10] Step [11/469]  acc1 0.875000 (0.819602)  loss 186.367477 (183.952946)\u001b[0m\n",
      "[2023-10-22 11:45:19] \u001b[32mEpoch [4/10] Step [21/469]  acc1 0.859375 (0.812500)  loss 167.174515 (161.450013)\u001b[0m\n",
      "[2023-10-22 11:45:20] \u001b[32mEpoch [4/10] Step [31/469]  acc1 0.718750 (0.812500)  loss 181.411636 (168.865013)\u001b[0m\n",
      "[2023-10-22 11:45:22] \u001b[32mEpoch [4/10] Step [41/469]  acc1 0.906250 (0.820503)  loss 2120.866455 (229.825953)\u001b[0m\n",
      "[2023-10-22 11:45:23] \u001b[32mEpoch [4/10] Step [51/469]  acc1 0.875000 (0.820772)  loss 117.581055 (211.930351)\u001b[0m\n",
      "[2023-10-22 11:45:24] \u001b[32mEpoch [4/10] Step [61/469]  acc1 0.859375 (0.815061)  loss 150.214691 (201.687384)\u001b[0m\n",
      "[2023-10-22 11:45:26] \u001b[32mEpoch [4/10] Step [71/469]  acc1 0.812500 (0.814481)  loss 27.426083 (187.557110)\u001b[0m\n",
      "[2023-10-22 11:45:27] \u001b[32mEpoch [4/10] Step [81/469]  acc1 0.796875 (0.814429)  loss 186.815369 (183.118915)\u001b[0m\n",
      "[2023-10-22 11:45:29] \u001b[32mEpoch [4/10] Step [91/469]  acc1 0.859375 (0.812157)  loss 179.345367 (183.080530)\u001b[0m\n",
      "[2023-10-22 11:45:30] \u001b[32mEpoch [4/10] Step [101/469]  acc1 0.828125 (0.813738)  loss 186.575058 (184.491129)\u001b[0m\n",
      "[2023-10-22 11:45:31] \u001b[32mEpoch [4/10] Step [111/469]  acc1 0.812500 (0.813908)  loss 25.797968 (183.558777)\u001b[0m\n",
      "[2023-10-22 11:45:33] \u001b[32mEpoch [4/10] Step [121/469]  acc1 0.796875 (0.813404)  loss 122.137962 (182.841900)\u001b[0m\n",
      "[2023-10-22 11:45:34] \u001b[32mEpoch [4/10] Step [131/469]  acc1 0.843750 (0.812858)  loss 184.340775 (183.551527)\u001b[0m\n",
      "[2023-10-22 11:45:35] \u001b[32mEpoch [4/10] Step [141/469]  acc1 0.750000 (0.809619)  loss 185.987930 (180.269045)\u001b[0m\n",
      "[2023-10-22 11:45:37] \u001b[32mEpoch [4/10] Step [151/469]  acc1 0.750000 (0.807533)  loss 179.997803 (178.112269)\u001b[0m\n",
      "[2023-10-22 11:45:38] \u001b[32mEpoch [4/10] Step [161/469]  acc1 0.796875 (0.803086)  loss 186.740082 (177.771095)\u001b[0m\n",
      "[2023-10-22 11:45:39] \u001b[32mEpoch [4/10] Step [171/469]  acc1 0.750000 (0.802175)  loss 186.726959 (177.198834)\u001b[0m\n",
      "[2023-10-22 11:45:41] \u001b[32mEpoch [4/10] Step [181/469]  acc1 0.875000 (0.800846)  loss 14.761419 (176.774107)\u001b[0m\n",
      "[2023-10-22 11:45:42] \u001b[32mEpoch [4/10] Step [191/469]  acc1 0.828125 (0.801129)  loss 186.657822 (177.282795)\u001b[0m\n",
      "[2023-10-22 11:45:43] \u001b[32mEpoch [4/10] Step [201/469]  acc1 0.781250 (0.800218)  loss 185.925674 (177.743255)\u001b[0m\n",
      "[2023-10-22 11:45:45] \u001b[32mEpoch [4/10] Step [211/469]  acc1 0.859375 (0.800504)  loss 185.929733 (178.162304)\u001b[0m\n",
      "[2023-10-22 11:45:46] \u001b[32mEpoch [4/10] Step [221/469]  acc1 0.750000 (0.800693)  loss 186.784988 (177.956604)\u001b[0m\n",
      "[2023-10-22 11:45:47] \u001b[32mEpoch [4/10] Step [231/469]  acc1 0.765625 (0.800866)  loss 175.016510 (178.266421)\u001b[0m\n",
      "[2023-10-22 11:45:49] \u001b[32mEpoch [4/10] Step [241/469]  acc1 0.828125 (0.800182)  loss 301.482086 (178.422708)\u001b[0m\n",
      "[2023-10-22 11:45:50] \u001b[32mEpoch [4/10] Step [251/469]  acc1 0.546875 (0.782682)  loss 307.401947 (179.992963)\u001b[0m\n",
      "[2023-10-22 11:45:51] \u001b[32mEpoch [4/10] Step [261/469]  acc1 0.812500 (0.782447)  loss 186.488678 (179.460034)\u001b[0m\n",
      "[2023-10-22 11:45:53] \u001b[32mEpoch [4/10] Step [271/469]  acc1 0.859375 (0.783210)  loss 186.445129 (179.724163)\u001b[0m\n",
      "[2023-10-22 11:45:54] \u001b[32mEpoch [4/10] Step [281/469]  acc1 0.828125 (0.784364)  loss 299.626892 (180.267760)\u001b[0m\n",
      "[2023-10-22 11:45:55] \u001b[32mEpoch [4/10] Step [291/469]  acc1 0.765625 (0.783935)  loss 129.334396 (180.218253)\u001b[0m\n",
      "[2023-10-22 11:45:57] \u001b[32mEpoch [4/10] Step [301/469]  acc1 0.812500 (0.785351)  loss 186.644623 (180.410062)\u001b[0m\n",
      "[2023-10-22 11:45:58] \u001b[32mEpoch [4/10] Step [311/469]  acc1 0.812500 (0.785320)  loss 185.729919 (180.602242)\u001b[0m\n",
      "[2023-10-22 11:45:59] \u001b[32mEpoch [4/10] Step [321/469]  acc1 0.828125 (0.785290)  loss 186.471329 (180.757021)\u001b[0m\n",
      "[2023-10-22 11:46:01] \u001b[32mEpoch [4/10] Step [331/469]  acc1 0.812500 (0.786159)  loss 186.606827 (180.934810)\u001b[0m\n",
      "[2023-10-22 11:46:02] \u001b[32mEpoch [4/10] Step [341/469]  acc1 0.765625 (0.786794)  loss 186.737686 (181.094753)\u001b[0m\n",
      "[2023-10-22 11:46:04] \u001b[32mEpoch [4/10] Step [351/469]  acc1 0.906250 (0.789040)  loss 186.413544 (181.248439)\u001b[0m\n",
      "[2023-10-22 11:46:05] \u001b[32mEpoch [4/10] Step [361/469]  acc1 0.843750 (0.789950)  loss 186.522018 (181.396500)\u001b[0m\n",
      "[2023-10-22 11:46:06] \u001b[32mEpoch [4/10] Step [371/469]  acc1 0.828125 (0.790852)  loss 186.630478 (181.407822)\u001b[0m\n",
      "[2023-10-22 11:46:08] \u001b[32mEpoch [4/10] Step [381/469]  acc1 0.859375 (0.791339)  loss 186.536484 (181.543112)\u001b[0m\n",
      "[2023-10-22 11:46:09] \u001b[32mEpoch [4/10] Step [391/469]  acc1 0.828125 (0.791480)  loss 186.546844 (181.673049)\u001b[0m\n",
      "[2023-10-22 11:46:10] \u001b[32mEpoch [4/10] Step [401/469]  acc1 0.796875 (0.791887)  loss 186.694275 (181.796921)\u001b[0m\n",
      "[2023-10-22 11:46:12] \u001b[32mEpoch [4/10] Step [411/469]  acc1 0.796875 (0.792503)  loss 186.664566 (181.558295)\u001b[0m\n",
      "[2023-10-22 11:46:13] \u001b[32mEpoch [4/10] Step [421/469]  acc1 0.859375 (0.793498)  loss 186.562897 (181.677710)\u001b[0m\n",
      "[2023-10-22 11:46:15] \u001b[32mEpoch [4/10] Step [431/469]  acc1 0.750000 (0.793830)  loss 186.904831 (181.792908)\u001b[0m\n",
      "[2023-10-22 11:46:16] \u001b[32mEpoch [4/10] Step [441/469]  acc1 0.906250 (0.794643)  loss 186.501526 (181.901517)\u001b[0m\n",
      "[2023-10-22 11:46:18] \u001b[32mEpoch [4/10] Step [451/469]  acc1 0.812500 (0.795524)  loss 186.648636 (182.005252)\u001b[0m\n",
      "[2023-10-22 11:46:19] \u001b[32mEpoch [4/10] Step [461/469]  acc1 0.843750 (0.795960)  loss 186.505096 (182.103356)\u001b[0m\n",
      "[2023-10-22 11:46:22] \u001b[32mEpoch [5/10] Step [1/469]  acc1 0.937500 (0.937500)  loss 186.356567 (186.356567)\u001b[0m\n",
      "[2023-10-22 11:46:24] \u001b[32mEpoch [5/10] Step [11/469]  acc1 0.828125 (0.821023)  loss 186.586746 (186.616490)\u001b[0m\n",
      "[2023-10-22 11:46:25] \u001b[32mEpoch [5/10] Step [21/469]  acc1 0.843750 (0.828869)  loss 186.544769 (186.601236)\u001b[0m\n",
      "[2023-10-22 11:46:27] \u001b[32mEpoch [5/10] Step [31/469]  acc1 0.843750 (0.831653)  loss 186.551514 (186.591163)\u001b[0m\n",
      "[2023-10-22 11:46:28] \u001b[32mEpoch [5/10] Step [41/469]  acc1 0.875000 (0.824314)  loss 186.476700 (186.592800)\u001b[0m\n",
      "[2023-10-22 11:46:29] \u001b[32mEpoch [5/10] Step [51/469]  acc1 0.250000 (0.802390)  loss 303.354126 (186.018842)\u001b[0m\n",
      "[2023-10-22 11:46:31] \u001b[32mEpoch [5/10] Step [61/469]  acc1 0.656250 (0.740266)  loss 302.102570 (205.118856)\u001b[0m\n",
      "[2023-10-22 11:46:32] \u001b[32mEpoch [5/10] Step [71/469]  acc1 0.656250 (0.723812)  loss 256.932617 (218.121902)\u001b[0m\n",
      "[2023-10-22 11:46:34] \u001b[32mEpoch [5/10] Step [81/469]  acc1 0.656250 (0.717785)  loss 301.911682 (228.540434)\u001b[0m\n",
      "[2023-10-22 11:46:35] \u001b[32mEpoch [5/10] Step [91/469]  acc1 0.781250 (0.720982)  loss 301.698303 (236.669123)\u001b[0m\n",
      "[2023-10-22 11:46:37] \u001b[32mEpoch [5/10] Step [101/469]  acc1 0.750000 (0.723700)  loss 300.924042 (242.277933)\u001b[0m\n",
      "[2023-10-22 11:46:38] \u001b[32mEpoch [5/10] Step [111/469]  acc1 0.796875 (0.725788)  loss 186.758881 (242.811596)\u001b[0m\n",
      "[2023-10-22 11:46:40] \u001b[32mEpoch [5/10] Step [121/469]  acc1 0.703125 (0.730372)  loss 186.839630 (238.177666)\u001b[0m\n",
      "[2023-10-22 11:46:41] \u001b[32mEpoch [5/10] Step [131/469]  acc1 0.828125 (0.735329)  loss 186.560303 (234.247789)\u001b[0m\n",
      "[2023-10-22 11:46:43] \u001b[32mEpoch [5/10] Step [141/469]  acc1 0.828125 (0.741356)  loss 186.475403 (230.869430)\u001b[0m\n",
      "[2023-10-22 11:46:44] \u001b[32mEpoch [5/10] Step [151/469]  acc1 0.765625 (0.745447)  loss 186.685486 (227.671522)\u001b[0m\n",
      "[2023-10-22 11:46:45] \u001b[32mEpoch [5/10] Step [161/469]  acc1 0.890625 (0.749515)  loss 186.510880 (225.105744)\u001b[0m\n",
      "[2023-10-22 11:46:47] \u001b[32mEpoch [5/10] Step [171/469]  acc1 0.875000 (0.753107)  loss 186.623169 (222.847768)\u001b[0m\n",
      "[2023-10-22 11:46:48] \u001b[32mEpoch [5/10] Step [181/469]  acc1 0.828125 (0.756043)  loss 186.540573 (221.466758)\u001b[0m\n",
      "[2023-10-22 11:46:49] \u001b[32mEpoch [5/10] Step [191/469]  acc1 0.796875 (0.760062)  loss 186.562393 (219.373830)\u001b[0m\n",
      "[2023-10-22 11:46:51] \u001b[32mEpoch [5/10] Step [201/469]  acc1 0.859375 (0.762049)  loss 186.545746 (217.686015)\u001b[0m\n",
      "[2023-10-22 11:46:52] \u001b[32mEpoch [5/10] Step [211/469]  acc1 0.843750 (0.764810)  loss 186.590637 (216.213539)\u001b[0m\n",
      "[2023-10-22 11:46:53] \u001b[32mEpoch [5/10] Step [221/469]  acc1 0.843750 (0.768241)  loss 186.645813 (214.874685)\u001b[0m\n",
      "[2023-10-22 11:46:55] \u001b[32mEpoch [5/10] Step [231/469]  acc1 0.765625 (0.770698)  loss 186.721069 (213.652090)\u001b[0m\n",
      "[2023-10-22 11:46:56] \u001b[32mEpoch [5/10] Step [241/469]  acc1 0.843750 (0.773275)  loss 186.603302 (212.530570)\u001b[0m\n",
      "[2023-10-22 11:46:57] \u001b[32mEpoch [5/10] Step [251/469]  acc1 0.859375 (0.776208)  loss 186.489929 (211.496125)\u001b[0m\n",
      "[2023-10-22 11:46:59] \u001b[32mEpoch [5/10] Step [261/469]  acc1 0.828125 (0.778436)  loss 186.573395 (210.540922)\u001b[0m\n",
      "[2023-10-22 11:47:00] \u001b[32mEpoch [5/10] Step [271/469]  acc1 0.906250 (0.780443)  loss 186.499863 (209.651958)\u001b[0m\n",
      "[2023-10-22 11:47:01] \u001b[32mEpoch [5/10] Step [281/469]  acc1 0.812500 (0.782306)  loss 186.586533 (208.831642)\u001b[0m\n",
      "[2023-10-22 11:47:03] \u001b[32mEpoch [5/10] Step [291/469]  acc1 0.921875 (0.784203)  loss 186.491714 (208.066429)\u001b[0m\n",
      "[2023-10-22 11:47:04] \u001b[32mEpoch [5/10] Step [301/469]  acc1 0.875000 (0.786701)  loss 186.432053 (207.350554)\u001b[0m\n",
      "[2023-10-22 11:47:05] \u001b[32mEpoch [5/10] Step [311/469]  acc1 0.953125 (0.789088)  loss 186.303070 (206.681063)\u001b[0m\n",
      "[2023-10-22 11:47:07] \u001b[32mEpoch [5/10] Step [321/469]  acc1 0.890625 (0.790888)  loss 186.443649 (206.054968)\u001b[0m\n",
      "[2023-10-22 11:47:08] \u001b[32mEpoch [5/10] Step [331/469]  acc1 0.781250 (0.792296)  loss 186.558319 (205.465615)\u001b[0m\n",
      "[2023-10-22 11:47:09] \u001b[32mEpoch [5/10] Step [341/469]  acc1 0.843750 (0.793163)  loss 186.545319 (204.834342)\u001b[0m\n",
      "[2023-10-22 11:47:11] \u001b[32mEpoch [5/10] Step [351/469]  acc1 0.859375 (0.794560)  loss 186.736084 (204.313235)\u001b[0m\n",
      "[2023-10-22 11:47:12] \u001b[32mEpoch [5/10] Step [361/469]  acc1 0.921875 (0.795836)  loss 186.427017 (203.820263)\u001b[0m\n",
      "[2023-10-22 11:47:13] \u001b[32mEpoch [5/10] Step [371/469]  acc1 0.859375 (0.796959)  loss 186.545334 (203.355255)\u001b[0m\n",
      "[2023-10-22 11:47:15] \u001b[32mEpoch [5/10] Step [381/469]  acc1 0.812500 (0.798228)  loss 186.581741 (202.804443)\u001b[0m\n",
      "[2023-10-22 11:47:16] \u001b[32mEpoch [5/10] Step [391/469]  acc1 0.843750 (0.799153)  loss 186.511887 (202.389134)\u001b[0m\n",
      "[2023-10-22 11:47:17] \u001b[32mEpoch [5/10] Step [401/469]  acc1 0.875000 (0.800031)  loss 186.516876 (201.965673)\u001b[0m\n",
      "[2023-10-22 11:47:19] \u001b[32mEpoch [5/10] Step [411/469]  acc1 0.750000 (0.801095)  loss 186.762009 (201.591236)\u001b[0m\n",
      "[2023-10-22 11:47:20] \u001b[32mEpoch [5/10] Step [421/469]  acc1 0.781250 (0.801217)  loss 186.675568 (201.235007)\u001b[0m\n",
      "[2023-10-22 11:47:21] \u001b[32mEpoch [5/10] Step [431/469]  acc1 0.843750 (0.801987)  loss 186.520950 (200.894641)\u001b[0m\n",
      "[2023-10-22 11:47:23] \u001b[32mEpoch [5/10] Step [441/469]  acc1 0.703125 (0.802721)  loss 183.904648 (200.564217)\u001b[0m\n",
      "[2023-10-22 11:47:24] \u001b[32mEpoch [5/10] Step [451/469]  acc1 0.828125 (0.802869)  loss 186.702240 (200.215534)\u001b[0m\n",
      "[2023-10-22 11:47:25] \u001b[32mEpoch [5/10] Step [461/469]  acc1 0.875000 (0.803755)  loss 186.415955 (199.918836)\u001b[0m\n",
      "[2023-10-22 11:47:28] \u001b[32mEpoch [6/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 301.358215 (301.358215)\u001b[0m\n",
      "[2023-10-22 11:47:30] \u001b[32mEpoch [6/10] Step [11/469]  acc1 0.875000 (0.846591)  loss 301.217804 (231.890246)\u001b[0m\n",
      "[2023-10-22 11:47:31] \u001b[32mEpoch [6/10] Step [21/469]  acc1 0.765625 (0.845982)  loss 301.159882 (242.478227)\u001b[0m\n",
      "[2023-10-22 11:47:32] \u001b[32mEpoch [6/10] Step [31/469]  acc1 0.875000 (0.839718)  loss 292.903381 (251.399391)\u001b[0m\n",
      "[2023-10-22 11:47:34] \u001b[32mEpoch [6/10] Step [41/469]  acc1 0.843750 (0.850610)  loss 186.551300 (230.388566)\u001b[0m\n",
      "[2023-10-22 11:47:35] \u001b[32mEpoch [6/10] Step [51/469]  acc1 0.875000 (0.848346)  loss 186.453186 (221.765041)\u001b[0m\n",
      "[2023-10-22 11:47:37] \u001b[32mEpoch [6/10] Step [61/469]  acc1 0.875000 (0.847848)  loss 186.511078 (215.986008)\u001b[0m\n",
      "[2023-10-22 11:47:38] \u001b[32mEpoch [6/10] Step [71/469]  acc1 0.718750 (0.843310)  loss 186.880539 (211.843585)\u001b[0m\n",
      "[2023-10-22 11:47:39] \u001b[32mEpoch [6/10] Step [81/469]  acc1 0.859375 (0.841628)  loss 186.536499 (208.723080)\u001b[0m\n",
      "[2023-10-22 11:47:41] \u001b[32mEpoch [6/10] Step [91/469]  acc1 0.906250 (0.842033)  loss 186.436325 (206.144539)\u001b[0m\n",
      "[2023-10-22 11:47:42] \u001b[32mEpoch [6/10] Step [101/469]  acc1 0.921875 (0.842358)  loss 186.403107 (204.200891)\u001b[0m\n",
      "[2023-10-22 11:47:43] \u001b[32mEpoch [6/10] Step [111/469]  acc1 0.843750 (0.844595)  loss 186.539993 (202.604701)\u001b[0m\n",
      "[2023-10-22 11:47:45] \u001b[32mEpoch [6/10] Step [121/469]  acc1 0.890625 (0.848011)  loss 186.481949 (201.270065)\u001b[0m\n",
      "[2023-10-22 11:47:46] \u001b[32mEpoch [6/10] Step [131/469]  acc1 0.890625 (0.848282)  loss 186.532486 (200.145882)\u001b[0m\n",
      "[2023-10-22 11:47:47] \u001b[32mEpoch [6/10] Step [141/469]  acc1 0.937500 (0.847739)  loss 186.390137 (198.139478)\u001b[0m\n",
      "[2023-10-22 11:47:49] \u001b[32mEpoch [6/10] Step [151/469]  acc1 0.812500 (0.847786)  loss 186.580170 (197.371778)\u001b[0m\n",
      "[2023-10-22 11:47:50] \u001b[32mEpoch [6/10] Step [161/469]  acc1 0.890625 (0.846759)  loss 186.470703 (196.691278)\u001b[0m\n",
      "[2023-10-22 11:47:51] \u001b[32mEpoch [6/10] Step [171/469]  acc1 0.843750 (0.847039)  loss 186.626312 (196.097270)\u001b[0m\n",
      "[2023-10-22 11:47:53] \u001b[32mEpoch [6/10] Step [181/469]  acc1 0.890625 (0.848325)  loss 186.453201 (193.993167)\u001b[0m\n",
      "[2023-10-22 11:47:54] \u001b[32mEpoch [6/10] Step [191/469]  acc1 0.843750 (0.848577)  loss 186.453079 (193.601529)\u001b[0m\n",
      "[2023-10-22 11:47:55] \u001b[32mEpoch [6/10] Step [201/469]  acc1 0.890625 (0.848725)  loss 186.482590 (193.249765)\u001b[0m\n",
      "[2023-10-22 11:47:57] \u001b[32mEpoch [6/10] Step [211/469]  acc1 0.843750 (0.849008)  loss 186.544907 (192.929461)\u001b[0m\n",
      "[2023-10-22 11:47:58] \u001b[32mEpoch [6/10] Step [221/469]  acc1 0.843750 (0.849123)  loss 90.962234 (191.610556)\u001b[0m\n",
      "[2023-10-22 11:47:59] \u001b[32mEpoch [6/10] Step [231/469]  acc1 0.890625 (0.850108)  loss 186.397949 (191.372239)\u001b[0m\n",
      "[2023-10-22 11:48:01] \u001b[32mEpoch [6/10] Step [241/469]  acc1 0.781250 (0.850558)  loss 186.739456 (191.119534)\u001b[0m\n",
      "[2023-10-22 11:48:02] \u001b[32mEpoch [6/10] Step [251/469]  acc1 0.859375 (0.850349)  loss 186.568100 (190.182658)\u001b[0m\n",
      "[2023-10-22 11:48:03] \u001b[32mEpoch [6/10] Step [261/469]  acc1 0.843750 (0.848420)  loss 179.951004 (189.074217)\u001b[0m\n",
      "[2023-10-22 11:48:05] \u001b[32mEpoch [6/10] Step [271/469]  acc1 0.828125 (0.848132)  loss 291.234741 (189.827793)\u001b[0m\n",
      "[2023-10-22 11:48:06] \u001b[32mEpoch [6/10] Step [281/469]  acc1 0.718750 (0.847754)  loss 296.897125 (189.185264)\u001b[0m\n",
      "[2023-10-22 11:48:08] \u001b[32mEpoch [6/10] Step [291/469]  acc1 0.906250 (0.846972)  loss 154.018753 (193.538014)\u001b[0m\n",
      "[2023-10-22 11:48:09] \u001b[32mEpoch [6/10] Step [301/469]  acc1 0.843750 (0.847228)  loss 186.559540 (192.943346)\u001b[0m\n",
      "[2023-10-22 11:48:10] \u001b[32mEpoch [6/10] Step [311/469]  acc1 0.812500 (0.847166)  loss 22.525024 (191.027614)\u001b[0m\n",
      "[2023-10-22 11:48:12] \u001b[32mEpoch [6/10] Step [321/469]  acc1 0.718750 (0.845551)  loss 304.143951 (188.805671)\u001b[0m\n",
      "[2023-10-22 11:48:13] \u001b[32mEpoch [6/10] Step [331/469]  acc1 0.890625 (0.845166)  loss 186.426666 (188.525373)\u001b[0m\n",
      "[2023-10-22 11:48:14] \u001b[32mEpoch [6/10] Step [341/469]  acc1 0.921875 (0.845400)  loss 186.556732 (188.467043)\u001b[0m\n",
      "[2023-10-22 11:48:16] \u001b[32mEpoch [6/10] Step [351/469]  acc1 0.875000 (0.846065)  loss 186.482224 (188.411837)\u001b[0m\n",
      "[2023-10-22 11:48:17] \u001b[32mEpoch [6/10] Step [361/469]  acc1 0.875000 (0.847256)  loss 186.469360 (188.358608)\u001b[0m\n",
      "[2023-10-22 11:48:19] \u001b[32mEpoch [6/10] Step [371/469]  acc1 0.859375 (0.847540)  loss 186.436569 (188.309333)\u001b[0m\n",
      "[2023-10-22 11:48:20] \u001b[32mEpoch [6/10] Step [381/469]  acc1 0.796875 (0.847851)  loss 186.583527 (188.262045)\u001b[0m\n",
      "[2023-10-22 11:48:21] \u001b[32mEpoch [6/10] Step [391/469]  acc1 0.796875 (0.848106)  loss 186.724426 (188.217899)\u001b[0m\n",
      "[2023-10-22 11:48:23] \u001b[32mEpoch [6/10] Step [401/469]  acc1 0.828125 (0.848036)  loss 186.558075 (188.173966)\u001b[0m\n",
      "[2023-10-22 11:48:24] \u001b[32mEpoch [6/10] Step [411/469]  acc1 0.906250 (0.848502)  loss 186.476776 (188.133956)\u001b[0m\n",
      "[2023-10-22 11:48:25] \u001b[32mEpoch [6/10] Step [421/469]  acc1 0.890625 (0.848129)  loss 186.503738 (188.097237)\u001b[0m\n",
      "[2023-10-22 11:48:27] \u001b[32mEpoch [6/10] Step [431/469]  acc1 0.921875 (0.847847)  loss 186.410233 (188.061580)\u001b[0m\n",
      "[2023-10-22 11:48:28] \u001b[32mEpoch [6/10] Step [441/469]  acc1 0.843750 (0.848285)  loss 186.521988 (188.026268)\u001b[0m\n",
      "[2023-10-22 11:48:30] \u001b[32mEpoch [6/10] Step [451/469]  acc1 0.843750 (0.848254)  loss 185.876953 (188.242690)\u001b[0m\n",
      "[2023-10-22 11:48:31] \u001b[32mEpoch [6/10] Step [461/469]  acc1 0.875000 (0.847682)  loss 186.468765 (188.206518)\u001b[0m\n",
      "[2023-10-22 11:48:34] \u001b[32mEpoch [7/10] Step [1/469]  acc1 0.890625 (0.890625)  loss 186.438614 (186.438614)\u001b[0m\n",
      "[2023-10-22 11:48:35] \u001b[32mEpoch [7/10] Step [11/469]  acc1 0.781250 (0.832386)  loss 186.617615 (186.523246)\u001b[0m\n",
      "[2023-10-22 11:48:36] \u001b[32mEpoch [7/10] Step [21/469]  acc1 0.921875 (0.853423)  loss 186.341064 (186.491980)\u001b[0m\n",
      "[2023-10-22 11:48:38] \u001b[32mEpoch [7/10] Step [31/469]  acc1 0.859375 (0.851815)  loss 186.437668 (186.480901)\u001b[0m\n",
      "[2023-10-22 11:48:39] \u001b[32mEpoch [7/10] Step [41/469]  acc1 0.859375 (0.860518)  loss 186.523087 (186.483890)\u001b[0m\n",
      "[2023-10-22 11:48:40] \u001b[32mEpoch [7/10] Step [51/469]  acc1 0.875000 (0.861520)  loss 186.439529 (186.483461)\u001b[0m\n",
      "[2023-10-22 11:48:42] \u001b[32mEpoch [7/10] Step [61/469]  acc1 0.890625 (0.859119)  loss 186.429535 (186.394295)\u001b[0m\n",
      "[2023-10-22 11:48:43] \u001b[32mEpoch [7/10] Step [71/469]  acc1 0.765625 (0.857394)  loss 186.593964 (186.365657)\u001b[0m\n",
      "[2023-10-22 11:48:45] \u001b[32mEpoch [7/10] Step [81/469]  acc1 0.828125 (0.855903)  loss 186.581268 (186.385881)\u001b[0m\n",
      "[2023-10-22 11:48:46] \u001b[32mEpoch [7/10] Step [91/469]  acc1 0.875000 (0.857143)  loss 186.546082 (186.399754)\u001b[0m\n",
      "[2023-10-22 11:48:47] \u001b[32mEpoch [7/10] Step [101/469]  acc1 0.796875 (0.856590)  loss 186.546188 (186.413544)\u001b[0m\n",
      "[2023-10-22 11:48:49] \u001b[32mEpoch [7/10] Step [111/469]  acc1 0.781250 (0.856700)  loss 186.580566 (186.424085)\u001b[0m\n",
      "[2023-10-22 11:48:50] \u001b[32mEpoch [7/10] Step [121/469]  acc1 0.859375 (0.854726)  loss 186.667633 (186.436212)\u001b[0m\n",
      "[2023-10-22 11:48:52] \u001b[32mEpoch [7/10] Step [131/469]  acc1 0.734375 (0.855200)  loss 186.765656 (186.442937)\u001b[0m\n",
      "[2023-10-22 11:48:53] \u001b[32mEpoch [7/10] Step [141/469]  acc1 0.906250 (0.854942)  loss 186.490509 (186.449849)\u001b[0m\n",
      "[2023-10-22 11:48:54] \u001b[32mEpoch [7/10] Step [151/469]  acc1 0.890625 (0.854925)  loss 186.422546 (186.454683)\u001b[0m\n",
      "[2023-10-22 11:48:56] \u001b[32mEpoch [7/10] Step [161/469]  acc1 0.859375 (0.855299)  loss 186.443481 (186.458706)\u001b[0m\n",
      "[2023-10-22 11:48:57] \u001b[32mEpoch [7/10] Step [171/469]  acc1 0.859375 (0.856451)  loss 186.472870 (186.459581)\u001b[0m\n",
      "[2023-10-22 11:48:58] \u001b[32mEpoch [7/10] Step [181/469]  acc1 0.828125 (0.856181)  loss 186.589920 (186.462933)\u001b[0m\n",
      "[2023-10-22 11:49:00] \u001b[32mEpoch [7/10] Step [191/469]  acc1 0.875000 (0.855530)  loss 186.534592 (186.467393)\u001b[0m\n",
      "[2023-10-22 11:49:01] \u001b[32mEpoch [7/10] Step [201/469]  acc1 0.875000 (0.855488)  loss 186.474030 (186.470280)\u001b[0m\n",
      "[2023-10-22 11:49:03] \u001b[32mEpoch [7/10] Step [211/469]  acc1 0.921875 (0.855154)  loss 186.394196 (186.473167)\u001b[0m\n",
      "[2023-10-22 11:49:04] \u001b[32mEpoch [7/10] Step [221/469]  acc1 0.796875 (0.855769)  loss 173.513489 (186.414317)\u001b[0m\n",
      "[2023-10-22 11:49:05] \u001b[32mEpoch [7/10] Step [231/469]  acc1 0.875000 (0.856264)  loss 186.471680 (186.416698)\u001b[0m\n",
      "[2023-10-22 11:49:07] \u001b[32mEpoch [7/10] Step [241/469]  acc1 0.843750 (0.856911)  loss 186.612213 (186.417458)\u001b[0m\n",
      "[2023-10-22 11:49:08] \u001b[32mEpoch [7/10] Step [251/469]  acc1 0.906250 (0.858254)  loss 186.371262 (186.418376)\u001b[0m\n",
      "[2023-10-22 11:49:09] \u001b[32mEpoch [7/10] Step [261/469]  acc1 0.843750 (0.859016)  loss 186.612366 (186.421909)\u001b[0m\n",
      "[2023-10-22 11:49:11] \u001b[32mEpoch [7/10] Step [271/469]  acc1 0.812500 (0.858568)  loss 187.760376 (186.431241)\u001b[0m\n",
      "[2023-10-22 11:49:12] \u001b[32mEpoch [7/10] Step [281/469]  acc1 0.812500 (0.858263)  loss 186.595932 (186.435702)\u001b[0m\n",
      "[2023-10-22 11:49:14] \u001b[32mEpoch [7/10] Step [291/469]  acc1 0.890625 (0.858838)  loss 186.426910 (186.358749)\u001b[0m\n",
      "[2023-10-22 11:49:15] \u001b[32mEpoch [7/10] Step [301/469]  acc1 0.859375 (0.859479)  loss 186.484360 (186.363179)\u001b[0m\n",
      "[2023-10-22 11:49:16] \u001b[32mEpoch [7/10] Step [311/469]  acc1 0.906250 (0.859877)  loss 186.459671 (186.366726)\u001b[0m\n",
      "[2023-10-22 11:49:18] \u001b[32mEpoch [7/10] Step [321/469]  acc1 0.859375 (0.860300)  loss 186.596466 (186.368028)\u001b[0m\n",
      "[2023-10-22 11:49:19] \u001b[32mEpoch [7/10] Step [331/469]  acc1 0.843750 (0.861405)  loss 186.580490 (186.369901)\u001b[0m\n",
      "[2023-10-22 11:49:20] \u001b[32mEpoch [7/10] Step [341/469]  acc1 0.890625 (0.861712)  loss 184.082275 (186.366547)\u001b[0m\n",
      "[2023-10-22 11:49:22] \u001b[32mEpoch [7/10] Step [351/469]  acc1 0.812500 (0.862224)  loss 186.536697 (186.370240)\u001b[0m\n",
      "[2023-10-22 11:49:23] \u001b[32mEpoch [7/10] Step [361/469]  acc1 0.875000 (0.862145)  loss 186.481888 (186.372837)\u001b[0m\n",
      "[2023-10-22 11:49:24] \u001b[32mEpoch [7/10] Step [371/469]  acc1 0.843750 (0.860681)  loss 186.581619 (185.393681)\u001b[0m\n",
      "[2023-10-22 11:49:26] \u001b[32mEpoch [7/10] Step [381/469]  acc1 0.843750 (0.860113)  loss 186.615128 (185.280313)\u001b[0m\n",
      "[2023-10-22 11:49:27] \u001b[32mEpoch [7/10] Step [391/469]  acc1 0.953125 (0.861133)  loss 180.750473 (185.295380)\u001b[0m\n",
      "[2023-10-22 11:49:28] \u001b[32mEpoch [7/10] Step [401/469]  acc1 0.812500 (0.861518)  loss 186.625687 (184.995921)\u001b[0m\n",
      "[2023-10-22 11:49:30] \u001b[32mEpoch [7/10] Step [411/469]  acc1 0.296875 (0.855231)  loss 295.517700 (184.431555)\u001b[0m\n",
      "[2023-10-22 11:49:31] \u001b[32mEpoch [7/10] Step [421/469]  acc1 0.578125 (0.846237)  loss 302.235809 (187.235575)\u001b[0m\n",
      "[2023-10-22 11:49:32] \u001b[32mEpoch [7/10] Step [431/469]  acc1 0.578125 (0.841575)  loss 302.105042 (189.896973)\u001b[0m\n",
      "[2023-10-22 11:49:34] \u001b[32mEpoch [7/10] Step [441/469]  acc1 0.671875 (0.838187)  loss 301.950836 (192.431623)\u001b[0m\n",
      "[2023-10-22 11:49:35] \u001b[32mEpoch [7/10] Step [451/469]  acc1 0.796875 (0.835608)  loss 301.402344 (194.853213)\u001b[0m\n",
      "[2023-10-22 11:49:36] \u001b[32mEpoch [7/10] Step [461/469]  acc1 0.687500 (0.833311)  loss 301.330444 (197.169114)\u001b[0m\n",
      "[2023-10-22 11:49:39] \u001b[32mEpoch [8/10] Step [1/469]  acc1 0.687500 (0.687500)  loss 301.679779 (301.679779)\u001b[0m\n",
      "[2023-10-22 11:49:40] \u001b[32mEpoch [8/10] Step [11/469]  acc1 0.812500 (0.759943)  loss 186.517273 (200.119599)\u001b[0m\n",
      "[2023-10-22 11:49:42] \u001b[32mEpoch [8/10] Step [21/469]  acc1 0.812500 (0.788690)  loss 186.647995 (193.708764)\u001b[0m\n",
      "[2023-10-22 11:49:43] \u001b[32mEpoch [8/10] Step [31/469]  acc1 0.906250 (0.802419)  loss 186.458466 (191.405756)\u001b[0m\n",
      "[2023-10-22 11:49:45] \u001b[32mEpoch [8/10] Step [41/469]  acc1 0.812500 (0.807546)  loss 186.629242 (190.221964)\u001b[0m\n",
      "[2023-10-22 11:49:46] \u001b[32mEpoch [8/10] Step [51/469]  acc1 0.875000 (0.818015)  loss 186.445557 (189.495309)\u001b[0m\n",
      "[2023-10-22 11:49:47] \u001b[32mEpoch [8/10] Step [61/469]  acc1 0.765625 (0.821977)  loss 186.651276 (189.016079)\u001b[0m\n",
      "[2023-10-22 11:49:49] \u001b[32mEpoch [8/10] Step [71/469]  acc1 0.828125 (0.827685)  loss 186.574890 (188.653204)\u001b[0m\n",
      "[2023-10-22 11:49:50] \u001b[32mEpoch [8/10] Step [81/469]  acc1 0.843750 (0.830826)  loss 186.637344 (188.388738)\u001b[0m\n",
      "[2023-10-22 11:49:51] \u001b[32mEpoch [8/10] Step [91/469]  acc1 0.843750 (0.833276)  loss 186.590973 (188.183453)\u001b[0m\n",
      "[2023-10-22 11:49:53] \u001b[32mEpoch [8/10] Step [101/469]  acc1 0.828125 (0.832611)  loss 186.682861 (188.030288)\u001b[0m\n",
      "[2023-10-22 11:49:54] \u001b[32mEpoch [8/10] Step [111/469]  acc1 0.875000 (0.834741)  loss 186.429825 (187.890286)\u001b[0m\n",
      "[2023-10-22 11:49:55] \u001b[32mEpoch [8/10] Step [121/469]  acc1 0.812500 (0.835356)  loss 150.217667 (187.479628)\u001b[0m\n",
      "[2023-10-22 11:49:57] \u001b[32mEpoch [8/10] Step [131/469]  acc1 0.828125 (0.836355)  loss 186.533554 (187.407690)\u001b[0m\n",
      "[2023-10-22 11:49:58] \u001b[32mEpoch [8/10] Step [141/469]  acc1 0.937500 (0.837544)  loss 186.402496 (187.345064)\u001b[0m\n",
      "[2023-10-22 11:49:59] \u001b[32mEpoch [8/10] Step [151/469]  acc1 0.796875 (0.838162)  loss 186.509155 (187.290009)\u001b[0m\n",
      "[2023-10-22 11:50:01] \u001b[32mEpoch [8/10] Step [161/469]  acc1 0.796875 (0.840062)  loss 186.598969 (187.241421)\u001b[0m\n",
      "[2023-10-22 11:50:02] \u001b[32mEpoch [8/10] Step [171/469]  acc1 0.906250 (0.843019)  loss 186.562561 (187.185779)\u001b[0m\n",
      "[2023-10-22 11:50:03] \u001b[32mEpoch [8/10] Step [181/469]  acc1 0.875000 (0.844354)  loss 186.416168 (187.148182)\u001b[0m\n",
      "[2023-10-22 11:50:05] \u001b[32mEpoch [8/10] Step [191/469]  acc1 0.875000 (0.845223)  loss 186.429810 (187.114059)\u001b[0m\n",
      "[2023-10-22 11:50:06] \u001b[32mEpoch [8/10] Step [201/469]  acc1 0.875000 (0.846471)  loss 186.514740 (187.083646)\u001b[0m\n",
      "[2023-10-22 11:50:07] \u001b[32mEpoch [8/10] Step [211/469]  acc1 0.890625 (0.847230)  loss 186.519440 (187.044890)\u001b[0m\n",
      "[2023-10-22 11:50:09] \u001b[32mEpoch [8/10] Step [221/469]  acc1 0.937500 (0.848699)  loss 186.397202 (187.019993)\u001b[0m\n",
      "[2023-10-22 11:50:10] \u001b[32mEpoch [8/10] Step [231/469]  acc1 0.953125 (0.849770)  loss 186.293015 (186.996861)\u001b[0m\n",
      "[2023-10-22 11:50:11] \u001b[32mEpoch [8/10] Step [241/469]  acc1 0.828125 (0.850558)  loss 186.470322 (186.974338)\u001b[0m\n",
      "[2023-10-22 11:50:13] \u001b[32mEpoch [8/10] Step [251/469]  acc1 0.859375 (0.850660)  loss 186.496353 (186.214486)\u001b[0m\n",
      "[2023-10-22 11:50:14] \u001b[32mEpoch [8/10] Step [261/469]  acc1 0.906250 (0.851353)  loss 186.374100 (186.224781)\u001b[0m\n",
      "[2023-10-22 11:50:15] \u001b[32mEpoch [8/10] Step [271/469]  acc1 0.796875 (0.851937)  loss 186.636124 (186.234231)\u001b[0m\n",
      "[2023-10-22 11:50:17] \u001b[32mEpoch [8/10] Step [281/469]  acc1 0.828125 (0.852591)  loss 186.635239 (186.232333)\u001b[0m\n",
      "[2023-10-22 11:50:18] \u001b[32mEpoch [8/10] Step [291/469]  acc1 0.843750 (0.853683)  loss 186.546463 (186.240721)\u001b[0m\n",
      "[2023-10-22 11:50:19] \u001b[32mEpoch [8/10] Step [301/469]  acc1 0.906250 (0.854651)  loss 186.354568 (186.247123)\u001b[0m\n",
      "[2023-10-22 11:50:21] \u001b[32mEpoch [8/10] Step [311/469]  acc1 0.828125 (0.855607)  loss 186.514465 (186.254132)\u001b[0m\n",
      "[2023-10-22 11:50:22] \u001b[32mEpoch [8/10] Step [321/469]  acc1 0.812500 (0.855870)  loss 186.581161 (186.261736)\u001b[0m\n",
      "[2023-10-22 11:50:23] \u001b[32mEpoch [8/10] Step [331/469]  acc1 0.875000 (0.856401)  loss 186.410370 (186.267750)\u001b[0m\n",
      "[2023-10-22 11:50:25] \u001b[32mEpoch [8/10] Step [341/469]  acc1 0.906250 (0.856809)  loss 186.403458 (186.274081)\u001b[0m\n",
      "[2023-10-22 11:50:26] \u001b[32mEpoch [8/10] Step [351/469]  acc1 0.890625 (0.856481)  loss 186.458847 (186.282873)\u001b[0m\n",
      "[2023-10-22 11:50:28] \u001b[32mEpoch [8/10] Step [361/469]  acc1 0.875000 (0.856518)  loss 186.489777 (186.283881)\u001b[0m\n",
      "[2023-10-22 11:50:29] \u001b[32mEpoch [8/10] Step [371/469]  acc1 0.953125 (0.856553)  loss 186.367447 (186.076511)\u001b[0m\n",
      "[2023-10-22 11:50:30] \u001b[32mEpoch [8/10] Step [381/469]  acc1 0.890625 (0.856053)  loss 186.385010 (186.089021)\u001b[0m\n",
      "[2023-10-22 11:50:32] \u001b[32mEpoch [8/10] Step [391/469]  acc1 0.828125 (0.856538)  loss 186.549759 (186.099020)\u001b[0m\n",
      "[2023-10-22 11:50:33] \u001b[32mEpoch [8/10] Step [401/469]  acc1 0.921875 (0.856998)  loss 186.387512 (186.108614)\u001b[0m\n",
      "[2023-10-22 11:50:34] \u001b[32mEpoch [8/10] Step [411/469]  acc1 0.937500 (0.856980)  loss 186.369690 (186.117893)\u001b[0m\n",
      "[2023-10-22 11:50:36] \u001b[32mEpoch [8/10] Step [421/469]  acc1 0.921875 (0.857297)  loss 186.503036 (186.127670)\u001b[0m\n",
      "[2023-10-22 11:50:37] \u001b[32mEpoch [8/10] Step [431/469]  acc1 0.875000 (0.857164)  loss 186.534271 (186.137191)\u001b[0m\n",
      "[2023-10-22 11:50:38] \u001b[32mEpoch [8/10] Step [441/469]  acc1 0.906250 (0.857497)  loss 186.498672 (186.145914)\u001b[0m\n",
      "[2023-10-22 11:50:40] \u001b[32mEpoch [8/10] Step [451/469]  acc1 0.937500 (0.857747)  loss 186.391800 (186.154272)\u001b[0m\n",
      "[2023-10-22 11:50:41] \u001b[32mEpoch [8/10] Step [461/469]  acc1 0.921875 (0.858189)  loss 186.372101 (186.160772)\u001b[0m\n",
      "[2023-10-22 11:50:44] \u001b[32mEpoch [9/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 186.404877 (186.404877)\u001b[0m\n",
      "[2023-10-22 11:50:45] \u001b[32mEpoch [9/10] Step [11/469]  acc1 0.859375 (0.853693)  loss 186.589127 (186.513870)\u001b[0m\n",
      "[2023-10-22 11:50:47] \u001b[32mEpoch [9/10] Step [21/469]  acc1 0.937500 (0.863095)  loss 186.407120 (186.499170)\u001b[0m\n",
      "[2023-10-22 11:50:48] \u001b[32mEpoch [9/10] Step [31/469]  acc1 0.859375 (0.872480)  loss 186.547211 (186.476360)\u001b[0m\n",
      "[2023-10-22 11:50:49] \u001b[32mEpoch [9/10] Step [41/469]  acc1 0.859375 (0.873476)  loss 186.489609 (186.479436)\u001b[0m\n",
      "[2023-10-22 11:50:50] \u001b[32mEpoch [9/10] Step [51/469]  acc1 0.906250 (0.876532)  loss 186.464386 (186.471295)\u001b[0m\n",
      "[2023-10-22 11:50:52] \u001b[32mEpoch [9/10] Step [61/469]  acc1 0.875000 (0.873975)  loss 186.525192 (186.479867)\u001b[0m\n",
      "[2023-10-22 11:50:53] \u001b[32mEpoch [9/10] Step [71/469]  acc1 0.828125 (0.869718)  loss 186.492889 (186.485160)\u001b[0m\n",
      "[2023-10-22 11:50:55] \u001b[32mEpoch [9/10] Step [81/469]  acc1 0.828125 (0.868634)  loss 186.621674 (186.485452)\u001b[0m\n",
      "[2023-10-22 11:50:56] \u001b[32mEpoch [9/10] Step [91/469]  acc1 0.875000 (0.869162)  loss 186.445633 (186.481787)\u001b[0m\n",
      "[2023-10-22 11:50:57] \u001b[32mEpoch [9/10] Step [101/469]  acc1 0.875000 (0.868967)  loss 186.416061 (186.478001)\u001b[0m\n",
      "[2023-10-22 11:50:59] \u001b[32mEpoch [9/10] Step [111/469]  acc1 0.859375 (0.869651)  loss 186.527328 (186.479685)\u001b[0m\n",
      "[2023-10-22 11:51:00] \u001b[32mEpoch [9/10] Step [121/469]  acc1 0.921875 (0.872030)  loss 186.509399 (186.476885)\u001b[0m\n",
      "[2023-10-22 11:51:02] \u001b[32mEpoch [9/10] Step [131/469]  acc1 0.843750 (0.871422)  loss 186.613892 (186.477486)\u001b[0m\n",
      "[2023-10-22 11:51:03] \u001b[32mEpoch [9/10] Step [141/469]  acc1 0.906250 (0.872008)  loss 186.273590 (186.477007)\u001b[0m\n",
      "[2023-10-22 11:51:04] \u001b[32mEpoch [9/10] Step [151/469]  acc1 0.937500 (0.873448)  loss 186.350143 (186.473093)\u001b[0m\n",
      "[2023-10-22 11:51:06] \u001b[32mEpoch [9/10] Step [161/469]  acc1 0.875000 (0.872962)  loss 186.473755 (186.473869)\u001b[0m\n",
      "[2023-10-22 11:51:07] \u001b[32mEpoch [9/10] Step [171/469]  acc1 0.953125 (0.873173)  loss 186.338745 (186.473546)\u001b[0m\n",
      "[2023-10-22 11:51:09] \u001b[32mEpoch [9/10] Step [181/469]  acc1 0.859375 (0.873532)  loss 186.524338 (186.472262)\u001b[0m\n",
      "[2023-10-22 11:51:10] \u001b[32mEpoch [9/10] Step [191/469]  acc1 0.859375 (0.873200)  loss 186.502258 (186.473431)\u001b[0m\n",
      "[2023-10-22 11:51:11] \u001b[32mEpoch [9/10] Step [201/469]  acc1 0.859375 (0.873445)  loss 186.502106 (186.472472)\u001b[0m\n",
      "[2023-10-22 11:51:13] \u001b[32mEpoch [9/10] Step [211/469]  acc1 0.921875 (0.874926)  loss 186.378845 (186.471006)\u001b[0m\n",
      "[2023-10-22 11:51:14] \u001b[32mEpoch [9/10] Step [221/469]  acc1 0.875000 (0.876061)  loss 186.412415 (186.468045)\u001b[0m\n",
      "[2023-10-22 11:51:15] \u001b[32mEpoch [9/10] Step [231/469]  acc1 0.859375 (0.875203)  loss 186.454865 (186.470236)\u001b[0m\n",
      "[2023-10-22 11:51:17] \u001b[32mEpoch [9/10] Step [241/469]  acc1 0.812500 (0.874870)  loss 186.533020 (186.471220)\u001b[0m\n",
      "[2023-10-22 11:51:18] \u001b[32mEpoch [9/10] Step [251/469]  acc1 0.765625 (0.874253)  loss 186.671509 (186.471334)\u001b[0m\n",
      "[2023-10-22 11:51:20] \u001b[32mEpoch [9/10] Step [261/469]  acc1 0.859375 (0.874820)  loss 186.566864 (186.470978)\u001b[0m\n",
      "[2023-10-22 11:51:21] \u001b[32mEpoch [9/10] Step [271/469]  acc1 0.828125 (0.874020)  loss 186.578262 (186.473211)\u001b[0m\n",
      "[2023-10-22 11:51:22] \u001b[32mEpoch [9/10] Step [281/469]  acc1 0.906250 (0.874444)  loss 186.377625 (186.471395)\u001b[0m\n",
      "[2023-10-22 11:51:24] \u001b[32mEpoch [9/10] Step [291/469]  acc1 0.921875 (0.874356)  loss 186.332077 (186.472289)\u001b[0m\n",
      "[2023-10-22 11:51:25] \u001b[32mEpoch [9/10] Step [301/469]  acc1 0.859375 (0.874169)  loss 186.592667 (186.472737)\u001b[0m\n",
      "[2023-10-22 11:51:26] \u001b[32mEpoch [9/10] Step [311/469]  acc1 0.812500 (0.874347)  loss 186.499146 (186.472641)\u001b[0m\n",
      "[2023-10-22 11:51:28] \u001b[32mEpoch [9/10] Step [321/469]  acc1 0.812500 (0.873248)  loss 186.687576 (186.475320)\u001b[0m\n",
      "[2023-10-22 11:51:29] \u001b[32mEpoch [9/10] Step [331/469]  acc1 0.906250 (0.873489)  loss 186.436874 (186.474828)\u001b[0m\n",
      "[2023-10-22 11:51:31] \u001b[32mEpoch [9/10] Step [341/469]  acc1 0.921875 (0.873213)  loss 186.297516 (186.474828)\u001b[0m\n",
      "[2023-10-22 11:51:32] \u001b[32mEpoch [9/10] Step [351/469]  acc1 0.828125 (0.872863)  loss 186.644241 (186.475380)\u001b[0m\n",
      "[2023-10-22 11:51:33] \u001b[32mEpoch [9/10] Step [361/469]  acc1 0.859375 (0.872619)  loss 186.580063 (186.475811)\u001b[0m\n",
      "[2023-10-22 11:51:35] \u001b[32mEpoch [9/10] Step [371/469]  acc1 0.828125 (0.873315)  loss 186.533890 (186.290037)\u001b[0m\n",
      "[2023-10-22 11:51:36] \u001b[32mEpoch [9/10] Step [381/469]  acc1 0.906250 (0.873237)  loss 186.356812 (186.294869)\u001b[0m\n",
      "[2023-10-22 11:51:37] \u001b[32mEpoch [9/10] Step [391/469]  acc1 0.875000 (0.873202)  loss 186.414139 (186.299948)\u001b[0m\n",
      "[2023-10-22 11:51:39] \u001b[32mEpoch [9/10] Step [401/469]  acc1 0.859375 (0.873675)  loss 186.428345 (186.303334)\u001b[0m\n",
      "[2023-10-22 11:51:40] \u001b[32mEpoch [9/10] Step [411/469]  acc1 0.906250 (0.874164)  loss 186.373642 (186.306877)\u001b[0m\n",
      "[2023-10-22 11:51:41] \u001b[32mEpoch [9/10] Step [421/469]  acc1 0.890625 (0.874109)  loss 186.470428 (186.310422)\u001b[0m\n",
      "[2023-10-22 11:51:43] \u001b[32mEpoch [9/10] Step [431/469]  acc1 0.875000 (0.874311)  loss 186.427017 (186.313392)\u001b[0m\n",
      "[2023-10-22 11:51:44] \u001b[32mEpoch [9/10] Step [441/469]  acc1 0.906250 (0.874150)  loss 186.391617 (186.317001)\u001b[0m\n",
      "[2023-10-22 11:51:45] \u001b[32mEpoch [9/10] Step [451/469]  acc1 0.843750 (0.874203)  loss 186.409576 (186.320338)\u001b[0m\n",
      "[2023-10-22 11:51:47] \u001b[32mEpoch [9/10] Step [461/469]  acc1 0.906250 (0.874017)  loss 186.351593 (186.323725)\u001b[0m\n",
      "[2023-10-22 11:51:51] \u001b[32mEpoch [10/10] Step [1/469]  acc1 0.906250 (0.906250)  loss 186.339005 (186.339005)\u001b[0m\n",
      "[2023-10-22 11:51:52] \u001b[32mEpoch [10/10] Step [11/469]  acc1 0.875000 (0.897727)  loss 186.409393 (186.432060)\u001b[0m\n",
      "[2023-10-22 11:51:54] \u001b[32mEpoch [10/10] Step [21/469]  acc1 0.921875 (0.896577)  loss 186.377640 (186.436074)\u001b[0m\n",
      "[2023-10-22 11:51:55] \u001b[32mEpoch [10/10] Step [31/469]  acc1 0.875000 (0.897177)  loss 186.452957 (186.422160)\u001b[0m\n",
      "[2023-10-22 11:51:56] \u001b[32mEpoch [10/10] Step [41/469]  acc1 0.906250 (0.891387)  loss 186.377029 (186.438720)\u001b[0m\n",
      "[2023-10-22 11:51:58] \u001b[32mEpoch [10/10] Step [51/469]  acc1 0.953125 (0.887255)  loss 186.353989 (186.451135)\u001b[0m\n",
      "[2023-10-22 11:51:59] \u001b[32mEpoch [10/10] Step [61/469]  acc1 0.828125 (0.888064)  loss 186.641556 (186.450694)\u001b[0m\n",
      "[2023-10-22 11:52:00] \u001b[32mEpoch [10/10] Step [71/469]  acc1 0.859375 (0.887324)  loss 186.436813 (186.448831)\u001b[0m\n",
      "[2023-10-22 11:52:02] \u001b[32mEpoch [10/10] Step [81/469]  acc1 0.937500 (0.887346)  loss 186.380081 (186.449287)\u001b[0m\n",
      "[2023-10-22 11:52:03] \u001b[32mEpoch [10/10] Step [91/469]  acc1 0.828125 (0.888565)  loss 186.482498 (186.446127)\u001b[0m\n",
      "[2023-10-22 11:52:04] \u001b[32mEpoch [10/10] Step [101/469]  acc1 0.921875 (0.890934)  loss 186.466171 (186.442551)\u001b[0m\n",
      "[2023-10-22 11:52:06] \u001b[32mEpoch [10/10] Step [111/469]  acc1 0.890625 (0.890343)  loss 186.469635 (186.444036)\u001b[0m\n",
      "[2023-10-22 11:52:07] \u001b[32mEpoch [10/10] Step [121/469]  acc1 0.890625 (0.888688)  loss 186.485596 (186.444469)\u001b[0m\n",
      "[2023-10-22 11:52:08] \u001b[32mEpoch [10/10] Step [131/469]  acc1 0.953125 (0.888597)  loss 186.304733 (185.699759)\u001b[0m\n",
      "[2023-10-22 11:52:10] \u001b[32mEpoch [10/10] Step [141/469]  acc1 0.875000 (0.887855)  loss 186.502441 (185.754229)\u001b[0m\n",
      "[2023-10-22 11:52:11] \u001b[32mEpoch [10/10] Step [151/469]  acc1 0.796875 (0.887314)  loss 186.547409 (185.800675)\u001b[0m\n",
      "[2023-10-22 11:52:12] \u001b[32mEpoch [10/10] Step [161/469]  acc1 0.906250 (0.886064)  loss 186.458176 (185.842889)\u001b[0m\n",
      "[2023-10-22 11:52:14] \u001b[32mEpoch [10/10] Step [171/469]  acc1 0.921875 (0.886056)  loss 186.431625 (185.879082)\u001b[0m\n",
      "[2023-10-22 11:52:15] \u001b[32mEpoch [10/10] Step [181/469]  acc1 0.906250 (0.885445)  loss 186.395630 (185.910804)\u001b[0m\n",
      "[2023-10-22 11:52:16] \u001b[32mEpoch [10/10] Step [191/469]  acc1 0.828125 (0.884244)  loss 186.567642 (185.941863)\u001b[0m\n",
      "[2023-10-22 11:52:18] \u001b[32mEpoch [10/10] Step [201/469]  acc1 0.906250 (0.883940)  loss 186.359955 (185.967680)\u001b[0m\n",
      "[2023-10-22 11:52:19] \u001b[32mEpoch [10/10] Step [211/469]  acc1 0.906250 (0.884775)  loss 186.337540 (185.988867)\u001b[0m\n",
      "[2023-10-22 11:52:21] \u001b[32mEpoch [10/10] Step [221/469]  acc1 0.984375 (0.885464)  loss 186.269592 (186.008542)\u001b[0m\n",
      "[2023-10-22 11:52:22] \u001b[32mEpoch [10/10] Step [231/469]  acc1 0.921875 (0.886228)  loss 186.340302 (186.025649)\u001b[0m\n",
      "[2023-10-22 11:52:24] \u001b[32mEpoch [10/10] Step [241/469]  acc1 0.843750 (0.884855)  loss 186.478668 (186.043841)\u001b[0m\n",
      "[2023-10-22 11:52:25] \u001b[32mEpoch [10/10] Step [251/469]  acc1 0.890625 (0.884773)  loss 186.387802 (186.058283)\u001b[0m\n",
      "[2023-10-22 11:52:27] \u001b[32mEpoch [10/10] Step [261/469]  acc1 0.812500 (0.883740)  loss 186.531265 (186.067898)\u001b[0m\n",
      "[2023-10-22 11:52:28] \u001b[32mEpoch [10/10] Step [271/469]  acc1 0.890625 (0.883994)  loss 186.366486 (186.081641)\u001b[0m\n",
      "[2023-10-22 11:52:30] \u001b[32mEpoch [10/10] Step [281/469]  acc1 0.890625 (0.884230)  loss 186.318741 (186.093059)\u001b[0m\n",
      "[2023-10-22 11:52:31] \u001b[32mEpoch [10/10] Step [291/469]  acc1 0.875000 (0.884128)  loss 186.423309 (186.105476)\u001b[0m\n",
      "[2023-10-22 11:52:33] \u001b[32mEpoch [10/10] Step [301/469]  acc1 0.906250 (0.883929)  loss 186.424271 (186.116611)\u001b[0m\n",
      "[2023-10-22 11:52:34] \u001b[32mEpoch [10/10] Step [311/469]  acc1 0.890625 (0.883641)  loss 186.432175 (186.128040)\u001b[0m\n",
      "[2023-10-22 11:52:36] \u001b[32mEpoch [10/10] Step [321/469]  acc1 0.890625 (0.882837)  loss 186.425995 (186.139428)\u001b[0m\n",
      "[2023-10-22 11:52:37] \u001b[32mEpoch [10/10] Step [331/469]  acc1 0.875000 (0.882600)  loss 186.439972 (186.091653)\u001b[0m\n",
      "[2023-10-22 11:52:39] \u001b[32mEpoch [10/10] Step [341/469]  acc1 0.859375 (0.882423)  loss 186.558472 (186.102449)\u001b[0m\n",
      "[2023-10-22 11:52:41] \u001b[32mEpoch [10/10] Step [351/469]  acc1 0.875000 (0.882434)  loss 186.414490 (186.112100)\u001b[0m\n",
      "[2023-10-22 11:52:42] \u001b[32mEpoch [10/10] Step [361/469]  acc1 0.953125 (0.882445)  loss 186.355743 (186.119756)\u001b[0m\n",
      "[2023-10-22 11:52:44] \u001b[32mEpoch [10/10] Step [371/469]  acc1 0.937500 (0.882328)  loss 186.284348 (186.128928)\u001b[0m\n",
      "[2023-10-22 11:52:45] \u001b[32mEpoch [10/10] Step [381/469]  acc1 0.906250 (0.882382)  loss 186.500900 (186.137961)\u001b[0m\n",
      "[2023-10-22 11:52:46] \u001b[32mEpoch [10/10] Step [391/469]  acc1 0.890625 (0.882593)  loss 186.399734 (186.145650)\u001b[0m\n",
      "[2023-10-22 11:52:48] \u001b[32mEpoch [10/10] Step [401/469]  acc1 0.906250 (0.882715)  loss 186.428726 (186.152561)\u001b[0m\n",
      "[2023-10-22 11:52:49] \u001b[32mEpoch [10/10] Step [411/469]  acc1 0.812500 (0.882185)  loss 186.535660 (186.159932)\u001b[0m\n",
      "[2023-10-22 11:52:50] \u001b[32mEpoch [10/10] Step [421/469]  acc1 0.921875 (0.882349)  loss 186.451111 (186.166684)\u001b[0m\n",
      "[2023-10-22 11:52:52] \u001b[32mEpoch [10/10] Step [431/469]  acc1 0.859375 (0.881743)  loss 186.514938 (186.174980)\u001b[0m\n",
      "[2023-10-22 11:52:53] \u001b[32mEpoch [10/10] Step [441/469]  acc1 0.859375 (0.881378)  loss 186.463730 (186.181855)\u001b[0m\n",
      "[2023-10-22 11:52:54] \u001b[32mEpoch [10/10] Step [451/469]  acc1 0.859375 (0.881513)  loss 186.483337 (186.188572)\u001b[0m\n",
      "[2023-10-22 11:52:56] \u001b[32mEpoch [10/10] Step [461/469]  acc1 0.875000 (0.881406)  loss 186.411194 (186.194344)\u001b[0m\n",
      "Final architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'maxpool', 'reduce_n3_p0': 'dilconv3x3', 'reduce_n3_p1': 'maxpool', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'dilconv5x5', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'sepconv3x3', 'reduce_n5_p2': 'dilconv5x5', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'avgpool', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [2, 1], 'reduce_n4_switch': [3, 0], 'reduce_n5_switch': [3, 4]}\n",
      "weight = 1000.0, lambd = 4\n",
      "[2023-10-22 11:52:58] \u001b[32mEpoch [1/10] Step [1/469]  acc1 0.187500 (0.187500)  loss 4940.245117 (4940.245117)\u001b[0m\n",
      "[2023-10-22 11:53:00] \u001b[32mEpoch [1/10] Step [11/469]  acc1 0.171875 (0.129261)  loss 6149.481445 (5740.405584)\u001b[0m\n",
      "[2023-10-22 11:53:01] \u001b[32mEpoch [1/10] Step [21/469]  acc1 0.125000 (0.141369)  loss 9.772446 (3295.673036)\u001b[0m\n",
      "[2023-10-22 11:53:03] \u001b[32mEpoch [1/10] Step [31/469]  acc1 0.187500 (0.154234)  loss 7.139411 (2277.002381)\u001b[0m\n",
      "[2023-10-22 11:53:04] \u001b[32mEpoch [1/10] Step [41/469]  acc1 0.265625 (0.167683)  loss 7.515579 (1778.547489)\u001b[0m\n",
      "[2023-10-22 11:53:05] \u001b[32mEpoch [1/10] Step [51/469]  acc1 0.468750 (0.194547)  loss 79.327827 (1451.407242)\u001b[0m\n",
      "[2023-10-22 11:53:07] \u001b[32mEpoch [1/10] Step [61/469]  acc1 0.296875 (0.214652)  loss 6.688682 (1237.844184)\u001b[0m\n",
      "[2023-10-22 11:53:08] \u001b[32mEpoch [1/10] Step [71/469]  acc1 0.437500 (0.239217)  loss 7.802828 (1064.843163)\u001b[0m\n",
      "[2023-10-22 11:53:09] \u001b[32mEpoch [1/10] Step [81/469]  acc1 0.406250 (0.262539)  loss 12.760131 (937.405728)\u001b[0m\n",
      "[2023-10-22 11:53:11] \u001b[32mEpoch [1/10] Step [91/469]  acc1 0.515625 (0.283826)  loss 6.242077 (847.268745)\u001b[0m\n",
      "[2023-10-22 11:53:12] \u001b[32mEpoch [1/10] Step [101/469]  acc1 0.515625 (0.305074)  loss 6.099413 (764.075317)\u001b[0m\n",
      "[2023-10-22 11:53:13] \u001b[32mEpoch [1/10] Step [111/469]  acc1 0.609375 (0.329392)  loss 6.033921 (696.139435)\u001b[0m\n",
      "[2023-10-22 11:53:15] \u001b[32mEpoch [1/10] Step [121/469]  acc1 0.625000 (0.349948)  loss 5.813436 (642.183288)\u001b[0m\n",
      "[2023-10-22 11:53:16] \u001b[32mEpoch [1/10] Step [131/469]  acc1 0.593750 (0.368440)  loss 37.800648 (599.402562)\u001b[0m\n",
      "[2023-10-22 11:53:17] \u001b[32mEpoch [1/10] Step [141/469]  acc1 0.656250 (0.387301)  loss 5.670367 (557.628167)\u001b[0m\n",
      "[2023-10-22 11:53:19] \u001b[32mEpoch [1/10] Step [151/469]  acc1 0.687500 (0.402214)  loss 5.666188 (522.892468)\u001b[0m\n",
      "[2023-10-22 11:53:20] \u001b[32mEpoch [1/10] Step [161/469]  acc1 0.656250 (0.417217)  loss 5.713132 (503.807846)\u001b[0m\n",
      "[2023-10-22 11:53:21] \u001b[32mEpoch [1/10] Step [171/469]  acc1 0.703125 (0.433023)  loss 5.757716 (479.801482)\u001b[0m\n",
      "[2023-10-22 11:53:23] \u001b[32mEpoch [1/10] Step [181/469]  acc1 0.609375 (0.444233)  loss 5.718289 (457.590272)\u001b[0m\n",
      "[2023-10-22 11:53:24] \u001b[32mEpoch [1/10] Step [191/469]  acc1 0.687500 (0.455906)  loss 6.496165 (434.018929)\u001b[0m\n",
      "[2023-10-22 11:53:25] \u001b[32mEpoch [1/10] Step [201/469]  acc1 0.671875 (0.466029)  loss 5.612447 (413.899224)\u001b[0m\n",
      "[2023-10-22 11:53:27] \u001b[32mEpoch [1/10] Step [211/469]  acc1 0.640625 (0.476525)  loss 5.741459 (394.737871)\u001b[0m\n",
      "[2023-10-22 11:53:28] \u001b[32mEpoch [1/10] Step [221/469]  acc1 0.625000 (0.485365)  loss 5.621548 (377.156155)\u001b[0m\n",
      "[2023-10-22 11:53:29] \u001b[32mEpoch [1/10] Step [231/469]  acc1 0.671875 (0.493709)  loss 5.664000 (362.050410)\u001b[0m\n",
      "[2023-10-22 11:53:31] \u001b[32mEpoch [1/10] Step [241/469]  acc1 0.687500 (0.501945)  loss 14.940701 (348.324567)\u001b[0m\n",
      "[2023-10-22 11:53:32] \u001b[32mEpoch [1/10] Step [251/469]  acc1 0.578125 (0.508342)  loss 5.655824 (338.007398)\u001b[0m\n",
      "[2023-10-22 11:53:33] \u001b[32mEpoch [1/10] Step [261/469]  acc1 0.703125 (0.515565)  loss 5.584001 (325.270026)\u001b[0m\n",
      "[2023-10-22 11:53:35] \u001b[32mEpoch [1/10] Step [271/469]  acc1 0.812500 (0.523293)  loss 5.416112 (313.470418)\u001b[0m\n",
      "[2023-10-22 11:53:36] \u001b[32mEpoch [1/10] Step [281/469]  acc1 0.687500 (0.530638)  loss 5.435682 (302.511735)\u001b[0m\n",
      "[2023-10-22 11:53:37] \u001b[32mEpoch [1/10] Step [291/469]  acc1 0.718750 (0.537586)  loss 6.480545 (292.344466)\u001b[0m\n",
      "[2023-10-22 11:53:39] \u001b[32mEpoch [1/10] Step [301/469]  acc1 0.687500 (0.543968)  loss 6.062778 (282.816655)\u001b[0m\n",
      "[2023-10-22 11:53:40] \u001b[32mEpoch [1/10] Step [311/469]  acc1 0.750000 (0.549789)  loss 5.555790 (273.898967)\u001b[0m\n",
      "[2023-10-22 11:53:41] \u001b[32mEpoch [1/10] Step [321/469]  acc1 0.828125 (0.555831)  loss 5.263709 (265.543302)\u001b[0m\n",
      "[2023-10-22 11:53:43] \u001b[32mEpoch [1/10] Step [331/469]  acc1 0.625000 (0.560801)  loss 6.711956 (257.689530)\u001b[0m\n",
      "[2023-10-22 11:53:44] \u001b[32mEpoch [1/10] Step [341/469]  acc1 0.734375 (0.565203)  loss 5.405375 (250.845482)\u001b[0m\n",
      "[2023-10-22 11:53:45] \u001b[32mEpoch [1/10] Step [351/469]  acc1 0.796875 (0.570735)  loss 5.296733 (245.022539)\u001b[0m\n",
      "[2023-10-22 11:53:47] \u001b[32mEpoch [1/10] Step [361/469]  acc1 0.828125 (0.575918)  loss 5.315862 (238.384525)\u001b[0m\n",
      "[2023-10-22 11:53:48] \u001b[32mEpoch [1/10] Step [371/469]  acc1 0.796875 (0.580694)  loss 5.298868 (232.104388)\u001b[0m\n",
      "[2023-10-22 11:53:49] \u001b[32mEpoch [1/10] Step [381/469]  acc1 0.796875 (0.584892)  loss 5.308434 (226.155251)\u001b[0m\n",
      "[2023-10-22 11:53:51] \u001b[32mEpoch [1/10] Step [391/469]  acc1 0.781250 (0.589035)  loss 5.319538 (223.142397)\u001b[0m\n",
      "[2023-10-22 11:53:52] \u001b[32mEpoch [1/10] Step [401/469]  acc1 0.781250 (0.593633)  loss 5.389598 (217.710882)\u001b[0m\n",
      "[2023-10-22 11:53:53] \u001b[32mEpoch [1/10] Step [411/469]  acc1 0.687500 (0.597362)  loss 5.731396 (214.644526)\u001b[0m\n",
      "[2023-10-22 11:53:55] \u001b[32mEpoch [1/10] Step [421/469]  acc1 0.750000 (0.601730)  loss 5.418816 (209.672533)\u001b[0m\n",
      "[2023-10-22 11:53:56] \u001b[32mEpoch [1/10] Step [431/469]  acc1 0.718750 (0.605278)  loss 5.468145 (205.922456)\u001b[0m\n",
      "[2023-10-22 11:53:57] \u001b[32mEpoch [1/10] Step [441/469]  acc1 0.765625 (0.608241)  loss 5.266307 (201.390022)\u001b[0m\n",
      "[2023-10-22 11:53:59] \u001b[32mEpoch [1/10] Step [451/469]  acc1 0.781250 (0.612597)  loss 5.338447 (197.042101)\u001b[0m\n",
      "[2023-10-22 11:54:00] \u001b[32mEpoch [1/10] Step [461/469]  acc1 0.765625 (0.615442)  loss 5.409358 (192.885145)\u001b[0m\n",
      "[2023-10-22 11:54:03] \u001b[32mEpoch [2/10] Step [1/469]  acc1 0.718750 (0.718750)  loss 5.594106 (5.594106)\u001b[0m\n",
      "[2023-10-22 11:54:04] \u001b[32mEpoch [2/10] Step [11/469]  acc1 0.796875 (0.776989)  loss 5.300447 (35.562393)\u001b[0m\n",
      "[2023-10-22 11:54:06] \u001b[32mEpoch [2/10] Step [21/469]  acc1 0.812500 (0.776042)  loss 5.288628 (21.163846)\u001b[0m\n",
      "[2023-10-22 11:54:07] \u001b[32mEpoch [2/10] Step [31/469]  acc1 0.796875 (0.775706)  loss 5.302599 (16.052910)\u001b[0m\n",
      "[2023-10-22 11:54:08] \u001b[32mEpoch [2/10] Step [41/469]  acc1 0.750000 (0.777058)  loss 5.399352 (13.441043)\u001b[0m\n",
      "[2023-10-22 11:54:10] \u001b[32mEpoch [2/10] Step [51/469]  acc1 0.765625 (0.780944)  loss 7.263623 (11.877964)\u001b[0m\n",
      "[2023-10-22 11:54:11] \u001b[32mEpoch [2/10] Step [61/469]  acc1 0.718750 (0.779969)  loss 5.450829 (10.844445)\u001b[0m\n",
      "[2023-10-22 11:54:12] \u001b[32mEpoch [2/10] Step [71/469]  acc1 0.812500 (0.779710)  loss 5.192605 (18.644928)\u001b[0m\n",
      "[2023-10-22 11:54:14] \u001b[32mEpoch [2/10] Step [81/469]  acc1 0.703125 (0.780285)  loss 5.306816 (19.392374)\u001b[0m\n",
      "[2023-10-22 11:54:15] \u001b[32mEpoch [2/10] Step [91/469]  acc1 0.734375 (0.779190)  loss 5.456615 (17.848651)\u001b[0m\n",
      "[2023-10-22 11:54:16] \u001b[32mEpoch [2/10] Step [101/469]  acc1 0.812500 (0.781250)  loss 5.277778 (16.604375)\u001b[0m\n",
      "[2023-10-22 11:54:18] \u001b[32mEpoch [2/10] Step [111/469]  acc1 0.781250 (0.782095)  loss 5.397108 (15.584106)\u001b[0m\n",
      "[2023-10-22 11:54:19] \u001b[32mEpoch [2/10] Step [121/469]  acc1 0.734375 (0.779830)  loss 7.537258 (14.760610)\u001b[0m\n",
      "[2023-10-22 11:54:21] \u001b[32mEpoch [2/10] Step [131/469]  acc1 0.671875 (0.780654)  loss 5.430676 (14.061179)\u001b[0m\n",
      "[2023-10-22 11:54:22] \u001b[32mEpoch [2/10] Step [141/469]  acc1 0.718750 (0.778369)  loss 5.445511 (13.446439)\u001b[0m\n",
      "[2023-10-22 11:54:23] \u001b[32mEpoch [2/10] Step [151/469]  acc1 0.765625 (0.779594)  loss 5.312235 (12.910902)\u001b[0m\n",
      "[2023-10-22 11:54:25] \u001b[32mEpoch [2/10] Step [161/469]  acc1 0.765625 (0.780765)  loss 998.987976 (18.611736)\u001b[0m\n",
      "[2023-10-22 11:54:26] \u001b[32mEpoch [2/10] Step [171/469]  acc1 0.859375 (0.780245)  loss 5.184179 (17.865835)\u001b[0m\n",
      "[2023-10-22 11:54:27] \u001b[32mEpoch [2/10] Step [181/469]  acc1 0.718750 (0.781336)  loss 5.422657 (17.169237)\u001b[0m\n",
      "[2023-10-22 11:54:29] \u001b[32mEpoch [2/10] Step [191/469]  acc1 0.796875 (0.782559)  loss 5.324498 (16.545118)\u001b[0m\n",
      "[2023-10-22 11:54:30] \u001b[32mEpoch [2/10] Step [201/469]  acc1 0.765625 (0.782494)  loss 5.308271 (15.985425)\u001b[0m\n",
      "[2023-10-22 11:54:32] \u001b[32mEpoch [2/10] Step [211/469]  acc1 0.812500 (0.782879)  loss 5.220840 (15.478214)\u001b[0m\n",
      "[2023-10-22 11:54:33] \u001b[32mEpoch [2/10] Step [221/469]  acc1 0.781250 (0.782947)  loss 5.316451 (15.017510)\u001b[0m\n",
      "[2023-10-22 11:54:35] \u001b[32mEpoch [2/10] Step [231/469]  acc1 0.765625 (0.783820)  loss 5.209713 (14.593713)\u001b[0m\n",
      "[2023-10-22 11:54:36] \u001b[32mEpoch [2/10] Step [241/469]  acc1 0.828125 (0.784686)  loss 5.132617 (14.204408)\u001b[0m\n",
      "[2023-10-22 11:54:37] \u001b[32mEpoch [2/10] Step [251/469]  acc1 0.875000 (0.784425)  loss 5.037697 (13.850322)\u001b[0m\n",
      "[2023-10-22 11:54:39] \u001b[32mEpoch [2/10] Step [261/469]  acc1 0.796875 (0.785141)  loss 5.262791 (13.559499)\u001b[0m\n",
      "[2023-10-22 11:54:40] \u001b[32mEpoch [2/10] Step [271/469]  acc1 0.828125 (0.786554)  loss 5.122343 (13.250356)\u001b[0m\n",
      "[2023-10-22 11:54:41] \u001b[32mEpoch [2/10] Step [281/469]  acc1 0.781250 (0.787144)  loss 5.257420 (14.042025)\u001b[0m\n",
      "[2023-10-22 11:54:43] \u001b[32mEpoch [2/10] Step [291/469]  acc1 0.750000 (0.786351)  loss 6.505438 (13.746155)\u001b[0m\n",
      "[2023-10-22 11:54:44] \u001b[32mEpoch [2/10] Step [301/469]  acc1 0.875000 (0.787168)  loss 5.080176 (13.484113)\u001b[0m\n",
      "[2023-10-22 11:54:46] \u001b[32mEpoch [2/10] Step [311/469]  acc1 0.843750 (0.788133)  loss 5.161105 (13.217248)\u001b[0m\n",
      "[2023-10-22 11:54:47] \u001b[32mEpoch [2/10] Step [321/469]  acc1 0.812500 (0.789768)  loss 5.253570 (12.966979)\u001b[0m\n",
      "[2023-10-22 11:54:48] \u001b[32mEpoch [2/10] Step [331/469]  acc1 0.765625 (0.791635)  loss 5.334041 (12.735270)\u001b[0m\n",
      "[2023-10-22 11:54:50] \u001b[32mEpoch [2/10] Step [341/469]  acc1 0.765625 (0.792201)  loss 5.240095 (12.515029)\u001b[0m\n",
      "[2023-10-22 11:54:51] \u001b[32mEpoch [2/10] Step [351/469]  acc1 0.718750 (0.791578)  loss 5.385775 (12.311099)\u001b[0m\n",
      "[2023-10-22 11:54:52] \u001b[32mEpoch [2/10] Step [361/469]  acc1 0.812500 (0.791768)  loss 5.257238 (12.115373)\u001b[0m\n",
      "[2023-10-22 11:54:54] \u001b[32mEpoch [2/10] Step [371/469]  acc1 0.765625 (0.792074)  loss 5.200506 (11.930019)\u001b[0m\n",
      "[2023-10-22 11:54:55] \u001b[32mEpoch [2/10] Step [381/469]  acc1 0.859375 (0.792487)  loss 5.080758 (11.754395)\u001b[0m\n",
      "[2023-10-22 11:54:56] \u001b[32mEpoch [2/10] Step [391/469]  acc1 0.812500 (0.793358)  loss 5.302557 (11.587033)\u001b[0m\n",
      "[2023-10-22 11:54:58] \u001b[32mEpoch [2/10] Step [401/469]  acc1 0.875000 (0.793641)  loss 5.102935 (11.429688)\u001b[0m\n",
      "[2023-10-22 11:54:59] \u001b[32mEpoch [2/10] Step [411/469]  acc1 0.796875 (0.793986)  loss 5.214972 (11.279194)\u001b[0m\n",
      "[2023-10-22 11:55:01] \u001b[32mEpoch [2/10] Step [421/469]  acc1 0.843750 (0.794834)  loss 5.145563 (11.135588)\u001b[0m\n",
      "[2023-10-22 11:55:02] \u001b[32mEpoch [2/10] Step [431/469]  acc1 0.843750 (0.795824)  loss 5.209090 (10.998715)\u001b[0m\n",
      "[2023-10-22 11:55:03] \u001b[32mEpoch [2/10] Step [441/469]  acc1 0.875000 (0.796769)  loss 5.021502 (10.866232)\u001b[0m\n",
      "[2023-10-22 11:55:05] \u001b[32mEpoch [2/10] Step [451/469]  acc1 0.796875 (0.796944)  loss 5.189122 (10.741339)\u001b[0m\n",
      "[2023-10-22 11:55:06] \u001b[32mEpoch [2/10] Step [461/469]  acc1 0.734375 (0.797451)  loss 5.234246 (10.620427)\u001b[0m\n",
      "[2023-10-22 11:55:09] \u001b[32mEpoch [3/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 5.202361 (5.202361)\u001b[0m\n",
      "[2023-10-22 11:55:10] \u001b[32mEpoch [3/10] Step [11/469]  acc1 0.843750 (0.832386)  loss 5.166330 (5.163845)\u001b[0m\n",
      "[2023-10-22 11:55:12] \u001b[32mEpoch [3/10] Step [21/469]  acc1 0.843750 (0.814732)  loss 5.187005 (5.216203)\u001b[0m\n",
      "[2023-10-22 11:55:13] \u001b[32mEpoch [3/10] Step [31/469]  acc1 0.828125 (0.823085)  loss 5.152720 (5.202738)\u001b[0m\n",
      "[2023-10-22 11:55:14] \u001b[32mEpoch [3/10] Step [41/469]  acc1 0.906250 (0.825457)  loss 5.045611 (5.195033)\u001b[0m\n",
      "[2023-10-22 11:55:16] \u001b[32mEpoch [3/10] Step [51/469]  acc1 0.812500 (0.826593)  loss 5.321710 (5.194394)\u001b[0m\n",
      "[2023-10-22 11:55:17] \u001b[32mEpoch [3/10] Step [61/469]  acc1 0.812500 (0.824027)  loss 5.213133 (5.225959)\u001b[0m\n",
      "[2023-10-22 11:55:18] \u001b[32mEpoch [3/10] Step [71/469]  acc1 0.828125 (0.826364)  loss 5.099442 (5.214774)\u001b[0m\n",
      "[2023-10-22 11:55:20] \u001b[32mEpoch [3/10] Step [81/469]  acc1 0.750000 (0.823688)  loss 5.338270 (5.213740)\u001b[0m\n",
      "[2023-10-22 11:55:21] \u001b[32mEpoch [3/10] Step [91/469]  acc1 0.750000 (0.823317)  loss 5.442377 (5.210004)\u001b[0m\n",
      "[2023-10-22 11:55:22] \u001b[32mEpoch [3/10] Step [101/469]  acc1 0.781250 (0.823175)  loss 5.264641 (5.206535)\u001b[0m\n",
      "[2023-10-22 11:55:24] \u001b[32mEpoch [3/10] Step [111/469]  acc1 0.875000 (0.823620)  loss 5.129527 (5.202832)\u001b[0m\n",
      "[2023-10-22 11:55:25] \u001b[32mEpoch [3/10] Step [121/469]  acc1 0.750000 (0.823605)  loss 5.215584 (5.199830)\u001b[0m\n",
      "[2023-10-22 11:55:26] \u001b[32mEpoch [3/10] Step [131/469]  acc1 0.843750 (0.824785)  loss 9.806532 (5.725909)\u001b[0m\n",
      "[2023-10-22 11:55:28] \u001b[32mEpoch [3/10] Step [141/469]  acc1 0.781250 (0.825244)  loss 5.291177 (5.717863)\u001b[0m\n",
      "[2023-10-22 11:55:29] \u001b[32mEpoch [3/10] Step [151/469]  acc1 0.750000 (0.824193)  loss 5.186531 (5.860077)\u001b[0m\n",
      "[2023-10-22 11:55:30] \u001b[32mEpoch [3/10] Step [161/469]  acc1 0.843750 (0.826960)  loss 5.958907 (12.619085)\u001b[0m\n",
      "[2023-10-22 11:55:32] \u001b[32mEpoch [3/10] Step [171/469]  acc1 0.828125 (0.827394)  loss 5.144951 (12.183191)\u001b[0m\n",
      "[2023-10-22 11:55:33] \u001b[32mEpoch [3/10] Step [181/469]  acc1 0.812500 (0.828211)  loss 5.142987 (11.798895)\u001b[0m\n",
      "[2023-10-22 11:55:34] \u001b[32mEpoch [3/10] Step [191/469]  acc1 0.781250 (0.828534)  loss 5.298066 (11.450960)\u001b[0m\n",
      "[2023-10-22 11:55:36] \u001b[32mEpoch [3/10] Step [201/469]  acc1 0.921875 (0.829602)  loss 5.017922 (11.136440)\u001b[0m\n",
      "[2023-10-22 11:55:37] \u001b[32mEpoch [3/10] Step [211/469]  acc1 0.875000 (0.830421)  loss 5.038230 (15.343557)\u001b[0m\n",
      "[2023-10-22 11:55:38] \u001b[32mEpoch [3/10] Step [221/469]  acc1 0.828125 (0.830246)  loss 5.130115 (14.884348)\u001b[0m\n",
      "[2023-10-22 11:55:40] \u001b[32mEpoch [3/10] Step [231/469]  acc1 0.828125 (0.830087)  loss 5.148456 (14.464498)\u001b[0m\n",
      "[2023-10-22 11:55:41] \u001b[32mEpoch [3/10] Step [241/469]  acc1 0.843750 (0.830524)  loss 5.107812 (14.078371)\u001b[0m\n",
      "[2023-10-22 11:55:42] \u001b[32mEpoch [3/10] Step [251/469]  acc1 0.843750 (0.830677)  loss 6.250896 (13.726662)\u001b[0m\n",
      "[2023-10-22 11:55:44] \u001b[32mEpoch [3/10] Step [261/469]  acc1 0.812500 (0.830999)  loss 5.250659 (13.397952)\u001b[0m\n",
      "[2023-10-22 11:55:45] \u001b[32mEpoch [3/10] Step [271/469]  acc1 0.859375 (0.831700)  loss 5.050302 (13.091860)\u001b[0m\n",
      "[2023-10-22 11:55:47] \u001b[32mEpoch [3/10] Step [281/469]  acc1 0.875000 (0.832407)  loss 4.999739 (12.807497)\u001b[0m\n",
      "[2023-10-22 11:55:48] \u001b[32mEpoch [3/10] Step [291/469]  acc1 0.750000 (0.832582)  loss 5.478386 (12.544506)\u001b[0m\n",
      "[2023-10-22 11:55:49] \u001b[32mEpoch [3/10] Step [301/469]  acc1 0.718750 (0.831966)  loss 5.438334 (12.299874)\u001b[0m\n",
      "[2023-10-22 11:55:51] \u001b[32mEpoch [3/10] Step [311/469]  acc1 0.812500 (0.831441)  loss 5.280728 (12.071387)\u001b[0m\n",
      "[2023-10-22 11:55:52] \u001b[32mEpoch [3/10] Step [321/469]  acc1 0.890625 (0.832944)  loss 5.053370 (11.853515)\u001b[0m\n",
      "[2023-10-22 11:55:53] \u001b[32mEpoch [3/10] Step [331/469]  acc1 0.812500 (0.833223)  loss 5.175711 (11.651025)\u001b[0m\n",
      "[2023-10-22 11:55:55] \u001b[32mEpoch [3/10] Step [341/469]  acc1 0.796875 (0.832661)  loss 5.197893 (11.460809)\u001b[0m\n",
      "[2023-10-22 11:55:56] \u001b[32mEpoch [3/10] Step [351/469]  acc1 0.828125 (0.832933)  loss 5.098370 (11.280791)\u001b[0m\n",
      "[2023-10-22 11:55:57] \u001b[32mEpoch [3/10] Step [361/469]  acc1 0.859375 (0.833362)  loss 5.101407 (11.109964)\u001b[0m\n",
      "[2023-10-22 11:55:59] \u001b[32mEpoch [3/10] Step [371/469]  acc1 0.906250 (0.833979)  loss 4.993258 (10.949285)\u001b[0m\n",
      "[2023-10-22 11:56:00] \u001b[32mEpoch [3/10] Step [381/469]  acc1 0.843750 (0.834195)  loss 5.103001 (10.796609)\u001b[0m\n",
      "[2023-10-22 11:56:01] \u001b[32mEpoch [3/10] Step [391/469]  acc1 0.890625 (0.834439)  loss 5.054688 (10.651171)\u001b[0m\n",
      "[2023-10-22 11:56:03] \u001b[32mEpoch [3/10] Step [401/469]  acc1 0.796875 (0.833736)  loss 5.262911 (10.515659)\u001b[0m\n",
      "[2023-10-22 11:56:04] \u001b[32mEpoch [3/10] Step [411/469]  acc1 0.859375 (0.834208)  loss 5.140150 (10.383928)\u001b[0m\n",
      "[2023-10-22 11:56:05] \u001b[32mEpoch [3/10] Step [421/469]  acc1 0.828125 (0.834434)  loss 5.197259 (10.259017)\u001b[0m\n",
      "[2023-10-22 11:56:07] \u001b[32mEpoch [3/10] Step [431/469]  acc1 0.828125 (0.835049)  loss 5.056215 (10.138875)\u001b[0m\n",
      "[2023-10-22 11:56:08] \u001b[32mEpoch [3/10] Step [441/469]  acc1 0.812500 (0.835282)  loss 5.132849 (10.024914)\u001b[0m\n",
      "[2023-10-22 11:56:09] \u001b[32mEpoch [3/10] Step [451/469]  acc1 0.890625 (0.835920)  loss 5.054206 (9.915513)\u001b[0m\n",
      "[2023-10-22 11:56:11] \u001b[32mEpoch [3/10] Step [461/469]  acc1 0.906250 (0.836361)  loss 5.055789 (9.835993)\u001b[0m\n",
      "[2023-10-22 11:56:14] \u001b[32mEpoch [4/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 5.162473 (5.162473)\u001b[0m\n",
      "[2023-10-22 11:56:15] \u001b[32mEpoch [4/10] Step [11/469]  acc1 0.828125 (0.840909)  loss 5.442366 (5.180519)\u001b[0m\n",
      "[2023-10-22 11:56:16] \u001b[32mEpoch [4/10] Step [21/469]  acc1 0.921875 (0.859375)  loss 5.054598 (5.126196)\u001b[0m\n",
      "[2023-10-22 11:56:18] \u001b[32mEpoch [4/10] Step [31/469]  acc1 0.859375 (0.859375)  loss 5.034669 (5.162230)\u001b[0m\n",
      "[2023-10-22 11:56:19] \u001b[32mEpoch [4/10] Step [41/469]  acc1 0.843750 (0.866616)  loss 5.070016 (5.130125)\u001b[0m\n",
      "[2023-10-22 11:56:20] \u001b[32mEpoch [4/10] Step [51/469]  acc1 0.906250 (0.867341)  loss 5.112962 (5.124180)\u001b[0m\n",
      "[2023-10-22 11:56:22] \u001b[32mEpoch [4/10] Step [61/469]  acc1 0.875000 (0.863986)  loss 5.051822 (5.118689)\u001b[0m\n",
      "[2023-10-22 11:56:23] \u001b[32mEpoch [4/10] Step [71/469]  acc1 0.875000 (0.863336)  loss 5.149295 (5.118764)\u001b[0m\n",
      "[2023-10-22 11:56:24] \u001b[32mEpoch [4/10] Step [81/469]  acc1 0.953125 (0.865162)  loss 4.968359 (5.367061)\u001b[0m\n",
      "[2023-10-22 11:56:26] \u001b[32mEpoch [4/10] Step [91/469]  acc1 0.765625 (0.866243)  loss 5.289461 (5.334743)\u001b[0m\n",
      "[2023-10-22 11:56:27] \u001b[32mEpoch [4/10] Step [101/469]  acc1 0.875000 (0.866491)  loss 5.017886 (5.306680)\u001b[0m\n",
      "[2023-10-22 11:56:28] \u001b[32mEpoch [4/10] Step [111/469]  acc1 0.828125 (0.866413)  loss 5.109416 (5.285458)\u001b[0m\n",
      "[2023-10-22 11:56:30] \u001b[32mEpoch [4/10] Step [121/469]  acc1 0.906250 (0.866994)  loss 5.019073 (5.267512)\u001b[0m\n",
      "[2023-10-22 11:56:31] \u001b[32mEpoch [4/10] Step [131/469]  acc1 0.875000 (0.865935)  loss 5.046076 (5.255660)\u001b[0m\n",
      "[2023-10-22 11:56:32] \u001b[32mEpoch [4/10] Step [141/469]  acc1 0.921875 (0.865359)  loss 4.957909 (5.242497)\u001b[0m\n",
      "[2023-10-22 11:56:34] \u001b[32mEpoch [4/10] Step [151/469]  acc1 0.843750 (0.865791)  loss 5.095078 (5.231385)\u001b[0m\n",
      "[2023-10-22 11:56:35] \u001b[32mEpoch [4/10] Step [161/469]  acc1 0.781250 (0.863063)  loss 5.204103 (5.228616)\u001b[0m\n",
      "[2023-10-22 11:56:37] \u001b[32mEpoch [4/10] Step [171/469]  acc1 0.859375 (0.861933)  loss 5.061456 (5.221714)\u001b[0m\n",
      "[2023-10-22 11:56:38] \u001b[32mEpoch [4/10] Step [181/469]  acc1 0.828125 (0.860670)  loss 5.139375 (5.217207)\u001b[0m\n",
      "[2023-10-22 11:56:40] \u001b[32mEpoch [4/10] Step [191/469]  acc1 0.812500 (0.859457)  loss 5.258360 (5.211487)\u001b[0m\n",
      "[2023-10-22 11:56:41] \u001b[32mEpoch [4/10] Step [201/469]  acc1 0.843750 (0.859453)  loss 5.069342 (5.224217)\u001b[0m\n",
      "[2023-10-22 11:56:43] \u001b[32mEpoch [4/10] Step [211/469]  acc1 0.875000 (0.859375)  loss 5.161748 (5.218966)\u001b[0m\n",
      "[2023-10-22 11:56:44] \u001b[32mEpoch [4/10] Step [221/469]  acc1 0.750000 (0.859658)  loss 5.191931 (5.213180)\u001b[0m\n",
      "[2023-10-22 11:56:45] \u001b[32mEpoch [4/10] Step [231/469]  acc1 0.843750 (0.860525)  loss 5.204430 (5.206524)\u001b[0m\n",
      "[2023-10-22 11:56:47] \u001b[32mEpoch [4/10] Step [241/469]  acc1 0.875000 (0.860996)  loss 5.034709 (5.200100)\u001b[0m\n",
      "[2023-10-22 11:56:48] \u001b[32mEpoch [4/10] Step [251/469]  acc1 0.906250 (0.860620)  loss 5.081466 (5.196221)\u001b[0m\n",
      "[2023-10-22 11:56:49] \u001b[32mEpoch [4/10] Step [261/469]  acc1 0.859375 (0.860752)  loss 5.084604 (5.192365)\u001b[0m\n",
      "[2023-10-22 11:56:51] \u001b[32mEpoch [4/10] Step [271/469]  acc1 0.921875 (0.861451)  loss 4.934078 (5.188760)\u001b[0m\n",
      "[2023-10-22 11:56:52] \u001b[32mEpoch [4/10] Step [281/469]  acc1 0.875000 (0.861043)  loss 5.007904 (5.184974)\u001b[0m\n",
      "[2023-10-22 11:56:53] \u001b[32mEpoch [4/10] Step [291/469]  acc1 0.937500 (0.860717)  loss 4.963376 (5.182721)\u001b[0m\n",
      "[2023-10-22 11:56:55] \u001b[32mEpoch [4/10] Step [301/469]  acc1 0.859375 (0.860569)  loss 5.043125 (5.179216)\u001b[0m\n",
      "[2023-10-22 11:56:56] \u001b[32mEpoch [4/10] Step [311/469]  acc1 0.828125 (0.860229)  loss 5.057300 (5.176524)\u001b[0m\n",
      "[2023-10-22 11:56:57] \u001b[32mEpoch [4/10] Step [321/469]  acc1 0.812500 (0.860202)  loss 5.104537 (5.173714)\u001b[0m\n",
      "[2023-10-22 11:56:59] \u001b[32mEpoch [4/10] Step [331/469]  acc1 0.796875 (0.859469)  loss 5.185558 (5.172390)\u001b[0m\n",
      "[2023-10-22 11:57:00] \u001b[32mEpoch [4/10] Step [341/469]  acc1 0.906250 (0.859604)  loss 4.987894 (5.169419)\u001b[0m\n",
      "[2023-10-22 11:57:01] \u001b[32mEpoch [4/10] Step [351/469]  acc1 0.859375 (0.859820)  loss 5.119140 (5.167200)\u001b[0m\n",
      "[2023-10-22 11:57:03] \u001b[32mEpoch [4/10] Step [361/469]  acc1 0.859375 (0.859462)  loss 5.053134 (5.165657)\u001b[0m\n",
      "[2023-10-22 11:57:04] \u001b[32mEpoch [4/10] Step [371/469]  acc1 0.890625 (0.859164)  loss 5.075392 (5.167833)\u001b[0m\n",
      "[2023-10-22 11:57:05] \u001b[32mEpoch [4/10] Step [381/469]  acc1 0.828125 (0.858473)  loss 5.185913 (5.168263)\u001b[0m\n",
      "[2023-10-22 11:57:07] \u001b[32mEpoch [4/10] Step [391/469]  acc1 0.890625 (0.858656)  loss 5.109961 (5.165582)\u001b[0m\n",
      "[2023-10-22 11:57:08] \u001b[32mEpoch [4/10] Step [401/469]  acc1 0.843750 (0.858674)  loss 5.077448 (5.163887)\u001b[0m\n",
      "[2023-10-22 11:57:09] \u001b[32mEpoch [4/10] Step [411/469]  acc1 0.859375 (0.858729)  loss 5.071677 (5.162205)\u001b[0m\n",
      "[2023-10-22 11:57:11] \u001b[32mEpoch [4/10] Step [421/469]  acc1 0.921875 (0.859115)  loss 5.051832 (5.159606)\u001b[0m\n",
      "[2023-10-22 11:57:12] \u001b[32mEpoch [4/10] Step [431/469]  acc1 0.906250 (0.859085)  loss 4.943503 (5.161155)\u001b[0m\n",
      "[2023-10-22 11:57:14] \u001b[32mEpoch [4/10] Step [441/469]  acc1 0.875000 (0.859410)  loss 4.998281 (7.209650)\u001b[0m\n",
      "[2023-10-22 11:57:15] \u001b[32mEpoch [4/10] Step [451/469]  acc1 0.875000 (0.858544)  loss 5.005579 (7.165714)\u001b[0m\n",
      "[2023-10-22 11:57:16] \u001b[32mEpoch [4/10] Step [461/469]  acc1 0.890625 (0.858968)  loss 4.956082 (7.119771)\u001b[0m\n",
      "[2023-10-22 11:57:19] \u001b[32mEpoch [5/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 5.023694 (5.023694)\u001b[0m\n",
      "[2023-10-22 11:57:20] \u001b[32mEpoch [5/10] Step [11/469]  acc1 0.828125 (0.870739)  loss 5.183704 (5.058231)\u001b[0m\n",
      "[2023-10-22 11:57:22] \u001b[32mEpoch [5/10] Step [21/469]  acc1 0.796875 (0.864583)  loss 5.113429 (5.072238)\u001b[0m\n",
      "[2023-10-22 11:57:23] \u001b[32mEpoch [5/10] Step [31/469]  acc1 0.859375 (0.862399)  loss 5.092928 (5.107839)\u001b[0m\n",
      "[2023-10-22 11:57:24] \u001b[32mEpoch [5/10] Step [41/469]  acc1 0.890625 (0.864329)  loss 4.987659 (5.095668)\u001b[0m\n",
      "[2023-10-22 11:57:26] \u001b[32mEpoch [5/10] Step [51/469]  acc1 0.906250 (0.861520)  loss 5.006782 (5.108315)\u001b[0m\n",
      "[2023-10-22 11:57:27] \u001b[32mEpoch [5/10] Step [61/469]  acc1 0.875000 (0.860656)  loss 5.042317 (5.102478)\u001b[0m\n",
      "[2023-10-22 11:57:28] \u001b[32mEpoch [5/10] Step [71/469]  acc1 0.796875 (0.863996)  loss 5.244203 (5.095006)\u001b[0m\n",
      "[2023-10-22 11:57:30] \u001b[32mEpoch [5/10] Step [81/469]  acc1 0.906250 (0.865162)  loss 4.983110 (5.092047)\u001b[0m\n",
      "[2023-10-22 11:57:31] \u001b[32mEpoch [5/10] Step [91/469]  acc1 0.765625 (0.862981)  loss 5.268006 (5.095324)\u001b[0m\n",
      "[2023-10-22 11:57:33] \u001b[32mEpoch [5/10] Step [101/469]  acc1 0.828125 (0.863707)  loss 5.140672 (5.091463)\u001b[0m\n",
      "[2023-10-22 11:57:34] \u001b[32mEpoch [5/10] Step [111/469]  acc1 0.812500 (0.862894)  loss 5.144787 (5.091489)\u001b[0m\n",
      "[2023-10-22 11:57:35] \u001b[32mEpoch [5/10] Step [121/469]  acc1 0.890625 (0.864282)  loss 5.068580 (5.089599)\u001b[0m\n",
      "[2023-10-22 11:57:37] \u001b[32mEpoch [5/10] Step [131/469]  acc1 0.859375 (0.865100)  loss 5.008909 (5.110199)\u001b[0m\n",
      "[2023-10-22 11:57:38] \u001b[32mEpoch [5/10] Step [141/469]  acc1 0.937500 (0.866356)  loss 4.916365 (5.104489)\u001b[0m\n",
      "[2023-10-22 11:57:39] \u001b[32mEpoch [5/10] Step [151/469]  acc1 0.859375 (0.867343)  loss 5.126662 (5.100220)\u001b[0m\n",
      "[2023-10-22 11:57:41] \u001b[32mEpoch [5/10] Step [161/469]  acc1 0.953125 (0.867915)  loss 4.892406 (5.095311)\u001b[0m\n",
      "[2023-10-22 11:57:42] \u001b[32mEpoch [5/10] Step [171/469]  acc1 0.828125 (0.867964)  loss 5.163115 (5.095097)\u001b[0m\n",
      "[2023-10-22 11:57:43] \u001b[32mEpoch [5/10] Step [181/469]  acc1 0.843750 (0.869130)  loss 5.092667 (5.091276)\u001b[0m\n",
      "[2023-10-22 11:57:45] \u001b[32mEpoch [5/10] Step [191/469]  acc1 0.921875 (0.870173)  loss 4.943336 (5.100022)\u001b[0m\n",
      "[2023-10-22 11:57:46] \u001b[32mEpoch [5/10] Step [201/469]  acc1 0.906250 (0.870258)  loss 5.020460 (5.098492)\u001b[0m\n",
      "[2023-10-22 11:57:47] \u001b[32mEpoch [5/10] Step [211/469]  acc1 0.812500 (0.870409)  loss 5.235898 (5.098956)\u001b[0m\n",
      "[2023-10-22 11:57:49] \u001b[32mEpoch [5/10] Step [221/469]  acc1 0.812500 (0.870758)  loss 5.222503 (5.095971)\u001b[0m\n",
      "[2023-10-22 11:57:50] \u001b[32mEpoch [5/10] Step [231/469]  acc1 0.921875 (0.869995)  loss 4.881775 (5.095963)\u001b[0m\n",
      "[2023-10-22 11:57:51] \u001b[32mEpoch [5/10] Step [241/469]  acc1 0.859375 (0.868517)  loss 5.095482 (5.099751)\u001b[0m\n",
      "[2023-10-22 11:57:53] \u001b[32mEpoch [5/10] Step [251/469]  acc1 0.953125 (0.869148)  loss 4.907131 (5.097648)\u001b[0m\n",
      "[2023-10-22 11:57:54] \u001b[32mEpoch [5/10] Step [261/469]  acc1 0.890625 (0.869073)  loss 4.975955 (5.096695)\u001b[0m\n",
      "[2023-10-22 11:57:55] \u001b[32mEpoch [5/10] Step [271/469]  acc1 0.859375 (0.869350)  loss 5.042149 (5.093936)\u001b[0m\n",
      "[2023-10-22 11:57:57] \u001b[32mEpoch [5/10] Step [281/469]  acc1 0.906250 (0.869217)  loss 4.930669 (5.093735)\u001b[0m\n",
      "[2023-10-22 11:57:58] \u001b[32mEpoch [5/10] Step [291/469]  acc1 0.843750 (0.868718)  loss 5.086063 (5.094571)\u001b[0m\n",
      "[2023-10-22 11:57:59] \u001b[32mEpoch [5/10] Step [301/469]  acc1 0.828125 (0.868771)  loss 5.112274 (5.092734)\u001b[0m\n",
      "[2023-10-22 11:58:01] \u001b[32mEpoch [5/10] Step [311/469]  acc1 0.906250 (0.868519)  loss 4.958943 (5.091964)\u001b[0m\n",
      "[2023-10-22 11:58:02] \u001b[32mEpoch [5/10] Step [321/469]  acc1 0.875000 (0.869062)  loss 5.006683 (5.090692)\u001b[0m\n",
      "[2023-10-22 11:58:03] \u001b[32mEpoch [5/10] Step [331/469]  acc1 0.859375 (0.868674)  loss 5.101699 (5.090796)\u001b[0m\n",
      "[2023-10-22 11:58:05] \u001b[32mEpoch [5/10] Step [341/469]  acc1 0.859375 (0.869501)  loss 4.991478 (5.087285)\u001b[0m\n",
      "[2023-10-22 11:58:06] \u001b[32mEpoch [5/10] Step [351/469]  acc1 0.859375 (0.869525)  loss 5.041686 (5.086973)\u001b[0m\n",
      "[2023-10-22 11:58:08] \u001b[32mEpoch [5/10] Step [361/469]  acc1 0.859375 (0.870239)  loss 5.052056 (5.087174)\u001b[0m\n",
      "[2023-10-22 11:58:09] \u001b[32mEpoch [5/10] Step [371/469]  acc1 0.875000 (0.870115)  loss 5.100700 (5.086744)\u001b[0m\n",
      "[2023-10-22 11:58:10] \u001b[32mEpoch [5/10] Step [381/469]  acc1 0.937500 (0.870202)  loss 4.932073 (5.085044)\u001b[0m\n",
      "[2023-10-22 11:58:12] \u001b[32mEpoch [5/10] Step [391/469]  acc1 0.953125 (0.870604)  loss 4.842654 (5.084901)\u001b[0m\n",
      "[2023-10-22 11:58:13] \u001b[32mEpoch [5/10] Step [401/469]  acc1 0.890625 (0.870870)  loss 4.972758 (5.084146)\u001b[0m\n",
      "[2023-10-22 11:58:14] \u001b[32mEpoch [5/10] Step [411/469]  acc1 0.828125 (0.870856)  loss 5.092034 (5.083351)\u001b[0m\n",
      "[2023-10-22 11:58:16] \u001b[32mEpoch [5/10] Step [421/469]  acc1 0.906250 (0.871177)  loss 5.096294 (5.082856)\u001b[0m\n",
      "[2023-10-22 11:58:17] \u001b[32mEpoch [5/10] Step [431/469]  acc1 0.796875 (0.870976)  loss 5.357236 (5.082840)\u001b[0m\n",
      "[2023-10-22 11:58:19] \u001b[32mEpoch [5/10] Step [441/469]  acc1 0.906250 (0.871103)  loss 4.946306 (5.081896)\u001b[0m\n",
      "[2023-10-22 11:58:20] \u001b[32mEpoch [5/10] Step [451/469]  acc1 0.843750 (0.870912)  loss 5.154253 (5.081239)\u001b[0m\n",
      "[2023-10-22 11:58:22] \u001b[32mEpoch [5/10] Step [461/469]  acc1 0.968750 (0.871204)  loss 4.900766 (5.080131)\u001b[0m\n",
      "[2023-10-22 11:58:25] \u001b[32mEpoch [6/10] Step [1/469]  acc1 0.859375 (0.859375)  loss 5.021482 (5.021482)\u001b[0m\n",
      "[2023-10-22 11:58:26] \u001b[32mEpoch [6/10] Step [11/469]  acc1 0.890625 (0.857955)  loss 5.012891 (5.065102)\u001b[0m\n",
      "[2023-10-22 11:58:27] \u001b[32mEpoch [6/10] Step [21/469]  acc1 0.921875 (0.865327)  loss 4.913800 (5.059543)\u001b[0m\n",
      "[2023-10-22 11:58:29] \u001b[32mEpoch [6/10] Step [31/469]  acc1 0.937500 (0.871976)  loss 4.907336 (5.043691)\u001b[0m\n",
      "[2023-10-22 11:58:30] \u001b[32mEpoch [6/10] Step [41/469]  acc1 0.921875 (0.874238)  loss 5.001080 (5.043227)\u001b[0m\n",
      "[2023-10-22 11:58:31] \u001b[32mEpoch [6/10] Step [51/469]  acc1 0.906250 (0.873468)  loss 4.982381 (5.054600)\u001b[0m\n",
      "[2023-10-22 11:58:33] \u001b[32mEpoch [6/10] Step [61/469]  acc1 0.828125 (0.870902)  loss 5.129318 (5.059892)\u001b[0m\n",
      "[2023-10-22 11:58:34] \u001b[32mEpoch [6/10] Step [71/469]  acc1 0.859375 (0.873019)  loss 5.091411 (5.056872)\u001b[0m\n",
      "[2023-10-22 11:58:36] \u001b[32mEpoch [6/10] Step [81/469]  acc1 0.859375 (0.874421)  loss 4.997738 (5.052244)\u001b[0m\n",
      "[2023-10-22 11:58:37] \u001b[32mEpoch [6/10] Step [91/469]  acc1 0.843750 (0.873111)  loss 5.053882 (5.051269)\u001b[0m\n",
      "[2023-10-22 11:58:39] \u001b[32mEpoch [6/10] Step [101/469]  acc1 0.859375 (0.874072)  loss 5.090627 (5.050217)\u001b[0m\n",
      "[2023-10-22 11:58:40] \u001b[32mEpoch [6/10] Step [111/469]  acc1 0.859375 (0.874718)  loss 5.054635 (5.048151)\u001b[0m\n",
      "[2023-10-22 11:58:42] \u001b[32mEpoch [6/10] Step [121/469]  acc1 0.906250 (0.874742)  loss 4.966286 (5.045668)\u001b[0m\n",
      "[2023-10-22 11:58:43] \u001b[32mEpoch [6/10] Step [131/469]  acc1 0.843750 (0.875954)  loss 5.142886 (5.043639)\u001b[0m\n",
      "[2023-10-22 11:58:44] \u001b[32mEpoch [6/10] Step [141/469]  acc1 0.875000 (0.876662)  loss 5.008913 (5.042925)\u001b[0m\n",
      "[2023-10-22 11:58:46] \u001b[32mEpoch [6/10] Step [151/469]  acc1 0.875000 (0.876656)  loss 4.956842 (5.041526)\u001b[0m\n",
      "[2023-10-22 11:58:47] \u001b[32mEpoch [6/10] Step [161/469]  acc1 0.921875 (0.877329)  loss 4.948975 (5.042247)\u001b[0m\n",
      "[2023-10-22 11:58:48] \u001b[32mEpoch [6/10] Step [171/469]  acc1 0.812500 (0.875822)  loss 5.169505 (5.044376)\u001b[0m\n",
      "[2023-10-22 11:58:50] \u001b[32mEpoch [6/10] Step [181/469]  acc1 0.828125 (0.875173)  loss 5.107620 (5.044039)\u001b[0m\n",
      "[2023-10-22 11:58:51] \u001b[32mEpoch [6/10] Step [191/469]  acc1 0.828125 (0.874918)  loss 5.088490 (5.044255)\u001b[0m\n",
      "[2023-10-22 11:58:52] \u001b[32mEpoch [6/10] Step [201/469]  acc1 0.875000 (0.875233)  loss 4.968124 (5.045188)\u001b[0m\n",
      "[2023-10-22 11:58:54] \u001b[32mEpoch [6/10] Step [211/469]  acc1 0.843750 (0.875222)  loss 5.098061 (5.044882)\u001b[0m\n",
      "[2023-10-22 11:58:55] \u001b[32mEpoch [6/10] Step [221/469]  acc1 0.921875 (0.874788)  loss 4.990845 (5.048526)\u001b[0m\n",
      "[2023-10-22 11:58:56] \u001b[32mEpoch [6/10] Step [231/469]  acc1 0.875000 (0.875609)  loss 5.083991 (5.045789)\u001b[0m\n",
      "[2023-10-22 11:58:58] \u001b[32mEpoch [6/10] Step [241/469]  acc1 0.906250 (0.876362)  loss 4.986138 (5.043981)\u001b[0m\n",
      "[2023-10-22 11:58:59] \u001b[32mEpoch [6/10] Step [251/469]  acc1 0.921875 (0.876494)  loss 4.922395 (5.043370)\u001b[0m\n",
      "[2023-10-22 11:59:00] \u001b[32mEpoch [6/10] Step [261/469]  acc1 0.890625 (0.875539)  loss 4.985370 (5.045320)\u001b[0m\n",
      "[2023-10-22 11:59:02] \u001b[32mEpoch [6/10] Step [271/469]  acc1 0.906250 (0.875692)  loss 5.033150 (5.045362)\u001b[0m\n",
      "[2023-10-22 11:59:03] \u001b[32mEpoch [6/10] Step [281/469]  acc1 0.765625 (0.875667)  loss 5.148540 (5.045252)\u001b[0m\n",
      "[2023-10-22 11:59:04] \u001b[32mEpoch [6/10] Step [291/469]  acc1 0.890625 (0.875698)  loss 5.024309 (5.044397)\u001b[0m\n",
      "[2023-10-22 11:59:06] \u001b[32mEpoch [6/10] Step [301/469]  acc1 0.875000 (0.875467)  loss 5.106875 (5.044064)\u001b[0m\n",
      "[2023-10-22 11:59:07] \u001b[32mEpoch [6/10] Step [311/469]  acc1 0.875000 (0.875553)  loss 4.999770 (5.042976)\u001b[0m\n",
      "[2023-10-22 11:59:08] \u001b[32mEpoch [6/10] Step [321/469]  acc1 0.875000 (0.875389)  loss 5.050976 (5.043506)\u001b[0m\n",
      "[2023-10-22 11:59:10] \u001b[32mEpoch [6/10] Step [331/469]  acc1 0.812500 (0.875283)  loss 5.115379 (5.055018)\u001b[0m\n",
      "[2023-10-22 11:59:11] \u001b[32mEpoch [6/10] Step [341/469]  acc1 0.937500 (0.875550)  loss 4.933438 (5.053812)\u001b[0m\n",
      "[2023-10-22 11:59:12] \u001b[32mEpoch [6/10] Step [351/469]  acc1 0.890625 (0.875579)  loss 5.093366 (5.053659)\u001b[0m\n",
      "[2023-10-22 11:59:14] \u001b[32mEpoch [6/10] Step [361/469]  acc1 0.843750 (0.875822)  loss 5.001177 (5.053062)\u001b[0m\n",
      "[2023-10-22 11:59:15] \u001b[32mEpoch [6/10] Step [371/469]  acc1 0.875000 (0.875842)  loss 4.959507 (5.052309)\u001b[0m\n",
      "[2023-10-22 11:59:16] \u001b[32mEpoch [6/10] Step [381/469]  acc1 0.906250 (0.875984)  loss 5.012852 (5.051288)\u001b[0m\n",
      "[2023-10-22 11:59:18] \u001b[32mEpoch [6/10] Step [391/469]  acc1 0.875000 (0.875999)  loss 5.102210 (5.052864)\u001b[0m\n",
      "[2023-10-22 11:59:19] \u001b[32mEpoch [6/10] Step [401/469]  acc1 0.828125 (0.875818)  loss 5.144382 (5.053401)\u001b[0m\n",
      "[2023-10-22 11:59:20] \u001b[32mEpoch [6/10] Step [411/469]  acc1 0.843750 (0.876141)  loss 5.015552 (5.053414)\u001b[0m\n",
      "[2023-10-22 11:59:22] \u001b[32mEpoch [6/10] Step [421/469]  acc1 0.875000 (0.876113)  loss 5.004328 (5.053473)\u001b[0m\n",
      "[2023-10-22 11:59:23] \u001b[32mEpoch [6/10] Step [431/469]  acc1 0.890625 (0.875906)  loss 4.997528 (5.052778)\u001b[0m\n",
      "[2023-10-22 11:59:25] \u001b[32mEpoch [6/10] Step [441/469]  acc1 0.906250 (0.875779)  loss 4.920179 (5.052869)\u001b[0m\n",
      "[2023-10-22 11:59:26] \u001b[32mEpoch [6/10] Step [451/469]  acc1 0.812500 (0.875866)  loss 5.095283 (5.052258)\u001b[0m\n",
      "[2023-10-22 11:59:27] \u001b[32mEpoch [6/10] Step [461/469]  acc1 0.921875 (0.876017)  loss 4.964204 (5.052456)\u001b[0m\n",
      "[2023-10-22 11:59:30] \u001b[32mEpoch [7/10] Step [1/469]  acc1 0.875000 (0.875000)  loss 5.043902 (5.043902)\u001b[0m\n",
      "[2023-10-22 11:59:31] \u001b[32mEpoch [7/10] Step [11/469]  acc1 0.906250 (0.899148)  loss 4.955210 (5.025005)\u001b[0m\n",
      "[2023-10-22 11:59:33] \u001b[32mEpoch [7/10] Step [21/469]  acc1 0.890625 (0.899554)  loss 4.999053 (5.008832)\u001b[0m\n",
      "[2023-10-22 11:59:34] \u001b[32mEpoch [7/10] Step [31/469]  acc1 0.937500 (0.894657)  loss 5.016221 (5.014634)\u001b[0m\n",
      "[2023-10-22 11:59:36] \u001b[32mEpoch [7/10] Step [41/469]  acc1 0.859375 (0.888720)  loss 5.145196 (5.020999)\u001b[0m\n",
      "[2023-10-22 11:59:37] \u001b[32mEpoch [7/10] Step [51/469]  acc1 0.921875 (0.890319)  loss 4.905494 (5.015137)\u001b[0m\n",
      "[2023-10-22 11:59:38] \u001b[32mEpoch [7/10] Step [61/469]  acc1 0.890625 (0.887551)  loss 5.113820 (5.018902)\u001b[0m\n",
      "[2023-10-22 11:59:40] \u001b[32mEpoch [7/10] Step [71/469]  acc1 0.984375 (0.890625)  loss 4.807725 (5.008464)\u001b[0m\n",
      "[2023-10-22 11:59:41] \u001b[32mEpoch [7/10] Step [81/469]  acc1 0.843750 (0.888889)  loss 5.227324 (5.013477)\u001b[0m\n",
      "[2023-10-22 11:59:43] \u001b[32mEpoch [7/10] Step [91/469]  acc1 0.953125 (0.886332)  loss 4.909744 (5.018131)\u001b[0m\n",
      "[2023-10-22 11:59:44] \u001b[32mEpoch [7/10] Step [101/469]  acc1 0.937500 (0.885984)  loss 4.909243 (5.022109)\u001b[0m\n",
      "[2023-10-22 11:59:45] \u001b[32mEpoch [7/10] Step [111/469]  acc1 0.953125 (0.887106)  loss 4.919860 (5.021629)\u001b[0m\n",
      "[2023-10-22 11:59:47] \u001b[32mEpoch [7/10] Step [121/469]  acc1 0.859375 (0.884943)  loss 5.112579 (5.025497)\u001b[0m\n",
      "[2023-10-22 11:59:48] \u001b[32mEpoch [7/10] Step [131/469]  acc1 0.875000 (0.884900)  loss 4.955128 (5.024551)\u001b[0m\n",
      "[2023-10-22 11:59:49] \u001b[32mEpoch [7/10] Step [141/469]  acc1 0.875000 (0.884087)  loss 5.047591 (5.026606)\u001b[0m\n",
      "[2023-10-22 11:59:51] \u001b[32mEpoch [7/10] Step [151/469]  acc1 0.875000 (0.883485)  loss 5.138177 (5.029148)\u001b[0m\n",
      "[2023-10-22 11:59:52] \u001b[32mEpoch [7/10] Step [161/469]  acc1 0.781250 (0.883249)  loss 5.260352 (5.028891)\u001b[0m\n",
      "[2023-10-22 11:59:53] \u001b[32mEpoch [7/10] Step [171/469]  acc1 0.875000 (0.883406)  loss 4.964238 (5.027233)\u001b[0m\n",
      "[2023-10-22 11:59:55] \u001b[32mEpoch [7/10] Step [181/469]  acc1 0.859375 (0.882338)  loss 5.168021 (5.028390)\u001b[0m\n",
      "[2023-10-22 11:59:56] \u001b[32mEpoch [7/10] Step [191/469]  acc1 0.890625 (0.881954)  loss 5.027421 (5.029394)\u001b[0m\n",
      "[2023-10-22 11:59:58] \u001b[32mEpoch [7/10] Step [201/469]  acc1 0.859375 (0.882463)  loss 5.060209 (5.029517)\u001b[0m\n",
      "[2023-10-22 11:59:59] \u001b[32mEpoch [7/10] Step [211/469]  acc1 0.812500 (0.881591)  loss 5.081169 (5.029922)\u001b[0m\n",
      "[2023-10-22 12:00:00] \u001b[32mEpoch [7/10] Step [221/469]  acc1 0.859375 (0.881292)  loss 5.088754 (5.030046)\u001b[0m\n",
      "[2023-10-22 12:00:02] \u001b[32mEpoch [7/10] Step [231/469]  acc1 0.875000 (0.881358)  loss 4.999975 (5.035363)\u001b[0m\n",
      "[2023-10-22 12:00:03] \u001b[32mEpoch [7/10] Step [241/469]  acc1 0.828125 (0.880576)  loss 5.085242 (5.036682)\u001b[0m\n",
      "[2023-10-22 12:00:04] \u001b[32mEpoch [7/10] Step [251/469]  acc1 0.796875 (0.880416)  loss 5.195147 (5.037017)\u001b[0m\n",
      "[2023-10-22 12:00:06] \u001b[32mEpoch [7/10] Step [261/469]  acc1 0.906250 (0.881346)  loss 4.936942 (5.033845)\u001b[0m\n",
      "[2023-10-22 12:00:07] \u001b[32mEpoch [7/10] Step [271/469]  acc1 0.921875 (0.881919)  loss 4.931715 (5.032148)\u001b[0m\n",
      "[2023-10-22 12:00:08] \u001b[32mEpoch [7/10] Step [281/469]  acc1 0.875000 (0.882173)  loss 4.988103 (5.031061)\u001b[0m\n",
      "[2023-10-22 12:00:10] \u001b[32mEpoch [7/10] Step [291/469]  acc1 0.812500 (0.881980)  loss 5.212858 (5.030848)\u001b[0m\n",
      "[2023-10-22 12:00:11] \u001b[32mEpoch [7/10] Step [301/469]  acc1 0.859375 (0.881904)  loss 5.006696 (5.030243)\u001b[0m\n",
      "[2023-10-22 12:00:12] \u001b[32mEpoch [7/10] Step [311/469]  acc1 0.953125 (0.882687)  loss 4.985446 (5.029218)\u001b[0m\n",
      "[2023-10-22 12:00:14] \u001b[32mEpoch [7/10] Step [321/469]  acc1 0.968750 (0.883080)  loss 4.828876 (5.027761)\u001b[0m\n",
      "[2023-10-22 12:00:15] \u001b[32mEpoch [7/10] Step [331/469]  acc1 0.921875 (0.883214)  loss 4.920757 (5.027557)\u001b[0m\n",
      "[2023-10-22 12:00:17] \u001b[32mEpoch [7/10] Step [341/469]  acc1 0.921875 (0.883843)  loss 4.986830 (5.115897)\u001b[0m\n",
      "[2023-10-22 12:00:18] \u001b[32mEpoch [7/10] Step [351/469]  acc1 0.906250 (0.883903)  loss 4.938259 (5.113024)\u001b[0m\n",
      "[2023-10-22 12:00:19] \u001b[32mEpoch [7/10] Step [361/469]  acc1 0.968750 (0.883700)  loss 4.839655 (5.110919)\u001b[0m\n",
      "[2023-10-22 12:00:21] \u001b[32mEpoch [7/10] Step [371/469]  acc1 0.843750 (0.883886)  loss 5.087656 (5.108656)\u001b[0m\n",
      "[2023-10-22 12:00:22] \u001b[32mEpoch [7/10] Step [381/469]  acc1 0.921875 (0.883817)  loss 4.884265 (5.106414)\u001b[0m\n",
      "[2023-10-22 12:00:23] \u001b[32mEpoch [7/10] Step [391/469]  acc1 0.937500 (0.883832)  loss 4.963685 (5.105376)\u001b[0m\n",
      "[2023-10-22 12:00:25] \u001b[32mEpoch [7/10] Step [401/469]  acc1 0.906250 (0.883767)  loss 4.963157 (5.103558)\u001b[0m\n",
      "[2023-10-22 12:00:26] \u001b[32mEpoch [7/10] Step [411/469]  acc1 0.906250 (0.883706)  loss 5.069707 (5.102624)\u001b[0m\n",
      "[2023-10-22 12:00:28] \u001b[32mEpoch [7/10] Step [421/469]  acc1 0.843750 (0.883722)  loss 5.087387 (5.100543)\u001b[0m\n",
      "[2023-10-22 12:00:29] \u001b[32mEpoch [7/10] Step [431/469]  acc1 0.968750 (0.883519)  loss 4.848035 (5.099140)\u001b[0m\n",
      "[2023-10-22 12:00:30] \u001b[32mEpoch [7/10] Step [441/469]  acc1 0.937500 (0.883539)  loss 5.018692 (5.097918)\u001b[0m\n",
      "[2023-10-22 12:00:32] \u001b[32mEpoch [7/10] Step [451/469]  acc1 0.953125 (0.883280)  loss 4.864560 (5.096661)\u001b[0m\n",
      "[2023-10-22 12:00:33] \u001b[32mEpoch [7/10] Step [461/469]  acc1 0.937500 (0.883575)  loss 4.882128 (5.094185)\u001b[0m\n",
      "[2023-10-22 12:00:36] \u001b[32mEpoch [8/10] Step [1/469]  acc1 0.890625 (0.890625)  loss 5.019759 (5.019759)\u001b[0m\n",
      "[2023-10-22 12:00:38] \u001b[32mEpoch [8/10] Step [11/469]  acc1 0.906250 (0.882102)  loss 4.983535 (5.014074)\u001b[0m\n",
      "[2023-10-22 12:00:39] \u001b[32mEpoch [8/10] Step [21/469]  acc1 0.968750 (0.887649)  loss 4.882105 (5.012413)\u001b[0m\n",
      "[2023-10-22 12:00:40] \u001b[32mEpoch [8/10] Step [31/469]  acc1 0.859375 (0.889617)  loss 4.996303 (5.016139)\u001b[0m\n",
      "[2023-10-22 12:00:42] \u001b[32mEpoch [8/10] Step [41/469]  acc1 0.890625 (0.885671)  loss 5.026280 (5.017386)\u001b[0m\n",
      "[2023-10-22 12:00:43] \u001b[32mEpoch [8/10] Step [51/469]  acc1 0.812500 (0.885110)  loss 5.297368 (5.024289)\u001b[0m\n",
      "[2023-10-22 12:00:44] \u001b[32mEpoch [8/10] Step [61/469]  acc1 0.890625 (0.883965)  loss 5.082588 (5.023671)\u001b[0m\n",
      "[2023-10-22 12:00:46] \u001b[32mEpoch [8/10] Step [71/469]  acc1 0.890625 (0.887324)  loss 4.934096 (5.016396)\u001b[0m\n",
      "[2023-10-22 12:00:47] \u001b[32mEpoch [8/10] Step [81/469]  acc1 0.875000 (0.885610)  loss 5.021567 (5.061679)\u001b[0m\n",
      "[2023-10-22 12:00:48] \u001b[32mEpoch [8/10] Step [91/469]  acc1 0.890625 (0.884959)  loss 4.973742 (5.057614)\u001b[0m\n",
      "[2023-10-22 12:00:50] \u001b[32mEpoch [8/10] Step [101/469]  acc1 0.906250 (0.887686)  loss 4.986838 (5.046808)\u001b[0m\n",
      "[2023-10-22 12:00:51] \u001b[32mEpoch [8/10] Step [111/469]  acc1 0.890625 (0.888091)  loss 5.040585 (5.045297)\u001b[0m\n",
      "[2023-10-22 12:00:53] \u001b[32mEpoch [8/10] Step [121/469]  acc1 0.890625 (0.888042)  loss 5.071313 (5.042308)\u001b[0m\n",
      "[2023-10-22 12:00:54] \u001b[32mEpoch [8/10] Step [131/469]  acc1 0.875000 (0.887047)  loss 5.023969 (5.043428)\u001b[0m\n",
      "[2023-10-22 12:00:55] \u001b[32mEpoch [8/10] Step [141/469]  acc1 0.937500 (0.887633)  loss 4.893470 (5.038461)\u001b[0m\n",
      "[2023-10-22 12:00:57] \u001b[32mEpoch [8/10] Step [151/469]  acc1 0.906250 (0.889590)  loss 4.903692 (5.032424)\u001b[0m\n",
      "[2023-10-22 12:00:58] \u001b[32mEpoch [8/10] Step [161/469]  acc1 0.843750 (0.888684)  loss 5.074231 (5.032100)\u001b[0m\n",
      "[2023-10-22 12:01:00] \u001b[32mEpoch [8/10] Step [171/469]  acc1 0.828125 (0.887884)  loss 5.012573 (5.031716)\u001b[0m\n",
      "[2023-10-22 12:01:01] \u001b[32mEpoch [8/10] Step [181/469]  acc1 0.859375 (0.887086)  loss 5.054924 (5.032603)\u001b[0m\n",
      "[2023-10-22 12:01:02] \u001b[32mEpoch [8/10] Step [191/469]  acc1 0.875000 (0.888498)  loss 5.053684 (5.029017)\u001b[0m\n",
      "[2023-10-22 12:01:04] \u001b[32mEpoch [8/10] Step [201/469]  acc1 0.843750 (0.887982)  loss 5.134860 (5.028677)\u001b[0m\n",
      "[2023-10-22 12:01:05] \u001b[32mEpoch [8/10] Step [211/469]  acc1 0.890625 (0.888329)  loss 5.076786 (5.027284)\u001b[0m\n",
      "[2023-10-22 12:01:06] \u001b[32mEpoch [8/10] Step [221/469]  acc1 0.859375 (0.889140)  loss 5.048737 (5.024992)\u001b[0m\n",
      "[2023-10-22 12:01:08] \u001b[32mEpoch [8/10] Step [231/469]  acc1 0.906250 (0.888460)  loss 4.947926 (5.026111)\u001b[0m\n",
      "[2023-10-22 12:01:09] \u001b[32mEpoch [8/10] Step [241/469]  acc1 0.937500 (0.887707)  loss 4.914856 (5.026363)\u001b[0m\n",
      "[2023-10-22 12:01:11] \u001b[32mEpoch [8/10] Step [251/469]  acc1 0.937500 (0.887450)  loss 4.940858 (5.027704)\u001b[0m\n",
      "[2023-10-22 12:01:12] \u001b[32mEpoch [8/10] Step [261/469]  acc1 0.937500 (0.887572)  loss 5.011747 (5.026432)\u001b[0m\n",
      "[2023-10-22 12:01:13] \u001b[32mEpoch [8/10] Step [271/469]  acc1 0.890625 (0.886993)  loss 5.043628 (5.027035)\u001b[0m\n",
      "[2023-10-22 12:01:15] \u001b[32mEpoch [8/10] Step [281/469]  acc1 0.781250 (0.887066)  loss 5.092479 (5.025628)\u001b[0m\n",
      "[2023-10-22 12:01:16] \u001b[32mEpoch [8/10] Step [291/469]  acc1 0.859375 (0.887779)  loss 5.028657 (5.023701)\u001b[0m\n",
      "[2023-10-22 12:01:17] \u001b[32mEpoch [8/10] Step [301/469]  acc1 0.875000 (0.887822)  loss 5.116380 (5.022872)\u001b[0m\n",
      "[2023-10-22 12:01:19] \u001b[32mEpoch [8/10] Step [311/469]  acc1 0.921875 (0.887962)  loss 4.990779 (5.022172)\u001b[0m\n",
      "[2023-10-22 12:01:20] \u001b[32mEpoch [8/10] Step [321/469]  acc1 0.906250 (0.888289)  loss 4.981077 (5.021420)\u001b[0m\n",
      "[2023-10-22 12:01:21] \u001b[32mEpoch [8/10] Step [331/469]  acc1 0.890625 (0.888218)  loss 5.042779 (5.021274)\u001b[0m\n",
      "[2023-10-22 12:01:23] \u001b[32mEpoch [8/10] Step [341/469]  acc1 0.875000 (0.888105)  loss 5.101596 (5.020794)\u001b[0m\n",
      "[2023-10-22 12:01:24] \u001b[32mEpoch [8/10] Step [351/469]  acc1 0.906250 (0.888355)  loss 4.963702 (5.019583)\u001b[0m\n",
      "[2023-10-22 12:01:25] \u001b[32mEpoch [8/10] Step [361/469]  acc1 0.859375 (0.888071)  loss 5.120111 (5.019670)\u001b[0m\n",
      "[2023-10-22 12:01:27] \u001b[32mEpoch [8/10] Step [371/469]  acc1 0.875000 (0.887761)  loss 5.021615 (5.018979)\u001b[0m\n",
      "[2023-10-22 12:01:28] \u001b[32mEpoch [8/10] Step [381/469]  acc1 0.890625 (0.887508)  loss 5.059357 (5.019822)\u001b[0m\n",
      "[2023-10-22 12:01:29] \u001b[32mEpoch [8/10] Step [391/469]  acc1 0.859375 (0.887428)  loss 5.075975 (5.024400)\u001b[0m\n",
      "[2023-10-22 12:01:31] \u001b[32mEpoch [8/10] Step [401/469]  acc1 0.875000 (0.887196)  loss 5.049176 (5.024538)\u001b[0m\n",
      "[2023-10-22 12:01:32] \u001b[32mEpoch [8/10] Step [411/469]  acc1 0.875000 (0.887013)  loss 5.016026 (5.024590)\u001b[0m\n",
      "[2023-10-22 12:01:33] \u001b[32mEpoch [8/10] Step [421/469]  acc1 0.859375 (0.886839)  loss 5.012582 (5.024268)\u001b[0m\n",
      "[2023-10-22 12:01:35] \u001b[32mEpoch [8/10] Step [431/469]  acc1 0.859375 (0.886927)  loss 5.062642 (5.024501)\u001b[0m\n",
      "[2023-10-22 12:01:36] \u001b[32mEpoch [8/10] Step [441/469]  acc1 0.890625 (0.887188)  loss 5.015133 (5.023855)\u001b[0m\n",
      "[2023-10-22 12:01:38] \u001b[32mEpoch [8/10] Step [451/469]  acc1 0.859375 (0.887299)  loss 5.023259 (5.023722)\u001b[0m\n",
      "[2023-10-22 12:01:39] \u001b[32mEpoch [8/10] Step [461/469]  acc1 0.906250 (0.887066)  loss 4.933723 (5.023193)\u001b[0m\n",
      "[2023-10-22 12:01:42] \u001b[32mEpoch [9/10] Step [1/469]  acc1 0.828125 (0.828125)  loss 5.034555 (5.034555)\u001b[0m\n",
      "[2023-10-22 12:01:43] \u001b[32mEpoch [9/10] Step [11/469]  acc1 0.953125 (0.893466)  loss 4.854673 (4.985888)\u001b[0m\n",
      "[2023-10-22 12:01:44] \u001b[32mEpoch [9/10] Step [21/469]  acc1 0.843750 (0.892113)  loss 5.064824 (4.990729)\u001b[0m\n",
      "[2023-10-22 12:01:46] \u001b[32mEpoch [9/10] Step [31/469]  acc1 0.906250 (0.894153)  loss 4.994697 (4.993876)\u001b[0m\n",
      "[2023-10-22 12:01:47] \u001b[32mEpoch [9/10] Step [41/469]  acc1 0.843750 (0.893674)  loss 5.073418 (14.200935)\u001b[0m\n",
      "[2023-10-22 12:01:48] \u001b[32mEpoch [9/10] Step [51/469]  acc1 0.968750 (0.896752)  loss 4.891589 (12.390865)\u001b[0m\n",
      "[2023-10-22 12:01:50] \u001b[32mEpoch [9/10] Step [61/469]  acc1 0.781250 (0.895492)  loss 5.181777 (11.181054)\u001b[0m\n",
      "[2023-10-22 12:01:51] \u001b[32mEpoch [9/10] Step [71/469]  acc1 0.875000 (0.891725)  loss 5.068432 (10.320729)\u001b[0m\n",
      "[2023-10-22 12:01:52] \u001b[32mEpoch [9/10] Step [81/469]  acc1 0.906250 (0.893133)  loss 5.031723 (9.661786)\u001b[0m\n",
      "[2023-10-22 12:01:54] \u001b[32mEpoch [9/10] Step [91/469]  acc1 0.875000 (0.891140)  loss 5.078587 (9.156382)\u001b[0m\n",
      "[2023-10-22 12:01:55] \u001b[32mEpoch [9/10] Step [101/469]  acc1 0.859375 (0.889233)  loss 4.994248 (8.748289)\u001b[0m\n",
      "[2023-10-22 12:01:56] \u001b[32mEpoch [9/10] Step [111/469]  acc1 0.906250 (0.890625)  loss 4.962617 (8.409709)\u001b[0m\n",
      "[2023-10-22 12:01:58] \u001b[32mEpoch [9/10] Step [121/469]  acc1 0.859375 (0.890367)  loss 5.050125 (8.128149)\u001b[0m\n",
      "[2023-10-22 12:01:59] \u001b[32mEpoch [9/10] Step [131/469]  acc1 0.921875 (0.890983)  loss 4.961613 (7.886528)\u001b[0m\n",
      "[2023-10-22 12:02:01] \u001b[32mEpoch [9/10] Step [141/469]  acc1 0.828125 (0.891179)  loss 5.157925 (7.681254)\u001b[0m\n",
      "[2023-10-22 12:02:02] \u001b[32mEpoch [9/10] Step [151/469]  acc1 0.953125 (0.891453)  loss 4.919393 (7.504848)\u001b[0m\n",
      "[2023-10-22 12:02:03] \u001b[32mEpoch [9/10] Step [161/469]  acc1 0.843750 (0.891790)  loss 5.015009 (7.347659)\u001b[0m\n",
      "[2023-10-22 12:02:05] \u001b[32mEpoch [9/10] Step [171/469]  acc1 0.906250 (0.890990)  loss 5.000549 (7.211346)\u001b[0m\n",
      "[2023-10-22 12:02:06] \u001b[32mEpoch [9/10] Step [181/469]  acc1 0.843750 (0.891488)  loss 5.042296 (7.087333)\u001b[0m\n",
      "[2023-10-22 12:02:07] \u001b[32mEpoch [9/10] Step [191/469]  acc1 0.921875 (0.891525)  loss 4.982097 (6.977681)\u001b[0m\n",
      "[2023-10-22 12:02:09] \u001b[32mEpoch [9/10] Step [201/469]  acc1 0.906250 (0.890780)  loss 4.996507 (6.880399)\u001b[0m\n",
      "[2023-10-22 12:02:10] \u001b[32mEpoch [9/10] Step [211/469]  acc1 0.937500 (0.890699)  loss 5.075412 (6.792451)\u001b[0m\n",
      "[2023-10-22 12:02:11] \u001b[32mEpoch [9/10] Step [221/469]  acc1 0.906250 (0.890908)  loss 4.988716 (6.712839)\u001b[0m\n",
      "[2023-10-22 12:02:13] \u001b[32mEpoch [9/10] Step [231/469]  acc1 0.875000 (0.891301)  loss 5.809309 (6.641191)\u001b[0m\n",
      "[2023-10-22 12:02:14] \u001b[32mEpoch [9/10] Step [241/469]  acc1 0.859375 (0.890625)  loss 5.032698 (6.574230)\u001b[0m\n",
      "[2023-10-22 12:02:15] \u001b[32mEpoch [9/10] Step [251/469]  acc1 0.937500 (0.891497)  loss 4.950801 (6.508481)\u001b[0m\n",
      "[2023-10-22 12:02:17] \u001b[32mEpoch [9/10] Step [261/469]  acc1 0.906250 (0.891343)  loss 4.976949 (6.450549)\u001b[0m\n",
      "[2023-10-22 12:02:18] \u001b[32mEpoch [9/10] Step [271/469]  acc1 0.953125 (0.890856)  loss 4.851694 (6.397761)\u001b[0m\n",
      "[2023-10-22 12:02:19] \u001b[32mEpoch [9/10] Step [281/469]  acc1 0.843750 (0.890847)  loss 5.091151 (6.348286)\u001b[0m\n",
      "[2023-10-22 12:02:21] \u001b[32mEpoch [9/10] Step [291/469]  acc1 0.906250 (0.891001)  loss 5.063289 (6.301532)\u001b[0m\n",
      "[2023-10-22 12:02:22] \u001b[32mEpoch [9/10] Step [301/469]  acc1 0.890625 (0.891300)  loss 4.939070 (6.256602)\u001b[0m\n",
      "[2023-10-22 12:02:23] \u001b[32mEpoch [9/10] Step [311/469]  acc1 0.859375 (0.891077)  loss 5.139494 (6.217063)\u001b[0m\n",
      "[2023-10-22 12:02:25] \u001b[32mEpoch [9/10] Step [321/469]  acc1 0.890625 (0.890674)  loss 4.945851 (6.179553)\u001b[0m\n",
      "[2023-10-22 12:02:26] \u001b[32mEpoch [9/10] Step [331/469]  acc1 0.890625 (0.891097)  loss 4.974755 (6.142494)\u001b[0m\n",
      "[2023-10-22 12:02:28] \u001b[32mEpoch [9/10] Step [341/469]  acc1 0.953125 (0.891358)  loss 4.886275 (6.109132)\u001b[0m\n",
      "[2023-10-22 12:02:29] \u001b[32mEpoch [9/10] Step [351/469]  acc1 0.828125 (0.891293)  loss 5.063678 (6.077429)\u001b[0m\n",
      "[2023-10-22 12:02:30] \u001b[32mEpoch [9/10] Step [361/469]  acc1 0.906250 (0.891577)  loss 4.975020 (6.046895)\u001b[0m\n",
      "[2023-10-22 12:02:32] \u001b[32mEpoch [9/10] Step [371/469]  acc1 0.843750 (0.891257)  loss 5.190487 (6.019959)\u001b[0m\n",
      "[2023-10-22 12:02:33] \u001b[32mEpoch [9/10] Step [381/469]  acc1 0.875000 (0.890625)  loss 4.996491 (5.994937)\u001b[0m\n",
      "[2023-10-22 12:02:34] \u001b[32mEpoch [9/10] Step [391/469]  acc1 0.906250 (0.890345)  loss 4.981699 (5.969713)\u001b[0m\n",
      "[2023-10-22 12:02:36] \u001b[32mEpoch [9/10] Step [401/469]  acc1 0.890625 (0.890703)  loss 5.143027 (5.945190)\u001b[0m\n",
      "[2023-10-22 12:02:37] \u001b[32mEpoch [9/10] Step [411/469]  acc1 0.953125 (0.890815)  loss 4.839665 (5.921823)\u001b[0m\n",
      "[2023-10-22 12:02:39] \u001b[32mEpoch [9/10] Step [421/469]  acc1 0.906250 (0.891107)  loss 5.084563 (5.899810)\u001b[0m\n",
      "[2023-10-22 12:02:40] \u001b[32mEpoch [9/10] Step [431/469]  acc1 0.859375 (0.891350)  loss 5.057945 (5.878525)\u001b[0m\n",
      "[2023-10-22 12:02:41] \u001b[32mEpoch [9/10] Step [441/469]  acc1 0.921875 (0.891617)  loss 4.851103 (5.857608)\u001b[0m\n",
      "[2023-10-22 12:02:43] \u001b[32mEpoch [9/10] Step [451/469]  acc1 0.875000 (0.891768)  loss 5.086097 (5.838625)\u001b[0m\n",
      "[2023-10-22 12:02:44] \u001b[32mEpoch [9/10] Step [461/469]  acc1 0.875000 (0.891540)  loss 4.964231 (5.821195)\u001b[0m\n",
      "[2023-10-22 12:02:47] \u001b[32mEpoch [10/10] Step [1/469]  acc1 0.906250 (0.906250)  loss 4.991790 (4.991790)\u001b[0m\n",
      "[2023-10-22 12:02:48] \u001b[32mEpoch [10/10] Step [11/469]  acc1 0.906250 (0.900568)  loss 5.123803 (4.989424)\u001b[0m\n",
      "[2023-10-22 12:02:49] \u001b[32mEpoch [10/10] Step [21/469]  acc1 0.828125 (0.895833)  loss 5.141664 (4.995462)\u001b[0m\n",
      "[2023-10-22 12:02:51] \u001b[32mEpoch [10/10] Step [31/469]  acc1 0.890625 (0.899194)  loss 4.952250 (4.981136)\u001b[0m\n",
      "[2023-10-22 12:02:52] \u001b[32mEpoch [10/10] Step [41/469]  acc1 0.906250 (0.895198)  loss 4.909185 (4.994583)\u001b[0m\n",
      "[2023-10-22 12:02:53] \u001b[32mEpoch [10/10] Step [51/469]  acc1 0.906250 (0.895833)  loss 4.985963 (4.996529)\u001b[0m\n",
      "[2023-10-22 12:02:55] \u001b[32mEpoch [10/10] Step [61/469]  acc1 0.890625 (0.898053)  loss 4.913480 (5.565148)\u001b[0m\n",
      "[2023-10-22 12:02:56] \u001b[32mEpoch [10/10] Step [71/469]  acc1 0.921875 (0.895246)  loss 4.924942 (5.487054)\u001b[0m\n",
      "[2023-10-22 12:02:57] \u001b[32mEpoch [10/10] Step [81/469]  acc1 0.890625 (0.893904)  loss 5.115694 (5.427847)\u001b[0m\n",
      "[2023-10-22 12:02:59] \u001b[32mEpoch [10/10] Step [91/469]  acc1 0.859375 (0.894059)  loss 5.167648 (5.381206)\u001b[0m\n",
      "[2023-10-22 12:03:00] \u001b[32mEpoch [10/10] Step [101/469]  acc1 0.906250 (0.894647)  loss 4.887668 (5.339528)\u001b[0m\n",
      "[2023-10-22 12:03:01] \u001b[32mEpoch [10/10] Step [111/469]  acc1 0.890625 (0.895411)  loss 5.055132 (5.308255)\u001b[0m\n",
      "[2023-10-22 12:03:03] \u001b[32mEpoch [10/10] Step [121/469]  acc1 0.906250 (0.895790)  loss 5.037419 (5.281186)\u001b[0m\n",
      "[2023-10-22 12:03:04] \u001b[32mEpoch [10/10] Step [131/469]  acc1 0.812500 (0.896112)  loss 5.107729 (5.260005)\u001b[0m\n",
      "[2023-10-22 12:03:05] \u001b[32mEpoch [10/10] Step [141/469]  acc1 0.859375 (0.895168)  loss 5.023267 (5.242199)\u001b[0m\n",
      "[2023-10-22 12:03:07] \u001b[32mEpoch [10/10] Step [151/469]  acc1 0.906250 (0.894040)  loss 4.948307 (5.227316)\u001b[0m\n",
      "[2023-10-22 12:03:08] \u001b[32mEpoch [10/10] Step [161/469]  acc1 0.890625 (0.895380)  loss 4.991003 (5.209835)\u001b[0m\n",
      "[2023-10-22 12:03:10] \u001b[32mEpoch [10/10] Step [171/469]  acc1 0.875000 (0.895559)  loss 5.036378 (5.197876)\u001b[0m\n",
      "[2023-10-22 12:03:11] \u001b[32mEpoch [10/10] Step [181/469]  acc1 0.921875 (0.895805)  loss 5.007407 (5.186592)\u001b[0m\n",
      "[2023-10-22 12:03:12] \u001b[32mEpoch [10/10] Step [191/469]  acc1 0.906250 (0.894797)  loss 5.003868 (5.177353)\u001b[0m\n",
      "[2023-10-22 12:03:14] \u001b[32mEpoch [10/10] Step [201/469]  acc1 0.843750 (0.893812)  loss 5.175833 (5.169760)\u001b[0m\n",
      "[2023-10-22 12:03:15] \u001b[32mEpoch [10/10] Step [211/469]  acc1 0.906250 (0.894105)  loss 5.029567 (5.161449)\u001b[0m\n",
      "[2023-10-22 12:03:16] \u001b[32mEpoch [10/10] Step [221/469]  acc1 0.890625 (0.894514)  loss 4.944563 (5.152724)\u001b[0m\n",
      "[2023-10-22 12:03:18] \u001b[32mEpoch [10/10] Step [231/469]  acc1 0.859375 (0.895495)  loss 4.989818 (5.143480)\u001b[0m\n",
      "[2023-10-22 12:03:19] \u001b[32mEpoch [10/10] Step [241/469]  acc1 0.921875 (0.895682)  loss 4.921116 (5.136910)\u001b[0m\n",
      "[2023-10-22 12:03:20] \u001b[32mEpoch [10/10] Step [251/469]  acc1 0.906250 (0.895730)  loss 4.854694 (5.130791)\u001b[0m\n",
      "[2023-10-22 12:03:22] \u001b[32mEpoch [10/10] Step [261/469]  acc1 0.890625 (0.896013)  loss 4.913313 (5.124651)\u001b[0m\n",
      "[2023-10-22 12:03:23] \u001b[32mEpoch [10/10] Step [271/469]  acc1 0.906250 (0.895065)  loss 4.914657 (5.121021)\u001b[0m\n",
      "[2023-10-22 12:03:24] \u001b[32mEpoch [10/10] Step [281/469]  acc1 0.796875 (0.894629)  loss 5.264991 (5.117654)\u001b[0m\n",
      "[2023-10-22 12:03:26] \u001b[32mEpoch [10/10] Step [291/469]  acc1 0.890625 (0.893954)  loss 5.004891 (5.114030)\u001b[0m\n",
      "[2023-10-22 12:03:27] \u001b[32mEpoch [10/10] Step [301/469]  acc1 0.937500 (0.894674)  loss 4.877048 (5.108777)\u001b[0m\n",
      "[2023-10-22 12:03:28] \u001b[32mEpoch [10/10] Step [311/469]  acc1 0.875000 (0.894192)  loss 5.034221 (5.106824)\u001b[0m\n",
      "[2023-10-22 12:03:30] \u001b[32mEpoch [10/10] Step [321/469]  acc1 0.875000 (0.894227)  loss 4.970248 (5.102686)\u001b[0m\n",
      "[2023-10-22 12:03:31] \u001b[32mEpoch [10/10] Step [331/469]  acc1 0.890625 (0.894307)  loss 4.975997 (5.099150)\u001b[0m\n",
      "[2023-10-22 12:03:32] \u001b[32mEpoch [10/10] Step [341/469]  acc1 0.828125 (0.894749)  loss 5.222406 (5.095062)\u001b[0m\n",
      "[2023-10-22 12:03:34] \u001b[32mEpoch [10/10] Step [351/469]  acc1 0.859375 (0.894320)  loss 5.137556 (5.093368)\u001b[0m\n",
      "[2023-10-22 12:03:35] \u001b[32mEpoch [10/10] Step [361/469]  acc1 0.921875 (0.894261)  loss 4.908566 (5.090865)\u001b[0m\n",
      "[2023-10-22 12:03:36] \u001b[32mEpoch [10/10] Step [371/469]  acc1 0.921875 (0.894121)  loss 4.937087 (5.087709)\u001b[0m\n",
      "[2023-10-22 12:03:38] \u001b[32mEpoch [10/10] Step [381/469]  acc1 0.828125 (0.894029)  loss 5.118838 (5.086158)\u001b[0m\n",
      "[2023-10-22 12:03:39] \u001b[32mEpoch [10/10] Step [391/469]  acc1 0.890625 (0.893662)  loss 5.035836 (5.084121)\u001b[0m\n",
      "[2023-10-22 12:03:40] \u001b[32mEpoch [10/10] Step [401/469]  acc1 0.828125 (0.893119)  loss 5.125701 (5.083406)\u001b[0m\n",
      "[2023-10-22 12:03:42] \u001b[32mEpoch [10/10] Step [411/469]  acc1 0.843750 (0.893172)  loss 5.118679 (5.081239)\u001b[0m\n",
      "[2023-10-22 12:03:43] \u001b[32mEpoch [10/10] Step [421/469]  acc1 0.921875 (0.893260)  loss 4.907990 (5.079142)\u001b[0m\n",
      "[2023-10-22 12:03:45] \u001b[32mEpoch [10/10] Step [431/469]  acc1 0.953125 (0.893126)  loss 4.900797 (5.077646)\u001b[0m\n",
      "[2023-10-22 12:03:46] \u001b[32mEpoch [10/10] Step [441/469]  acc1 0.906250 (0.893389)  loss 4.948647 (5.075663)\u001b[0m\n",
      "[2023-10-22 12:03:47] \u001b[32mEpoch [10/10] Step [451/469]  acc1 0.828125 (0.893050)  loss 5.145242 (5.074431)\u001b[0m\n",
      "[2023-10-22 12:03:49] \u001b[32mEpoch [10/10] Step [461/469]  acc1 0.937500 (0.893404)  loss 4.892768 (5.071410)\u001b[0m\n",
      "Final architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'avgpool', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'skipconnect', 'reduce_n4_p1': 'sepconv3x3', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'skipconnect', 'reduce_n5_p2': 'skipconnect', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [2, 0], 'reduce_n4_switch': [2, 3], 'reduce_n5_switch': [4, 3]}\n"
     ]
    }
   ],
   "source": [
    "save_folder = 'checkpoints/0'\n",
    "epochs = 10\n",
    "lambdas = [1, 2]\n",
    "weight = 2\n",
    "\n",
    "for lambd in lambdas:\n",
    "    print(f\"weight = {weight}, lambd = {lambd}\")\n",
    "    if dataset == \"fashionmnist\":\n",
    "        model = CNN(32, 1, channels, 10, layers)\n",
    "    if dataset == \"cifar10\":\n",
    "        model = CNN(32, 3, channels, 10, layers)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # mycriterion()\n",
    "    optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, epochs, eta_min=0.001)\n",
    "    trainer = utils.MyDartsTrainer( # MyDartsTrainer\n",
    "        model=model,\n",
    "        loss=criterion, # =mycriterion,\n",
    "        metrics=lambda output, target: utils.accuracy(output, target, topk=(1,)),\n",
    "        optimizer=optim,\n",
    "        num_epochs=epochs,\n",
    "        dataset=dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        log_frequency=log_frequency,\n",
    "        unrolled=unrolled,\n",
    "        weight=weight, # вес регуляризатора\n",
    "        lambd=lambd, # количество общих ребер\n",
    "        train_as_optimal=False,\n",
    "        optimalPath='checkpoints/' + dataset + '/optimal/arc.json',\n",
    "        tau=0.98,\n",
    "        learning_rate=2.5E-3,\n",
    "        arc_learning_rate=3.0E-1,\n",
    "    )\n",
    "    trainer.fit()\n",
    "    final_architecture = trainer.export()\n",
    "    print('Final architecture:', final_architecture)\n",
    "    json.dump(trainer.export(), open(f'checkpoints/' + dataset + '/lambd={lambd}/arc.json', 'w+'))\n",
    "    # json.dump(trainer.export(), open(f\"checkpoints/lambd={lambd}\" + '/arc.json', 'w+'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1, 2, 2, 3, 5, 6, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "arcs = []\n",
    "\n",
    "with open(f'checkpoints/fashionMNIST/optimal/arc.json') as f:\n",
    "    arc = json.load(f) # оптимальная архитектура в виде словаря\n",
    "    arcs.append(arc)\n",
    "\n",
    "lambds = [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4]\n",
    "for lamb in lambds:\n",
    "    with open(f'checkpoints/fashionMNIST/lambd={lamb}/arc.json') as f:\n",
    "        arc = json.load(f)\n",
    "        arcs.append(arc)\n",
    "\n",
    "all_intersections = []\n",
    "for arc in arcs:\n",
    "    intersections = []\n",
    "    for other_arc in arcs:\n",
    "        same = 0\n",
    "        for n in range(2, 6):\n",
    "            common_parents = set(arc[f\"reduce_n{n}_switch\"]) & set(other_arc[f\"reduce_n{n}_switch\"])\n",
    "            for p in common_parents:\n",
    "                key = f\"reduce_n{n}_p{p}\"\n",
    "                if arc[key] == other_arc[key]:\n",
    "                    same += 1\n",
    "        intersections.append(same)\n",
    "    all_intersections.append(intersections)\n",
    "intersections_with_opt = all_intersections[0]\n",
    "print(intersections_with_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'lambda')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = intersections_with_opt[1:]\n",
    "plt.plot(lambds, y)\n",
    "plt.ylabel('common edges')\n",
    "plt.xlabel('lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 2\n",
    "batch_size = 96\n",
    "log_frequency = 20\n",
    "channels = 16\n",
    "unrolled = False\n",
    "visualization = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/fashionMNIST/lambd=0.5/\n",
      "[2023-10-23 13:58:48] \u001b[32mFixed architecture: {'reduce_n2_p0': 'avgpool', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv3x3', 'reduce_n3_p1': 'avgpool', 'reduce_n3_p2': 'maxpool', 'reduce_n4_p0': 'maxpool', 'reduce_n4_p1': 'sepconv5x5', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv5x5', 'reduce_n5_p1': 'sepconv3x3', 'reduce_n5_p2': 'skipconnect', 'reduce_n5_p3': 'skipconnect', 'reduce_n5_p4': 'sepconv5x5', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [2, 0], 'reduce_n4_switch': [0, 1], 'reduce_n5_switch': [4, 0]}\u001b[0m\n",
      "[2023-10-23 13:58:49] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 13:58:51] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.353 Prec@(1,5) (6.2%, 46.9%)\u001b[0m\n",
      "[2023-10-23 13:58:52] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.719 Prec@(1,5) (25.7%, 70.8%)\u001b[0m\n",
      "[2023-10-23 13:58:53] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.274 Prec@(1,5) (36.8%, 82.2%)\u001b[0m\n",
      "[2023-10-23 13:58:53] \u001b[32mTrain: [  1/10] Step 060/624 Loss 2.023 Prec@(1,5) (43.9%, 87.1%)\u001b[0m\n",
      "[2023-10-23 13:58:54] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.829 Prec@(1,5) (49.5%, 89.8%)\u001b[0m\n",
      "[2023-10-23 13:58:54] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.701 Prec@(1,5) (53.1%, 91.5%)\u001b[0m\n",
      "[2023-10-23 13:58:55] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.609 Prec@(1,5) (55.6%, 92.7%)\u001b[0m\n",
      "[2023-10-23 13:58:55] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.537 Prec@(1,5) (57.7%, 93.6%)\u001b[0m\n",
      "[2023-10-23 13:58:56] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.472 Prec@(1,5) (59.6%, 94.3%)\u001b[0m\n",
      "[2023-10-23 13:58:56] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.420 Prec@(1,5) (61.0%, 94.8%)\u001b[0m\n",
      "[2023-10-23 13:58:57] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.377 Prec@(1,5) (62.3%, 95.3%)\u001b[0m\n",
      "[2023-10-23 13:58:57] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.347 Prec@(1,5) (63.0%, 95.6%)\u001b[0m\n",
      "[2023-10-23 13:58:58] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.316 Prec@(1,5) (63.9%, 95.9%)\u001b[0m\n",
      "[2023-10-23 13:58:58] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.289 Prec@(1,5) (64.7%, 96.1%)\u001b[0m\n",
      "[2023-10-23 13:58:59] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.263 Prec@(1,5) (65.3%, 96.4%)\u001b[0m\n",
      "[2023-10-23 13:58:59] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.239 Prec@(1,5) (66.0%, 96.6%)\u001b[0m\n",
      "[2023-10-23 13:59:00] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.216 Prec@(1,5) (66.7%, 96.7%)\u001b[0m\n",
      "[2023-10-23 13:59:00] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.197 Prec@(1,5) (67.2%, 96.9%)\u001b[0m\n",
      "[2023-10-23 13:59:01] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.179 Prec@(1,5) (67.7%, 97.0%)\u001b[0m\n",
      "[2023-10-23 13:59:01] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.158 Prec@(1,5) (68.2%, 97.1%)\u001b[0m\n",
      "[2023-10-23 13:59:02] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.138 Prec@(1,5) (68.8%, 97.2%)\u001b[0m\n",
      "[2023-10-23 13:59:02] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.120 Prec@(1,5) (69.4%, 97.3%)\u001b[0m\n",
      "[2023-10-23 13:59:03] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.104 Prec@(1,5) (69.8%, 97.4%)\u001b[0m\n",
      "[2023-10-23 13:59:03] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.089 Prec@(1,5) (70.2%, 97.5%)\u001b[0m\n",
      "[2023-10-23 13:59:04] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.075 Prec@(1,5) (70.6%, 97.6%)\u001b[0m\n",
      "[2023-10-23 13:59:04] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.063 Prec@(1,5) (71.0%, 97.7%)\u001b[0m\n",
      "[2023-10-23 13:59:05] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.050 Prec@(1,5) (71.3%, 97.8%)\u001b[0m\n",
      "[2023-10-23 13:59:05] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.040 Prec@(1,5) (71.6%, 97.8%)\u001b[0m\n",
      "[2023-10-23 13:59:06] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.029 Prec@(1,5) (71.9%, 97.9%)\u001b[0m\n",
      "[2023-10-23 13:59:06] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.019 Prec@(1,5) (72.2%, 98.0%)\u001b[0m\n",
      "[2023-10-23 13:59:07] \u001b[32mTrain: [  1/10] Step 600/624 Loss 1.008 Prec@(1,5) (72.5%, 98.0%)\u001b[0m\n",
      "[2023-10-23 13:59:07] \u001b[32mTrain: [  1/10] Step 620/624 Loss 1.000 Prec@(1,5) (72.7%, 98.1%)\u001b[0m\n",
      "[2023-10-23 13:59:07] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.997 Prec@(1,5) (72.8%, 98.1%)\u001b[0m\n",
      "[2023-10-23 13:59:08] \u001b[32mTrain: [  1/10] Final Prec@1 72.7833%\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.956 Prec@(1,5) (79.2%, 99.0%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.686 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.754 Prec@(1,5) (80.7%, 99.4%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.771 Prec@(1,5) (80.3%, 99.4%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.754 Prec@(1,5) (80.5%, 99.4%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.758 Prec@(1,5) (80.3%, 99.4%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.754 Prec@(1,5) (80.3%, 99.4%)\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mValid: [  1/10] Final Prec@1 80.3200%\u001b[0m\n",
      "[2023-10-23 13:59:09] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 13:59:10] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.822 Prec@(1,5) (82.3%, 99.0%)\u001b[0m\n",
      "[2023-10-23 13:59:11] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.711 Prec@(1,5) (80.6%, 99.5%)\u001b[0m\n",
      "[2023-10-23 13:59:12] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.690 Prec@(1,5) (81.3%, 99.5%)\u001b[0m\n",
      "[2023-10-23 13:59:12] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.702 Prec@(1,5) (81.0%, 99.5%)\u001b[0m\n",
      "[2023-10-23 13:59:13] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.689 Prec@(1,5) (81.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 13:59:13] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.687 Prec@(1,5) (81.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:14] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.693 Prec@(1,5) (81.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:14] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.700 Prec@(1,5) (80.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:15] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.696 Prec@(1,5) (80.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:15] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.691 Prec@(1,5) (81.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:16] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.687 Prec@(1,5) (81.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:17] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.685 Prec@(1,5) (81.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:17] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.683 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:18] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.683 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:18] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.680 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:19] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.677 Prec@(1,5) (81.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:19] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.673 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:20] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.671 Prec@(1,5) (81.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:21] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.667 Prec@(1,5) (81.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:21] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.663 Prec@(1,5) (81.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:22] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.659 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:22] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.655 Prec@(1,5) (82.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:23] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.653 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:23] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.651 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:24] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.649 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:24] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.645 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:25] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.644 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:26] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.644 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:27] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.643 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:28] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.640 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:28] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.641 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:29] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.639 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:29] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.639 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 13:59:30] \u001b[32mTrain: [  2/10] Final Prec@1 82.6917%\u001b[0m\n",
      "[2023-10-23 13:59:31] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.558 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 13:59:31] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.446 Prec@(1,5) (87.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 13:59:31] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.501 Prec@(1,5) (86.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:32] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.513 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:32] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.498 Prec@(1,5) (86.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:32] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.501 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:32] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.496 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:32] \u001b[32mValid: [  2/10] Final Prec@1 86.7400%\u001b[0m\n",
      "[2023-10-23 13:59:32] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 13:59:33] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.618 Prec@(1,5) (84.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 13:59:34] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.614 Prec@(1,5) (83.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 13:59:35] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.600 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:36] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.596 Prec@(1,5) (83.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:36] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.585 Prec@(1,5) (84.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 13:59:37] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.577 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:38] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.585 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:39] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.578 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:40] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.577 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:40] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.573 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:41] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.572 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 13:59:42] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.571 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 13:59:43] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.567 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 13:59:44] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.567 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 13:59:45] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.570 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:46] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.568 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:47] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.568 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:47] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.568 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:48] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.571 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:49] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.568 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:50] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.567 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:51] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.566 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:52] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.567 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:53] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.567 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:54] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.564 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:54] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.565 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:55] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:56] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:57] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:58] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 13:59:59] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.562 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:00:00] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.561 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:00:00] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.560 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:00:00] \u001b[32mTrain: [  3/10] Final Prec@1 85.0000%\u001b[0m\n",
      "[2023-10-23 14:00:01] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.620 Prec@(1,5) (81.2%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:00:01] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.472 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:00:02] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.511 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:02] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.521 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:02] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.512 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:02] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.514 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:02] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.509 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:03] \u001b[32mValid: [  3/10] Final Prec@1 87.1200%\u001b[0m\n",
      "[2023-10-23 14:00:03] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:00:04] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.517 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:00:04] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.523 Prec@(1,5) (85.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:00:05] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.512 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:06] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.509 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:07] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.525 Prec@(1,5) (85.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:08] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.518 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:08] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.507 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:09] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.515 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:10] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.515 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:11] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.515 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:12] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.518 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:13] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.518 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:13] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.519 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:14] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.519 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:15] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.517 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:16] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.513 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:17] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.515 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:18] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.514 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:19] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.515 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:19] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.516 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:20] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.516 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:21] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.515 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:22] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.514 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:23] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.515 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:24] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.512 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:25] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.511 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:25] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.512 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:26] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.512 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:27] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.512 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:28] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.513 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:29] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.514 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:30] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.514 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:30] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.514 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:30] \u001b[32mTrain: [  4/10] Final Prec@1 86.2700%\u001b[0m\n",
      "[2023-10-23 14:00:31] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.667 Prec@(1,5) (83.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:00:32] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.519 Prec@(1,5) (86.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:00:32] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.569 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:32] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.563 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:32] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.536 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:33] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.544 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:33] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.541 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:33] \u001b[32mValid: [  4/10] Final Prec@1 86.2600%\u001b[0m\n",
      "[2023-10-23 14:00:33] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:00:34] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.595 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:00:35] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.457 Prec@(1,5) (87.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:00:36] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.460 Prec@(1,5) (88.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:00:36] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.460 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:37] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.471 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:38] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.466 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:39] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.471 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:40] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.473 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:40] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.479 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:41] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.479 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:42] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.480 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:43] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.481 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:44] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.482 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:45] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.479 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:45] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.476 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:46] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.475 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:47] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.474 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:48] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.475 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:49] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.475 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:50] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.473 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:51] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.474 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:51] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.474 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:52] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.473 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:53] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.473 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:54] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.475 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:55] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.477 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:56] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.476 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:57] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.475 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:58] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.475 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:58] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.474 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:00:59] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.472 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:00] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.472 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:00] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.472 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:01] \u001b[32mTrain: [  5/10] Final Prec@1 87.5183%\u001b[0m\n",
      "[2023-10-23 14:01:02] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.545 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:01:02] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.372 Prec@(1,5) (89.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:01:02] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.428 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:02] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.420 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:03] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.410 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:03] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.416 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:03] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.412 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:03] \u001b[32mValid: [  5/10] Final Prec@1 88.6200%\u001b[0m\n",
      "[2023-10-23 14:01:03] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:01:04] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.435 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:01:05] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.462 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:06] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.461 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:07] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.457 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:07] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.452 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:08] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.449 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:09] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.448 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:10] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.453 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:11] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.453 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:11] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.450 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:12] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.450 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:13] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.447 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:14] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.448 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:15] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.448 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:16] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.445 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:17] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.442 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:18] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.444 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:18] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.446 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:19] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.446 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:20] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.446 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:21] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.444 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:22] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.444 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:23] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.445 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:24] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.446 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:25] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.446 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:25] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.445 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:26] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.447 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:27] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.447 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:28] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.448 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:29] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.448 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:30] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.446 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:31] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.446 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:31] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.445 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:31] \u001b[32mTrain: [  6/10] Final Prec@1 88.3500%\u001b[0m\n",
      "[2023-10-23 14:01:32] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.481 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:01:32] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.357 Prec@(1,5) (90.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:01:33] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.394 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:33] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.395 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:33] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.381 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:33] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.383 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:33] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.379 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:34] \u001b[32mValid: [  6/10] Final Prec@1 89.9600%\u001b[0m\n",
      "[2023-10-23 14:01:34] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:01:35] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.475 Prec@(1,5) (85.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:01:36] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.449 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:01:36] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.447 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:37] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.436 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:38] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.438 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:39] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.433 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:40] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.430 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:40] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.424 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:41] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.428 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:42] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.426 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:43] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.430 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:44] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.427 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:45] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:45] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.430 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:46] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.428 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:47] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.429 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:48] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.428 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:49] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:50] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.426 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:51] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.426 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:52] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:53] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:53] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:54] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:55] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:56] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:57] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:58] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:01:59] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.424 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:00] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.424 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:01] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.424 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:01] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.423 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:02] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.423 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:02] \u001b[32mTrain: [  7/10] Final Prec@1 88.7600%\u001b[0m\n",
      "[2023-10-23 14:02:03] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.653 Prec@(1,5) (85.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:02:03] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.479 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:03] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.525 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:04] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.521 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:04] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.519 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:04] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.518 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:04] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.513 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:04] \u001b[32mValid: [  7/10] Final Prec@1 88.0100%\u001b[0m\n",
      "[2023-10-23 14:02:04] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:02:06] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.371 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:02:06] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.404 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:07] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.404 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:08] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.418 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:09] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.423 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:10] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.425 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:10] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.423 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:11] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.422 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:12] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.417 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:13] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.415 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:14] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.414 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:14] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.417 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:15] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.416 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:16] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.413 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:17] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.412 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:18] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.413 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:19] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.412 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:20] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.412 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:21] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.412 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:21] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.409 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:22] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.409 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:23] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.410 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:24] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.411 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:25] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.412 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:26] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.412 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:27] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.410 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:27] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.409 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:28] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.409 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:29] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.410 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:30] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.410 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:31] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.410 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:32] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.410 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:32] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.410 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:32] \u001b[32mTrain: [  8/10] Final Prec@1 89.3800%\u001b[0m\n",
      "[2023-10-23 14:02:33] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.394 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:02:33] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.359 Prec@(1,5) (90.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:02:34] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.403 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:34] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.402 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:34] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.394 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:34] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.398 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:35] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.394 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:35] \u001b[32mValid: [  8/10] Final Prec@1 89.9200%\u001b[0m\n",
      "[2023-10-23 14:02:35] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:02:36] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.430 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:02:37] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.424 Prec@(1,5) (89.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:02:37] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.425 Prec@(1,5) (89.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:02:38] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.416 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:39] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.415 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:40] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.413 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:41] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.413 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:41] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.412 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:42] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.409 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:02:43] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.409 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:44] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.407 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:45] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.407 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:46] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.409 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:46] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.406 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:47] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.404 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:48] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.401 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:49] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.401 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:50] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.399 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:51] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.399 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:52] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.400 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:53] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.398 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:53] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.397 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:54] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.396 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:55] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.396 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:56] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.396 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:57] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.396 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:58] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.396 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:02:59] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.397 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:00] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.397 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:00] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.397 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:01] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.398 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:02] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.397 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:02] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.397 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:03] \u001b[32mTrain: [  9/10] Final Prec@1 89.9250%\u001b[0m\n",
      "[2023-10-23 14:03:04] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.481 Prec@(1,5) (85.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:03:04] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.361 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:04] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.416 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:04] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.410 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:05] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.400 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:05] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.398 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:05] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.394 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:05] \u001b[32mValid: [  9/10] Final Prec@1 89.9200%\u001b[0m\n",
      "[2023-10-23 14:03:05] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:03:06] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.336 Prec@(1,5) (93.8%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:03:07] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.422 Prec@(1,5) (89.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:03:08] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.435 Prec@(1,5) (89.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:03:09] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.425 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:09] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.423 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:10] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.423 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:11] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.420 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:12] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.416 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:13] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.414 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:13] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.411 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:14] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.408 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:15] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.406 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:16] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.408 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:17] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.405 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:18] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.403 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:03:19] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.402 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:19] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.400 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:20] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.399 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:21] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:22] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:23] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.397 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:24] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.397 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:25] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.396 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:26] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.396 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:26] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.395 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:27] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.396 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:28] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.395 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:29] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.395 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:30] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.397 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:31] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.397 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:32] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.395 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:32] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.394 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:33] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.394 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:33] \u001b[32mTrain: [ 10/10] Final Prec@1 89.7467%\u001b[0m\n",
      "[2023-10-23 14:03:34] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.395 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:03:34] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.330 Prec@(1,5) (92.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:34] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.378 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:35] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.374 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:35] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.362 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:35] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.362 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:35] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.359 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:03:35] \u001b[32mValid: [ 10/10] Final Prec@1 90.7800%\u001b[0m\n",
      "Final best Prec@1 = 90.7800%\n",
      "[0.9078000211715698]\n",
      "./checkpoints/fashionMNIST/lambd=1/\n",
      "[2023-10-23 14:03:35] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'dilconv3x3', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'maxpool', 'reduce_n4_p0': 'sepconv5x5', 'reduce_n4_p1': 'sepconv3x3', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'sepconv5x5', 'reduce_n5_p0': 'dilconv3x3', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'sepconv5x5', 'reduce_n5_p3': 'dilconv5x5', 'reduce_n5_p4': 'avgpool', 'reduce_n2_switch': [1, 0], 'reduce_n3_switch': [1, 0], 'reduce_n4_switch': [0, 1], 'reduce_n5_switch': [2, 0]}\u001b[0m\n",
      "[2023-10-23 14:03:36] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:03:37] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.325 Prec@(1,5) (5.2%, 52.1%)\u001b[0m\n",
      "[2023-10-23 14:03:38] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.781 Prec@(1,5) (22.0%, 70.9%)\u001b[0m\n",
      "[2023-10-23 14:03:38] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.333 Prec@(1,5) (33.5%, 82.3%)\u001b[0m\n",
      "[2023-10-23 14:03:39] \u001b[32mTrain: [  1/10] Step 060/624 Loss 2.093 Prec@(1,5) (40.1%, 87.1%)\u001b[0m\n",
      "[2023-10-23 14:03:40] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.926 Prec@(1,5) (44.5%, 89.7%)\u001b[0m\n",
      "[2023-10-23 14:03:41] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.823 Prec@(1,5) (47.8%, 91.4%)\u001b[0m\n",
      "[2023-10-23 14:03:42] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.724 Prec@(1,5) (50.8%, 92.5%)\u001b[0m\n",
      "[2023-10-23 14:03:43] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.652 Prec@(1,5) (52.9%, 93.2%)\u001b[0m\n",
      "[2023-10-23 14:03:44] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.586 Prec@(1,5) (54.9%, 93.9%)\u001b[0m\n",
      "[2023-10-23 14:03:44] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.530 Prec@(1,5) (56.5%, 94.5%)\u001b[0m\n",
      "[2023-10-23 14:03:45] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.493 Prec@(1,5) (57.7%, 95.0%)\u001b[0m\n",
      "[2023-10-23 14:03:46] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.457 Prec@(1,5) (58.9%, 95.3%)\u001b[0m\n",
      "[2023-10-23 14:03:47] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.417 Prec@(1,5) (60.0%, 95.6%)\u001b[0m\n",
      "[2023-10-23 14:03:48] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.383 Prec@(1,5) (60.9%, 95.9%)\u001b[0m\n",
      "[2023-10-23 14:03:49] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.351 Prec@(1,5) (61.8%, 96.2%)\u001b[0m\n",
      "[2023-10-23 14:03:50] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.328 Prec@(1,5) (62.7%, 96.4%)\u001b[0m\n",
      "[2023-10-23 14:03:51] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.302 Prec@(1,5) (63.4%, 96.6%)\u001b[0m\n",
      "[2023-10-23 14:03:52] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.280 Prec@(1,5) (64.1%, 96.7%)\u001b[0m\n",
      "[2023-10-23 14:03:53] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.261 Prec@(1,5) (64.7%, 96.9%)\u001b[0m\n",
      "[2023-10-23 14:03:54] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.241 Prec@(1,5) (65.3%, 97.0%)\u001b[0m\n",
      "[2023-10-23 14:03:55] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.222 Prec@(1,5) (65.8%, 97.1%)\u001b[0m\n",
      "[2023-10-23 14:03:56] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.204 Prec@(1,5) (66.3%, 97.2%)\u001b[0m\n",
      "[2023-10-23 14:03:57] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.189 Prec@(1,5) (66.7%, 97.3%)\u001b[0m\n",
      "[2023-10-23 14:03:57] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.174 Prec@(1,5) (67.1%, 97.4%)\u001b[0m\n",
      "[2023-10-23 14:03:58] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.160 Prec@(1,5) (67.6%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:03:59] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.146 Prec@(1,5) (68.0%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:04:00] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.135 Prec@(1,5) (68.3%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:04:01] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.122 Prec@(1,5) (68.7%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:04:02] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.112 Prec@(1,5) (69.0%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:04:03] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.102 Prec@(1,5) (69.3%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:04:04] \u001b[32mTrain: [  1/10] Step 600/624 Loss 1.091 Prec@(1,5) (69.6%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:04:05] \u001b[32mTrain: [  1/10] Step 620/624 Loss 1.080 Prec@(1,5) (69.9%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:04:05] \u001b[32mTrain: [  1/10] Step 624/624 Loss 1.078 Prec@(1,5) (70.0%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:04:05] \u001b[32mTrain: [  1/10] Final Prec@1 69.9633%\u001b[0m\n",
      "[2023-10-23 14:04:06] \u001b[32mValid: [  1/10] Step 000/104 Loss 1.316 Prec@(1,5) (68.8%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:04:07] \u001b[32mValid: [  1/10] Step 020/104 Loss 1.099 Prec@(1,5) (74.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:04:07] \u001b[32mValid: [  1/10] Step 040/104 Loss 1.130 Prec@(1,5) (74.1%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:04:07] \u001b[32mValid: [  1/10] Step 060/104 Loss 1.154 Prec@(1,5) (74.1%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:04:08] \u001b[32mValid: [  1/10] Step 080/104 Loss 1.147 Prec@(1,5) (74.2%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:04:08] \u001b[32mValid: [  1/10] Step 100/104 Loss 1.161 Prec@(1,5) (74.2%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:04:08] \u001b[32mValid: [  1/10] Step 104/104 Loss 1.153 Prec@(1,5) (74.4%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:04:08] \u001b[32mValid: [  1/10] Final Prec@1 74.3500%\u001b[0m\n",
      "[2023-10-23 14:04:08] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:04:09] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.679 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:04:10] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.759 Prec@(1,5) (79.0%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:04:11] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.740 Prec@(1,5) (80.0%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:04:12] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.743 Prec@(1,5) (79.7%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:04:13] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.727 Prec@(1,5) (80.1%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:04:14] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.733 Prec@(1,5) (80.1%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:04:15] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.724 Prec@(1,5) (80.4%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:04:16] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.722 Prec@(1,5) (80.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:17] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.720 Prec@(1,5) (80.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:18] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.716 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:19] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.715 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:20] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.711 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:21] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.714 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:22] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.712 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:23] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.714 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:24] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.710 Prec@(1,5) (80.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:25] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.708 Prec@(1,5) (80.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:26] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.703 Prec@(1,5) (80.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:27] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.701 Prec@(1,5) (80.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:28] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.698 Prec@(1,5) (81.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:29] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.695 Prec@(1,5) (81.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:30] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.694 Prec@(1,5) (81.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:31] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.690 Prec@(1,5) (81.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:32] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.688 Prec@(1,5) (81.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:33] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.685 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:34] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.684 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:35] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.681 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:36] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.679 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:37] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.680 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:38] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.678 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:39] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.677 Prec@(1,5) (81.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:40] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.675 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:40] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.674 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:41] \u001b[32mTrain: [  2/10] Final Prec@1 81.6583%\u001b[0m\n",
      "[2023-10-23 14:04:42] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.840 Prec@(1,5) (81.2%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:04:42] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.561 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:42] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.627 Prec@(1,5) (83.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:42] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.640 Prec@(1,5) (83.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:43] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.639 Prec@(1,5) (83.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:43] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.643 Prec@(1,5) (83.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:43] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.637 Prec@(1,5) (83.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:43] \u001b[32mValid: [  2/10] Final Prec@1 83.9300%\u001b[0m\n",
      "[2023-10-23 14:04:43] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:04:44] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.973 Prec@(1,5) (78.1%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:04:45] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.644 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:46] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.608 Prec@(1,5) (83.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:04:47] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.590 Prec@(1,5) (83.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:48] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.594 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:49] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.595 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:50] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.603 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:51] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.606 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:52] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.605 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:53] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.601 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:54] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.603 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:55] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.606 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:56] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.607 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:57] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.603 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:58] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.598 Prec@(1,5) (83.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:04:59] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.600 Prec@(1,5) (83.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:00] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.596 Prec@(1,5) (83.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:01] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.596 Prec@(1,5) (83.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:02] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.593 Prec@(1,5) (83.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:03] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.594 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:04] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.591 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:05] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.592 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:06] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.591 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:08] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.590 Prec@(1,5) (84.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:09] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.589 Prec@(1,5) (84.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:10] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.589 Prec@(1,5) (84.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:11] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.588 Prec@(1,5) (84.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:12] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.586 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:13] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.585 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:14] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.584 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:15] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.584 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:16] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.584 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:16] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.584 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:16] \u001b[32mTrain: [  3/10] Final Prec@1 84.2100%\u001b[0m\n",
      "[2023-10-23 14:05:17] \u001b[32mValid: [  3/10] Step 000/104 Loss 1.115 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:05:17] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.869 Prec@(1,5) (80.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:05:18] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.960 Prec@(1,5) (80.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:18] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.971 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:05:18] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.957 Prec@(1,5) (80.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:19] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.964 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:05:19] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.955 Prec@(1,5) (80.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:05:19] \u001b[32mValid: [  3/10] Final Prec@1 80.7000%\u001b[0m\n",
      "[2023-10-23 14:05:19] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:05:20] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.693 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:05:21] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.572 Prec@(1,5) (83.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:05:22] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.563 Prec@(1,5) (84.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:05:23] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.558 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:24] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.558 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:25] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.560 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:26] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.569 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:26] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.562 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:27] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.561 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:28] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.560 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:30] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.560 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:31] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.552 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:32] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.548 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:33] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.544 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:34] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.541 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:35] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.542 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:36] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.541 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:37] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.541 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:38] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.540 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:39] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.538 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:40] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.538 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:41] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.537 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:42] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.536 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:43] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.534 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:44] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.535 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:45] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.534 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:46] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.533 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:47] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.535 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:48] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.536 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:49] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.538 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:50] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.537 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:51] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.537 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:51] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.538 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:51] \u001b[32mTrain: [  4/10] Final Prec@1 85.5350%\u001b[0m\n",
      "[2023-10-23 14:05:52] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.935 Prec@(1,5) (78.1%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:05:53] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.653 Prec@(1,5) (83.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:05:53] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.740 Prec@(1,5) (82.4%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:05:53] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.741 Prec@(1,5) (82.5%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:05:54] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.726 Prec@(1,5) (83.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:05:54] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.730 Prec@(1,5) (83.0%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:05:54] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.724 Prec@(1,5) (83.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:05:54] \u001b[32mValid: [  4/10] Final Prec@1 83.2100%\u001b[0m\n",
      "[2023-10-23 14:05:54] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:05:55] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.355 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:05:56] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.509 Prec@(1,5) (86.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:05:57] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.508 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:05:58] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.521 Prec@(1,5) (86.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:05:59] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.516 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:00] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.509 Prec@(1,5) (86.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:06:01] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.508 Prec@(1,5) (86.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:06:02] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.511 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:06:03] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.512 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:04] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.506 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:05] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.505 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:06] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.509 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:07] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.512 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:08] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.511 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:09] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.511 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:10] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.513 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:11] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.511 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:12] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.511 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:13] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.510 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:14] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.507 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:15] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:16] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:17] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:18] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.506 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:19] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.504 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:20] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.503 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:21] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.501 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:22] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.498 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:23] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.496 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:24] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.494 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:25] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.495 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:26] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.494 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:26] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.493 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:27] \u001b[32mTrain: [  5/10] Final Prec@1 86.8817%\u001b[0m\n",
      "[2023-10-23 14:06:28] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.564 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:06:28] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.452 Prec@(1,5) (89.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:06:28] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.495 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:29] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.502 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:29] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.478 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:29] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.489 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:29] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.484 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:29] \u001b[32mValid: [  5/10] Final Prec@1 88.7000%\u001b[0m\n",
      "[2023-10-23 14:06:29] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:06:31] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.598 Prec@(1,5) (83.3%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:06:32] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.507 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:32] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.479 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:33] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.496 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:06:34] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.494 Prec@(1,5) (86.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:35] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.485 Prec@(1,5) (86.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:36] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.479 Prec@(1,5) (87.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:37] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.480 Prec@(1,5) (87.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:38] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.479 Prec@(1,5) (87.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:39] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.477 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:40] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.474 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:41] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.477 Prec@(1,5) (87.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:42] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.476 Prec@(1,5) (87.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:43] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.474 Prec@(1,5) (87.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:44] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.471 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:45] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.471 Prec@(1,5) (87.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:46] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.472 Prec@(1,5) (87.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:47] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.472 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:48] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.469 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:49] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.468 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:50] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.467 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:51] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.466 Prec@(1,5) (87.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:52] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.468 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:53] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.468 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:54] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.467 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:55] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.468 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:56] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.469 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:57] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.469 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:58] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.468 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:06:59] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.468 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:00] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.466 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:01] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.467 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:02] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.466 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:02] \u001b[32mTrain: [  6/10] Final Prec@1 87.5067%\u001b[0m\n",
      "[2023-10-23 14:07:03] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.447 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:07:03] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.394 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:03] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.442 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:04] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.443 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:04] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.432 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:04] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.433 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:04] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.428 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:05] \u001b[32mValid: [  6/10] Final Prec@1 89.6900%\u001b[0m\n",
      "[2023-10-23 14:07:05] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:07:06] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.340 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:07:07] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.442 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:08] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.462 Prec@(1,5) (88.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:07:08] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.454 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:09] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.452 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:10] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.449 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:11] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.450 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:12] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.441 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:13] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.440 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:14] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.439 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:15] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.442 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:16] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.444 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:17] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.446 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:18] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.445 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:19] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.445 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:20] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.444 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:21] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.446 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:22] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.446 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:23] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.445 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:24] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.447 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:25] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.447 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:26] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.445 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:27] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.444 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:28] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.444 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:29] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.443 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:30] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.442 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:31] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.443 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:32] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.442 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:34] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.443 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:35] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.444 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:36] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.443 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:37] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.442 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:37] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.441 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:37] \u001b[32mTrain: [  7/10] Final Prec@1 88.2450%\u001b[0m\n",
      "[2023-10-23 14:07:38] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.469 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:07:38] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.350 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:39] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.402 Prec@(1,5) (90.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:07:39] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.403 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:39] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.389 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:39] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.386 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:40] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.382 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:40] \u001b[32mValid: [  7/10] Final Prec@1 90.1300%\u001b[0m\n",
      "[2023-10-23 14:07:40] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:07:41] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.377 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:07:42] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.430 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:43] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.460 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:44] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.442 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:45] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.433 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:46] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.424 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:46] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.425 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:47] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.422 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:48] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.420 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:49] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.417 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:50] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.420 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:52] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.420 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:53] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.419 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:54] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.415 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:07:55] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.415 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:56] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.415 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:57] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.415 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:58] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.418 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:07:59] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.417 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:00] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.415 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:01] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.416 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:02] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.415 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:03] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.416 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:04] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.414 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:05] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.414 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:06] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.414 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:07] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.412 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:08] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.413 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:09] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.412 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:10] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.412 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:11] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.412 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:12] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.411 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:12] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.411 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:12] \u001b[32mTrain: [  8/10] Final Prec@1 89.1183%\u001b[0m\n",
      "[2023-10-23 14:08:13] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.504 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:08:14] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.350 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:14] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.398 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:14] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.399 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:14] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.383 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:15] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.382 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:15] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.377 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:15] \u001b[32mValid: [  8/10] Final Prec@1 90.8000%\u001b[0m\n",
      "[2023-10-23 14:08:15] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:08:16] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.342 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:08:17] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.417 Prec@(1,5) (88.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:08:18] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.404 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:19] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.400 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:20] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:21] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:22] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.394 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:23] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.393 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:24] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.393 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:25] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.391 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:26] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.392 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:27] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.388 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:28] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:29] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:30] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.385 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:31] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.388 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:32] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.388 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:33] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:34] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.385 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:35] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:36] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:37] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.388 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:38] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.388 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:39] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.389 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:40] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:41] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.388 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:42] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.390 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:43] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.390 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:44] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.391 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:45] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.391 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:46] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:47] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.393 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:47] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.393 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:47] \u001b[32mTrain: [  9/10] Final Prec@1 89.7950%\u001b[0m\n",
      "[2023-10-23 14:08:48] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.464 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:08:49] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.342 Prec@(1,5) (91.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:49] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.379 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:49] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.373 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:49] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.358 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:50] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.359 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:50] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.354 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:50] \u001b[32mValid: [  9/10] Final Prec@1 91.3600%\u001b[0m\n",
      "[2023-10-23 14:08:50] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:08:51] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.472 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:08:52] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.406 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:08:53] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.395 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:54] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.396 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:55] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:56] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.391 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:57] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.390 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:58] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.384 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:08:59] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.383 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:00] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.381 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:01] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.380 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:02] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.384 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:03] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.385 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:04] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.383 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:05] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.385 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:06] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.384 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:07] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:08] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.385 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:09] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.384 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:10] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.383 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:11] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.385 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:12] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.385 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:13] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.385 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:14] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:15] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:16] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:17] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:18] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:19] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:20] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:21] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:22] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.387 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:22] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.386 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:23] \u001b[32mTrain: [ 10/10] Final Prec@1 89.9967%\u001b[0m\n",
      "[2023-10-23 14:09:24] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.435 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:09:24] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.329 Prec@(1,5) (91.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:24] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.375 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:24] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.374 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:25] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.360 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:25] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.359 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:25] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.355 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:09:25] \u001b[32mValid: [ 10/10] Final Prec@1 91.2800%\u001b[0m\n",
      "Final best Prec@1 = 91.3600%\n",
      "[0.9078000211715698, 0.9136000185012817]\n",
      "./checkpoints/fashionMNIST/lambd=1.5/\n",
      "[2023-10-23 14:09:25] \u001b[32mFixed architecture: {'reduce_n2_p0': 'avgpool', 'reduce_n2_p1': 'skipconnect', 'reduce_n3_p0': 'avgpool', 'reduce_n3_p1': 'sepconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'sepconv5x5', 'reduce_n4_p1': 'dilconv5x5', 'reduce_n4_p2': 'maxpool', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'maxpool', 'reduce_n5_p1': 'sepconv3x3', 'reduce_n5_p2': 'maxpool', 'reduce_n5_p3': 'sepconv3x3', 'reduce_n5_p4': 'skipconnect', 'reduce_n2_switch': [1, 0], 'reduce_n3_switch': [2, 1], 'reduce_n4_switch': [3, 1], 'reduce_n5_switch': [2, 0]}\u001b[0m\n",
      "[2023-10-23 14:09:25] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:09:26] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.299 Prec@(1,5) (9.4%, 50.0%)\u001b[0m\n",
      "[2023-10-23 14:09:27] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.669 Prec@(1,5) (25.9%, 71.4%)\u001b[0m\n",
      "[2023-10-23 14:09:28] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.200 Prec@(1,5) (38.2%, 83.0%)\u001b[0m\n",
      "[2023-10-23 14:09:29] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.952 Prec@(1,5) (44.8%, 87.7%)\u001b[0m\n",
      "[2023-10-23 14:09:30] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.798 Prec@(1,5) (48.7%, 90.2%)\u001b[0m\n",
      "[2023-10-23 14:09:30] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.687 Prec@(1,5) (52.2%, 91.9%)\u001b[0m\n",
      "[2023-10-23 14:09:31] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.597 Prec@(1,5) (55.0%, 93.0%)\u001b[0m\n",
      "[2023-10-23 14:09:32] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.525 Prec@(1,5) (57.1%, 93.9%)\u001b[0m\n",
      "[2023-10-23 14:09:32] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.463 Prec@(1,5) (59.0%, 94.6%)\u001b[0m\n",
      "[2023-10-23 14:09:33] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.410 Prec@(1,5) (60.6%, 95.1%)\u001b[0m\n",
      "[2023-10-23 14:09:34] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.365 Prec@(1,5) (61.9%, 95.4%)\u001b[0m\n",
      "[2023-10-23 14:09:35] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.325 Prec@(1,5) (63.1%, 95.8%)\u001b[0m\n",
      "[2023-10-23 14:09:36] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.287 Prec@(1,5) (64.3%, 96.1%)\u001b[0m\n",
      "[2023-10-23 14:09:37] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.260 Prec@(1,5) (65.0%, 96.3%)\u001b[0m\n",
      "[2023-10-23 14:09:37] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.228 Prec@(1,5) (65.8%, 96.5%)\u001b[0m\n",
      "[2023-10-23 14:09:38] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.203 Prec@(1,5) (66.5%, 96.7%)\u001b[0m\n",
      "[2023-10-23 14:09:39] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.181 Prec@(1,5) (67.2%, 96.9%)\u001b[0m\n",
      "[2023-10-23 14:09:40] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.164 Prec@(1,5) (67.7%, 97.0%)\u001b[0m\n",
      "[2023-10-23 14:09:41] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.147 Prec@(1,5) (68.2%, 97.2%)\u001b[0m\n",
      "[2023-10-23 14:09:42] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.133 Prec@(1,5) (68.6%, 97.3%)\u001b[0m\n",
      "[2023-10-23 14:09:43] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.113 Prec@(1,5) (69.2%, 97.4%)\u001b[0m\n",
      "[2023-10-23 14:09:43] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.096 Prec@(1,5) (69.7%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:09:44] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.081 Prec@(1,5) (70.2%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:09:45] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.068 Prec@(1,5) (70.5%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:09:46] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.055 Prec@(1,5) (70.9%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:09:47] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.040 Prec@(1,5) (71.3%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:09:48] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.029 Prec@(1,5) (71.6%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:09:49] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.015 Prec@(1,5) (72.0%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:09:49] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.002 Prec@(1,5) (72.4%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:09:50] \u001b[32mTrain: [  1/10] Step 580/624 Loss 0.992 Prec@(1,5) (72.7%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:09:51] \u001b[32mTrain: [  1/10] Step 600/624 Loss 0.982 Prec@(1,5) (73.0%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:09:52] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.973 Prec@(1,5) (73.2%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:09:52] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.972 Prec@(1,5) (73.3%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:09:52] \u001b[32mTrain: [  1/10] Final Prec@1 73.2783%\u001b[0m\n",
      "[2023-10-23 14:09:53] \u001b[32mValid: [  1/10] Step 000/104 Loss 1.428 Prec@(1,5) (68.8%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:09:54] \u001b[32mValid: [  1/10] Step 020/104 Loss 1.128 Prec@(1,5) (72.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:09:54] \u001b[32mValid: [  1/10] Step 040/104 Loss 1.188 Prec@(1,5) (72.7%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:09:54] \u001b[32mValid: [  1/10] Step 060/104 Loss 1.199 Prec@(1,5) (72.6%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:09:54] \u001b[32mValid: [  1/10] Step 080/104 Loss 1.173 Prec@(1,5) (73.1%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:09:55] \u001b[32mValid: [  1/10] Step 100/104 Loss 1.169 Prec@(1,5) (72.8%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:09:55] \u001b[32mValid: [  1/10] Step 104/104 Loss 1.162 Prec@(1,5) (72.9%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:09:55] \u001b[32mValid: [  1/10] Final Prec@1 72.8900%\u001b[0m\n",
      "[2023-10-23 14:09:55] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:09:56] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.604 Prec@(1,5) (79.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:09:57] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.774 Prec@(1,5) (77.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:09:58] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.754 Prec@(1,5) (78.8%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:09:58] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.721 Prec@(1,5) (80.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:09:59] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.701 Prec@(1,5) (80.7%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:10:00] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.687 Prec@(1,5) (81.1%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:10:01] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.686 Prec@(1,5) (81.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:02] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.683 Prec@(1,5) (81.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:03] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.679 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:03] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.673 Prec@(1,5) (81.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:05] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.670 Prec@(1,5) (81.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:06] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.662 Prec@(1,5) (82.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:07] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.659 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:08] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.657 Prec@(1,5) (82.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:09] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.657 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:09] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.659 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:10] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.658 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:11] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.656 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:11] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.653 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:12] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.650 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:12] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.648 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:13] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.645 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:13] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.643 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:14] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.642 Prec@(1,5) (82.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:15] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.640 Prec@(1,5) (82.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:15] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.637 Prec@(1,5) (82.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:16] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.636 Prec@(1,5) (82.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:16] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.632 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:17] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.629 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:17] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.628 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:18] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.626 Prec@(1,5) (83.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:19] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.623 Prec@(1,5) (83.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:19] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.623 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:19] \u001b[32mTrain: [  2/10] Final Prec@1 83.2533%\u001b[0m\n",
      "[2023-10-23 14:10:20] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.566 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:10:20] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.454 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:10:20] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.516 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:20] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.521 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:20] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.502 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:20] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.501 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:21] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.498 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:21] \u001b[32mValid: [  2/10] Final Prec@1 87.3800%\u001b[0m\n",
      "[2023-10-23 14:10:21] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:10:22] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.483 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:10:22] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.620 Prec@(1,5) (83.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:23] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.586 Prec@(1,5) (84.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:23] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.601 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:24] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.600 Prec@(1,5) (83.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:25] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.584 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:25] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.583 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:26] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.580 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:26] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.575 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:27] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.571 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:27] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.573 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:28] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.572 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:29] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.572 Prec@(1,5) (84.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:29] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.566 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:30] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.564 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:30] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.563 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:31] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.562 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:32] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.560 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:32] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.558 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:33] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.558 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:33] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.559 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:34] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.561 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:34] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.561 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:35] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.560 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:36] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.557 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:36] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.557 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:37] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.555 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:37] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.554 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:38] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.552 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:39] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.552 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:39] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.550 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:40] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.550 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:40] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.550 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:40] \u001b[32mTrain: [  3/10] Final Prec@1 85.1800%\u001b[0m\n",
      "[2023-10-23 14:10:41] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.876 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:10:41] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.743 Prec@(1,5) (82.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:10:41] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.806 Prec@(1,5) (81.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:42] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.823 Prec@(1,5) (81.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:42] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.789 Prec@(1,5) (82.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:42] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.792 Prec@(1,5) (82.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:42] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.787 Prec@(1,5) (82.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:42] \u001b[32mValid: [  3/10] Final Prec@1 82.2900%\u001b[0m\n",
      "[2023-10-23 14:10:42] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:10:43] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.634 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:10:44] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.557 Prec@(1,5) (85.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:10:44] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.551 Prec@(1,5) (85.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:45] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.540 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:45] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.530 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:46] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.538 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:46] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.537 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:47] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.536 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:10:48] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.530 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:48] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.528 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:49] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.524 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:49] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.520 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:50] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.520 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:51] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.517 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:51] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.520 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:52] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.522 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:52] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.523 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:53] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.521 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:53] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.518 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:54] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.517 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:55] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.514 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:55] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.512 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:56] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.511 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:56] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.510 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:57] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.510 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:57] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.508 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:58] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.508 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:59] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.507 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:10:59] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.508 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:00] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.509 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:00] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.509 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:01] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.508 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:01] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.508 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:01] \u001b[32mTrain: [  4/10] Final Prec@1 86.3833%\u001b[0m\n",
      "[2023-10-23 14:11:02] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.560 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:11:02] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.433 Prec@(1,5) (88.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:11:02] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.475 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:03] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.499 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:03] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.481 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:03] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.481 Prec@(1,5) (87.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:03] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.477 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:03] \u001b[32mValid: [  4/10] Final Prec@1 87.7600%\u001b[0m\n",
      "[2023-10-23 14:11:03] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:11:04] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.369 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:11:05] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.496 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:05] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.474 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:06] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.477 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:06] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.480 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:07] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.474 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:07] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.476 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:08] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.470 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:09] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.472 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:09] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.471 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:10] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.467 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:10] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.464 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:11] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.464 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:11] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.465 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:12] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.464 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:13] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.465 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:13] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.469 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:14] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.468 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:14] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:15] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.466 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:15] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.465 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:16] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.466 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:17] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.466 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:17] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.466 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:18] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.466 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:18] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.467 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:19] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.467 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:19] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:20] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:21] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.467 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:21] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.467 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:22] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:22] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:22] \u001b[32mTrain: [  5/10] Final Prec@1 87.6167%\u001b[0m\n",
      "[2023-10-23 14:11:23] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.441 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:11:23] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.362 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:23] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.412 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:23] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.406 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:24] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.400 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:24] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.401 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:24] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:24] \u001b[32mValid: [  5/10] Final Prec@1 89.6200%\u001b[0m\n",
      "[2023-10-23 14:11:24] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:11:25] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.495 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:11:25] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.426 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:26] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:27] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.433 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:27] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.439 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:28] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.439 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:28] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.439 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:29] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.444 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:29] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.439 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:30] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.436 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:31] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.439 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:31] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.437 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:32] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.433 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:32] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.436 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:33] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.435 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:33] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.435 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:34] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.435 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:35] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.434 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:35] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.436 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:36] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.436 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:36] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.436 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:37] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.435 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:37] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.435 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:38] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.438 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:38] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.437 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:39] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.439 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:40] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.440 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:40] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.440 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:41] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.440 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:41] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.439 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:42] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.438 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:43] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.437 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:43] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.437 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:43] \u001b[32mTrain: [  6/10] Final Prec@1 88.3950%\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.431 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.366 Prec@(1,5) (89.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.415 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.413 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.399 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.401 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:44] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.398 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:45] \u001b[32mValid: [  6/10] Final Prec@1 89.8200%\u001b[0m\n",
      "[2023-10-23 14:11:45] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:11:46] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.470 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:11:46] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.423 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:47] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.423 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:47] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.408 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:48] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.407 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:48] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.420 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:49] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.423 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:50] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.423 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:50] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.425 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:51] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.426 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:51] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.424 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:52] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.426 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:11:52] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.426 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:53] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.423 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:54] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.424 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:54] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.425 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:55] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:55] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.425 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:56] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.424 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:56] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.423 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:57] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.422 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:58] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:58] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:59] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:11:59] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.420 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:00] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.420 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:01] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:01] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:02] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:02] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.422 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:03] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.420 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:03] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.420 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:03] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.421 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:04] \u001b[32mTrain: [  7/10] Final Prec@1 88.9017%\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.406 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.339 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.380 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.367 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.355 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.354 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:05] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.350 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:06] \u001b[32mValid: [  7/10] Final Prec@1 90.5800%\u001b[0m\n",
      "[2023-10-23 14:12:06] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:12:07] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.409 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:12:07] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.400 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:08] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.391 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:08] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.385 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:09] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.391 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:10] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.397 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:10] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.400 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:11] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.402 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:11] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.402 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:12] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.401 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:12] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.404 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:13] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.404 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:14] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.404 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:14] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.404 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:15] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.403 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:15] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.403 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:16] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.403 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:16] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.402 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:17] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.401 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:18] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.400 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:18] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.401 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:19] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.399 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:19] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:20] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.398 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:20] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:21] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:22] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:22] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.399 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:23] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.398 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:23] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.398 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:24] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:24] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:25] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:25] \u001b[32mTrain: [  8/10] Final Prec@1 89.5800%\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.417 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.339 Prec@(1,5) (91.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.390 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.385 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.370 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.370 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:26] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.367 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:27] \u001b[32mValid: [  8/10] Final Prec@1 90.9000%\u001b[0m\n",
      "[2023-10-23 14:12:27] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:12:28] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.460 Prec@(1,5) (88.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:12:28] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.393 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:29] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.392 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:29] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.390 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:30] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.387 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:12:30] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.390 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:31] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.391 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:32] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.386 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:32] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.387 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:33] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.390 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:33] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.387 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:34] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.389 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:34] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.388 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:35] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.389 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:36] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.389 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:36] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.388 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:37] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.388 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:37] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.388 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:38] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.387 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:38] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.387 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:39] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.388 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:40] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.388 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:40] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.387 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:41] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.387 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:41] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.387 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:42] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.387 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:42] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.386 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:43] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.387 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:44] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.386 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:44] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.385 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:45] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.384 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:45] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.383 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:45] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.383 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:46] \u001b[32mTrain: [  9/10] Final Prec@1 89.9567%\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.315 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.277 Prec@(1,5) (92.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.322 Prec@(1,5) (91.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.316 Prec@(1,5) (91.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.301 Prec@(1,5) (92.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.300 Prec@(1,5) (92.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.297 Prec@(1,5) (92.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mValid: [  9/10] Final Prec@1 92.1200%\u001b[0m\n",
      "[2023-10-23 14:12:47] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:12:48] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.297 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:12:49] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.365 Prec@(1,5) (90.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:50] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.374 Prec@(1,5) (90.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:50] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.366 Prec@(1,5) (91.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:51] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.366 Prec@(1,5) (90.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:51] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.365 Prec@(1,5) (90.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:52] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.369 Prec@(1,5) (90.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:52] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.369 Prec@(1,5) (90.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:12:53] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.366 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:54] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.365 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:54] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.364 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:55] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.364 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:55] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.366 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:56] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.367 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:56] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.365 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:57] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.368 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:58] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.369 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:58] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.370 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:59] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.370 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:12:59] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.371 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:00] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.370 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:00] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.371 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:01] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.370 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:02] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.372 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:02] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.371 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:03] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:03] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:04] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.374 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:04] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:05] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:06] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.372 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:06] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:06] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.373 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:06] \u001b[32mTrain: [ 10/10] Final Prec@1 90.4800%\u001b[0m\n",
      "[2023-10-23 14:13:07] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.327 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.290 Prec@(1,5) (92.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.332 Prec@(1,5) (91.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.325 Prec@(1,5) (91.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.313 Prec@(1,5) (91.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.312 Prec@(1,5) (91.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.309 Prec@(1,5) (91.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mValid: [ 10/10] Final Prec@1 91.9400%\u001b[0m\n",
      "Final best Prec@1 = 92.1200%\n",
      "[0.9078000211715698, 0.9136000185012817, 0.9212000221252441]\n",
      "./checkpoints/fashionMNIST/lambd=2/\n",
      "[2023-10-23 14:13:08] \u001b[32mFixed architecture: {'reduce_n2_p0': 'dilconv5x5', 'reduce_n2_p1': 'sepconv5x5', 'reduce_n3_p0': 'avgpool', 'reduce_n3_p1': 'avgpool', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'avgpool', 'reduce_n5_p1': 'maxpool', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'sepconv3x3', 'reduce_n5_p4': 'dilconv3x3', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [0, 1], 'reduce_n4_switch': [3, 2], 'reduce_n5_switch': [2, 1]}\u001b[0m\n",
      "[2023-10-23 14:13:08] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:13:09] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.401 Prec@(1,5) (6.2%, 33.3%)\u001b[0m\n",
      "[2023-10-23 14:13:10] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.667 Prec@(1,5) (25.4%, 72.1%)\u001b[0m\n",
      "[2023-10-23 14:13:11] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.249 Prec@(1,5) (35.6%, 82.8%)\u001b[0m\n",
      "[2023-10-23 14:13:11] \u001b[32mTrain: [  1/10] Step 060/624 Loss 2.016 Prec@(1,5) (42.0%, 87.6%)\u001b[0m\n",
      "[2023-10-23 14:13:12] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.850 Prec@(1,5) (46.7%, 90.2%)\u001b[0m\n",
      "[2023-10-23 14:13:12] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.733 Prec@(1,5) (50.3%, 91.8%)\u001b[0m\n",
      "[2023-10-23 14:13:13] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.632 Prec@(1,5) (53.4%, 92.9%)\u001b[0m\n",
      "[2023-10-23 14:13:13] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.559 Prec@(1,5) (55.7%, 93.8%)\u001b[0m\n",
      "[2023-10-23 14:13:14] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.502 Prec@(1,5) (57.4%, 94.4%)\u001b[0m\n",
      "[2023-10-23 14:13:14] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.454 Prec@(1,5) (58.8%, 94.9%)\u001b[0m\n",
      "[2023-10-23 14:13:15] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.405 Prec@(1,5) (60.2%, 95.3%)\u001b[0m\n",
      "[2023-10-23 14:13:15] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.371 Prec@(1,5) (61.4%, 95.6%)\u001b[0m\n",
      "[2023-10-23 14:13:16] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.335 Prec@(1,5) (62.4%, 95.9%)\u001b[0m\n",
      "[2023-10-23 14:13:16] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.305 Prec@(1,5) (63.3%, 96.2%)\u001b[0m\n",
      "[2023-10-23 14:13:17] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.278 Prec@(1,5) (64.2%, 96.5%)\u001b[0m\n",
      "[2023-10-23 14:13:17] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.251 Prec@(1,5) (64.9%, 96.7%)\u001b[0m\n",
      "[2023-10-23 14:13:18] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.230 Prec@(1,5) (65.5%, 96.8%)\u001b[0m\n",
      "[2023-10-23 14:13:18] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.207 Prec@(1,5) (66.2%, 97.0%)\u001b[0m\n",
      "[2023-10-23 14:13:19] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.185 Prec@(1,5) (66.8%, 97.1%)\u001b[0m\n",
      "[2023-10-23 14:13:19] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.166 Prec@(1,5) (67.4%, 97.2%)\u001b[0m\n",
      "[2023-10-23 14:13:20] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.149 Prec@(1,5) (67.8%, 97.3%)\u001b[0m\n",
      "[2023-10-23 14:13:20] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.127 Prec@(1,5) (68.5%, 97.4%)\u001b[0m\n",
      "[2023-10-23 14:13:21] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.112 Prec@(1,5) (69.0%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:13:21] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.099 Prec@(1,5) (69.3%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:13:22] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.085 Prec@(1,5) (69.8%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:13:22] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.070 Prec@(1,5) (70.3%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:13:23] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.058 Prec@(1,5) (70.7%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:13:23] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.045 Prec@(1,5) (71.0%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:13:24] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.033 Prec@(1,5) (71.4%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:13:24] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.022 Prec@(1,5) (71.7%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:13:25] \u001b[32mTrain: [  1/10] Step 600/624 Loss 1.011 Prec@(1,5) (72.0%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:13:25] \u001b[32mTrain: [  1/10] Step 620/624 Loss 1.001 Prec@(1,5) (72.3%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:13:25] \u001b[32mTrain: [  1/10] Step 624/624 Loss 1.000 Prec@(1,5) (72.3%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:13:26] \u001b[32mTrain: [  1/10] Final Prec@1 72.3333%\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.573 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.521 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.579 Prec@(1,5) (84.4%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.584 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.569 Prec@(1,5) (84.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.581 Prec@(1,5) (84.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.577 Prec@(1,5) (84.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mValid: [  1/10] Final Prec@1 84.5900%\u001b[0m\n",
      "[2023-10-23 14:13:27] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:13:29] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.856 Prec@(1,5) (77.1%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:13:29] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.687 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:13:30] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.683 Prec@(1,5) (82.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:30] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.671 Prec@(1,5) (82.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:31] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.666 Prec@(1,5) (82.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:31] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.661 Prec@(1,5) (82.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:32] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.660 Prec@(1,5) (82.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:32] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.659 Prec@(1,5) (82.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:33] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.650 Prec@(1,5) (82.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:33] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.643 Prec@(1,5) (82.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:34] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.642 Prec@(1,5) (82.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:35] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.643 Prec@(1,5) (82.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:35] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.636 Prec@(1,5) (82.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:36] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.635 Prec@(1,5) (82.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:36] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.632 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:37] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.630 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:37] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.630 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:38] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.629 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:38] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.630 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:39] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.630 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:39] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.630 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:40] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.629 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:40] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.627 Prec@(1,5) (83.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:41] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.623 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:42] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.623 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:42] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.622 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:43] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.619 Prec@(1,5) (83.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:43] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.616 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:44] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.615 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:44] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.614 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:45] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.614 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:45] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.613 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:45] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.613 Prec@(1,5) (83.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:46] \u001b[32mTrain: [  2/10] Final Prec@1 83.5617%\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.562 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.447 Prec@(1,5) (87.9%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.499 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.508 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.501 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.505 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.502 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mValid: [  2/10] Final Prec@1 87.5200%\u001b[0m\n",
      "[2023-10-23 14:13:47] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:13:48] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.580 Prec@(1,5) (82.3%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:13:49] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.600 Prec@(1,5) (84.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:50] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.587 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:50] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.563 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:51] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.572 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:51] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.574 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:52] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.575 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:52] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.576 Prec@(1,5) (84.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:13:53] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.571 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:53] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.568 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:54] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.565 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:54] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.561 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:55] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.563 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:56] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.565 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:56] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.563 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:57] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.563 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:57] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.560 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:58] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.559 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:58] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.556 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:59] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.554 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:13:59] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.551 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:00] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.551 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:00] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.551 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:01] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.551 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:02] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.550 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:02] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.550 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:03] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.548 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:03] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.547 Prec@(1,5) (85.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:04] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.547 Prec@(1,5) (85.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:04] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.546 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:05] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.546 Prec@(1,5) (85.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:05] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.545 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:06] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.546 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:06] \u001b[32mTrain: [  3/10] Final Prec@1 85.4533%\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.605 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.442 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.500 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.491 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.485 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.496 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:07] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.492 Prec@(1,5) (88.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:14:08] \u001b[32mValid: [  3/10] Final Prec@1 88.1800%\u001b[0m\n",
      "[2023-10-23 14:14:08] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:14:09] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.454 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:14:09] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.529 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:10] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.510 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:10] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.520 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:11] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.517 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:11] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.513 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:12] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.508 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:12] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.502 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:13] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.501 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:14] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.500 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:14] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.501 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:15] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.504 Prec@(1,5) (86.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:14:15] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.507 Prec@(1,5) (86.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:14:16] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.506 Prec@(1,5) (86.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:14:16] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.504 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:17] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.505 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:17] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.503 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:18] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.504 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:18] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:19] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:20] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.505 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:20] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:21] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.505 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:21] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.503 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:22] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.502 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:22] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.502 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:23] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.503 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:23] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.502 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:24] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.501 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:24] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.500 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:25] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.500 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:26] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.499 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:26] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.499 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:26] \u001b[32mTrain: [  4/10] Final Prec@1 86.6633%\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.705 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.610 Prec@(1,5) (86.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.670 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.699 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.685 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.691 Prec@(1,5) (85.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:27] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.685 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:28] \u001b[32mValid: [  4/10] Final Prec@1 85.8800%\u001b[0m\n",
      "[2023-10-23 14:14:28] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:14:29] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.446 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:29] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.451 Prec@(1,5) (88.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:30] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.463 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:30] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.464 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:31] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.456 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:31] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.454 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:32] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.461 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:33] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.461 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:33] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.464 Prec@(1,5) (87.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:34] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.459 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:34] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.464 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:35] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.463 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:35] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.464 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:36] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.463 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:36] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.465 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:37] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.466 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:37] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.465 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:38] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:39] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.470 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:39] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.470 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:40] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.469 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:40] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:41] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:41] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.467 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:42] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:42] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:43] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:43] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.467 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:44] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.466 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:45] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.465 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:45] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.466 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:46] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.465 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:46] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.466 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:46] \u001b[32mTrain: [  5/10] Final Prec@1 87.7700%\u001b[0m\n",
      "[2023-10-23 14:14:47] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.452 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:47] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.387 Prec@(1,5) (90.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:47] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.430 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:47] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.439 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:47] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.427 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:48] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.436 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:48] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.431 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:48] \u001b[32mValid: [  5/10] Final Prec@1 89.4900%\u001b[0m\n",
      "[2023-10-23 14:14:48] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:14:49] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.410 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:49] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.427 Prec@(1,5) (88.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:14:50] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.428 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:50] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.443 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:51] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.452 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:51] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.448 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:14:52] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.453 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:53] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.455 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:53] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.448 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:54] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.446 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:54] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.443 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:55] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.437 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:55] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.437 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:56] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.438 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:56] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.442 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:57] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.441 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:58] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.444 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:58] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.443 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:59] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.441 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:14:59] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.442 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:00] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.442 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:00] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:01] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:01] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:02] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:02] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:03] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:04] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:04] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:05] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:05] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:06] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:06] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:06] \u001b[32mTrain: [  6/10] Final Prec@1 88.4983%\u001b[0m\n",
      "[2023-10-23 14:15:07] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.342 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:07] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.357 Prec@(1,5) (90.9%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:07] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.419 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:07] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.421 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:08] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.411 Prec@(1,5) (90.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:08] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.416 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:08] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.411 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:08] \u001b[32mValid: [  6/10] Final Prec@1 90.3000%\u001b[0m\n",
      "[2023-10-23 14:15:08] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:15:09] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.418 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:09] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.418 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:10] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.418 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:11] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.413 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:11] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.411 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:12] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.410 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:12] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.410 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:13] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.410 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:13] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.410 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:14] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.410 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:14] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.408 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:15] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.407 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:16] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.407 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:16] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.407 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:17] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.408 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:17] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.408 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:18] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.408 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:18] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.409 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:19] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.408 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:19] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.409 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:20] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.408 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:20] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.408 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:21] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.410 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:21] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.411 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:22] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.413 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:23] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.412 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:23] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.412 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:24] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.413 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:24] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.413 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:25] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.411 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:25] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.411 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:26] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.410 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:26] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.410 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:26] \u001b[32mTrain: [  7/10] Final Prec@1 89.2317%\u001b[0m\n",
      "[2023-10-23 14:15:27] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.464 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:15:27] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.402 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:27] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.456 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:28] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.461 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:28] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.452 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:28] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.459 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:28] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.453 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:28] \u001b[32mValid: [  7/10] Final Prec@1 89.9100%\u001b[0m\n",
      "[2023-10-23 14:15:28] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:15:29] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.358 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:30] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.383 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:30] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.401 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:31] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.404 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:31] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.398 Prec@(1,5) (89.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:32] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.396 Prec@(1,5) (89.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:32] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.398 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:33] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.393 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:33] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.394 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:34] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.394 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:35] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.398 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:35] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.398 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:36] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.398 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:36] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:37] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.397 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:37] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.394 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:38] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.393 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:38] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.392 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:39] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.392 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:39] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.392 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:40] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.391 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:40] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.393 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:41] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.391 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:42] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:42] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:43] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:43] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.390 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:44] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.390 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:44] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.390 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:45] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.390 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:45] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:46] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.390 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:46] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.390 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:46] \u001b[32mTrain: [  8/10] Final Prec@1 89.8200%\u001b[0m\n",
      "[2023-10-23 14:15:47] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.444 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:47] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.439 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.492 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.491 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.484 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.492 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.486 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mValid: [  8/10] Final Prec@1 89.6300%\u001b[0m\n",
      "[2023-10-23 14:15:48] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:15:49] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.404 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:15:50] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.413 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:50] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.406 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:51] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:51] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.388 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:52] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.390 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:53] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.389 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:53] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.388 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:54] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.388 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:54] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.391 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:55] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.389 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:55] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.383 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:56] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.383 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:56] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.383 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:57] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.384 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:57] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.384 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:58] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.382 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:59] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.382 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:15:59] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.380 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:00] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.380 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:00] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.380 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:01] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.380 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:01] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.380 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:02] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.378 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:02] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.380 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:03] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.379 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:04] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.378 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:04] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.377 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:05] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.378 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:05] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.378 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:06] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.378 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:06] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.378 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:06] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.378 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:07] \u001b[32mTrain: [  9/10] Final Prec@1 90.1467%\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.436 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.373 Prec@(1,5) (92.0%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.416 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.418 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.412 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.416 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.410 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mValid: [  9/10] Final Prec@1 91.1800%\u001b[0m\n",
      "[2023-10-23 14:16:08] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:16:09] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.415 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:10] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.386 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:11] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.389 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:11] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.393 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:12] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.382 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:12] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.378 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:13] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.375 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:13] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.377 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:14] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.379 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:15] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.378 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:15] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.375 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:16] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.376 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:16] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.380 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:17] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.379 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:17] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.379 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:18] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:18] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:19] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.377 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:20] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.377 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:20] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:21] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.375 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:21] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.375 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:22] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.375 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:22] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.375 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:23] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.374 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:24] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.374 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:24] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.373 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:25] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.373 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:25] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.372 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:26] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.372 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:26] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.370 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:27] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.368 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:27] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.368 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:27] \u001b[32mTrain: [ 10/10] Final Prec@1 90.5117%\u001b[0m\n",
      "[2023-10-23 14:16:28] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.394 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:28] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.362 Prec@(1,5) (91.9%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.400 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.403 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.393 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.398 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.393 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mValid: [ 10/10] Final Prec@1 91.3300%\u001b[0m\n",
      "Final best Prec@1 = 91.3300%\n",
      "[0.9078000211715698, 0.9136000185012817, 0.9212000221252441, 0.9133000202178955]\n",
      "./checkpoints/fashionMNIST/lambd=2.5/\n",
      "[2023-10-23 14:16:29] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'avgpool', 'reduce_n3_p1': 'skipconnect', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'sepconv5x5', 'reduce_n4_p1': 'maxpool', 'reduce_n4_p2': 'dilconv5x5', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'maxpool', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'maxpool', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'sepconv3x3', 'reduce_n2_switch': [1, 0], 'reduce_n3_switch': [0, 2], 'reduce_n4_switch': [3, 0], 'reduce_n5_switch': [3, 2]}\u001b[0m\n",
      "[2023-10-23 14:16:29] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:16:30] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.289 Prec@(1,5) (11.5%, 55.2%)\u001b[0m\n",
      "[2023-10-23 14:16:31] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.647 Prec@(1,5) (26.1%, 74.9%)\u001b[0m\n",
      "[2023-10-23 14:16:31] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.174 Prec@(1,5) (39.2%, 85.2%)\u001b[0m\n",
      "[2023-10-23 14:16:32] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.934 Prec@(1,5) (46.0%, 89.2%)\u001b[0m\n",
      "[2023-10-23 14:16:32] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.772 Prec@(1,5) (50.6%, 91.4%)\u001b[0m\n",
      "[2023-10-23 14:16:33] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.649 Prec@(1,5) (54.3%, 92.9%)\u001b[0m\n",
      "[2023-10-23 14:16:33] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.562 Prec@(1,5) (57.1%, 93.8%)\u001b[0m\n",
      "[2023-10-23 14:16:34] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.496 Prec@(1,5) (59.1%, 94.5%)\u001b[0m\n",
      "[2023-10-23 14:16:34] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.437 Prec@(1,5) (60.6%, 95.1%)\u001b[0m\n",
      "[2023-10-23 14:16:35] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.390 Prec@(1,5) (61.9%, 95.5%)\u001b[0m\n",
      "[2023-10-23 14:16:35] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.351 Prec@(1,5) (63.0%, 95.8%)\u001b[0m\n",
      "[2023-10-23 14:16:36] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.308 Prec@(1,5) (64.3%, 96.1%)\u001b[0m\n",
      "[2023-10-23 14:16:36] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.273 Prec@(1,5) (65.2%, 96.4%)\u001b[0m\n",
      "[2023-10-23 14:16:37] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.244 Prec@(1,5) (65.9%, 96.6%)\u001b[0m\n",
      "[2023-10-23 14:16:37] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.219 Prec@(1,5) (66.7%, 96.8%)\u001b[0m\n",
      "[2023-10-23 14:16:38] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.199 Prec@(1,5) (67.1%, 97.0%)\u001b[0m\n",
      "[2023-10-23 14:16:38] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.180 Prec@(1,5) (67.8%, 97.1%)\u001b[0m\n",
      "[2023-10-23 14:16:39] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.159 Prec@(1,5) (68.4%, 97.3%)\u001b[0m\n",
      "[2023-10-23 14:16:39] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.141 Prec@(1,5) (68.9%, 97.4%)\u001b[0m\n",
      "[2023-10-23 14:16:40] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.123 Prec@(1,5) (69.4%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:16:40] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.103 Prec@(1,5) (69.9%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:16:41] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.085 Prec@(1,5) (70.4%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:16:41] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.068 Prec@(1,5) (70.9%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:16:42] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.053 Prec@(1,5) (71.3%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:16:42] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.041 Prec@(1,5) (71.7%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:16:43] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.027 Prec@(1,5) (72.1%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:16:43] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.013 Prec@(1,5) (72.4%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:16:44] \u001b[32mTrain: [  1/10] Step 540/624 Loss 0.999 Prec@(1,5) (72.8%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:16:44] \u001b[32mTrain: [  1/10] Step 560/624 Loss 0.988 Prec@(1,5) (73.2%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:16:45] \u001b[32mTrain: [  1/10] Step 580/624 Loss 0.980 Prec@(1,5) (73.4%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:16:45] \u001b[32mTrain: [  1/10] Step 600/624 Loss 0.970 Prec@(1,5) (73.7%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:16:46] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.961 Prec@(1,5) (73.9%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:16:46] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.959 Prec@(1,5) (74.0%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:16:46] \u001b[32mTrain: [  1/10] Final Prec@1 73.9667%\u001b[0m\n",
      "[2023-10-23 14:16:47] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.741 Prec@(1,5) (79.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:47] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.729 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:47] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.763 Prec@(1,5) (80.8%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:47] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.774 Prec@(1,5) (80.8%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:16:48] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.753 Prec@(1,5) (81.1%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:48] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.761 Prec@(1,5) (80.9%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:48] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.757 Prec@(1,5) (81.0%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:48] \u001b[32mValid: [  1/10] Final Prec@1 80.9500%\u001b[0m\n",
      "[2023-10-23 14:16:48] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:16:49] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.820 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:16:49] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.696 Prec@(1,5) (81.4%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:16:50] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.694 Prec@(1,5) (82.0%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:51] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.692 Prec@(1,5) (82.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:51] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.686 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:52] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.680 Prec@(1,5) (82.3%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:52] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.670 Prec@(1,5) (82.5%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:53] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.665 Prec@(1,5) (82.5%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:53] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.664 Prec@(1,5) (82.7%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:54] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.664 Prec@(1,5) (82.7%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:16:54] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.660 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:55] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.659 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:55] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.658 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:56] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.656 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:56] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.655 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:57] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.652 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:57] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.653 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:58] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.655 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:58] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.654 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:59] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.650 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:16:59] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.650 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:00] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.645 Prec@(1,5) (82.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:01] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.643 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:01] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.642 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:02] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.640 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:02] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.639 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:03] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.637 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:03] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.636 Prec@(1,5) (83.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:04] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.633 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:04] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.632 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:05] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.630 Prec@(1,5) (83.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:05] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.627 Prec@(1,5) (83.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:05] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.626 Prec@(1,5) (83.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:06] \u001b[32mTrain: [  2/10] Final Prec@1 83.4350%\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.628 Prec@(1,5) (81.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.507 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.574 Prec@(1,5) (85.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.577 Prec@(1,5) (85.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.569 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.577 Prec@(1,5) (85.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.571 Prec@(1,5) (85.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mValid: [  2/10] Final Prec@1 85.9200%\u001b[0m\n",
      "[2023-10-23 14:17:07] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:17:08] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.633 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:09] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.555 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:09] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.564 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:10] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.567 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:10] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.557 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:11] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.553 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:12] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.558 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:12] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.563 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:13] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.564 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:13] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.566 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:14] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.563 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:14] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.562 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:15] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.565 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:15] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.568 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:16] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.570 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:17] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.569 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:17] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.568 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:18] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.566 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:18] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.566 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:19] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.566 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:19] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:20] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.563 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:20] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.561 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:21] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.563 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:21] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.562 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:22] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.562 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:22] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.560 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:23] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.560 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:24] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.558 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:24] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.558 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:25] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.558 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:25] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.557 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:25] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.557 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:25] \u001b[32mTrain: [  3/10] Final Prec@1 85.2200%\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.512 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.547 Prec@(1,5) (86.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.597 Prec@(1,5) (85.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.597 Prec@(1,5) (85.0%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.584 Prec@(1,5) (85.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.593 Prec@(1,5) (85.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.590 Prec@(1,5) (85.2%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mValid: [  3/10] Final Prec@1 85.2300%\u001b[0m\n",
      "[2023-10-23 14:17:27] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:17:28] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.589 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:29] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.538 Prec@(1,5) (85.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:30] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.564 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:30] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.562 Prec@(1,5) (85.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:31] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.552 Prec@(1,5) (85.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:31] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.544 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:32] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.543 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:32] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.541 Prec@(1,5) (85.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:33] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.537 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:33] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.534 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:34] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.530 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:34] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.529 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:35] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.523 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:36] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.524 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:36] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.521 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:37] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.520 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:37] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.522 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:38] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.520 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:38] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.521 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:39] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.519 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:39] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.516 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:40] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.514 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:40] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.516 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:41] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.516 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:42] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.516 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:42] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.517 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:43] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.516 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:43] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.518 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:44] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.518 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:44] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.518 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:45] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.519 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:45] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.521 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:45] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.521 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:46] \u001b[32mTrain: [  4/10] Final Prec@1 86.3233%\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.483 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.389 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.413 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.421 Prec@(1,5) (88.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.409 Prec@(1,5) (88.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.409 Prec@(1,5) (88.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.407 Prec@(1,5) (88.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mValid: [  4/10] Final Prec@1 88.7100%\u001b[0m\n",
      "[2023-10-23 14:17:47] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:17:48] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.416 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:17:49] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.514 Prec@(1,5) (86.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:17:50] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.505 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:50] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.495 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:51] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.501 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:51] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.502 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:52] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.498 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:52] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.502 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:53] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.497 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:53] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.498 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:54] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.500 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:54] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.500 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:55] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.498 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:55] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.498 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:56] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.498 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:56] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.500 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:57] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.498 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:58] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.497 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:58] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.497 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:59] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.495 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:17:59] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.494 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:00] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.494 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:00] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.493 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:01] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.493 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:01] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.493 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:02] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.492 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:02] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.493 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:03] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.492 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:03] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.492 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:04] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.492 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:04] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.491 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:05] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.492 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:05] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.492 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:05] \u001b[32mTrain: [  5/10] Final Prec@1 87.1100%\u001b[0m\n",
      "[2023-10-23 14:18:06] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.428 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:06] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.364 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:06] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.405 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:07] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.417 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:07] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.404 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:07] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.410 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:07] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.409 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:07] \u001b[32mValid: [  5/10] Final Prec@1 89.2000%\u001b[0m\n",
      "[2023-10-23 14:18:07] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:18:08] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.617 Prec@(1,5) (84.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:18:09] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.460 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:09] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.472 Prec@(1,5) (87.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:10] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.488 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:10] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.485 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:11] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.490 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:11] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.492 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:12] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.491 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:12] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.488 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:13] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.490 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:13] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.485 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:14] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.482 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:14] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.479 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:15] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.480 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:16] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.480 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:16] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.477 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:17] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.478 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:17] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.479 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:18] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.475 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:18] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.476 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:19] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.476 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:19] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.474 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:20] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.475 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:20] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.474 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:21] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.474 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:21] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.473 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:22] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.470 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:22] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.470 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:23] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.471 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:23] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.471 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:24] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.470 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:25] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.470 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:25] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.470 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:25] \u001b[32mTrain: [  6/10] Final Prec@1 87.8250%\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.424 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.381 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.411 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.416 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.400 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.404 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:26] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.402 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:27] \u001b[32mValid: [  6/10] Final Prec@1 89.9400%\u001b[0m\n",
      "[2023-10-23 14:18:27] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:18:28] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.408 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:28] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.417 Prec@(1,5) (89.1%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:29] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.434 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:29] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.449 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:30] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.448 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:30] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.450 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:31] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.449 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:31] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.447 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:32] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.449 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:33] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.449 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:33] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.450 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:34] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:34] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.448 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:35] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.446 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:35] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.446 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:36] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.444 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:36] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.443 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:37] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.443 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:37] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.445 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:38] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:38] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:39] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:39] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.444 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:40] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.444 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:41] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.445 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:41] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:42] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.445 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:42] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:43] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.447 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:43] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:44] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:44] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.445 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:44] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.445 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:45] \u001b[32mTrain: [  7/10] Final Prec@1 88.3483%\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.397 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.353 Prec@(1,5) (90.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.377 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.376 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.364 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.372 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.369 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mValid: [  7/10] Final Prec@1 90.2200%\u001b[0m\n",
      "[2023-10-23 14:18:46] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:18:47] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.450 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:18:48] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.432 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:18:49] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.434 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:49] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.451 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:50] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.439 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:50] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.431 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:51] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.436 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:51] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.433 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:52] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.436 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:52] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.432 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:53] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.433 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:53] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.436 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:54] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.435 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:54] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.437 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:55] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.436 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:55] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.434 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:56] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.433 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:56] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.433 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:57] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.430 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:58] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.431 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:58] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.432 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:59] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.434 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:18:59] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.434 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:00] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.433 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:00] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.432 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:01] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.431 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:01] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.431 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:02] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.430 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:02] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.430 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:03] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.430 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:03] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.429 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:04] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.429 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:04] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.429 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:04] \u001b[32mTrain: [  8/10] Final Prec@1 89.0250%\u001b[0m\n",
      "[2023-10-23 14:19:05] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.478 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:05] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.350 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:05] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.378 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:06] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.377 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:06] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.362 Prec@(1,5) (90.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:06] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.365 Prec@(1,5) (90.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:06] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.364 Prec@(1,5) (90.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:06] \u001b[32mValid: [  8/10] Final Prec@1 90.6800%\u001b[0m\n",
      "[2023-10-23 14:19:06] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:19:07] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.474 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:08] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.427 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:08] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.429 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:09] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.427 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:09] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.417 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:10] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.415 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:10] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.408 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:11] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.411 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:11] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.409 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:12] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.409 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:12] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.407 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:13] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.409 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:13] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.411 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:14] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.414 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:14] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:15] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:15] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.416 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:16] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:16] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:17] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:17] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.416 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:18] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.413 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:18] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.414 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:19] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:20] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:20] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.414 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:21] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:21] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.416 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:22] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.416 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:22] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.416 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:23] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:23] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.416 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:23] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:23] \u001b[32mTrain: [  9/10] Final Prec@1 89.3233%\u001b[0m\n",
      "[2023-10-23 14:19:24] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.427 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.338 Prec@(1,5) (91.1%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.368 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.366 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.351 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.356 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.355 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mValid: [  9/10] Final Prec@1 90.7500%\u001b[0m\n",
      "[2023-10-23 14:19:25] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:19:26] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.515 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:27] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.399 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:27] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.385 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:28] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.401 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:28] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.410 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:29] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.419 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:29] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.423 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:30] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.425 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:30] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.423 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:31] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.427 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:32] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.427 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:32] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.426 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:33] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.423 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:33] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.421 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:34] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.421 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:34] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.422 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:35] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.421 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:35] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.420 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:36] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.418 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:36] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.418 Prec@(1,5) (89.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:37] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.416 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:37] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.415 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:38] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.415 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:38] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.413 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:39] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.414 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:39] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.414 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:40] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.415 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:40] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.416 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:41] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.417 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:42] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.417 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:42] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.418 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:43] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.418 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:43] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.418 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:19:43] \u001b[32mTrain: [ 10/10] Final Prec@1 89.2683%\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.437 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.338 Prec@(1,5) (91.0%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.369 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.365 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.352 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.357 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:44] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.355 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:19:45] \u001b[32mValid: [ 10/10] Final Prec@1 90.7700%\u001b[0m\n",
      "Final best Prec@1 = 90.7700%\n",
      "[0.9078000211715698, 0.9136000185012817, 0.9212000221252441, 0.9133000202178955, 0.9077000192642212]\n",
      "./checkpoints/fashionMNIST/lambd=3/\n",
      "[2023-10-23 14:19:45] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'avgpool', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'dilconv3x3', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv3x3', 'reduce_n5_p1': 'skipconnect', 'reduce_n5_p2': 'avgpool', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'avgpool', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [2, 0], 'reduce_n4_switch': [2, 3], 'reduce_n5_switch': [4, 2]}\u001b[0m\n",
      "[2023-10-23 14:19:45] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:19:46] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.320 Prec@(1,5) (12.5%, 44.8%)\u001b[0m\n",
      "[2023-10-23 14:19:46] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.566 Prec@(1,5) (29.4%, 78.8%)\u001b[0m\n",
      "[2023-10-23 14:19:47] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.096 Prec@(1,5) (41.1%, 87.9%)\u001b[0m\n",
      "[2023-10-23 14:19:47] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.878 Prec@(1,5) (47.6%, 91.1%)\u001b[0m\n",
      "[2023-10-23 14:19:48] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.737 Prec@(1,5) (51.7%, 92.8%)\u001b[0m\n",
      "[2023-10-23 14:19:48] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.617 Prec@(1,5) (55.2%, 94.0%)\u001b[0m\n",
      "[2023-10-23 14:19:49] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.544 Prec@(1,5) (57.5%, 94.8%)\u001b[0m\n",
      "[2023-10-23 14:19:49] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.483 Prec@(1,5) (59.1%, 95.4%)\u001b[0m\n",
      "[2023-10-23 14:19:50] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.429 Prec@(1,5) (60.7%, 95.8%)\u001b[0m\n",
      "[2023-10-23 14:19:50] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.388 Prec@(1,5) (61.8%, 96.2%)\u001b[0m\n",
      "[2023-10-23 14:19:51] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.356 Prec@(1,5) (62.7%, 96.5%)\u001b[0m\n",
      "[2023-10-23 14:19:51] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.324 Prec@(1,5) (63.7%, 96.7%)\u001b[0m\n",
      "[2023-10-23 14:19:52] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.292 Prec@(1,5) (64.5%, 96.9%)\u001b[0m\n",
      "[2023-10-23 14:19:52] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.269 Prec@(1,5) (65.3%, 97.1%)\u001b[0m\n",
      "[2023-10-23 14:19:53] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.240 Prec@(1,5) (66.1%, 97.2%)\u001b[0m\n",
      "[2023-10-23 14:19:53] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.215 Prec@(1,5) (66.7%, 97.4%)\u001b[0m\n",
      "[2023-10-23 14:19:54] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.193 Prec@(1,5) (67.3%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:19:54] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.171 Prec@(1,5) (67.9%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:19:55] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.152 Prec@(1,5) (68.5%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:19:55] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.139 Prec@(1,5) (68.9%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:19:56] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.123 Prec@(1,5) (69.3%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:19:56] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.107 Prec@(1,5) (69.7%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:19:57] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.093 Prec@(1,5) (70.1%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:19:57] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.080 Prec@(1,5) (70.5%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:19:58] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.067 Prec@(1,5) (70.8%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:19:58] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.055 Prec@(1,5) (71.2%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:19:59] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.046 Prec@(1,5) (71.5%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:19:59] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.036 Prec@(1,5) (71.7%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:20:00] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.025 Prec@(1,5) (72.1%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:20:00] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.015 Prec@(1,5) (72.3%, 98.4%)\u001b[0m\n",
      "[2023-10-23 14:20:01] \u001b[32mTrain: [  1/10] Step 600/624 Loss 1.004 Prec@(1,5) (72.6%, 98.4%)\u001b[0m\n",
      "[2023-10-23 14:20:01] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.994 Prec@(1,5) (72.9%, 98.5%)\u001b[0m\n",
      "[2023-10-23 14:20:02] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.993 Prec@(1,5) (73.0%, 98.5%)\u001b[0m\n",
      "[2023-10-23 14:20:02] \u001b[32mTrain: [  1/10] Final Prec@1 72.9817%\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 000/104 Loss 1.133 Prec@(1,5) (72.9%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.847 Prec@(1,5) (76.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.891 Prec@(1,5) (76.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.897 Prec@(1,5) (76.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.886 Prec@(1,5) (76.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.890 Prec@(1,5) (76.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.885 Prec@(1,5) (77.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mValid: [  1/10] Final Prec@1 76.9800%\u001b[0m\n",
      "[2023-10-23 14:20:03] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:20:05] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.753 Prec@(1,5) (79.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:20:05] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.732 Prec@(1,5) (79.9%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:20:06] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.731 Prec@(1,5) (79.9%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:20:06] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.728 Prec@(1,5) (79.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:07] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.723 Prec@(1,5) (80.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:07] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.722 Prec@(1,5) (80.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:08] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.706 Prec@(1,5) (80.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:08] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.707 Prec@(1,5) (80.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:09] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.705 Prec@(1,5) (80.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:09] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.701 Prec@(1,5) (80.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:10] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.695 Prec@(1,5) (81.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:11] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.689 Prec@(1,5) (81.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:11] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.682 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:12] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.682 Prec@(1,5) (81.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:12] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.680 Prec@(1,5) (81.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:13] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.678 Prec@(1,5) (81.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:13] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.675 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:14] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.672 Prec@(1,5) (81.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:14] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.670 Prec@(1,5) (81.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:15] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.668 Prec@(1,5) (82.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:16] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.666 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:16] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.666 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:17] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.662 Prec@(1,5) (82.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:17] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.660 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:18] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.660 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:18] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.659 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:19] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.656 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:19] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.653 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:20] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.651 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:20] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.649 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:21] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.647 Prec@(1,5) (82.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:22] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.644 Prec@(1,5) (82.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:22] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.643 Prec@(1,5) (82.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:20:22] \u001b[32mTrain: [  2/10] Final Prec@1 82.8133%\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.662 Prec@(1,5) (83.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.514 Prec@(1,5) (86.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.606 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.616 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.594 Prec@(1,5) (85.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.602 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:23] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.596 Prec@(1,5) (85.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:24] \u001b[32mValid: [  2/10] Final Prec@1 85.6200%\u001b[0m\n",
      "[2023-10-23 14:20:24] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:20:25] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.633 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:20:25] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.566 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:26] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.591 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:26] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.580 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:27] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.575 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:27] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.583 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:28] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.584 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:28] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.587 Prec@(1,5) (84.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:29] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.588 Prec@(1,5) (84.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:30] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.584 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:30] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.582 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:31] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.581 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:31] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.585 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:32] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.584 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:32] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.581 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:33] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.580 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:33] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.581 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:34] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.582 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:34] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.581 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:35] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.580 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:36] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.576 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:36] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.574 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:37] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.571 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:37] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.572 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:38] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.573 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:38] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.572 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:39] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.573 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:39] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.572 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:40] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.572 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:40] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.570 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:41] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.570 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:42] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.571 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:42] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.570 Prec@(1,5) (84.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:42] \u001b[32mTrain: [  3/10] Final Prec@1 84.8200%\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.666 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.561 Prec@(1,5) (86.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.595 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.621 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.598 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.609 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:43] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.605 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:44] \u001b[32mValid: [  3/10] Final Prec@1 86.4200%\u001b[0m\n",
      "[2023-10-23 14:20:44] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:20:45] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.565 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:20:45] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.522 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:46] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.518 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:46] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.549 Prec@(1,5) (85.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:47] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.554 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:47] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.549 Prec@(1,5) (86.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:48] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.539 Prec@(1,5) (86.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:48] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.542 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:49] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.541 Prec@(1,5) (86.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:50] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.539 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:50] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.539 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:51] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.541 Prec@(1,5) (85.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:51] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.544 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:52] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.543 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:52] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.544 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:53] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.545 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:53] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.545 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:54] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.543 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:54] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.542 Prec@(1,5) (85.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:55] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.540 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:55] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.537 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:56] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.536 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:57] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.535 Prec@(1,5) (86.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:57] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.534 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:20:58] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.534 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:58] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.531 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:59] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.529 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:20:59] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.528 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:00] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.528 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:00] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.528 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:01] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.525 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:01] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.525 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:01] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.525 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:02] \u001b[32mTrain: [  4/10] Final Prec@1 86.2183%\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.556 Prec@(1,5) (84.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.472 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.538 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.542 Prec@(1,5) (87.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.530 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.542 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:03] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.537 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:04] \u001b[32mValid: [  4/10] Final Prec@1 87.9700%\u001b[0m\n",
      "[2023-10-23 14:21:04] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:21:05] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.466 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:21:05] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.493 Prec@(1,5) (86.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:06] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.495 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:06] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.488 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:07] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.497 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:07] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.506 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:08] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.499 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:08] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.499 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:09] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.504 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:09] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.506 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:10] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.508 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:10] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.509 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:11] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.506 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:12] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.502 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:12] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.501 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:13] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.501 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:13] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.498 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:14] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.495 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:14] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.493 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:15] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.494 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:15] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.491 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:16] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.494 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:16] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.494 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:17] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.494 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:18] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.495 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:18] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.495 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:19] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.495 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:19] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.496 Prec@(1,5) (86.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:20] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.495 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:20] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.494 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:21] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.493 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:21] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.494 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:21] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.494 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:22] \u001b[32mTrain: [  5/10] Final Prec@1 86.7550%\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.690 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.517 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.594 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.599 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.571 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.581 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.578 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mValid: [  5/10] Final Prec@1 87.6000%\u001b[0m\n",
      "[2023-10-23 14:21:23] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:21:24] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.323 Prec@(1,5) (92.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:21:25] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.505 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:26] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.508 Prec@(1,5) (86.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:21:26] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.507 Prec@(1,5) (86.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:21:27] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.495 Prec@(1,5) (86.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:21:27] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.489 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:28] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.494 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:28] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.490 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:29] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.486 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:29] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.489 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:30] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.485 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:30] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.484 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:31] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.483 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:31] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.480 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:32] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.476 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:32] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.477 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:33] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.472 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:34] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.473 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:34] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.473 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:35] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.472 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:35] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.471 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:36] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.469 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:36] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.471 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:37] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.471 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:37] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.471 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:38] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.470 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:38] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.469 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:39] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:39] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.467 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:40] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.466 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:41] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.466 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:41] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:41] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:41] \u001b[32mTrain: [  6/10] Final Prec@1 87.5917%\u001b[0m\n",
      "[2023-10-23 14:21:42] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.530 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:21:42] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.507 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.572 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.584 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.551 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.559 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.559 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mValid: [  6/10] Final Prec@1 88.8000%\u001b[0m\n",
      "[2023-10-23 14:21:43] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:21:44] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.397 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:21:45] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.452 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:45] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.450 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:46] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.450 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:46] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.455 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:47] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.454 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:47] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.455 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:48] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.458 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:48] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.456 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:49] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.455 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:50] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.452 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:50] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.450 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:51] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.451 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:51] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.448 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:52] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.447 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:52] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.446 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:53] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.444 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:53] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.443 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:54] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.442 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:21:54] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.444 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:55] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.444 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:55] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.443 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:56] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.442 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:56] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.441 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:57] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.442 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:58] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.441 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:58] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.440 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:59] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.441 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:21:59] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.442 Prec@(1,5) (88.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:00] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.441 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:00] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.440 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:01] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.439 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:01] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.439 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:01] \u001b[32mTrain: [  7/10] Final Prec@1 88.4533%\u001b[0m\n",
      "[2023-10-23 14:22:02] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.457 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:22:02] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.421 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:02] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.475 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:02] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.480 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:03] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.453 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:03] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.460 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:03] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.456 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:03] \u001b[32mValid: [  7/10] Final Prec@1 90.1900%\u001b[0m\n",
      "[2023-10-23 14:22:03] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:22:04] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.306 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:22:04] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.425 Prec@(1,5) (89.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:22:05] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.446 Prec@(1,5) (88.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:22:06] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.448 Prec@(1,5) (88.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:22:06] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.435 Prec@(1,5) (88.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:07] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.433 Prec@(1,5) (88.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:07] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.435 Prec@(1,5) (88.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:08] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.437 Prec@(1,5) (88.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:08] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.436 Prec@(1,5) (88.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:09] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.432 Prec@(1,5) (88.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:09] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.432 Prec@(1,5) (88.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:10] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.432 Prec@(1,5) (88.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:10] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.432 Prec@(1,5) (88.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:11] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.429 Prec@(1,5) (88.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:12] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.428 Prec@(1,5) (88.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:12] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.426 Prec@(1,5) (88.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:13] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.426 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:13] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.426 Prec@(1,5) (88.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:14] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.424 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:14] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.422 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:15] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.423 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:15] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.419 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:16] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.418 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:17] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.418 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:17] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.419 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:18] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.419 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:18] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.419 Prec@(1,5) (89.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:19] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.420 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:19] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.422 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:20] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.421 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:20] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.420 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:21] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.419 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:21] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.419 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:21] \u001b[32mTrain: [  8/10] Final Prec@1 89.0450%\u001b[0m\n",
      "[2023-10-23 14:22:22] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.481 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:22:22] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.380 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.427 Prec@(1,5) (91.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.429 Prec@(1,5) (91.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.404 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.407 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.403 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mValid: [  8/10] Final Prec@1 91.3000%\u001b[0m\n",
      "[2023-10-23 14:22:23] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:22:24] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.447 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:22:25] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.449 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:25] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.440 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:26] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.425 Prec@(1,5) (89.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:26] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.423 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:27] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.412 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:27] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.407 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:28] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:29] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.411 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:29] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.413 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:30] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.412 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:30] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.413 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:31] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.410 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:31] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.410 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:32] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.409 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:33] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.409 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:33] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.411 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:34] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.410 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:34] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:35] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.409 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:35] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:36] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:36] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:37] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:37] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:38] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:39] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:39] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:40] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.406 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:40] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.405 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:41] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.404 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:41] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.404 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:41] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.404 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:42] \u001b[32mTrain: [  9/10] Final Prec@1 89.6417%\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.481 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.369 Prec@(1,5) (92.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.420 Prec@(1,5) (91.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.422 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.397 Prec@(1,5) (91.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.400 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.396 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mValid: [  9/10] Final Prec@1 91.4900%\u001b[0m\n",
      "[2023-10-23 14:22:43] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:22:44] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.397 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:22:45] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.397 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:46] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.391 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:46] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.398 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:47] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.398 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:47] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.404 Prec@(1,5) (89.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:22:48] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.399 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:48] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.398 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:49] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.394 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:49] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.391 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:50] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.392 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:50] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.395 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:51] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.397 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:51] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.397 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:52] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.397 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:52] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.396 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:53] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.395 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:54] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.397 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:54] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.394 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:55] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.397 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:55] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.399 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:56] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.399 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:56] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.400 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:57] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.400 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:57] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.401 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:58] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.399 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:58] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.398 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:59] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.397 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:22:59] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.397 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:00] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.399 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:01] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.399 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:01] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.399 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:01] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.399 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:01] \u001b[32mTrain: [ 10/10] Final Prec@1 89.7567%\u001b[0m\n",
      "[2023-10-23 14:23:02] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.539 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:02] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.384 Prec@(1,5) (91.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.434 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.436 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.413 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.417 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.413 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mValid: [ 10/10] Final Prec@1 91.5000%\u001b[0m\n",
      "Final best Prec@1 = 91.5000%\n",
      "[0.9078000211715698, 0.9136000185012817, 0.9212000221252441, 0.9133000202178955, 0.9077000192642212, 0.915000019454956]\n",
      "./checkpoints/fashionMNIST/lambd=3.5/\n",
      "[2023-10-23 14:23:03] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'maxpool', 'reduce_n3_p0': 'dilconv3x3', 'reduce_n3_p1': 'maxpool', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'dilconv5x5', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'sepconv3x3', 'reduce_n5_p2': 'dilconv5x5', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'avgpool', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [2, 1], 'reduce_n4_switch': [3, 0], 'reduce_n5_switch': [3, 4]}\u001b[0m\n",
      "[2023-10-23 14:23:03] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:23:04] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.336 Prec@(1,5) (9.4%, 46.9%)\u001b[0m\n",
      "[2023-10-23 14:23:05] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.608 Prec@(1,5) (29.7%, 74.7%)\u001b[0m\n",
      "[2023-10-23 14:23:05] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.102 Prec@(1,5) (42.8%, 85.4%)\u001b[0m\n",
      "[2023-10-23 14:23:06] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.833 Prec@(1,5) (49.9%, 89.7%)\u001b[0m\n",
      "[2023-10-23 14:23:06] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.645 Prec@(1,5) (55.0%, 92.0%)\u001b[0m\n",
      "[2023-10-23 14:23:07] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.527 Prec@(1,5) (58.3%, 93.4%)\u001b[0m\n",
      "[2023-10-23 14:23:07] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.446 Prec@(1,5) (60.5%, 94.3%)\u001b[0m\n",
      "[2023-10-23 14:23:08] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.378 Prec@(1,5) (62.1%, 95.0%)\u001b[0m\n",
      "[2023-10-23 14:23:08] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.316 Prec@(1,5) (63.9%, 95.5%)\u001b[0m\n",
      "[2023-10-23 14:23:08] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.273 Prec@(1,5) (65.0%, 96.0%)\u001b[0m\n",
      "[2023-10-23 14:23:09] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.234 Prec@(1,5) (66.1%, 96.3%)\u001b[0m\n",
      "[2023-10-23 14:23:09] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.204 Prec@(1,5) (67.0%, 96.6%)\u001b[0m\n",
      "[2023-10-23 14:23:10] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.183 Prec@(1,5) (67.7%, 96.8%)\u001b[0m\n",
      "[2023-10-23 14:23:10] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.157 Prec@(1,5) (68.4%, 97.0%)\u001b[0m\n",
      "[2023-10-23 14:23:11] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.129 Prec@(1,5) (69.2%, 97.2%)\u001b[0m\n",
      "[2023-10-23 14:23:11] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.104 Prec@(1,5) (69.9%, 97.3%)\u001b[0m\n",
      "[2023-10-23 14:23:12] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.082 Prec@(1,5) (70.5%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:23:12] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.059 Prec@(1,5) (71.1%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:23:13] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.041 Prec@(1,5) (71.7%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:23:13] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.023 Prec@(1,5) (72.1%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:23:14] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.004 Prec@(1,5) (72.7%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:23:14] \u001b[32mTrain: [  1/10] Step 420/624 Loss 0.988 Prec@(1,5) (73.2%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:23:15] \u001b[32mTrain: [  1/10] Step 440/624 Loss 0.975 Prec@(1,5) (73.5%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:23:15] \u001b[32mTrain: [  1/10] Step 460/624 Loss 0.960 Prec@(1,5) (73.9%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:23:16] \u001b[32mTrain: [  1/10] Step 480/624 Loss 0.946 Prec@(1,5) (74.3%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:23:16] \u001b[32mTrain: [  1/10] Step 500/624 Loss 0.934 Prec@(1,5) (74.7%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:23:16] \u001b[32mTrain: [  1/10] Step 520/624 Loss 0.921 Prec@(1,5) (75.1%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:23:17] \u001b[32mTrain: [  1/10] Step 540/624 Loss 0.909 Prec@(1,5) (75.4%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:23:17] \u001b[32mTrain: [  1/10] Step 560/624 Loss 0.899 Prec@(1,5) (75.7%, 98.4%)\u001b[0m\n",
      "[2023-10-23 14:23:18] \u001b[32mTrain: [  1/10] Step 580/624 Loss 0.888 Prec@(1,5) (76.0%, 98.4%)\u001b[0m\n",
      "[2023-10-23 14:23:18] \u001b[32mTrain: [  1/10] Step 600/624 Loss 0.878 Prec@(1,5) (76.3%, 98.5%)\u001b[0m\n",
      "[2023-10-23 14:23:19] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.868 Prec@(1,5) (76.5%, 98.5%)\u001b[0m\n",
      "[2023-10-23 14:23:19] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.866 Prec@(1,5) (76.6%, 98.5%)\u001b[0m\n",
      "[2023-10-23 14:23:19] \u001b[32mTrain: [  1/10] Final Prec@1 76.5950%\u001b[0m\n",
      "[2023-10-23 14:23:20] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.720 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:20] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.588 Prec@(1,5) (85.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:20] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.668 Prec@(1,5) (84.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:21] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.670 Prec@(1,5) (84.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:21] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.651 Prec@(1,5) (84.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:21] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.648 Prec@(1,5) (84.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:21] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.643 Prec@(1,5) (84.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:21] \u001b[32mValid: [  1/10] Final Prec@1 84.6600%\u001b[0m\n",
      "[2023-10-23 14:23:21] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:23:22] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.650 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:23:23] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.671 Prec@(1,5) (82.5%, 99.5%)\u001b[0m\n",
      "[2023-10-23 14:23:23] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.637 Prec@(1,5) (83.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:24] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.611 Prec@(1,5) (83.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:24] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.618 Prec@(1,5) (83.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:25] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.615 Prec@(1,5) (83.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:25] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.611 Prec@(1,5) (83.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:26] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.611 Prec@(1,5) (83.8%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:26] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.609 Prec@(1,5) (83.9%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:27] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.604 Prec@(1,5) (84.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:27] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.600 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:28] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.599 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:28] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.596 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:29] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.596 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:29] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.599 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:30] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.597 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:30] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.599 Prec@(1,5) (84.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:31] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.596 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:31] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.594 Prec@(1,5) (84.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:32] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.590 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:32] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.588 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:33] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.586 Prec@(1,5) (84.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:33] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.585 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:34] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.583 Prec@(1,5) (84.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:34] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.581 Prec@(1,5) (84.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:35] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.581 Prec@(1,5) (84.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:35] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.580 Prec@(1,5) (84.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:36] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.578 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:36] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.577 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:37] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.577 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:37] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.575 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:38] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.575 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:38] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.575 Prec@(1,5) (84.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:38] \u001b[32mTrain: [  2/10] Final Prec@1 84.5200%\u001b[0m\n",
      "[2023-10-23 14:23:39] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.928 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:39] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.739 Prec@(1,5) (83.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:39] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.797 Prec@(1,5) (83.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:40] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.803 Prec@(1,5) (83.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:23:40] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.788 Prec@(1,5) (83.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:40] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.790 Prec@(1,5) (83.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:40] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.785 Prec@(1,5) (83.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:40] \u001b[32mValid: [  2/10] Final Prec@1 83.4000%\u001b[0m\n",
      "[2023-10-23 14:23:40] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:23:41] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.663 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:42] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.555 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:23:42] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.556 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:43] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.537 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:43] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.529 Prec@(1,5) (85.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:44] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.524 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:44] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.514 Prec@(1,5) (86.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:45] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.512 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:45] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.515 Prec@(1,5) (86.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:45] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.518 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:46] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.516 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:46] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.518 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:47] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.514 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:47] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.513 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:48] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.513 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:48] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.517 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:49] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.516 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:49] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.513 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:50] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.514 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:50] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.515 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:51] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.513 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:51] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.511 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:52] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.511 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:52] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.510 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:53] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.508 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:53] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.508 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:54] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.507 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:54] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:55] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:55] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.507 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:56] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:57] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.504 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:57] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.504 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:57] \u001b[32mTrain: [  3/10] Final Prec@1 86.6350%\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.745 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.662 Prec@(1,5) (85.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.713 Prec@(1,5) (85.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.734 Prec@(1,5) (84.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.718 Prec@(1,5) (85.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.728 Prec@(1,5) (84.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:58] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.721 Prec@(1,5) (85.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:23:59] \u001b[32mValid: [  3/10] Final Prec@1 84.9600%\u001b[0m\n",
      "[2023-10-23 14:23:59] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:24:00] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.455 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:24:00] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.471 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:01] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.498 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:01] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.494 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:02] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.494 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:02] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.498 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:03] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.487 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:03] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.490 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:04] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.490 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:04] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.487 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:05] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.488 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:05] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.487 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:06] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.489 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:06] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.487 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:07] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.488 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:07] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.487 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:08] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.487 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:08] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.488 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:09] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.488 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:09] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.489 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:10] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.491 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:10] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.490 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:11] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.486 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:11] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.482 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:12] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.481 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:12] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.481 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:13] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.480 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:13] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.481 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:14] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.481 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:14] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.480 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:15] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.479 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:15] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.478 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:15] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.478 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:16] \u001b[32mTrain: [  4/10] Final Prec@1 87.3250%\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.889 Prec@(1,5) (82.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.742 Prec@(1,5) (84.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.754 Prec@(1,5) (84.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.752 Prec@(1,5) (84.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.749 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.768 Prec@(1,5) (84.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.759 Prec@(1,5) (84.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mValid: [  4/10] Final Prec@1 84.5600%\u001b[0m\n",
      "[2023-10-23 14:24:17] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:24:18] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.772 Prec@(1,5) (78.1%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:24:19] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.458 Prec@(1,5) (87.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:19] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.453 Prec@(1,5) (87.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:20] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.459 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:20] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.465 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:21] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.466 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:21] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.468 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:22] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.463 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:22] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.462 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:23] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.457 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:23] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.458 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:24] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.460 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:24] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.461 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:25] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.460 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:25] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.461 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:26] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.461 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:26] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.461 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:27] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.459 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:27] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.459 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:28] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.458 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:28] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.457 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:29] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.457 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:29] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.456 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:30] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.456 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:31] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.454 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:31] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.453 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:32] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.451 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:32] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.452 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:33] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.451 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:33] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.450 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:34] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.449 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:34] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.447 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:34] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.447 Prec@(1,5) (88.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:34] \u001b[32mTrain: [  5/10] Final Prec@1 88.1583%\u001b[0m\n",
      "[2023-10-23 14:24:35] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.582 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.469 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.506 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.494 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.479 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.485 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.479 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mValid: [  5/10] Final Prec@1 88.8800%\u001b[0m\n",
      "[2023-10-23 14:24:36] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:24:37] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.487 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:24:38] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.450 Prec@(1,5) (87.9%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:24:38] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.452 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:39] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.444 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:39] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.442 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:40] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.449 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:41] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.449 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:42] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.446 Prec@(1,5) (88.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:43] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.437 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:43] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.433 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:44] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.436 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:45] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.436 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:46] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.434 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:47] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.435 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:47] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.433 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:48] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.431 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:49] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.432 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:50] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.430 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:51] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.429 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:52] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.428 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:53] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:53] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:54] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:55] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.426 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:56] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.426 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:57] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.425 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:58] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.425 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:59] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.425 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:24:59] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.426 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:00] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.425 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:01] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.424 Prec@(1,5) (88.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:02] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.424 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:02] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.424 Prec@(1,5) (89.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:02] \u001b[32mTrain: [  6/10] Final Prec@1 88.9583%\u001b[0m\n",
      "[2023-10-23 14:25:03] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.567 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:25:04] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.419 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:04] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.460 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:04] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.456 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:04] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.440 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:05] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.445 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:05] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.440 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:05] \u001b[32mValid: [  6/10] Final Prec@1 89.6000%\u001b[0m\n",
      "[2023-10-23 14:25:05] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:25:06] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.653 Prec@(1,5) (80.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:25:07] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.437 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:08] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.414 Prec@(1,5) (89.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:08] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.416 Prec@(1,5) (89.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:09] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.417 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:10] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.407 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:11] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.405 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:12] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.408 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:12] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.404 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:13] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.403 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:14] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.403 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:15] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.405 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:16] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.406 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:17] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.407 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:17] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:18] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:19] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.410 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:20] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.410 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:21] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:22] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:23] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.406 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:23] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:24] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:25] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.407 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:26] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:27] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.408 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:28] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.408 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:28] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.408 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:29] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.406 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:30] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.405 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:31] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.405 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:32] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.405 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:32] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.405 Prec@(1,5) (89.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:32] \u001b[32mTrain: [  7/10] Final Prec@1 89.5800%\u001b[0m\n",
      "[2023-10-23 14:25:33] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.754 Prec@(1,5) (86.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:25:34] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.605 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:34] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.638 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:34] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.621 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:34] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.615 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:35] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.630 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:35] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.622 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:35] \u001b[32mValid: [  7/10] Final Prec@1 87.7400%\u001b[0m\n",
      "[2023-10-23 14:25:35] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:25:36] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.379 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:25:37] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.426 Prec@(1,5) (88.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:25:38] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.401 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:38] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.404 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:39] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.400 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:40] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.401 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:41] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.400 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:42] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.394 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:42] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:43] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.397 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:44] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.399 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:45] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.398 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:46] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.399 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:46] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.398 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:47] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.396 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:48] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.396 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:49] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.395 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:50] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.393 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:51] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.392 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:52] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:52] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:53] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.392 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:54] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.392 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:55] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:56] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.391 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:57] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.391 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:25:57] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.391 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:58] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.391 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:25:59] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.391 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:00] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.393 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:01] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.392 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:02] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.391 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:02] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.392 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:02] \u001b[32mTrain: [  8/10] Final Prec@1 89.8400%\u001b[0m\n",
      "[2023-10-23 14:26:03] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.482 Prec@(1,5) (89.6%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:26:03] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.392 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:04] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.425 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:04] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.418 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:04] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.402 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:04] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.410 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:04] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.405 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:05] \u001b[32mValid: [  8/10] Final Prec@1 90.6900%\u001b[0m\n",
      "[2023-10-23 14:26:05] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:26:06] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.525 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:26:07] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.430 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:07] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.415 Prec@(1,5) (89.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:08] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.406 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:09] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.400 Prec@(1,5) (89.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:10] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.399 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:10] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.392 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:11] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.386 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:12] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.386 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:13] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.382 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:14] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.382 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:14] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.385 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:15] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.386 Prec@(1,5) (90.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:16] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.383 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:17] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.383 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:18] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.381 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:19] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.380 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:20] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.380 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:20] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.380 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:21] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.379 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:22] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.380 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:23] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.381 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:24] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.380 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:25] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.381 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:25] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.381 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:26] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.382 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:27] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.381 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:28] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.380 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:29] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.380 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:30] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.379 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:31] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.378 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:31] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.377 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:32] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.378 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:32] \u001b[32mTrain: [  9/10] Final Prec@1 90.3267%\u001b[0m\n",
      "[2023-10-23 14:26:33] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.399 Prec@(1,5) (87.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:26:33] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.372 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:33] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.411 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:34] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.402 Prec@(1,5) (90.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:26:34] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.383 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:34] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.391 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:34] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.386 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:34] \u001b[32mValid: [  9/10] Final Prec@1 90.9700%\u001b[0m\n",
      "[2023-10-23 14:26:34] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:26:35] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.371 Prec@(1,5) (91.7%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:26:36] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.348 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:37] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.342 Prec@(1,5) (91.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:38] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.363 Prec@(1,5) (91.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:39] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.374 Prec@(1,5) (90.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:39] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.378 Prec@(1,5) (90.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:40] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.381 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:41] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.383 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:42] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.385 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:42] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.389 Prec@(1,5) (90.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:43] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.387 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:44] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.386 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:45] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.385 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:46] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.382 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:47] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.383 Prec@(1,5) (90.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:48] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.381 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:48] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.380 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:49] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.380 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:50] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.379 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:51] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.379 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:52] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.378 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:53] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.377 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:54] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:54] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.377 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:55] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.377 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:56] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:57] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:58] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:59] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:26:59] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.376 Prec@(1,5) (90.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:00] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.377 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:01] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.377 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:01] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.377 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:02] \u001b[32mTrain: [ 10/10] Final Prec@1 90.3250%\u001b[0m\n",
      "[2023-10-23 14:27:03] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.427 Prec@(1,5) (88.5%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:27:03] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.382 Prec@(1,5) (90.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:03] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.423 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:03] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.415 Prec@(1,5) (90.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:27:04] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.398 Prec@(1,5) (90.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:27:04] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.405 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:04] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.400 Prec@(1,5) (90.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:27:04] \u001b[32mValid: [ 10/10] Final Prec@1 90.7800%\u001b[0m\n",
      "Final best Prec@1 = 90.9700%\n",
      "[0.9078000211715698, 0.9136000185012817, 0.9212000221252441, 0.9133000202178955, 0.9077000192642212, 0.915000019454956, 0.9097000205993653]\n",
      "./checkpoints/fashionMNIST/lambd=4/\n",
      "[2023-10-23 14:27:04] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'dilconv5x5', 'reduce_n3_p0': 'avgpool', 'reduce_n3_p1': 'dilconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'skipconnect', 'reduce_n4_p1': 'sepconv3x3', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'skipconnect', 'reduce_n5_p2': 'skipconnect', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [0, 1], 'reduce_n3_switch': [2, 0], 'reduce_n4_switch': [2, 3], 'reduce_n5_switch': [4, 3]}\u001b[0m\n",
      "[2023-10-23 14:27:04] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2023-10-23 14:27:05] \u001b[32mTrain: [  1/10] Step 000/624 Loss 3.262 Prec@(1,5) (12.5%, 60.4%)\u001b[0m\n",
      "[2023-10-23 14:27:06] \u001b[32mTrain: [  1/10] Step 020/624 Loss 2.657 Prec@(1,5) (27.9%, 74.9%)\u001b[0m\n",
      "[2023-10-23 14:27:07] \u001b[32mTrain: [  1/10] Step 040/624 Loss 2.230 Prec@(1,5) (39.0%, 84.8%)\u001b[0m\n",
      "[2023-10-23 14:27:08] \u001b[32mTrain: [  1/10] Step 060/624 Loss 1.984 Prec@(1,5) (45.2%, 89.0%)\u001b[0m\n",
      "[2023-10-23 14:27:08] \u001b[32mTrain: [  1/10] Step 080/624 Loss 1.804 Prec@(1,5) (50.3%, 91.4%)\u001b[0m\n",
      "[2023-10-23 14:27:09] \u001b[32mTrain: [  1/10] Step 100/624 Loss 1.705 Prec@(1,5) (52.9%, 92.8%)\u001b[0m\n",
      "[2023-10-23 14:27:10] \u001b[32mTrain: [  1/10] Step 120/624 Loss 1.615 Prec@(1,5) (55.5%, 93.9%)\u001b[0m\n",
      "[2023-10-23 14:27:11] \u001b[32mTrain: [  1/10] Step 140/624 Loss 1.537 Prec@(1,5) (57.6%, 94.5%)\u001b[0m\n",
      "[2023-10-23 14:27:11] \u001b[32mTrain: [  1/10] Step 160/624 Loss 1.477 Prec@(1,5) (59.3%, 95.1%)\u001b[0m\n",
      "[2023-10-23 14:27:12] \u001b[32mTrain: [  1/10] Step 180/624 Loss 1.427 Prec@(1,5) (60.7%, 95.5%)\u001b[0m\n",
      "[2023-10-23 14:27:13] \u001b[32mTrain: [  1/10] Step 200/624 Loss 1.386 Prec@(1,5) (61.8%, 95.9%)\u001b[0m\n",
      "[2023-10-23 14:27:14] \u001b[32mTrain: [  1/10] Step 220/624 Loss 1.355 Prec@(1,5) (62.7%, 96.2%)\u001b[0m\n",
      "[2023-10-23 14:27:15] \u001b[32mTrain: [  1/10] Step 240/624 Loss 1.316 Prec@(1,5) (63.8%, 96.5%)\u001b[0m\n",
      "[2023-10-23 14:27:16] \u001b[32mTrain: [  1/10] Step 260/624 Loss 1.286 Prec@(1,5) (64.8%, 96.7%)\u001b[0m\n",
      "[2023-10-23 14:27:16] \u001b[32mTrain: [  1/10] Step 280/624 Loss 1.259 Prec@(1,5) (65.5%, 96.9%)\u001b[0m\n",
      "[2023-10-23 14:27:17] \u001b[32mTrain: [  1/10] Step 300/624 Loss 1.235 Prec@(1,5) (66.2%, 97.0%)\u001b[0m\n",
      "[2023-10-23 14:27:18] \u001b[32mTrain: [  1/10] Step 320/624 Loss 1.213 Prec@(1,5) (66.8%, 97.2%)\u001b[0m\n",
      "[2023-10-23 14:27:19] \u001b[32mTrain: [  1/10] Step 340/624 Loss 1.192 Prec@(1,5) (67.3%, 97.3%)\u001b[0m\n",
      "[2023-10-23 14:27:20] \u001b[32mTrain: [  1/10] Step 360/624 Loss 1.169 Prec@(1,5) (67.9%, 97.4%)\u001b[0m\n",
      "[2023-10-23 14:27:21] \u001b[32mTrain: [  1/10] Step 380/624 Loss 1.146 Prec@(1,5) (68.6%, 97.5%)\u001b[0m\n",
      "[2023-10-23 14:27:21] \u001b[32mTrain: [  1/10] Step 400/624 Loss 1.129 Prec@(1,5) (69.1%, 97.6%)\u001b[0m\n",
      "[2023-10-23 14:27:22] \u001b[32mTrain: [  1/10] Step 420/624 Loss 1.108 Prec@(1,5) (69.7%, 97.7%)\u001b[0m\n",
      "[2023-10-23 14:27:23] \u001b[32mTrain: [  1/10] Step 440/624 Loss 1.093 Prec@(1,5) (70.1%, 97.8%)\u001b[0m\n",
      "[2023-10-23 14:27:24] \u001b[32mTrain: [  1/10] Step 460/624 Loss 1.079 Prec@(1,5) (70.5%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:27:25] \u001b[32mTrain: [  1/10] Step 480/624 Loss 1.066 Prec@(1,5) (70.9%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:27:26] \u001b[32mTrain: [  1/10] Step 500/624 Loss 1.053 Prec@(1,5) (71.2%, 98.0%)\u001b[0m\n",
      "[2023-10-23 14:27:26] \u001b[32mTrain: [  1/10] Step 520/624 Loss 1.039 Prec@(1,5) (71.5%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:27:27] \u001b[32mTrain: [  1/10] Step 540/624 Loss 1.027 Prec@(1,5) (71.8%, 98.1%)\u001b[0m\n",
      "[2023-10-23 14:27:28] \u001b[32mTrain: [  1/10] Step 560/624 Loss 1.015 Prec@(1,5) (72.1%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:27:29] \u001b[32mTrain: [  1/10] Step 580/624 Loss 1.006 Prec@(1,5) (72.4%, 98.2%)\u001b[0m\n",
      "[2023-10-23 14:27:30] \u001b[32mTrain: [  1/10] Step 600/624 Loss 0.993 Prec@(1,5) (72.8%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:27:31] \u001b[32mTrain: [  1/10] Step 620/624 Loss 0.984 Prec@(1,5) (73.1%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:27:31] \u001b[32mTrain: [  1/10] Step 624/624 Loss 0.982 Prec@(1,5) (73.1%, 98.3%)\u001b[0m\n",
      "[2023-10-23 14:27:31] \u001b[32mTrain: [  1/10] Final Prec@1 73.1000%\u001b[0m\n",
      "[2023-10-23 14:27:32] \u001b[32mValid: [  1/10] Step 000/104 Loss 0.755 Prec@(1,5) (81.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:27:32] \u001b[32mValid: [  1/10] Step 020/104 Loss 0.609 Prec@(1,5) (84.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:33] \u001b[32mValid: [  1/10] Step 040/104 Loss 0.661 Prec@(1,5) (84.1%, 99.3%)\u001b[0m\n",
      "[2023-10-23 14:27:33] \u001b[32mValid: [  1/10] Step 060/104 Loss 0.680 Prec@(1,5) (83.8%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:27:33] \u001b[32mValid: [  1/10] Step 080/104 Loss 0.663 Prec@(1,5) (84.1%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:27:34] \u001b[32mValid: [  1/10] Step 100/104 Loss 0.678 Prec@(1,5) (83.7%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:27:34] \u001b[32mValid: [  1/10] Step 104/104 Loss 0.675 Prec@(1,5) (83.8%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:27:34] \u001b[32mValid: [  1/10] Final Prec@1 83.8400%\u001b[0m\n",
      "[2023-10-23 14:27:34] \u001b[32mEpoch 1 LR 0.024388\u001b[0m\n",
      "[2023-10-23 14:27:35] \u001b[32mTrain: [  2/10] Step 000/624 Loss 0.628 Prec@(1,5) (82.3%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:27:36] \u001b[32mTrain: [  2/10] Step 020/624 Loss 0.732 Prec@(1,5) (80.1%, 99.4%)\u001b[0m\n",
      "[2023-10-23 14:27:37] \u001b[32mTrain: [  2/10] Step 040/624 Loss 0.708 Prec@(1,5) (80.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:37] \u001b[32mTrain: [  2/10] Step 060/624 Loss 0.689 Prec@(1,5) (81.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:38] \u001b[32mTrain: [  2/10] Step 080/624 Loss 0.678 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:39] \u001b[32mTrain: [  2/10] Step 100/624 Loss 0.678 Prec@(1,5) (81.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:40] \u001b[32mTrain: [  2/10] Step 120/624 Loss 0.683 Prec@(1,5) (81.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:41] \u001b[32mTrain: [  2/10] Step 140/624 Loss 0.681 Prec@(1,5) (81.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:42] \u001b[32mTrain: [  2/10] Step 160/624 Loss 0.679 Prec@(1,5) (81.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:43] \u001b[32mTrain: [  2/10] Step 180/624 Loss 0.674 Prec@(1,5) (81.7%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:44] \u001b[32mTrain: [  2/10] Step 200/624 Loss 0.666 Prec@(1,5) (82.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:45] \u001b[32mTrain: [  2/10] Step 220/624 Loss 0.667 Prec@(1,5) (82.0%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:45] \u001b[32mTrain: [  2/10] Step 240/624 Loss 0.662 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:46] \u001b[32mTrain: [  2/10] Step 260/624 Loss 0.661 Prec@(1,5) (82.1%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:47] \u001b[32mTrain: [  2/10] Step 280/624 Loss 0.660 Prec@(1,5) (82.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:48] \u001b[32mTrain: [  2/10] Step 300/624 Loss 0.656 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:49] \u001b[32mTrain: [  2/10] Step 320/624 Loss 0.656 Prec@(1,5) (82.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:50] \u001b[32mTrain: [  2/10] Step 340/624 Loss 0.658 Prec@(1,5) (82.2%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:51] \u001b[32mTrain: [  2/10] Step 360/624 Loss 0.656 Prec@(1,5) (82.3%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:52] \u001b[32mTrain: [  2/10] Step 380/624 Loss 0.654 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:53] \u001b[32mTrain: [  2/10] Step 400/624 Loss 0.652 Prec@(1,5) (82.4%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:54] \u001b[32mTrain: [  2/10] Step 420/624 Loss 0.649 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:55] \u001b[32mTrain: [  2/10] Step 440/624 Loss 0.647 Prec@(1,5) (82.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:27:55] \u001b[32mTrain: [  2/10] Step 460/624 Loss 0.646 Prec@(1,5) (82.5%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:56] \u001b[32mTrain: [  2/10] Step 480/624 Loss 0.643 Prec@(1,5) (82.6%, 99.6%)\u001b[0m\n",
      "[2023-10-23 14:27:57] \u001b[32mTrain: [  2/10] Step 500/624 Loss 0.642 Prec@(1,5) (82.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:27:58] \u001b[32mTrain: [  2/10] Step 520/624 Loss 0.641 Prec@(1,5) (82.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:27:59] \u001b[32mTrain: [  2/10] Step 540/624 Loss 0.640 Prec@(1,5) (82.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:00] \u001b[32mTrain: [  2/10] Step 560/624 Loss 0.639 Prec@(1,5) (82.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:01] \u001b[32mTrain: [  2/10] Step 580/624 Loss 0.637 Prec@(1,5) (82.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:02] \u001b[32mTrain: [  2/10] Step 600/624 Loss 0.636 Prec@(1,5) (82.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:03] \u001b[32mTrain: [  2/10] Step 620/624 Loss 0.634 Prec@(1,5) (82.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:03] \u001b[32mTrain: [  2/10] Step 624/624 Loss 0.633 Prec@(1,5) (83.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:03] \u001b[32mTrain: [  2/10] Final Prec@1 82.9600%\u001b[0m\n",
      "[2023-10-23 14:28:04] \u001b[32mValid: [  2/10] Step 000/104 Loss 0.618 Prec@(1,5) (83.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:28:05] \u001b[32mValid: [  2/10] Step 020/104 Loss 0.407 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:28:05] \u001b[32mValid: [  2/10] Step 040/104 Loss 0.453 Prec@(1,5) (87.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:05] \u001b[32mValid: [  2/10] Step 060/104 Loss 0.450 Prec@(1,5) (87.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:05] \u001b[32mValid: [  2/10] Step 080/104 Loss 0.435 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:06] \u001b[32mValid: [  2/10] Step 100/104 Loss 0.433 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:06] \u001b[32mValid: [  2/10] Step 104/104 Loss 0.430 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:06] \u001b[32mValid: [  2/10] Final Prec@1 87.6800%\u001b[0m\n",
      "[2023-10-23 14:28:06] \u001b[32mEpoch 2 LR 0.022613\u001b[0m\n",
      "[2023-10-23 14:28:07] \u001b[32mTrain: [  3/10] Step 000/624 Loss 0.617 Prec@(1,5) (79.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:28:08] \u001b[32mTrain: [  3/10] Step 020/624 Loss 0.559 Prec@(1,5) (85.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:28:09] \u001b[32mTrain: [  3/10] Step 040/624 Loss 0.564 Prec@(1,5) (85.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:28:10] \u001b[32mTrain: [  3/10] Step 060/624 Loss 0.556 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:10] \u001b[32mTrain: [  3/10] Step 080/624 Loss 0.554 Prec@(1,5) (85.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:11] \u001b[32mTrain: [  3/10] Step 100/624 Loss 0.563 Prec@(1,5) (85.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:12] \u001b[32mTrain: [  3/10] Step 120/624 Loss 0.570 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:13] \u001b[32mTrain: [  3/10] Step 140/624 Loss 0.573 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:14] \u001b[32mTrain: [  3/10] Step 160/624 Loss 0.572 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:15] \u001b[32mTrain: [  3/10] Step 180/624 Loss 0.569 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:16] \u001b[32mTrain: [  3/10] Step 200/624 Loss 0.574 Prec@(1,5) (84.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:17] \u001b[32mTrain: [  3/10] Step 220/624 Loss 0.577 Prec@(1,5) (84.6%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:17] \u001b[32mTrain: [  3/10] Step 240/624 Loss 0.579 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:18] \u001b[32mTrain: [  3/10] Step 260/624 Loss 0.577 Prec@(1,5) (84.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:19] \u001b[32mTrain: [  3/10] Step 280/624 Loss 0.577 Prec@(1,5) (84.8%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:20] \u001b[32mTrain: [  3/10] Step 300/624 Loss 0.572 Prec@(1,5) (84.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:21] \u001b[32mTrain: [  3/10] Step 320/624 Loss 0.570 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:22] \u001b[32mTrain: [  3/10] Step 340/624 Loss 0.569 Prec@(1,5) (85.0%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:23] \u001b[32mTrain: [  3/10] Step 360/624 Loss 0.567 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:24] \u001b[32mTrain: [  3/10] Step 380/624 Loss 0.564 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:25] \u001b[32mTrain: [  3/10] Step 400/624 Loss 0.561 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:26] \u001b[32mTrain: [  3/10] Step 420/624 Loss 0.562 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:27] \u001b[32mTrain: [  3/10] Step 440/624 Loss 0.564 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:27] \u001b[32mTrain: [  3/10] Step 460/624 Loss 0.563 Prec@(1,5) (85.1%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:28] \u001b[32mTrain: [  3/10] Step 480/624 Loss 0.561 Prec@(1,5) (85.2%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:29] \u001b[32mTrain: [  3/10] Step 500/624 Loss 0.560 Prec@(1,5) (85.3%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:30] \u001b[32mTrain: [  3/10] Step 520/624 Loss 0.557 Prec@(1,5) (85.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:31] \u001b[32mTrain: [  3/10] Step 540/624 Loss 0.555 Prec@(1,5) (85.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:32] \u001b[32mTrain: [  3/10] Step 560/624 Loss 0.553 Prec@(1,5) (85.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:33] \u001b[32mTrain: [  3/10] Step 580/624 Loss 0.554 Prec@(1,5) (85.4%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:34] \u001b[32mTrain: [  3/10] Step 600/624 Loss 0.553 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:35] \u001b[32mTrain: [  3/10] Step 620/624 Loss 0.552 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:35] \u001b[32mTrain: [  3/10] Step 624/624 Loss 0.553 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:35] \u001b[32mTrain: [  3/10] Final Prec@1 85.4567%\u001b[0m\n",
      "[2023-10-23 14:28:36] \u001b[32mValid: [  3/10] Step 000/104 Loss 0.537 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:28:37] \u001b[32mValid: [  3/10] Step 020/104 Loss 0.474 Prec@(1,5) (87.8%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:28:37] \u001b[32mValid: [  3/10] Step 040/104 Loss 0.505 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:37] \u001b[32mValid: [  3/10] Step 060/104 Loss 0.509 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:37] \u001b[32mValid: [  3/10] Step 080/104 Loss 0.492 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:38] \u001b[32mValid: [  3/10] Step 100/104 Loss 0.498 Prec@(1,5) (87.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:28:38] \u001b[32mValid: [  3/10] Step 104/104 Loss 0.494 Prec@(1,5) (87.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:28:38] \u001b[32mValid: [  3/10] Final Prec@1 87.6500%\u001b[0m\n",
      "[2023-10-23 14:28:38] \u001b[32mEpoch 3 LR 0.019848\u001b[0m\n",
      "[2023-10-23 14:28:39] \u001b[32mTrain: [  4/10] Step 000/624 Loss 0.659 Prec@(1,5) (84.4%, 99.0%)\u001b[0m\n",
      "[2023-10-23 14:28:40] \u001b[32mTrain: [  4/10] Step 020/624 Loss 0.579 Prec@(1,5) (84.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:28:41] \u001b[32mTrain: [  4/10] Step 040/624 Loss 0.547 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:42] \u001b[32mTrain: [  4/10] Step 060/624 Loss 0.548 Prec@(1,5) (85.5%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:28:42] \u001b[32mTrain: [  4/10] Step 080/624 Loss 0.540 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:43] \u001b[32mTrain: [  4/10] Step 100/624 Loss 0.534 Prec@(1,5) (85.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:44] \u001b[32mTrain: [  4/10] Step 120/624 Loss 0.530 Prec@(1,5) (85.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:45] \u001b[32mTrain: [  4/10] Step 140/624 Loss 0.527 Prec@(1,5) (86.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:46] \u001b[32mTrain: [  4/10] Step 160/624 Loss 0.525 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:47] \u001b[32mTrain: [  4/10] Step 180/624 Loss 0.523 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:48] \u001b[32mTrain: [  4/10] Step 200/624 Loss 0.523 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:49] \u001b[32mTrain: [  4/10] Step 220/624 Loss 0.521 Prec@(1,5) (86.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:49] \u001b[32mTrain: [  4/10] Step 240/624 Loss 0.521 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:50] \u001b[32mTrain: [  4/10] Step 260/624 Loss 0.525 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:51] \u001b[32mTrain: [  4/10] Step 280/624 Loss 0.527 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:52] \u001b[32mTrain: [  4/10] Step 300/624 Loss 0.524 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:53] \u001b[32mTrain: [  4/10] Step 320/624 Loss 0.523 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:54] \u001b[32mTrain: [  4/10] Step 340/624 Loss 0.521 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:55] \u001b[32mTrain: [  4/10] Step 360/624 Loss 0.520 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:56] \u001b[32mTrain: [  4/10] Step 380/624 Loss 0.519 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:57] \u001b[32mTrain: [  4/10] Step 400/624 Loss 0.517 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:58] \u001b[32mTrain: [  4/10] Step 420/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:59] \u001b[32mTrain: [  4/10] Step 440/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:28:59] \u001b[32mTrain: [  4/10] Step 460/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:00] \u001b[32mTrain: [  4/10] Step 480/624 Loss 0.516 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:01] \u001b[32mTrain: [  4/10] Step 500/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:02] \u001b[32mTrain: [  4/10] Step 520/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:03] \u001b[32mTrain: [  4/10] Step 540/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:04] \u001b[32mTrain: [  4/10] Step 560/624 Loss 0.515 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:05] \u001b[32mTrain: [  4/10] Step 580/624 Loss 0.513 Prec@(1,5) (86.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:06] \u001b[32mTrain: [  4/10] Step 600/624 Loss 0.513 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:07] \u001b[32mTrain: [  4/10] Step 620/624 Loss 0.513 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:07] \u001b[32mTrain: [  4/10] Step 624/624 Loss 0.513 Prec@(1,5) (86.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:07] \u001b[32mTrain: [  4/10] Final Prec@1 86.4583%\u001b[0m\n",
      "[2023-10-23 14:29:08] \u001b[32mValid: [  4/10] Step 000/104 Loss 0.522 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:29:09] \u001b[32mValid: [  4/10] Step 020/104 Loss 0.408 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:29:09] \u001b[32mValid: [  4/10] Step 040/104 Loss 0.448 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:09] \u001b[32mValid: [  4/10] Step 060/104 Loss 0.451 Prec@(1,5) (88.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:09] \u001b[32mValid: [  4/10] Step 080/104 Loss 0.431 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:10] \u001b[32mValid: [  4/10] Step 100/104 Loss 0.428 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:10] \u001b[32mValid: [  4/10] Step 104/104 Loss 0.424 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:10] \u001b[32mValid: [  4/10] Final Prec@1 88.7200%\u001b[0m\n",
      "[2023-10-23 14:29:10] \u001b[32mEpoch 4 LR 0.016363\u001b[0m\n",
      "[2023-10-23 14:29:11] \u001b[32mTrain: [  5/10] Step 000/624 Loss 0.531 Prec@(1,5) (90.6%, 97.9%)\u001b[0m\n",
      "[2023-10-23 14:29:12] \u001b[32mTrain: [  5/10] Step 020/624 Loss 0.521 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:13] \u001b[32mTrain: [  5/10] Step 040/624 Loss 0.536 Prec@(1,5) (85.7%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:29:14] \u001b[32mTrain: [  5/10] Step 060/624 Loss 0.526 Prec@(1,5) (85.9%, 99.7%)\u001b[0m\n",
      "[2023-10-23 14:29:14] \u001b[32mTrain: [  5/10] Step 080/624 Loss 0.517 Prec@(1,5) (86.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:15] \u001b[32mTrain: [  5/10] Step 100/624 Loss 0.514 Prec@(1,5) (86.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:16] \u001b[32mTrain: [  5/10] Step 120/624 Loss 0.506 Prec@(1,5) (86.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:17] \u001b[32mTrain: [  5/10] Step 140/624 Loss 0.500 Prec@(1,5) (86.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:18] \u001b[32mTrain: [  5/10] Step 160/624 Loss 0.495 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:19] \u001b[32mTrain: [  5/10] Step 180/624 Loss 0.496 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:20] \u001b[32mTrain: [  5/10] Step 200/624 Loss 0.495 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:21] \u001b[32mTrain: [  5/10] Step 220/624 Loss 0.494 Prec@(1,5) (87.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:21] \u001b[32mTrain: [  5/10] Step 240/624 Loss 0.491 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:22] \u001b[32mTrain: [  5/10] Step 260/624 Loss 0.490 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:23] \u001b[32mTrain: [  5/10] Step 280/624 Loss 0.490 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:24] \u001b[32mTrain: [  5/10] Step 300/624 Loss 0.491 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:25] \u001b[32mTrain: [  5/10] Step 320/624 Loss 0.489 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:26] \u001b[32mTrain: [  5/10] Step 340/624 Loss 0.487 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:27] \u001b[32mTrain: [  5/10] Step 360/624 Loss 0.488 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:28] \u001b[32mTrain: [  5/10] Step 380/624 Loss 0.488 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:29] \u001b[32mTrain: [  5/10] Step 400/624 Loss 0.487 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:30] \u001b[32mTrain: [  5/10] Step 420/624 Loss 0.484 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:31] \u001b[32mTrain: [  5/10] Step 440/624 Loss 0.484 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:32] \u001b[32mTrain: [  5/10] Step 460/624 Loss 0.484 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:32] \u001b[32mTrain: [  5/10] Step 480/624 Loss 0.483 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:33] \u001b[32mTrain: [  5/10] Step 500/624 Loss 0.483 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:34] \u001b[32mTrain: [  5/10] Step 520/624 Loss 0.482 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:35] \u001b[32mTrain: [  5/10] Step 540/624 Loss 0.483 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:36] \u001b[32mTrain: [  5/10] Step 560/624 Loss 0.482 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:37] \u001b[32mTrain: [  5/10] Step 580/624 Loss 0.482 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:38] \u001b[32mTrain: [  5/10] Step 600/624 Loss 0.482 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:39] \u001b[32mTrain: [  5/10] Step 620/624 Loss 0.483 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:39] \u001b[32mTrain: [  5/10] Step 624/624 Loss 0.482 Prec@(1,5) (87.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:39] \u001b[32mTrain: [  5/10] Final Prec@1 87.3750%\u001b[0m\n",
      "[2023-10-23 14:29:40] \u001b[32mValid: [  5/10] Step 000/104 Loss 0.364 Prec@(1,5) (88.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:29:41] \u001b[32mValid: [  5/10] Step 020/104 Loss 0.357 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:29:41] \u001b[32mValid: [  5/10] Step 040/104 Loss 0.395 Prec@(1,5) (90.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:41] \u001b[32mValid: [  5/10] Step 060/104 Loss 0.402 Prec@(1,5) (90.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:41] \u001b[32mValid: [  5/10] Step 080/104 Loss 0.388 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:42] \u001b[32mValid: [  5/10] Step 100/104 Loss 0.395 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:42] \u001b[32mValid: [  5/10] Step 104/104 Loss 0.393 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:42] \u001b[32mValid: [  5/10] Final Prec@1 89.9100%\u001b[0m\n",
      "[2023-10-23 14:29:42] \u001b[32mEpoch 5 LR 0.012500\u001b[0m\n",
      "[2023-10-23 14:29:43] \u001b[32mTrain: [  6/10] Step 000/624 Loss 0.625 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:29:44] \u001b[32mTrain: [  6/10] Step 020/624 Loss 0.494 Prec@(1,5) (86.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:45] \u001b[32mTrain: [  6/10] Step 040/624 Loss 0.486 Prec@(1,5) (87.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:46] \u001b[32mTrain: [  6/10] Step 060/624 Loss 0.490 Prec@(1,5) (87.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:46] \u001b[32mTrain: [  6/10] Step 080/624 Loss 0.489 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:47] \u001b[32mTrain: [  6/10] Step 100/624 Loss 0.484 Prec@(1,5) (87.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:48] \u001b[32mTrain: [  6/10] Step 120/624 Loss 0.476 Prec@(1,5) (87.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:29:49] \u001b[32mTrain: [  6/10] Step 140/624 Loss 0.470 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:50] \u001b[32mTrain: [  6/10] Step 160/624 Loss 0.474 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:29:51] \u001b[32mTrain: [  6/10] Step 180/624 Loss 0.471 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:29:52] \u001b[32mTrain: [  6/10] Step 200/624 Loss 0.474 Prec@(1,5) (87.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:53] \u001b[32mTrain: [  6/10] Step 220/624 Loss 0.471 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:53] \u001b[32mTrain: [  6/10] Step 240/624 Loss 0.470 Prec@(1,5) (87.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:54] \u001b[32mTrain: [  6/10] Step 260/624 Loss 0.467 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:55] \u001b[32mTrain: [  6/10] Step 280/624 Loss 0.468 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:56] \u001b[32mTrain: [  6/10] Step 300/624 Loss 0.466 Prec@(1,5) (87.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:57] \u001b[32mTrain: [  6/10] Step 320/624 Loss 0.465 Prec@(1,5) (87.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:29:58] \u001b[32mTrain: [  6/10] Step 340/624 Loss 0.464 Prec@(1,5) (87.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:29:59] \u001b[32mTrain: [  6/10] Step 360/624 Loss 0.463 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:00] \u001b[32mTrain: [  6/10] Step 380/624 Loss 0.462 Prec@(1,5) (87.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:01] \u001b[32mTrain: [  6/10] Step 400/624 Loss 0.462 Prec@(1,5) (87.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:02] \u001b[32mTrain: [  6/10] Step 420/624 Loss 0.462 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:03] \u001b[32mTrain: [  6/10] Step 440/624 Loss 0.459 Prec@(1,5) (88.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:04] \u001b[32mTrain: [  6/10] Step 460/624 Loss 0.458 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:04] \u001b[32mTrain: [  6/10] Step 480/624 Loss 0.460 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:05] \u001b[32mTrain: [  6/10] Step 500/624 Loss 0.459 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:06] \u001b[32mTrain: [  6/10] Step 520/624 Loss 0.458 Prec@(1,5) (88.0%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:07] \u001b[32mTrain: [  6/10] Step 540/624 Loss 0.458 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:08] \u001b[32mTrain: [  6/10] Step 560/624 Loss 0.457 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:09] \u001b[32mTrain: [  6/10] Step 580/624 Loss 0.457 Prec@(1,5) (88.0%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:10] \u001b[32mTrain: [  6/10] Step 600/624 Loss 0.456 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:11] \u001b[32mTrain: [  6/10] Step 620/624 Loss 0.456 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:11] \u001b[32mTrain: [  6/10] Step 624/624 Loss 0.456 Prec@(1,5) (88.1%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:11] \u001b[32mTrain: [  6/10] Final Prec@1 88.0650%\u001b[0m\n",
      "[2023-10-23 14:30:12] \u001b[32mValid: [  6/10] Step 000/104 Loss 0.475 Prec@(1,5) (86.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:30:13] \u001b[32mValid: [  6/10] Step 020/104 Loss 0.414 Prec@(1,5) (89.3%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:30:13] \u001b[32mValid: [  6/10] Step 040/104 Loss 0.465 Prec@(1,5) (88.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:13] \u001b[32mValid: [  6/10] Step 060/104 Loss 0.463 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:13] \u001b[32mValid: [  6/10] Step 080/104 Loss 0.444 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:14] \u001b[32mValid: [  6/10] Step 100/104 Loss 0.448 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:14] \u001b[32mValid: [  6/10] Step 104/104 Loss 0.445 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:14] \u001b[32mValid: [  6/10] Final Prec@1 88.8000%\u001b[0m\n",
      "[2023-10-23 14:30:14] \u001b[32mEpoch 6 LR 0.008638\u001b[0m\n",
      "[2023-10-23 14:30:15] \u001b[32mTrain: [  7/10] Step 000/624 Loss 0.524 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:30:16] \u001b[32mTrain: [  7/10] Step 020/624 Loss 0.456 Prec@(1,5) (87.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:17] \u001b[32mTrain: [  7/10] Step 040/624 Loss 0.435 Prec@(1,5) (88.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:18] \u001b[32mTrain: [  7/10] Step 060/624 Loss 0.443 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:19] \u001b[32mTrain: [  7/10] Step 080/624 Loss 0.441 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:19] \u001b[32mTrain: [  7/10] Step 100/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:20] \u001b[32mTrain: [  7/10] Step 120/624 Loss 0.445 Prec@(1,5) (88.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:21] \u001b[32mTrain: [  7/10] Step 140/624 Loss 0.438 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:22] \u001b[32mTrain: [  7/10] Step 160/624 Loss 0.437 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:23] \u001b[32mTrain: [  7/10] Step 180/624 Loss 0.440 Prec@(1,5) (88.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:24] \u001b[32mTrain: [  7/10] Step 200/624 Loss 0.443 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:25] \u001b[32mTrain: [  7/10] Step 220/624 Loss 0.442 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:26] \u001b[32mTrain: [  7/10] Step 240/624 Loss 0.436 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:26] \u001b[32mTrain: [  7/10] Step 260/624 Loss 0.440 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:27] \u001b[32mTrain: [  7/10] Step 280/624 Loss 0.440 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:28] \u001b[32mTrain: [  7/10] Step 300/624 Loss 0.442 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:29] \u001b[32mTrain: [  7/10] Step 320/624 Loss 0.445 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:30] \u001b[32mTrain: [  7/10] Step 340/624 Loss 0.446 Prec@(1,5) (88.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:31] \u001b[32mTrain: [  7/10] Step 360/624 Loss 0.446 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:32] \u001b[32mTrain: [  7/10] Step 380/624 Loss 0.446 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:33] \u001b[32mTrain: [  7/10] Step 400/624 Loss 0.445 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:34] \u001b[32mTrain: [  7/10] Step 420/624 Loss 0.445 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:35] \u001b[32mTrain: [  7/10] Step 440/624 Loss 0.444 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:36] \u001b[32mTrain: [  7/10] Step 460/624 Loss 0.443 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:37] \u001b[32mTrain: [  7/10] Step 480/624 Loss 0.442 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:37] \u001b[32mTrain: [  7/10] Step 500/624 Loss 0.443 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:38] \u001b[32mTrain: [  7/10] Step 520/624 Loss 0.444 Prec@(1,5) (88.6%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:39] \u001b[32mTrain: [  7/10] Step 540/624 Loss 0.443 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:40] \u001b[32mTrain: [  7/10] Step 560/624 Loss 0.442 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:41] \u001b[32mTrain: [  7/10] Step 580/624 Loss 0.442 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:42] \u001b[32mTrain: [  7/10] Step 600/624 Loss 0.440 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:43] \u001b[32mTrain: [  7/10] Step 620/624 Loss 0.439 Prec@(1,5) (88.7%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:43] \u001b[32mTrain: [  7/10] Step 624/624 Loss 0.440 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:43] \u001b[32mTrain: [  7/10] Final Prec@1 88.7517%\u001b[0m\n",
      "[2023-10-23 14:30:44] \u001b[32mValid: [  7/10] Step 000/104 Loss 0.477 Prec@(1,5) (85.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:30:45] \u001b[32mValid: [  7/10] Step 020/104 Loss 0.376 Prec@(1,5) (90.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:45] \u001b[32mValid: [  7/10] Step 040/104 Loss 0.414 Prec@(1,5) (90.2%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:45] \u001b[32mValid: [  7/10] Step 060/104 Loss 0.414 Prec@(1,5) (90.3%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:46] \u001b[32mValid: [  7/10] Step 080/104 Loss 0.398 Prec@(1,5) (90.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:46] \u001b[32mValid: [  7/10] Step 100/104 Loss 0.403 Prec@(1,5) (90.4%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:46] \u001b[32mValid: [  7/10] Step 104/104 Loss 0.401 Prec@(1,5) (90.5%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:30:46] \u001b[32mValid: [  7/10] Final Prec@1 90.4700%\u001b[0m\n",
      "[2023-10-23 14:30:46] \u001b[32mEpoch 7 LR 0.005153\u001b[0m\n",
      "[2023-10-23 14:30:47] \u001b[32mTrain: [  8/10] Step 000/624 Loss 0.423 Prec@(1,5) (84.4%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:30:48] \u001b[32mTrain: [  8/10] Step 020/624 Loss 0.435 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:49] \u001b[32mTrain: [  8/10] Step 040/624 Loss 0.418 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:50] \u001b[32mTrain: [  8/10] Step 060/624 Loss 0.427 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:51] \u001b[32mTrain: [  8/10] Step 080/624 Loss 0.426 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:51] \u001b[32mTrain: [  8/10] Step 100/624 Loss 0.420 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:52] \u001b[32mTrain: [  8/10] Step 120/624 Loss 0.418 Prec@(1,5) (88.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:53] \u001b[32mTrain: [  8/10] Step 140/624 Loss 0.422 Prec@(1,5) (88.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:54] \u001b[32mTrain: [  8/10] Step 160/624 Loss 0.419 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:55] \u001b[32mTrain: [  8/10] Step 180/624 Loss 0.419 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:56] \u001b[32mTrain: [  8/10] Step 200/624 Loss 0.420 Prec@(1,5) (88.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:57] \u001b[32mTrain: [  8/10] Step 220/624 Loss 0.415 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:58] \u001b[32mTrain: [  8/10] Step 240/624 Loss 0.415 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:59] \u001b[32mTrain: [  8/10] Step 260/624 Loss 0.416 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:30:59] \u001b[32mTrain: [  8/10] Step 280/624 Loss 0.415 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:00] \u001b[32mTrain: [  8/10] Step 300/624 Loss 0.413 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:01] \u001b[32mTrain: [  8/10] Step 320/624 Loss 0.413 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:02] \u001b[32mTrain: [  8/10] Step 340/624 Loss 0.411 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:03] \u001b[32mTrain: [  8/10] Step 360/624 Loss 0.412 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:04] \u001b[32mTrain: [  8/10] Step 380/624 Loss 0.412 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:05] \u001b[32mTrain: [  8/10] Step 400/624 Loss 0.410 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:06] \u001b[32mTrain: [  8/10] Step 420/624 Loss 0.409 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:07] \u001b[32mTrain: [  8/10] Step 440/624 Loss 0.409 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:08] \u001b[32mTrain: [  8/10] Step 460/624 Loss 0.410 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:09] \u001b[32mTrain: [  8/10] Step 480/624 Loss 0.411 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:09] \u001b[32mTrain: [  8/10] Step 500/624 Loss 0.411 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:10] \u001b[32mTrain: [  8/10] Step 520/624 Loss 0.412 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:11] \u001b[32mTrain: [  8/10] Step 540/624 Loss 0.413 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:12] \u001b[32mTrain: [  8/10] Step 560/624 Loss 0.414 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:13] \u001b[32mTrain: [  8/10] Step 580/624 Loss 0.414 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:14] \u001b[32mTrain: [  8/10] Step 600/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:15] \u001b[32mTrain: [  8/10] Step 620/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:15] \u001b[32mTrain: [  8/10] Step 624/624 Loss 0.415 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:15] \u001b[32mTrain: [  8/10] Final Prec@1 89.2617%\u001b[0m\n",
      "[2023-10-23 14:31:16] \u001b[32mValid: [  8/10] Step 000/104 Loss 0.496 Prec@(1,5) (89.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:17] \u001b[32mValid: [  8/10] Step 020/104 Loss 0.355 Prec@(1,5) (91.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:17] \u001b[32mValid: [  8/10] Step 040/104 Loss 0.377 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:17] \u001b[32mValid: [  8/10] Step 060/104 Loss 0.374 Prec@(1,5) (91.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:18] \u001b[32mValid: [  8/10] Step 080/104 Loss 0.361 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:18] \u001b[32mValid: [  8/10] Step 100/104 Loss 0.362 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:18] \u001b[32mValid: [  8/10] Step 104/104 Loss 0.360 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:18] \u001b[32mValid: [  8/10] Final Prec@1 91.2400%\u001b[0m\n",
      "[2023-10-23 14:31:18] \u001b[32mEpoch 8 LR 0.002388\u001b[0m\n",
      "[2023-10-23 14:31:19] \u001b[32mTrain: [  9/10] Step 000/624 Loss 0.337 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:20] \u001b[32mTrain: [  9/10] Step 020/624 Loss 0.388 Prec@(1,5) (89.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:21] \u001b[32mTrain: [  9/10] Step 040/624 Loss 0.413 Prec@(1,5) (88.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:21] \u001b[32mTrain: [  9/10] Step 060/624 Loss 0.410 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:22] \u001b[32mTrain: [  9/10] Step 080/624 Loss 0.409 Prec@(1,5) (89.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:22] \u001b[32mTrain: [  9/10] Step 100/624 Loss 0.413 Prec@(1,5) (89.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:23] \u001b[32mTrain: [  9/10] Step 120/624 Loss 0.408 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:24] \u001b[32mTrain: [  9/10] Step 140/624 Loss 0.405 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:24] \u001b[32mTrain: [  9/10] Step 160/624 Loss 0.406 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:25] \u001b[32mTrain: [  9/10] Step 180/624 Loss 0.407 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:25] \u001b[32mTrain: [  9/10] Step 200/624 Loss 0.408 Prec@(1,5) (89.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:26] \u001b[32mTrain: [  9/10] Step 220/624 Loss 0.404 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:26] \u001b[32mTrain: [  9/10] Step 240/624 Loss 0.404 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:27] \u001b[32mTrain: [  9/10] Step 260/624 Loss 0.406 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:27] \u001b[32mTrain: [  9/10] Step 280/624 Loss 0.405 Prec@(1,5) (89.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:28] \u001b[32mTrain: [  9/10] Step 300/624 Loss 0.404 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:29] \u001b[32mTrain: [  9/10] Step 320/624 Loss 0.404 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:29] \u001b[32mTrain: [  9/10] Step 340/624 Loss 0.405 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:30] \u001b[32mTrain: [  9/10] Step 360/624 Loss 0.404 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:30] \u001b[32mTrain: [  9/10] Step 380/624 Loss 0.406 Prec@(1,5) (89.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:31] \u001b[32mTrain: [  9/10] Step 400/624 Loss 0.404 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:31] \u001b[32mTrain: [  9/10] Step 420/624 Loss 0.403 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:32] \u001b[32mTrain: [  9/10] Step 440/624 Loss 0.402 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:32] \u001b[32mTrain: [  9/10] Step 460/624 Loss 0.402 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:33] \u001b[32mTrain: [  9/10] Step 480/624 Loss 0.401 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:33] \u001b[32mTrain: [  9/10] Step 500/624 Loss 0.400 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:34] \u001b[32mTrain: [  9/10] Step 520/624 Loss 0.400 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:35] \u001b[32mTrain: [  9/10] Step 540/624 Loss 0.400 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:35] \u001b[32mTrain: [  9/10] Step 560/624 Loss 0.400 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:36] \u001b[32mTrain: [  9/10] Step 580/624 Loss 0.401 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:36] \u001b[32mTrain: [  9/10] Step 600/624 Loss 0.401 Prec@(1,5) (89.7%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:37] \u001b[32mTrain: [  9/10] Step 620/624 Loss 0.402 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:37] \u001b[32mTrain: [  9/10] Step 624/624 Loss 0.402 Prec@(1,5) (89.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:37] \u001b[32mTrain: [  9/10] Final Prec@1 89.6317%\u001b[0m\n",
      "[2023-10-23 14:31:38] \u001b[32mValid: [  9/10] Step 000/104 Loss 0.348 Prec@(1,5) (91.7%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:38] \u001b[32mValid: [  9/10] Step 020/104 Loss 0.333 Prec@(1,5) (92.2%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:38] \u001b[32mValid: [  9/10] Step 040/104 Loss 0.362 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:38] \u001b[32mValid: [  9/10] Step 060/104 Loss 0.362 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:39] \u001b[32mValid: [  9/10] Step 080/104 Loss 0.347 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:39] \u001b[32mValid: [  9/10] Step 100/104 Loss 0.350 Prec@(1,5) (91.4%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:39] \u001b[32mValid: [  9/10] Step 104/104 Loss 0.347 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:39] \u001b[32mValid: [  9/10] Final Prec@1 91.4700%\u001b[0m\n",
      "[2023-10-23 14:31:39] \u001b[32mEpoch 9 LR 0.000613\u001b[0m\n",
      "[2023-10-23 14:31:40] \u001b[32mTrain: [ 10/10] Step 000/624 Loss 0.393 Prec@(1,5) (87.5%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:40] \u001b[32mTrain: [ 10/10] Step 020/624 Loss 0.378 Prec@(1,5) (90.3%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:41] \u001b[32mTrain: [ 10/10] Step 040/624 Loss 0.392 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:42] \u001b[32mTrain: [ 10/10] Step 060/624 Loss 0.402 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:42] \u001b[32mTrain: [ 10/10] Step 080/624 Loss 0.401 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:43] \u001b[32mTrain: [ 10/10] Step 100/624 Loss 0.403 Prec@(1,5) (89.9%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:43] \u001b[32mTrain: [ 10/10] Step 120/624 Loss 0.397 Prec@(1,5) (90.1%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:44] \u001b[32mTrain: [ 10/10] Step 140/624 Loss 0.400 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:44] \u001b[32mTrain: [ 10/10] Step 160/624 Loss 0.402 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:45] \u001b[32mTrain: [ 10/10] Step 180/624 Loss 0.400 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:45] \u001b[32mTrain: [ 10/10] Step 200/624 Loss 0.402 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:46] \u001b[32mTrain: [ 10/10] Step 220/624 Loss 0.400 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:46] \u001b[32mTrain: [ 10/10] Step 240/624 Loss 0.402 Prec@(1,5) (89.8%, 99.8%)\u001b[0m\n",
      "[2023-10-23 14:31:47] \u001b[32mTrain: [ 10/10] Step 260/624 Loss 0.402 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:48] \u001b[32mTrain: [ 10/10] Step 280/624 Loss 0.401 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:48] \u001b[32mTrain: [ 10/10] Step 300/624 Loss 0.403 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:49] \u001b[32mTrain: [ 10/10] Step 320/624 Loss 0.402 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:49] \u001b[32mTrain: [ 10/10] Step 340/624 Loss 0.404 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:50] \u001b[32mTrain: [ 10/10] Step 360/624 Loss 0.402 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:50] \u001b[32mTrain: [ 10/10] Step 380/624 Loss 0.403 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:51] \u001b[32mTrain: [ 10/10] Step 400/624 Loss 0.402 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:51] \u001b[32mTrain: [ 10/10] Step 420/624 Loss 0.401 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:52] \u001b[32mTrain: [ 10/10] Step 440/624 Loss 0.403 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:52] \u001b[32mTrain: [ 10/10] Step 460/624 Loss 0.403 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:53] \u001b[32mTrain: [ 10/10] Step 480/624 Loss 0.403 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:53] \u001b[32mTrain: [ 10/10] Step 500/624 Loss 0.403 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:54] \u001b[32mTrain: [ 10/10] Step 520/624 Loss 0.404 Prec@(1,5) (89.8%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:55] \u001b[32mTrain: [ 10/10] Step 540/624 Loss 0.402 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:55] \u001b[32mTrain: [ 10/10] Step 560/624 Loss 0.402 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:56] \u001b[32mTrain: [ 10/10] Step 580/624 Loss 0.402 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:56] \u001b[32mTrain: [ 10/10] Step 600/624 Loss 0.401 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:57] \u001b[32mTrain: [ 10/10] Step 620/624 Loss 0.401 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:57] \u001b[32mTrain: [ 10/10] Step 624/624 Loss 0.400 Prec@(1,5) (89.9%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:57] \u001b[32mTrain: [ 10/10] Final Prec@1 89.9050%\u001b[0m\n",
      "[2023-10-23 14:31:58] \u001b[32mValid: [ 10/10] Step 000/104 Loss 0.385 Prec@(1,5) (90.6%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:58] \u001b[32mValid: [ 10/10] Step 020/104 Loss 0.333 Prec@(1,5) (91.9%, 100.0%)\u001b[0m\n",
      "[2023-10-23 14:31:58] \u001b[32mValid: [ 10/10] Step 040/104 Loss 0.361 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:58] \u001b[32mValid: [ 10/10] Step 060/104 Loss 0.361 Prec@(1,5) (91.2%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:58] \u001b[32mValid: [ 10/10] Step 080/104 Loss 0.346 Prec@(1,5) (91.6%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:59] \u001b[32mValid: [ 10/10] Step 100/104 Loss 0.348 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:59] \u001b[32mValid: [ 10/10] Step 104/104 Loss 0.346 Prec@(1,5) (91.5%, 99.9%)\u001b[0m\n",
      "[2023-10-23 14:31:59] \u001b[32mValid: [ 10/10] Final Prec@1 91.5400%\u001b[0m\n",
      "Final best Prec@1 = 91.5400%\n",
      "[0.9078000211715698, 0.9136000185012817, 0.9212000221252441, 0.9133000202178955, 0.9077000192642212, 0.915000019454956, 0.9097000205993653, 0.9154000215530396]\n"
     ]
    }
   ],
   "source": [
    "from retrain import train, validate, fixed_arch\n",
    "# reload(train)\n",
    "\n",
    "config = {\n",
    "'layers' : layers,\n",
    "'batch_size' : batch_size,\n",
    "'log_frequency' : log_frequency,\n",
    "'epochs' : 10,\n",
    "'aux_weight' : 0.4,\n",
    "'drop_path_prob' : 0.1,\n",
    "'workers' : 4,\n",
    "'grad_clip' : 5.,\n",
    "'save_folder' : \"./checkpoints/fashionMNIST/\",\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_train, dataset_valid = datasets.get_dataset(\"fashionmnist\", cutout_length=16)\n",
    "\n",
    "best_top1s = []\n",
    "for lambd in [1, 2]:\n",
    "    if lambd == 0:\n",
    "        folder = config['save_folder'] + \"optimal/\"\n",
    "    else:\n",
    "        folder = config['save_folder'] + f\"lambd={lambd}/\"\n",
    "    print(folder)\n",
    "    with fixed_arch(folder + 'arc.json'):\n",
    "    # with fixed_arch(args.save_folder + \"/arc.json\"):\n",
    "        model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'], eta_min=1E-6)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=config['workers'],\n",
    "                                            pin_memory=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset_valid,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config['workers'],\n",
    "                                            pin_memory=True)\n",
    "\n",
    "    best_top1 = 0.\n",
    "    for epoch in range(config['epochs']):\n",
    "        drop_prob = config['drop_path_prob'] * epoch / config['epochs']\n",
    "        model.drop_path_prob(drop_prob)\n",
    "\n",
    "        # training\n",
    "        train(config, train_loader, model, optimizer, criterion, epoch)\n",
    "\n",
    "        # validation\n",
    "        cur_step = (epoch + 1) * len(train_loader)\n",
    "        top1 = validate(config, valid_loader, model, criterion, epoch, cur_step)\n",
    "        best_top1 = max(best_top1, top1)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(), folder + \"mod.json\")\n",
    "    # torch.save(model.state_dict(), args.save_folder + \"/mod.json\")\n",
    "    print(\"Final best Prec@1 = {:.4%}\".format(best_top1))\n",
    "    best_top1s.append(best_top1)\n",
    "    print(best_top1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from nni.retiarii.oneshot.pytorch.utils import AverageMeter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger('nni')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "config = {\n",
    "'layers' : 2,\n",
    "'batch_size' : 96,\n",
    "'log_frequency' : 30,\n",
    "'epochs' : 10,\n",
    "'aux_weight' : 0.4,\n",
    "'drop_path_prob' : 0.1,\n",
    "'workers' : 4,\n",
    "'grad_clip' : 5.,\n",
    "'save_folder' : \"./checkpoints/fashionMNIST/\",\n",
    "}\n",
    "\n",
    "dataset_train, dataset_valid = datasets.get_dataset(\"fashionmnist\", cutout_length=16)\n",
    "\n",
    "res_dict_accur = {}\n",
    "models = []\n",
    "\n",
    "# chosen_lambdas = np.random.choice(8, size=3, replace=False) # выбранные lambda\n",
    "chosen_lambdas = (1)\n",
    "\n",
    "print(chosen_lambdas)\n",
    "\n",
    "with fixed_arch(config['save_folder'] + \"optimal/arc.json\"):\n",
    "    model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(config['save_folder'] + \"optimal/mod.json\"))\n",
    "    model.eval()\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "for dir in glob(config['save_folder'] + \"*\"):\n",
    "    if dir.split('\\\\')[-1] != 'optimal' and float(dir.split('\\\\')[-1].split('=')[-1]) in chosen_lambdas:\n",
    "    # if dir == \"./checkpoints\\\\0\":\n",
    "        print(dir)\n",
    "        with fixed_arch(dir + \"/arc.json\"):\n",
    "            model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(dir + \"/mod.json\"))\n",
    "        model.eval()\n",
    "        \n",
    "        models.append(model)\n",
    "\n",
    "print(f\"Models in ensemble: {len(models)}\")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config['workers'],\n",
    "                                            pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "top1 = AverageMeter(\"top1\")\n",
    "top5 = AverageMeter(\"top5\")\n",
    "losses = AverageMeter(\"losses\")\n",
    "\n",
    "# validation\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for step, (X, y) in enumerate(valid_loader):\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        bs = X.size(0)\n",
    "\n",
    "        probabilities = softmax(models[0](X))\n",
    "        for i in range(1, len(models)):\n",
    "            probabilities += softmax(models[i](X))\n",
    "        probabilities = probabilities / len(models)\n",
    "        loss = criterion(probabilities, y)\n",
    "\n",
    "        accuracy = utils.accuracy(probabilities, y, topk=(1, 5))\n",
    "        losses.update(loss.item(), bs)\n",
    "        top1.update(accuracy[\"acc1\"], bs)\n",
    "        top5.update(accuracy[\"acc5\"], bs)\n",
    "\n",
    "        if step % config['log_frequency'] == 0 or step == len(valid_loader) - 1:\n",
    "            logger.info(\n",
    "                \"Valid: Step {:03d}/{:03d} Loss {losses.avg:.3f} \"\n",
    "                \"Prec@(1,5) ({top1.avg:.1%}, {top5.avg:.1%})\".format(\n",
    "                    step, len(valid_loader) - 1, losses=losses,\n",
    "                    top1=top1, top5=top5))\n",
    "\n",
    "logger.info(\"Final best Prec@1 = {:.4%}\".format(top1.avg))\n",
    "\n",
    "res_dict_accur[chosen_lambdas] = top1.avg\n",
    "print(res_dict_accur)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
