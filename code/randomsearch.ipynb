{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pkbab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import datasets\n",
    "from model import CNN, MyDartsTrainer\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 1\n",
    "batch_size = 64\n",
    "log_frequency = 40\n",
    "channels = 16\n",
    "unrolled = False\n",
    "visualization = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cifar100\"\n",
    "\n",
    "dataset_train, dataset_valid = datasets.get_dataset(dataset)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture search for a range of $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Step [1/391]  acc1 0.015625 (0.015625)  loss 4.607870 (4.607870)\n",
      "Epoch [1/50] Step [41/391]  acc1 0.031250 (0.020579)  loss 4.443074 (4.575597)\n",
      "Epoch [1/50] Step [81/391]  acc1 0.046875 (0.027199)  loss 4.323179 (4.490315)\n",
      "Epoch [1/50] Step [121/391]  acc1 0.031250 (0.037319)  loss 4.183802 (4.405519)\n",
      "Epoch [1/50] Step [161/391]  acc1 0.046875 (0.042508)  loss 4.205765 (4.353607)\n",
      "Epoch [1/50] Step [201/391]  acc1 0.109375 (0.048896)  loss 3.841792 (4.297391)\n",
      "Epoch [1/50] Step [241/391]  acc1 0.093750 (0.054914)  loss 4.039520 (4.254007)\n",
      "Epoch [1/50] Step [281/391]  acc1 0.093750 (0.058997)  loss 3.999937 (4.225382)\n",
      "Epoch [1/50] Step [321/391]  acc1 0.093750 (0.062646)  loss 3.935636 (4.198299)\n",
      "Epoch [1/50] Step [361/391]  acc1 0.062500 (0.065789)  loss 3.965824 (4.169684)\n",
      "Epoch [2/50] Step [1/391]  acc1 0.093750 (0.093750)  loss 3.886081 (3.886081)\n",
      "Epoch [2/50] Step [41/391]  acc1 0.125000 (0.117759)  loss 3.647569 (3.896741)\n",
      "Epoch [2/50] Step [81/391]  acc1 0.125000 (0.113812)  loss 3.875706 (3.893932)\n",
      "Epoch [2/50] Step [121/391]  acc1 0.171875 (0.111183)  loss 3.861147 (3.888481)\n",
      "Epoch [2/50] Step [161/391]  acc1 0.109375 (0.112869)  loss 3.842346 (3.876326)\n",
      "Epoch [2/50] Step [201/391]  acc1 0.093750 (0.113961)  loss 3.748374 (3.868476)\n",
      "Epoch [2/50] Step [241/391]  acc1 0.125000 (0.115340)  loss 3.756811 (3.862275)\n",
      "Epoch [2/50] Step [281/391]  acc1 0.125000 (0.116270)  loss 3.860070 (3.852349)\n",
      "Epoch [2/50] Step [321/391]  acc1 0.203125 (0.117650)  loss 3.566267 (3.843735)\n",
      "Epoch [2/50] Step [361/391]  acc1 0.093750 (0.118421)  loss 3.819485 (3.836830)\n",
      "Epoch [3/50] Step [1/391]  acc1 0.093750 (0.093750)  loss 3.749399 (3.749399)\n",
      "Epoch [3/50] Step [41/391]  acc1 0.109375 (0.127287)  loss 3.886918 (3.730100)\n",
      "Epoch [3/50] Step [81/391]  acc1 0.156250 (0.131173)  loss 3.787801 (3.717025)\n",
      "Epoch [3/50] Step [121/391]  acc1 0.140625 (0.137526)  loss 3.722206 (3.697499)\n",
      "Epoch [3/50] Step [161/391]  acc1 0.078125 (0.139946)  loss 3.494662 (3.679415)\n",
      "Epoch [3/50] Step [201/391]  acc1 0.109375 (0.142802)  loss 3.759283 (3.662979)\n",
      "Epoch [3/50] Step [241/391]  acc1 0.156250 (0.144191)  loss 3.724141 (3.652866)\n",
      "Epoch [3/50] Step [281/391]  acc1 0.109375 (0.146742)  loss 3.564386 (3.644238)\n",
      "Epoch [3/50] Step [321/391]  acc1 0.156250 (0.147780)  loss 3.507321 (3.637749)\n",
      "Epoch [3/50] Step [361/391]  acc1 0.109375 (0.149152)  loss 3.600938 (3.631827)\n",
      "Epoch [4/50] Step [1/391]  acc1 0.156250 (0.156250)  loss 3.704835 (3.704835)\n",
      "Epoch [4/50] Step [41/391]  acc1 0.187500 (0.180640)  loss 3.328113 (3.501189)\n",
      "Epoch [4/50] Step [81/391]  acc1 0.140625 (0.179205)  loss 3.590291 (3.496758)\n",
      "Epoch [4/50] Step [121/391]  acc1 0.140625 (0.179106)  loss 3.653552 (3.488545)\n",
      "Epoch [4/50] Step [161/391]  acc1 0.187500 (0.178960)  loss 3.306753 (3.480721)\n",
      "Epoch [4/50] Step [201/391]  acc1 0.218750 (0.178716)  loss 3.321978 (3.489021)\n",
      "Epoch [4/50] Step [241/391]  acc1 0.140625 (0.180109)  loss 3.439290 (3.478919)\n",
      "Epoch [4/50] Step [281/391]  acc1 0.203125 (0.181773)  loss 3.382185 (3.469201)\n",
      "Epoch [4/50] Step [321/391]  acc1 0.171875 (0.181659)  loss 3.268214 (3.460552)\n",
      "Epoch [4/50] Step [361/391]  acc1 0.203125 (0.179623)  loss 3.351324 (3.460676)\n",
      "Epoch [5/50] Step [1/391]  acc1 0.093750 (0.093750)  loss 3.720115 (3.720115)\n",
      "Epoch [5/50] Step [41/391]  acc1 0.156250 (0.177591)  loss 3.481983 (3.441269)\n",
      "Epoch [5/50] Step [81/391]  acc1 0.187500 (0.187114)  loss 3.389743 (3.402106)\n",
      "Epoch [5/50] Step [121/391]  acc1 0.171875 (0.189566)  loss 3.344031 (3.374157)\n",
      "Epoch [5/50] Step [161/391]  acc1 0.234375 (0.195361)  loss 3.245409 (3.360960)\n",
      "Epoch [5/50] Step [201/391]  acc1 0.171875 (0.195740)  loss 3.452677 (3.351817)\n",
      "Epoch [5/50] Step [241/391]  acc1 0.250000 (0.196253)  loss 3.218917 (3.349928)\n",
      "Epoch [5/50] Step [281/391]  acc1 0.140625 (0.197398)  loss 3.495861 (3.335174)\n",
      "Epoch [5/50] Step [321/391]  acc1 0.140625 (0.199085)  loss 3.375937 (3.327004)\n",
      "Epoch [5/50] Step [361/391]  acc1 0.218750 (0.200701)  loss 3.097971 (3.315220)\n",
      "Epoch [6/50] Step [1/391]  acc1 0.187500 (0.187500)  loss 3.451498 (3.451498)\n",
      "Epoch [6/50] Step [41/391]  acc1 0.265625 (0.214939)  loss 3.207874 (3.229701)\n",
      "Epoch [6/50] Step [81/391]  acc1 0.281250 (0.220100)  loss 3.051041 (3.221478)\n",
      "Epoch [6/50] Step [121/391]  acc1 0.234375 (0.221074)  loss 3.308714 (3.226647)\n",
      "Epoch [6/50] Step [161/391]  acc1 0.171875 (0.222438)  loss 3.371397 (3.220515)\n",
      "Epoch [6/50] Step [201/391]  acc1 0.187500 (0.220227)  loss 3.165218 (3.220396)\n",
      "Epoch [6/50] Step [241/391]  acc1 0.171875 (0.223094)  loss 3.544248 (3.211100)\n",
      "Epoch [6/50] Step [281/391]  acc1 0.296875 (0.224310)  loss 3.116038 (3.202098)\n",
      "Epoch [6/50] Step [321/391]  acc1 0.359375 (0.224153)  loss 2.995168 (3.197168)\n",
      "Epoch [6/50] Step [361/391]  acc1 0.312500 (0.222126)  loss 2.871490 (3.192402)\n",
      "Epoch [7/50] Step [1/391]  acc1 0.203125 (0.203125)  loss 3.189541 (3.189541)\n",
      "Epoch [7/50] Step [41/391]  acc1 0.281250 (0.245427)  loss 2.807693 (3.074438)\n",
      "Epoch [7/50] Step [81/391]  acc1 0.281250 (0.247878)  loss 3.097625 (3.071134)\n",
      "Epoch [7/50] Step [121/391]  acc1 0.234375 (0.239282)  loss 3.247918 (3.109909)\n",
      "Epoch [7/50] Step [161/391]  acc1 0.250000 (0.236801)  loss 3.147001 (3.118486)\n",
      "Epoch [7/50] Step [201/391]  acc1 0.218750 (0.239661)  loss 3.042047 (3.101907)\n",
      "Epoch [7/50] Step [241/391]  acc1 0.203125 (0.239367)  loss 3.354783 (3.096576)\n",
      "Epoch [7/50] Step [281/391]  acc1 0.234375 (0.240658)  loss 3.288947 (3.096472)\n",
      "Epoch [7/50] Step [321/391]  acc1 0.265625 (0.240654)  loss 3.009858 (3.091552)\n",
      "Epoch [7/50] Step [361/391]  acc1 0.312500 (0.241214)  loss 2.689063 (3.085898)\n",
      "Epoch [8/50] Step [1/391]  acc1 0.296875 (0.296875)  loss 3.035257 (3.035257)\n",
      "Epoch [8/50] Step [41/391]  acc1 0.187500 (0.243902)  loss 3.407148 (3.023080)\n",
      "Epoch [8/50] Step [81/391]  acc1 0.250000 (0.250193)  loss 2.799804 (3.011398)\n",
      "Epoch [8/50] Step [121/391]  acc1 0.171875 (0.255165)  loss 3.057309 (2.998610)\n",
      "Epoch [8/50] Step [161/391]  acc1 0.281250 (0.254852)  loss 2.987143 (3.008880)\n",
      "Epoch [8/50] Step [201/391]  acc1 0.312500 (0.257618)  loss 2.780698 (2.998617)\n",
      "Epoch [8/50] Step [241/391]  acc1 0.203125 (0.257845)  loss 2.831497 (2.989608)\n",
      "Epoch [8/50] Step [281/391]  acc1 0.281250 (0.259342)  loss 3.267165 (2.986373)\n",
      "Epoch [8/50] Step [321/391]  acc1 0.281250 (0.257058)  loss 2.937996 (2.988822)\n",
      "Epoch [8/50] Step [361/391]  acc1 0.250000 (0.256579)  loss 2.921400 (2.988898)\n",
      "Epoch [9/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.835187 (2.835187)\n",
      "Epoch [9/50] Step [41/391]  acc1 0.375000 (0.272485)  loss 2.744162 (2.919177)\n",
      "Epoch [9/50] Step [81/391]  acc1 0.343750 (0.265432)  loss 2.699915 (2.933920)\n",
      "Epoch [9/50] Step [121/391]  acc1 0.312500 (0.268466)  loss 2.724798 (2.919300)\n",
      "Epoch [9/50] Step [161/391]  acc1 0.234375 (0.266887)  loss 2.827679 (2.923950)\n",
      "Epoch [9/50] Step [201/391]  acc1 0.328125 (0.269823)  loss 2.638386 (2.910107)\n",
      "Epoch [9/50] Step [241/391]  acc1 0.312500 (0.268478)  loss 2.926319 (2.914235)\n",
      "Epoch [9/50] Step [281/391]  acc1 0.203125 (0.270518)  loss 3.166637 (2.912384)\n",
      "Epoch [9/50] Step [321/391]  acc1 0.296875 (0.269860)  loss 2.928653 (2.910208)\n",
      "Epoch [9/50] Step [361/391]  acc1 0.265625 (0.269650)  loss 2.965155 (2.908951)\n",
      "Epoch [10/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.809120 (2.809120)\n",
      "Epoch [10/50] Step [41/391]  acc1 0.312500 (0.278582)  loss 2.804044 (2.843819)\n",
      "Epoch [10/50] Step [81/391]  acc1 0.265625 (0.284915)  loss 2.893159 (2.831137)\n",
      "Epoch [10/50] Step [121/391]  acc1 0.218750 (0.284607)  loss 3.247776 (2.829794)\n",
      "Epoch [10/50] Step [161/391]  acc1 0.375000 (0.288238)  loss 2.657644 (2.820383)\n",
      "Epoch [10/50] Step [201/391]  acc1 0.250000 (0.287702)  loss 3.129960 (2.820845)\n",
      "Epoch [10/50] Step [241/391]  acc1 0.312500 (0.286177)  loss 3.128254 (2.822972)\n",
      "Epoch [10/50] Step [281/391]  acc1 0.312500 (0.285476)  loss 2.855991 (2.825837)\n",
      "Epoch [10/50] Step [321/391]  acc1 0.296875 (0.286750)  loss 3.049321 (2.824181)\n",
      "Epoch [10/50] Step [361/391]  acc1 0.328125 (0.285795)  loss 2.698693 (2.824573)\n",
      "Epoch [11/50] Step [1/391]  acc1 0.265625 (0.265625)  loss 2.963032 (2.963032)\n",
      "Epoch [11/50] Step [41/391]  acc1 0.203125 (0.292683)  loss 2.727712 (2.778445)\n",
      "Epoch [11/50] Step [81/391]  acc1 0.312500 (0.293210)  loss 2.807388 (2.765742)\n",
      "Epoch [11/50] Step [121/391]  acc1 0.312500 (0.289644)  loss 2.793040 (2.774743)\n",
      "Epoch [11/50] Step [161/391]  acc1 0.312500 (0.291246)  loss 2.728274 (2.768929)\n",
      "Epoch [11/50] Step [201/391]  acc1 0.312500 (0.290889)  loss 2.573215 (2.771311)\n",
      "Epoch [11/50] Step [241/391]  acc1 0.328125 (0.293244)  loss 2.608317 (2.767305)\n",
      "Epoch [11/50] Step [281/391]  acc1 0.484375 (0.295040)  loss 2.413002 (2.763972)\n",
      "Epoch [11/50] Step [321/391]  acc1 0.343750 (0.296096)  loss 2.773974 (2.763702)\n",
      "Epoch [11/50] Step [361/391]  acc1 0.312500 (0.296572)  loss 2.675743 (2.759209)\n",
      "Epoch [12/50] Step [1/391]  acc1 0.250000 (0.250000)  loss 2.502127 (2.502127)\n",
      "Epoch [12/50] Step [41/391]  acc1 0.281250 (0.319360)  loss 2.648958 (2.693052)\n",
      "Epoch [12/50] Step [81/391]  acc1 0.281250 (0.322917)  loss 2.748729 (2.676571)\n",
      "Epoch [12/50] Step [121/391]  acc1 0.343750 (0.322056)  loss 2.677524 (2.678743)\n",
      "Epoch [12/50] Step [161/391]  acc1 0.328125 (0.314053)  loss 2.932310 (2.693885)\n",
      "Epoch [12/50] Step [201/391]  acc1 0.265625 (0.311723)  loss 3.055059 (2.701618)\n",
      "Epoch [12/50] Step [241/391]  acc1 0.390625 (0.314251)  loss 2.436983 (2.696932)\n",
      "Epoch [12/50] Step [281/391]  acc1 0.343750 (0.312556)  loss 2.529955 (2.698988)\n",
      "Epoch [12/50] Step [321/391]  acc1 0.265625 (0.313571)  loss 2.807630 (2.693760)\n",
      "Epoch [12/50] Step [361/391]  acc1 0.218750 (0.310552)  loss 3.022268 (2.702382)\n",
      "Epoch [13/50] Step [1/391]  acc1 0.265625 (0.265625)  loss 2.887841 (2.887841)\n",
      "Epoch [13/50] Step [41/391]  acc1 0.328125 (0.339558)  loss 2.754237 (2.609887)\n",
      "Epoch [13/50] Step [81/391]  acc1 0.281250 (0.324267)  loss 2.643426 (2.649739)\n",
      "Epoch [13/50] Step [121/391]  acc1 0.312500 (0.319990)  loss 2.568803 (2.658672)\n",
      "Epoch [13/50] Step [161/391]  acc1 0.234375 (0.316964)  loss 2.826668 (2.657793)\n",
      "Epoch [13/50] Step [201/391]  acc1 0.390625 (0.316154)  loss 2.374949 (2.660642)\n",
      "Epoch [13/50] Step [241/391]  acc1 0.265625 (0.317946)  loss 2.447945 (2.651091)\n",
      "Epoch [13/50] Step [281/391]  acc1 0.265625 (0.317171)  loss 2.613837 (2.649824)\n",
      "Epoch [13/50] Step [321/391]  acc1 0.375000 (0.317806)  loss 2.516339 (2.644543)\n",
      "Epoch [13/50] Step [361/391]  acc1 0.343750 (0.319901)  loss 2.629990 (2.641590)\n",
      "Epoch [14/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.447127 (2.447127)\n",
      "Epoch [14/50] Step [41/391]  acc1 0.296875 (0.324695)  loss 2.467529 (2.585813)\n",
      "Epoch [14/50] Step [81/391]  acc1 0.375000 (0.326582)  loss 2.554657 (2.591016)\n",
      "Epoch [14/50] Step [121/391]  acc1 0.296875 (0.326446)  loss 2.586671 (2.577645)\n",
      "Epoch [14/50] Step [161/391]  acc1 0.390625 (0.326863)  loss 2.482359 (2.584363)\n",
      "Epoch [14/50] Step [201/391]  acc1 0.375000 (0.330146)  loss 2.572698 (2.585164)\n",
      "Epoch [14/50] Step [241/391]  acc1 0.312500 (0.330848)  loss 2.521174 (2.589277)\n",
      "Epoch [14/50] Step [281/391]  acc1 0.375000 (0.331183)  loss 2.392997 (2.589083)\n",
      "Epoch [14/50] Step [321/391]  acc1 0.296875 (0.333869)  loss 2.457219 (2.583871)\n",
      "Epoch [14/50] Step [361/391]  acc1 0.390625 (0.332626)  loss 2.579703 (2.583026)\n",
      "Epoch [15/50] Step [1/391]  acc1 0.312500 (0.312500)  loss 2.621011 (2.621011)\n",
      "Epoch [15/50] Step [41/391]  acc1 0.390625 (0.349848)  loss 2.384753 (2.553503)\n",
      "Epoch [15/50] Step [81/391]  acc1 0.359375 (0.339120)  loss 2.591517 (2.557833)\n",
      "Epoch [15/50] Step [121/391]  acc1 0.281250 (0.344783)  loss 2.762480 (2.551251)\n",
      "Epoch [15/50] Step [161/391]  acc1 0.234375 (0.341809)  loss 2.805264 (2.561748)\n",
      "Epoch [15/50] Step [201/391]  acc1 0.421875 (0.345538)  loss 2.267715 (2.550384)\n",
      "Epoch [15/50] Step [241/391]  acc1 0.437500 (0.345501)  loss 2.451176 (2.547049)\n",
      "Epoch [15/50] Step [281/391]  acc1 0.359375 (0.345363)  loss 2.542510 (2.544824)\n",
      "Epoch [15/50] Step [321/391]  acc1 0.296875 (0.346086)  loss 2.696910 (2.537562)\n",
      "Epoch [15/50] Step [361/391]  acc1 0.375000 (0.348684)  loss 2.582852 (2.531800)\n",
      "Epoch [16/50] Step [1/391]  acc1 0.375000 (0.375000)  loss 2.567477 (2.567477)\n",
      "Epoch [16/50] Step [41/391]  acc1 0.421875 (0.357851)  loss 2.573775 (2.527971)\n",
      "Epoch [16/50] Step [81/391]  acc1 0.312500 (0.360532)  loss 2.678561 (2.483568)\n",
      "Epoch [16/50] Step [121/391]  acc1 0.359375 (0.362216)  loss 2.478471 (2.476687)\n",
      "Epoch [16/50] Step [161/391]  acc1 0.328125 (0.359569)  loss 2.310369 (2.479428)\n",
      "Epoch [16/50] Step [201/391]  acc1 0.250000 (0.357587)  loss 2.515867 (2.483102)\n",
      "Epoch [16/50] Step [241/391]  acc1 0.296875 (0.358013)  loss 2.565376 (2.480359)\n",
      "Epoch [16/50] Step [281/391]  acc1 0.265625 (0.358263)  loss 2.638832 (2.481080)\n",
      "Epoch [16/50] Step [321/391]  acc1 0.375000 (0.356552)  loss 2.349762 (2.482133)\n",
      "Epoch [16/50] Step [361/391]  acc1 0.546875 (0.356562)  loss 2.052803 (2.481287)\n",
      "Epoch [17/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.677270 (2.677270)\n",
      "Epoch [17/50] Step [41/391]  acc1 0.265625 (0.368902)  loss 2.791356 (2.447274)\n",
      "Epoch [17/50] Step [81/391]  acc1 0.453125 (0.365162)  loss 2.041854 (2.447203)\n",
      "Epoch [17/50] Step [121/391]  acc1 0.359375 (0.362603)  loss 2.655900 (2.445165)\n",
      "Epoch [17/50] Step [161/391]  acc1 0.437500 (0.358307)  loss 2.101997 (2.451162)\n",
      "Epoch [17/50] Step [201/391]  acc1 0.546875 (0.360930)  loss 2.205100 (2.448178)\n",
      "Epoch [17/50] Step [241/391]  acc1 0.375000 (0.361385)  loss 2.342165 (2.447562)\n",
      "Epoch [17/50] Step [281/391]  acc1 0.343750 (0.359987)  loss 2.595747 (2.449580)\n",
      "Epoch [17/50] Step [321/391]  acc1 0.406250 (0.361273)  loss 2.301561 (2.446527)\n",
      "Epoch [17/50] Step [361/391]  acc1 0.453125 (0.361063)  loss 2.096486 (2.445707)\n",
      "Epoch [18/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.272559 (2.272559)\n",
      "Epoch [18/50] Step [41/391]  acc1 0.406250 (0.362805)  loss 2.214477 (2.401979)\n",
      "Epoch [18/50] Step [81/391]  acc1 0.406250 (0.379051)  loss 2.120808 (2.382184)\n",
      "Epoch [18/50] Step [121/391]  acc1 0.343750 (0.378487)  loss 2.670120 (2.375708)\n",
      "Epoch [18/50] Step [161/391]  acc1 0.390625 (0.378397)  loss 2.229251 (2.385050)\n",
      "Epoch [18/50] Step [201/391]  acc1 0.390625 (0.377410)  loss 2.269588 (2.391900)\n",
      "Epoch [18/50] Step [241/391]  acc1 0.328125 (0.376686)  loss 2.435259 (2.390335)\n",
      "Epoch [18/50] Step [281/391]  acc1 0.390625 (0.373832)  loss 2.224502 (2.398786)\n",
      "Epoch [18/50] Step [321/391]  acc1 0.234375 (0.373053)  loss 2.791155 (2.401339)\n",
      "Epoch [18/50] Step [361/391]  acc1 0.390625 (0.372663)  loss 2.603360 (2.399341)\n",
      "Epoch [19/50] Step [1/391]  acc1 0.296875 (0.296875)  loss 2.495806 (2.495806)\n",
      "Epoch [19/50] Step [41/391]  acc1 0.296875 (0.383003)  loss 2.502228 (2.369292)\n",
      "Epoch [19/50] Step [81/391]  acc1 0.406250 (0.381366)  loss 2.287282 (2.361939)\n",
      "Epoch [19/50] Step [121/391]  acc1 0.406250 (0.388688)  loss 2.176581 (2.338162)\n",
      "Epoch [19/50] Step [161/391]  acc1 0.500000 (0.388975)  loss 2.068248 (2.338654)\n",
      "Epoch [19/50] Step [201/391]  acc1 0.515625 (0.385650)  loss 2.130096 (2.343066)\n",
      "Epoch [19/50] Step [241/391]  acc1 0.312500 (0.383688)  loss 2.440789 (2.349226)\n",
      "Epoch [19/50] Step [281/391]  acc1 0.437500 (0.382340)  loss 2.205644 (2.350765)\n",
      "Epoch [19/50] Step [321/391]  acc1 0.312500 (0.381620)  loss 2.305099 (2.351619)\n",
      "Epoch [19/50] Step [361/391]  acc1 0.437500 (0.382445)  loss 2.391528 (2.351011)\n",
      "Epoch [20/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.494564 (2.494564)\n",
      "Epoch [20/50] Step [41/391]  acc1 0.484375 (0.381479)  loss 2.187479 (2.346197)\n",
      "Epoch [20/50] Step [81/391]  acc1 0.421875 (0.390432)  loss 2.272333 (2.314703)\n",
      "Epoch [20/50] Step [121/391]  acc1 0.296875 (0.386105)  loss 2.443347 (2.329172)\n",
      "Epoch [20/50] Step [161/391]  acc1 0.390625 (0.383637)  loss 2.235591 (2.340517)\n",
      "Epoch [20/50] Step [201/391]  acc1 0.406250 (0.383784)  loss 2.354876 (2.329597)\n",
      "Epoch [20/50] Step [241/391]  acc1 0.406250 (0.386087)  loss 2.263400 (2.329361)\n",
      "Epoch [20/50] Step [281/391]  acc1 0.328125 (0.385899)  loss 2.530464 (2.330229)\n",
      "Epoch [20/50] Step [321/391]  acc1 0.328125 (0.385027)  loss 2.323487 (2.333213)\n",
      "Epoch [20/50] Step [361/391]  acc1 0.421875 (0.385474)  loss 2.301410 (2.329631)\n",
      "Epoch [21/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 2.189146 (2.189146)\n",
      "Epoch [21/50] Step [41/391]  acc1 0.531250 (0.403582)  loss 1.969879 (2.238137)\n",
      "Epoch [21/50] Step [81/391]  acc1 0.265625 (0.407600)  loss 2.622696 (2.228160)\n",
      "Epoch [21/50] Step [121/391]  acc1 0.343750 (0.403796)  loss 2.459032 (2.251537)\n",
      "Epoch [21/50] Step [161/391]  acc1 0.328125 (0.397904)  loss 2.278388 (2.262044)\n",
      "Epoch [21/50] Step [201/391]  acc1 0.250000 (0.391169)  loss 2.405702 (2.286274)\n",
      "Epoch [21/50] Step [241/391]  acc1 0.390625 (0.393024)  loss 2.272039 (2.288379)\n",
      "Epoch [21/50] Step [281/391]  acc1 0.437500 (0.393072)  loss 2.296484 (2.283283)\n",
      "Epoch [21/50] Step [321/391]  acc1 0.484375 (0.393302)  loss 2.220088 (2.285858)\n",
      "Epoch [21/50] Step [361/391]  acc1 0.375000 (0.394174)  loss 2.142958 (2.285904)\n",
      "Epoch [22/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 2.247013 (2.247013)\n",
      "Epoch [22/50] Step [41/391]  acc1 0.468750 (0.412729)  loss 2.146557 (2.213127)\n",
      "Epoch [22/50] Step [81/391]  acc1 0.484375 (0.401427)  loss 1.977081 (2.258396)\n",
      "Epoch [22/50] Step [121/391]  acc1 0.437500 (0.402247)  loss 2.161864 (2.248356)\n",
      "Epoch [22/50] Step [161/391]  acc1 0.312500 (0.400815)  loss 2.420158 (2.251516)\n",
      "Epoch [22/50] Step [201/391]  acc1 0.343750 (0.399876)  loss 2.261240 (2.259273)\n",
      "Epoch [22/50] Step [241/391]  acc1 0.453125 (0.402036)  loss 2.173867 (2.255956)\n",
      "Epoch [22/50] Step [281/391]  acc1 0.421875 (0.402747)  loss 2.409149 (2.258283)\n",
      "Epoch [22/50] Step [321/391]  acc1 0.328125 (0.401674)  loss 2.489642 (2.264143)\n",
      "Epoch [22/50] Step [361/391]  acc1 0.406250 (0.399238)  loss 2.242988 (2.269269)\n",
      "Epoch [23/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 1.957320 (1.957320)\n",
      "Epoch [23/50] Step [41/391]  acc1 0.437500 (0.431402)  loss 2.266739 (2.195040)\n",
      "Epoch [23/50] Step [81/391]  acc1 0.312500 (0.414352)  loss 2.182296 (2.221122)\n",
      "Epoch [23/50] Step [121/391]  acc1 0.328125 (0.413094)  loss 2.321942 (2.235855)\n",
      "Epoch [23/50] Step [161/391]  acc1 0.453125 (0.406541)  loss 2.059198 (2.249715)\n",
      "Epoch [23/50] Step [201/391]  acc1 0.453125 (0.406950)  loss 2.227593 (2.245349)\n",
      "Epoch [23/50] Step [241/391]  acc1 0.390625 (0.403786)  loss 2.191944 (2.254468)\n",
      "Epoch [23/50] Step [281/391]  acc1 0.453125 (0.403581)  loss 2.383126 (2.250746)\n",
      "Epoch [23/50] Step [321/391]  acc1 0.359375 (0.403524)  loss 2.255714 (2.249087)\n",
      "Epoch [23/50] Step [361/391]  acc1 0.296875 (0.403826)  loss 2.307732 (2.243685)\n",
      "Epoch [24/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.231078 (2.231078)\n",
      "Epoch [24/50] Step [41/391]  acc1 0.390625 (0.419588)  loss 2.019823 (2.183840)\n",
      "Epoch [24/50] Step [81/391]  acc1 0.437500 (0.423032)  loss 2.243897 (2.182217)\n",
      "Epoch [24/50] Step [121/391]  acc1 0.406250 (0.416322)  loss 2.502243 (2.207378)\n",
      "Epoch [24/50] Step [161/391]  acc1 0.390625 (0.412655)  loss 2.200796 (2.223598)\n",
      "Epoch [24/50] Step [201/391]  acc1 0.484375 (0.411614)  loss 1.857318 (2.222397)\n",
      "Epoch [24/50] Step [241/391]  acc1 0.390625 (0.411242)  loss 2.210903 (2.221064)\n",
      "Epoch [24/50] Step [281/391]  acc1 0.562500 (0.411977)  loss 1.980485 (2.214368)\n",
      "Epoch [24/50] Step [321/391]  acc1 0.359375 (0.411896)  loss 2.255451 (2.214490)\n",
      "Epoch [24/50] Step [361/391]  acc1 0.406250 (0.412483)  loss 2.378711 (2.214010)\n",
      "Epoch [25/50] Step [1/391]  acc1 0.484375 (0.484375)  loss 2.105857 (2.105857)\n",
      "Epoch [25/50] Step [41/391]  acc1 0.421875 (0.421494)  loss 2.067677 (2.174289)\n",
      "Epoch [25/50] Step [81/391]  acc1 0.515625 (0.423611)  loss 1.923323 (2.156755)\n",
      "Epoch [25/50] Step [121/391]  acc1 0.375000 (0.415677)  loss 2.318435 (2.180876)\n",
      "Epoch [25/50] Step [161/391]  acc1 0.421875 (0.418964)  loss 2.030426 (2.175905)\n",
      "Epoch [25/50] Step [201/391]  acc1 0.453125 (0.421020)  loss 1.980208 (2.167532)\n",
      "Epoch [25/50] Step [241/391]  acc1 0.421875 (0.419735)  loss 2.347590 (2.175152)\n",
      "Epoch [25/50] Step [281/391]  acc1 0.625000 (0.421319)  loss 1.689934 (2.172536)\n",
      "Epoch [25/50] Step [321/391]  acc1 0.453125 (0.420755)  loss 2.256598 (2.181531)\n",
      "Epoch [25/50] Step [361/391]  acc1 0.406250 (0.422005)  loss 2.381889 (2.175953)\n",
      "Epoch [26/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.266551 (2.266551)\n",
      "Epoch [26/50] Step [41/391]  acc1 0.437500 (0.421494)  loss 2.205076 (2.123758)\n",
      "Epoch [26/50] Step [81/391]  acc1 0.437500 (0.433063)  loss 2.164840 (2.123732)\n",
      "Epoch [26/50] Step [121/391]  acc1 0.500000 (0.433368)  loss 2.104609 (2.119869)\n",
      "Epoch [26/50] Step [161/391]  acc1 0.328125 (0.430609)  loss 2.658762 (2.128468)\n",
      "Epoch [26/50] Step [201/391]  acc1 0.468750 (0.430581)  loss 1.962790 (2.126311)\n",
      "Epoch [26/50] Step [241/391]  acc1 0.328125 (0.427321)  loss 2.550214 (2.150941)\n",
      "Epoch [26/50] Step [281/391]  acc1 0.390625 (0.425767)  loss 2.255955 (2.157788)\n",
      "Epoch [26/50] Step [321/391]  acc1 0.390625 (0.426694)  loss 2.317466 (2.152305)\n",
      "Epoch [26/50] Step [361/391]  acc1 0.468750 (0.424775)  loss 1.983782 (2.153624)\n",
      "Epoch [27/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.254914 (2.254914)\n",
      "Epoch [27/50] Step [41/391]  acc1 0.265625 (0.410061)  loss 2.454614 (2.176723)\n",
      "Epoch [27/50] Step [81/391]  acc1 0.312500 (0.421296)  loss 2.444651 (2.167550)\n",
      "Epoch [27/50] Step [121/391]  acc1 0.359375 (0.424974)  loss 2.147321 (2.144255)\n",
      "Epoch [27/50] Step [161/391]  acc1 0.453125 (0.426825)  loss 2.049924 (2.143186)\n",
      "Epoch [27/50] Step [201/391]  acc1 0.375000 (0.426539)  loss 2.274338 (2.140341)\n",
      "Epoch [27/50] Step [241/391]  acc1 0.500000 (0.426284)  loss 2.145530 (2.145140)\n",
      "Epoch [27/50] Step [281/391]  acc1 0.343750 (0.425990)  loss 2.332818 (2.146395)\n",
      "Epoch [27/50] Step [321/391]  acc1 0.390625 (0.426110)  loss 2.277954 (2.142949)\n",
      "Epoch [27/50] Step [361/391]  acc1 0.343750 (0.424861)  loss 2.177448 (2.141458)\n",
      "Epoch [28/50] Step [1/391]  acc1 0.343750 (0.343750)  loss 2.144500 (2.144500)\n",
      "Epoch [28/50] Step [41/391]  acc1 0.406250 (0.420351)  loss 2.156679 (2.142536)\n",
      "Epoch [28/50] Step [81/391]  acc1 0.421875 (0.431520)  loss 2.012528 (2.116082)\n",
      "Epoch [28/50] Step [121/391]  acc1 0.500000 (0.433755)  loss 1.913578 (2.110536)\n",
      "Epoch [28/50] Step [161/391]  acc1 0.515625 (0.434103)  loss 1.788147 (2.114269)\n",
      "Epoch [28/50] Step [201/391]  acc1 0.453125 (0.433380)  loss 1.891843 (2.113870)\n",
      "Epoch [28/50] Step [241/391]  acc1 0.406250 (0.434518)  loss 2.086893 (2.114158)\n",
      "Epoch [28/50] Step [281/391]  acc1 0.515625 (0.434442)  loss 2.066487 (2.115001)\n",
      "Epoch [28/50] Step [321/391]  acc1 0.421875 (0.434044)  loss 2.179272 (2.112081)\n",
      "Epoch [28/50] Step [361/391]  acc1 0.421875 (0.434643)  loss 2.071455 (2.108012)\n",
      "Epoch [29/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.143194 (2.143194)\n",
      "Epoch [29/50] Step [41/391]  acc1 0.468750 (0.429116)  loss 2.005916 (2.087034)\n",
      "Epoch [29/50] Step [81/391]  acc1 0.468750 (0.431520)  loss 1.913298 (2.105884)\n",
      "Epoch [29/50] Step [121/391]  acc1 0.421875 (0.437500)  loss 1.713929 (2.101679)\n",
      "Epoch [29/50] Step [161/391]  acc1 0.453125 (0.440314)  loss 2.376935 (2.082945)\n",
      "Epoch [29/50] Step [201/391]  acc1 0.531250 (0.443797)  loss 1.764197 (2.079762)\n",
      "Epoch [29/50] Step [241/391]  acc1 0.500000 (0.444243)  loss 2.143554 (2.073235)\n",
      "Epoch [29/50] Step [281/391]  acc1 0.546875 (0.442504)  loss 1.728660 (2.079630)\n",
      "Epoch [29/50] Step [321/391]  acc1 0.437500 (0.440956)  loss 2.035613 (2.088179)\n",
      "Epoch [29/50] Step [361/391]  acc1 0.359375 (0.440357)  loss 2.172256 (2.090818)\n",
      "Epoch [30/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.059510 (2.059510)\n",
      "Epoch [30/50] Step [41/391]  acc1 0.500000 (0.459223)  loss 1.627331 (2.001340)\n",
      "Epoch [30/50] Step [81/391]  acc1 0.484375 (0.454861)  loss 1.802824 (2.029682)\n",
      "Epoch [30/50] Step [121/391]  acc1 0.531250 (0.447056)  loss 1.842452 (2.048344)\n",
      "Epoch [30/50] Step [161/391]  acc1 0.437500 (0.447205)  loss 2.051610 (2.056600)\n",
      "Epoch [30/50] Step [201/391]  acc1 0.437500 (0.449238)  loss 1.838425 (2.054447)\n",
      "Epoch [30/50] Step [241/391]  acc1 0.484375 (0.448911)  loss 1.920619 (2.051686)\n",
      "Epoch [30/50] Step [281/391]  acc1 0.390625 (0.447843)  loss 2.030229 (2.052788)\n",
      "Epoch [30/50] Step [321/391]  acc1 0.421875 (0.446213)  loss 2.257404 (2.060827)\n",
      "Epoch [30/50] Step [361/391]  acc1 0.484375 (0.444901)  loss 2.046441 (2.066142)\n",
      "Epoch [31/50] Step [1/391]  acc1 0.484375 (0.484375)  loss 1.934888 (1.934888)\n",
      "Epoch [31/50] Step [41/391]  acc1 0.593750 (0.478659)  loss 1.669267 (1.943142)\n",
      "Epoch [31/50] Step [81/391]  acc1 0.375000 (0.462384)  loss 2.032119 (1.993860)\n",
      "Epoch [31/50] Step [121/391]  acc1 0.406250 (0.458678)  loss 2.097008 (2.006590)\n",
      "Epoch [31/50] Step [161/391]  acc1 0.343750 (0.459627)  loss 2.092089 (2.011982)\n",
      "Epoch [31/50] Step [201/391]  acc1 0.468750 (0.459188)  loss 2.149187 (2.016175)\n",
      "Epoch [31/50] Step [241/391]  acc1 0.500000 (0.456821)  loss 1.932066 (2.029541)\n",
      "Epoch [31/50] Step [281/391]  acc1 0.500000 (0.454070)  loss 1.942333 (2.037468)\n",
      "Epoch [31/50] Step [321/391]  acc1 0.375000 (0.454147)  loss 2.199271 (2.034274)\n",
      "Epoch [31/50] Step [361/391]  acc1 0.375000 (0.453255)  loss 2.358182 (2.037889)\n",
      "Epoch [32/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 1.968899 (1.968899)\n",
      "Epoch [32/50] Step [41/391]  acc1 0.390625 (0.461509)  loss 2.011518 (2.035481)\n",
      "Epoch [32/50] Step [81/391]  acc1 0.343750 (0.459684)  loss 2.435166 (2.021393)\n",
      "Epoch [32/50] Step [121/391]  acc1 0.390625 (0.454416)  loss 2.212559 (2.012545)\n",
      "Epoch [32/50] Step [161/391]  acc1 0.562500 (0.454969)  loss 1.725277 (2.018565)\n",
      "Epoch [32/50] Step [201/391]  acc1 0.578125 (0.452814)  loss 1.678743 (2.024218)\n",
      "Epoch [32/50] Step [241/391]  acc1 0.484375 (0.455589)  loss 1.955189 (2.023404)\n",
      "Epoch [32/50] Step [281/391]  acc1 0.484375 (0.454849)  loss 2.089608 (2.025017)\n",
      "Epoch [32/50] Step [321/391]  acc1 0.375000 (0.453514)  loss 2.070680 (2.024293)\n",
      "Epoch [32/50] Step [361/391]  acc1 0.484375 (0.452995)  loss 2.106006 (2.031840)\n",
      "Epoch [33/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.056128 (2.056128)\n",
      "Epoch [33/50] Step [41/391]  acc1 0.437500 (0.454649)  loss 1.986131 (2.056514)\n",
      "Epoch [33/50] Step [81/391]  acc1 0.406250 (0.455826)  loss 1.985420 (1.999659)\n",
      "Epoch [33/50] Step [121/391]  acc1 0.515625 (0.455062)  loss 1.753408 (2.013088)\n",
      "Epoch [33/50] Step [161/391]  acc1 0.328125 (0.455648)  loss 2.330877 (2.015944)\n",
      "Epoch [33/50] Step [201/391]  acc1 0.421875 (0.457867)  loss 2.041275 (2.009357)\n",
      "Epoch [33/50] Step [241/391]  acc1 0.484375 (0.458312)  loss 1.988651 (2.015891)\n",
      "Epoch [33/50] Step [281/391]  acc1 0.421875 (0.457796)  loss 2.264444 (2.014597)\n",
      "Epoch [33/50] Step [321/391]  acc1 0.515625 (0.459258)  loss 1.862374 (2.012033)\n",
      "Epoch [33/50] Step [361/391]  acc1 0.421875 (0.459141)  loss 1.960229 (2.009565)\n",
      "Epoch [34/50] Step [1/391]  acc1 0.546875 (0.546875)  loss 1.813562 (1.813562)\n",
      "Epoch [34/50] Step [41/391]  acc1 0.562500 (0.482088)  loss 1.749959 (1.966598)\n",
      "Epoch [34/50] Step [81/391]  acc1 0.468750 (0.475116)  loss 2.291329 (1.956852)\n",
      "Epoch [34/50] Step [121/391]  acc1 0.515625 (0.474948)  loss 1.662481 (1.953498)\n",
      "Epoch [34/50] Step [161/391]  acc1 0.500000 (0.464674)  loss 1.817311 (1.966663)\n",
      "Epoch [34/50] Step [201/391]  acc1 0.546875 (0.462298)  loss 1.770004 (1.980170)\n",
      "Epoch [34/50] Step [241/391]  acc1 0.437500 (0.461294)  loss 2.190536 (1.987490)\n",
      "Epoch [34/50] Step [281/391]  acc1 0.546875 (0.463690)  loss 1.942613 (1.985415)\n",
      "Epoch [34/50] Step [321/391]  acc1 0.546875 (0.463834)  loss 1.738608 (1.986495)\n",
      "Epoch [34/50] Step [361/391]  acc1 0.453125 (0.462474)  loss 2.031438 (1.987631)\n",
      "Epoch [35/50] Step [1/391]  acc1 0.578125 (0.578125)  loss 1.720784 (1.720784)\n",
      "Epoch [35/50] Step [41/391]  acc1 0.390625 (0.493140)  loss 1.745447 (1.922937)\n",
      "Epoch [35/50] Step [81/391]  acc1 0.500000 (0.484568)  loss 1.802058 (1.933054)\n",
      "Epoch [35/50] Step [121/391]  acc1 0.375000 (0.477273)  loss 2.257200 (1.942923)\n",
      "Epoch [35/50] Step [161/391]  acc1 0.406250 (0.473505)  loss 2.343535 (1.952129)\n",
      "Epoch [35/50] Step [201/391]  acc1 0.562500 (0.467895)  loss 1.590434 (1.958481)\n",
      "Epoch [35/50] Step [241/391]  acc1 0.406250 (0.467324)  loss 2.304986 (1.962341)\n",
      "Epoch [35/50] Step [281/391]  acc1 0.437500 (0.467082)  loss 2.249250 (1.966597)\n",
      "Epoch [35/50] Step [321/391]  acc1 0.500000 (0.463931)  loss 2.010431 (1.974131)\n",
      "Epoch [35/50] Step [361/391]  acc1 0.562500 (0.462950)  loss 1.999919 (1.978662)\n",
      "Epoch [36/50] Step [1/391]  acc1 0.593750 (0.593750)  loss 1.674560 (1.674560)\n",
      "Epoch [36/50] Step [41/391]  acc1 0.500000 (0.483994)  loss 1.780438 (1.916747)\n",
      "Epoch [36/50] Step [81/391]  acc1 0.593750 (0.482253)  loss 1.783972 (1.914426)\n",
      "Epoch [36/50] Step [121/391]  acc1 0.390625 (0.478306)  loss 1.973580 (1.929131)\n",
      "Epoch [36/50] Step [161/391]  acc1 0.500000 (0.472632)  loss 1.666790 (1.939862)\n",
      "Epoch [36/50] Step [201/391]  acc1 0.562500 (0.470227)  loss 1.730103 (1.947369)\n",
      "Epoch [36/50] Step [241/391]  acc1 0.437500 (0.466546)  loss 2.010230 (1.956497)\n",
      "Epoch [36/50] Step [281/391]  acc1 0.406250 (0.465247)  loss 2.076842 (1.958011)\n",
      "Epoch [36/50] Step [321/391]  acc1 0.421875 (0.463152)  loss 2.435204 (1.961789)\n",
      "Epoch [36/50] Step [361/391]  acc1 0.421875 (0.462301)  loss 1.918240 (1.966176)\n",
      "Epoch [37/50] Step [1/391]  acc1 0.406250 (0.406250)  loss 2.158684 (2.158684)\n",
      "Epoch [37/50] Step [41/391]  acc1 0.406250 (0.469512)  loss 2.109418 (1.916621)\n",
      "Epoch [37/50] Step [81/391]  acc1 0.484375 (0.470872)  loss 1.735126 (1.944566)\n",
      "Epoch [37/50] Step [121/391]  acc1 0.515625 (0.471591)  loss 1.756032 (1.950044)\n",
      "Epoch [37/50] Step [161/391]  acc1 0.375000 (0.472050)  loss 1.911875 (1.939137)\n",
      "Epoch [37/50] Step [201/391]  acc1 0.484375 (0.470305)  loss 1.966201 (1.939882)\n",
      "Epoch [37/50] Step [241/391]  acc1 0.468750 (0.470501)  loss 1.839955 (1.946234)\n",
      "Epoch [37/50] Step [281/391]  acc1 0.453125 (0.471475)  loss 1.954246 (1.944084)\n",
      "Epoch [37/50] Step [321/391]  acc1 0.375000 (0.469577)  loss 2.134673 (1.951603)\n",
      "Epoch [37/50] Step [361/391]  acc1 0.484375 (0.469572)  loss 1.956753 (1.950703)\n",
      "Epoch [38/50] Step [1/391]  acc1 0.406250 (0.406250)  loss 1.988630 (1.988630)\n",
      "Epoch [38/50] Step [41/391]  acc1 0.406250 (0.481326)  loss 2.089329 (1.904819)\n",
      "Epoch [38/50] Step [81/391]  acc1 0.421875 (0.482639)  loss 2.009335 (1.928347)\n",
      "Epoch [38/50] Step [121/391]  acc1 0.484375 (0.479855)  loss 2.291065 (1.932849)\n",
      "Epoch [38/50] Step [161/391]  acc1 0.375000 (0.477290)  loss 2.056429 (1.933402)\n",
      "Epoch [38/50] Step [201/391]  acc1 0.437500 (0.477379)  loss 1.883073 (1.933096)\n",
      "Epoch [38/50] Step [241/391]  acc1 0.390625 (0.476725)  loss 2.094722 (1.933258)\n",
      "Epoch [38/50] Step [281/391]  acc1 0.421875 (0.476201)  loss 1.976650 (1.934114)\n",
      "Epoch [38/50] Step [321/391]  acc1 0.421875 (0.478485)  loss 1.905671 (1.925634)\n",
      "Epoch [38/50] Step [361/391]  acc1 0.453125 (0.476281)  loss 1.894792 (1.928961)\n",
      "Epoch [39/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 1.938798 (1.938798)\n",
      "Epoch [39/50] Step [41/391]  acc1 0.531250 (0.504573)  loss 1.798273 (1.872558)\n",
      "Epoch [39/50] Step [81/391]  acc1 0.453125 (0.487461)  loss 2.056754 (1.909030)\n",
      "Epoch [39/50] Step [121/391]  acc1 0.484375 (0.485150)  loss 1.888753 (1.915075)\n",
      "Epoch [39/50] Step [161/391]  acc1 0.531250 (0.483016)  loss 1.864640 (1.914313)\n",
      "Epoch [39/50] Step [201/391]  acc1 0.468750 (0.483131)  loss 1.806571 (1.910470)\n",
      "Epoch [39/50] Step [241/391]  acc1 0.515625 (0.484051)  loss 1.929677 (1.907503)\n",
      "Epoch [39/50] Step [281/391]  acc1 0.453125 (0.480705)  loss 1.994827 (1.913041)\n",
      "Epoch [39/50] Step [321/391]  acc1 0.515625 (0.480140)  loss 1.765579 (1.912978)\n",
      "Epoch [39/50] Step [361/391]  acc1 0.406250 (0.477320)  loss 2.060612 (1.922927)\n",
      "Epoch [40/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 1.727479 (1.727479)\n",
      "Epoch [40/50] Step [41/391]  acc1 0.578125 (0.487805)  loss 1.911408 (1.893870)\n",
      "Epoch [40/50] Step [81/391]  acc1 0.437500 (0.489583)  loss 2.062968 (1.897562)\n",
      "Epoch [40/50] Step [121/391]  acc1 0.546875 (0.490186)  loss 1.840512 (1.886045)\n",
      "Epoch [40/50] Step [161/391]  acc1 0.484375 (0.486801)  loss 1.758041 (1.907067)\n",
      "Epoch [40/50] Step [201/391]  acc1 0.468750 (0.485230)  loss 2.104818 (1.906270)\n",
      "Epoch [40/50] Step [241/391]  acc1 0.453125 (0.480744)  loss 1.906161 (1.915124)\n",
      "Epoch [40/50] Step [281/391]  acc1 0.437500 (0.477702)  loss 1.979670 (1.922910)\n",
      "Epoch [40/50] Step [321/391]  acc1 0.421875 (0.479215)  loss 1.848543 (1.917850)\n",
      "Epoch [40/50] Step [361/391]  acc1 0.453125 (0.479744)  loss 1.939297 (1.915503)\n",
      "Epoch [41/50] Step [1/391]  acc1 0.578125 (0.578125)  loss 1.597878 (1.597878)\n",
      "Epoch [41/50] Step [41/391]  acc1 0.562500 (0.498857)  loss 1.643121 (1.883139)\n",
      "Epoch [41/50] Step [81/391]  acc1 0.453125 (0.495370)  loss 1.842424 (1.875113)\n",
      "Epoch [41/50] Step [121/391]  acc1 0.468750 (0.494189)  loss 1.966758 (1.871555)\n",
      "Epoch [41/50] Step [161/391]  acc1 0.453125 (0.490101)  loss 1.864496 (1.885573)\n",
      "Epoch [41/50] Step [201/391]  acc1 0.515625 (0.487718)  loss 1.991132 (1.885368)\n",
      "Epoch [41/50] Step [241/391]  acc1 0.546875 (0.487163)  loss 1.687903 (1.885567)\n",
      "Epoch [41/50] Step [281/391]  acc1 0.312500 (0.483485)  loss 2.523492 (1.899527)\n",
      "Epoch [41/50] Step [321/391]  acc1 0.437500 (0.482623)  loss 1.983054 (1.897410)\n",
      "Epoch [41/50] Step [361/391]  acc1 0.531250 (0.483293)  loss 1.711103 (1.896544)\n",
      "Epoch [42/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 1.740014 (1.740014)\n",
      "Epoch [42/50] Step [41/391]  acc1 0.500000 (0.500762)  loss 1.877442 (1.834969)\n",
      "Epoch [42/50] Step [81/391]  acc1 0.515625 (0.499614)  loss 1.778248 (1.822730)\n",
      "Epoch [42/50] Step [121/391]  acc1 0.578125 (0.497417)  loss 1.726901 (1.839026)\n",
      "Epoch [42/50] Step [161/391]  acc1 0.500000 (0.497380)  loss 2.066027 (1.844805)\n",
      "Epoch [42/50] Step [201/391]  acc1 0.421875 (0.496191)  loss 2.312820 (1.859823)\n",
      "Epoch [42/50] Step [241/391]  acc1 0.406250 (0.493841)  loss 2.017981 (1.867119)\n",
      "Epoch [42/50] Step [281/391]  acc1 0.421875 (0.490992)  loss 1.962824 (1.875219)\n",
      "Epoch [42/50] Step [321/391]  acc1 0.546875 (0.491384)  loss 1.891708 (1.875682)\n",
      "Epoch [42/50] Step [361/391]  acc1 0.390625 (0.489742)  loss 2.250525 (1.881013)\n",
      "Epoch [43/50] Step [1/391]  acc1 0.453125 (0.453125)  loss 1.895824 (1.895824)\n",
      "Epoch [43/50] Step [41/391]  acc1 0.468750 (0.490854)  loss 1.942046 (1.859357)\n",
      "Epoch [43/50] Step [81/391]  acc1 0.500000 (0.499421)  loss 1.851105 (1.838328)\n",
      "Epoch [43/50] Step [121/391]  acc1 0.578125 (0.491477)  loss 1.640583 (1.850339)\n",
      "Epoch [43/50] Step [161/391]  acc1 0.562500 (0.487675)  loss 1.810854 (1.864841)\n",
      "Epoch [43/50] Step [201/391]  acc1 0.468750 (0.488262)  loss 1.900173 (1.866511)\n",
      "Epoch [43/50] Step [241/391]  acc1 0.546875 (0.484505)  loss 1.723381 (1.872323)\n",
      "Epoch [43/50] Step [281/391]  acc1 0.515625 (0.487433)  loss 2.013350 (1.870709)\n",
      "Epoch [43/50] Step [321/391]  acc1 0.484375 (0.487247)  loss 1.830658 (1.873941)\n",
      "Epoch [43/50] Step [361/391]  acc1 0.546875 (0.485544)  loss 1.745633 (1.879475)\n",
      "Epoch [44/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.068821 (2.068821)\n",
      "Epoch [44/50] Step [41/391]  acc1 0.500000 (0.491616)  loss 1.992579 (1.875828)\n",
      "Epoch [44/50] Step [81/391]  acc1 0.406250 (0.485918)  loss 2.090517 (1.876454)\n",
      "Epoch [44/50] Step [121/391]  acc1 0.390625 (0.500387)  loss 2.068499 (1.858078)\n",
      "Epoch [44/50] Step [161/391]  acc1 0.578125 (0.501359)  loss 1.808396 (1.843538)\n",
      "Epoch [44/50] Step [201/391]  acc1 0.515625 (0.496035)  loss 1.842292 (1.858299)\n",
      "Epoch [44/50] Step [241/391]  acc1 0.562500 (0.496758)  loss 1.707609 (1.857334)\n",
      "Epoch [44/50] Step [281/391]  acc1 0.500000 (0.494495)  loss 1.747016 (1.863280)\n",
      "Epoch [44/50] Step [321/391]  acc1 0.468750 (0.493818)  loss 1.954276 (1.865278)\n",
      "Epoch [44/50] Step [361/391]  acc1 0.453125 (0.492685)  loss 1.944106 (1.867055)\n",
      "Epoch [45/50] Step [1/391]  acc1 0.562500 (0.562500)  loss 1.824216 (1.824216)\n",
      "Epoch [45/50] Step [41/391]  acc1 0.515625 (0.502287)  loss 1.844352 (1.825680)\n",
      "Epoch [45/50] Step [81/391]  acc1 0.468750 (0.502508)  loss 1.975961 (1.831882)\n",
      "Epoch [45/50] Step [121/391]  acc1 0.484375 (0.496772)  loss 1.914519 (1.830364)\n",
      "Epoch [45/50] Step [161/391]  acc1 0.500000 (0.497186)  loss 1.872563 (1.839003)\n",
      "Epoch [45/50] Step [201/391]  acc1 0.484375 (0.496813)  loss 1.637976 (1.830258)\n",
      "Epoch [45/50] Step [241/391]  acc1 0.531250 (0.495915)  loss 1.887976 (1.833767)\n",
      "Epoch [45/50] Step [281/391]  acc1 0.484375 (0.493327)  loss 1.678256 (1.840937)\n",
      "Epoch [45/50] Step [321/391]  acc1 0.531250 (0.493088)  loss 1.814996 (1.844266)\n",
      "Epoch [45/50] Step [361/391]  acc1 0.468750 (0.493334)  loss 2.090353 (1.847466)\n",
      "Epoch [46/50] Step [1/391]  acc1 0.562500 (0.562500)  loss 1.546547 (1.546547)\n",
      "Epoch [46/50] Step [41/391]  acc1 0.562500 (0.500000)  loss 1.635818 (1.798205)\n",
      "Epoch [46/50] Step [81/391]  acc1 0.515625 (0.504051)  loss 1.925524 (1.801028)\n",
      "Epoch [46/50] Step [121/391]  acc1 0.500000 (0.501291)  loss 1.928622 (1.817358)\n",
      "Epoch [46/50] Step [161/391]  acc1 0.437500 (0.499709)  loss 2.081224 (1.824839)\n",
      "Epoch [46/50] Step [201/391]  acc1 0.562500 (0.498756)  loss 1.695078 (1.832625)\n",
      "Epoch [46/50] Step [241/391]  acc1 0.562500 (0.496758)  loss 1.714949 (1.839959)\n",
      "Epoch [46/50] Step [281/391]  acc1 0.562500 (0.496719)  loss 1.774451 (1.838062)\n",
      "Epoch [46/50] Step [321/391]  acc1 0.500000 (0.496593)  loss 2.081453 (1.839746)\n",
      "Epoch [46/50] Step [361/391]  acc1 0.468750 (0.494373)  loss 1.968569 (1.846648)\n",
      "Epoch [47/50] Step [1/391]  acc1 0.515625 (0.515625)  loss 1.726116 (1.726116)\n",
      "Epoch [47/50] Step [41/391]  acc1 0.546875 (0.509909)  loss 1.793759 (1.779055)\n",
      "Epoch [47/50] Step [81/391]  acc1 0.375000 (0.506173)  loss 2.268451 (1.806015)\n",
      "Epoch [47/50] Step [121/391]  acc1 0.421875 (0.503874)  loss 2.110111 (1.815662)\n",
      "Epoch [47/50] Step [161/391]  acc1 0.500000 (0.500485)  loss 1.664691 (1.827617)\n",
      "Epoch [47/50] Step [201/391]  acc1 0.437500 (0.496580)  loss 2.040000 (1.837305)\n",
      "Epoch [47/50] Step [241/391]  acc1 0.500000 (0.494230)  loss 1.783710 (1.838822)\n",
      "Epoch [47/50] Step [281/391]  acc1 0.437500 (0.495718)  loss 1.983820 (1.839463)\n",
      "Epoch [47/50] Step [321/391]  acc1 0.484375 (0.498004)  loss 1.868705 (1.836496)\n",
      "Epoch [47/50] Step [361/391]  acc1 0.484375 (0.496667)  loss 1.899514 (1.843127)\n",
      "Epoch [48/50] Step [1/391]  acc1 0.546875 (0.546875)  loss 1.909239 (1.909239)\n",
      "Epoch [48/50] Step [41/391]  acc1 0.546875 (0.500381)  loss 1.640881 (1.782850)\n",
      "Epoch [48/50] Step [81/391]  acc1 0.593750 (0.503472)  loss 1.524614 (1.787475)\n",
      "Epoch [48/50] Step [121/391]  acc1 0.515625 (0.501420)  loss 1.873702 (1.811764)\n",
      "Epoch [48/50] Step [161/391]  acc1 0.515625 (0.503203)  loss 1.859090 (1.816229)\n",
      "Epoch [48/50] Step [201/391]  acc1 0.406250 (0.504509)  loss 1.966185 (1.813619)\n",
      "Epoch [48/50] Step [241/391]  acc1 0.468750 (0.502658)  loss 2.146478 (1.818841)\n",
      "Epoch [48/50] Step [281/391]  acc1 0.484375 (0.502169)  loss 1.810870 (1.820448)\n",
      "Epoch [48/50] Step [321/391]  acc1 0.468750 (0.501314)  loss 1.718651 (1.822817)\n",
      "Epoch [48/50] Step [361/391]  acc1 0.515625 (0.501169)  loss 2.013780 (1.825183)\n",
      "Epoch [49/50] Step [1/391]  acc1 0.562500 (0.562500)  loss 1.871257 (1.871257)\n",
      "Epoch [49/50] Step [41/391]  acc1 0.531250 (0.530107)  loss 1.691395 (1.737004)\n",
      "Epoch [49/50] Step [81/391]  acc1 0.515625 (0.515818)  loss 1.694330 (1.780598)\n",
      "Epoch [49/50] Step [121/391]  acc1 0.546875 (0.508910)  loss 1.738640 (1.795167)\n",
      "Epoch [49/50] Step [161/391]  acc1 0.500000 (0.505144)  loss 1.847354 (1.799677)\n",
      "Epoch [49/50] Step [201/391]  acc1 0.500000 (0.505752)  loss 1.823572 (1.797553)\n",
      "Epoch [49/50] Step [241/391]  acc1 0.578125 (0.503242)  loss 1.582262 (1.802209)\n",
      "Epoch [49/50] Step [281/391]  acc1 0.578125 (0.505282)  loss 1.803860 (1.800679)\n",
      "Epoch [49/50] Step [321/391]  acc1 0.515625 (0.502969)  loss 1.905011 (1.810055)\n",
      "Epoch [49/50] Step [361/391]  acc1 0.515625 (0.502813)  loss 1.779549 (1.812565)\n",
      "Epoch [50/50] Step [1/391]  acc1 0.531250 (0.531250)  loss 1.630410 (1.630410)\n",
      "Epoch [50/50] Step [41/391]  acc1 0.468750 (0.520579)  loss 1.904673 (1.779204)\n",
      "Epoch [50/50] Step [81/391]  acc1 0.453125 (0.515239)  loss 2.021242 (1.788743)\n",
      "Epoch [50/50] Step [121/391]  acc1 0.468750 (0.517433)  loss 1.948075 (1.788039)\n",
      "Epoch [50/50] Step [161/391]  acc1 0.468750 (0.511064)  loss 2.077362 (1.803372)\n",
      "Epoch [50/50] Step [201/391]  acc1 0.515625 (0.510417)  loss 1.735374 (1.811784)\n",
      "Epoch [50/50] Step [241/391]  acc1 0.406250 (0.510114)  loss 2.476763 (1.811755)\n",
      "Epoch [50/50] Step [281/391]  acc1 0.406250 (0.508841)  loss 2.033389 (1.812542)\n",
      "Epoch [50/50] Step [321/391]  acc1 0.578125 (0.508859)  loss 1.606970 (1.814572)\n",
      "Epoch [50/50] Step [361/391]  acc1 0.531250 (0.507271)  loss 1.883612 (1.811742)\n",
      "Final architecture: {'reduce_n2_p0': 'sepconv3x3', 'reduce_n2_p1': 'sepconv5x5', 'reduce_n3_p0': 'maxpool', 'reduce_n3_p1': 'sepconv5x5', 'reduce_n3_p2': 'dilconv5x5', 'reduce_n4_p0': 'dilconv3x3', 'reduce_n4_p1': 'sepconv5x5', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv5x5', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'skipconnect', 'reduce_n5_p2': 'dilconv3x3', 'reduce_n5_p3': 'sepconv3x3', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [0], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [4]}\n",
      "Epoch [1/50] Step [1/391]  acc1 0.015625 (0.015625)  loss 4.623779 (4.623779)\n",
      "Epoch [1/50] Step [41/391]  acc1 0.015625 (0.027820)  loss 4.474413 (4.565049)\n",
      "Epoch [1/50] Step [81/391]  acc1 0.046875 (0.037037)  loss 4.461713 (4.473145)\n",
      "Epoch [1/50] Step [121/391]  acc1 0.031250 (0.039902)  loss 4.360511 (4.411735)\n",
      "Epoch [1/50] Step [161/391]  acc1 0.078125 (0.043575)  loss 4.305247 (4.369619)\n",
      "Epoch [1/50] Step [201/391]  acc1 0.062500 (0.047264)  loss 3.939708 (4.328762)\n",
      "Epoch [1/50] Step [241/391]  acc1 0.078125 (0.052516)  loss 4.161717 (4.289291)\n",
      "Epoch [1/50] Step [281/391]  acc1 0.046875 (0.055049)  loss 4.045519 (4.258060)\n",
      "Epoch [1/50] Step [321/391]  acc1 0.109375 (0.059044)  loss 4.118788 (4.223720)\n",
      "Epoch [1/50] Step [361/391]  acc1 0.125000 (0.063193)  loss 4.001006 (4.198929)\n",
      "Epoch [2/50] Step [1/391]  acc1 0.093750 (0.093750)  loss 3.868139 (3.868139)\n",
      "Epoch [2/50] Step [41/391]  acc1 0.125000 (0.094512)  loss 3.800742 (3.918827)\n",
      "Epoch [2/50] Step [81/391]  acc1 0.109375 (0.105131)  loss 3.921059 (3.914419)\n",
      "Epoch [2/50] Step [121/391]  acc1 0.125000 (0.104855)  loss 3.709223 (3.906263)\n",
      "Epoch [2/50] Step [161/391]  acc1 0.109375 (0.106075)  loss 3.793828 (3.900211)\n",
      "Epoch [2/50] Step [201/391]  acc1 0.093750 (0.105877)  loss 3.745683 (3.890878)\n",
      "Epoch [2/50] Step [241/391]  acc1 0.093750 (0.108013)  loss 3.731275 (3.879873)\n",
      "Epoch [2/50] Step [281/391]  acc1 0.093750 (0.110153)  loss 3.811425 (3.868870)\n",
      "Epoch [2/50] Step [321/391]  acc1 0.140625 (0.112928)  loss 3.906814 (3.858439)\n",
      "Epoch [2/50] Step [361/391]  acc1 0.125000 (0.114699)  loss 3.644289 (3.850345)\n",
      "Epoch [3/50] Step [1/391]  acc1 0.109375 (0.109375)  loss 3.718822 (3.718822)\n",
      "Epoch [3/50] Step [41/391]  acc1 0.156250 (0.137195)  loss 3.551101 (3.757769)\n",
      "Epoch [3/50] Step [81/391]  acc1 0.109375 (0.138889)  loss 3.825993 (3.744356)\n",
      "Epoch [3/50] Step [121/391]  acc1 0.093750 (0.136105)  loss 3.893803 (3.742610)\n",
      "Epoch [3/50] Step [161/391]  acc1 0.125000 (0.138199)  loss 3.784437 (3.731381)\n",
      "Epoch [3/50] Step [201/391]  acc1 0.078125 (0.137982)  loss 3.777240 (3.728525)\n",
      "Epoch [3/50] Step [241/391]  acc1 0.078125 (0.140041)  loss 3.883428 (3.717229)\n",
      "Epoch [3/50] Step [281/391]  acc1 0.125000 (0.140236)  loss 3.881043 (3.711130)\n",
      "Epoch [3/50] Step [321/391]  acc1 0.250000 (0.141404)  loss 3.488136 (3.703654)\n",
      "Epoch [3/50] Step [361/391]  acc1 0.140625 (0.141880)  loss 3.678673 (3.699241)\n",
      "Epoch [4/50] Step [1/391]  acc1 0.187500 (0.187500)  loss 3.603995 (3.603995)\n",
      "Epoch [4/50] Step [41/391]  acc1 0.156250 (0.147866)  loss 3.570133 (3.622890)\n",
      "Epoch [4/50] Step [81/391]  acc1 0.109375 (0.161651)  loss 3.625301 (3.599144)\n",
      "Epoch [4/50] Step [121/391]  acc1 0.187500 (0.157929)  loss 3.681623 (3.587965)\n",
      "Epoch [4/50] Step [161/391]  acc1 0.093750 (0.156832)  loss 3.907580 (3.593090)\n",
      "Epoch [4/50] Step [201/391]  acc1 0.218750 (0.158193)  loss 3.500538 (3.586097)\n",
      "Epoch [4/50] Step [241/391]  acc1 0.171875 (0.159232)  loss 3.452532 (3.584312)\n",
      "Epoch [4/50] Step [281/391]  acc1 0.171875 (0.159475)  loss 3.651130 (3.577996)\n",
      "Epoch [4/50] Step [321/391]  acc1 0.203125 (0.159998)  loss 3.404562 (3.576328)\n",
      "Epoch [4/50] Step [361/391]  acc1 0.125000 (0.160838)  loss 3.257195 (3.573450)\n",
      "Epoch [5/50] Step [1/391]  acc1 0.203125 (0.203125)  loss 3.595336 (3.595336)\n",
      "Epoch [5/50] Step [41/391]  acc1 0.203125 (0.171875)  loss 3.343837 (3.477523)\n",
      "Epoch [5/50] Step [81/391]  acc1 0.171875 (0.175154)  loss 3.439079 (3.495179)\n",
      "Epoch [5/50] Step [121/391]  acc1 0.062500 (0.175491)  loss 3.891953 (3.486042)\n",
      "Epoch [5/50] Step [161/391]  acc1 0.140625 (0.178766)  loss 3.574952 (3.472719)\n",
      "Epoch [5/50] Step [201/391]  acc1 0.156250 (0.178405)  loss 3.643876 (3.473841)\n",
      "Epoch [5/50] Step [241/391]  acc1 0.234375 (0.179850)  loss 3.467703 (3.467453)\n",
      "Epoch [5/50] Step [281/391]  acc1 0.140625 (0.180049)  loss 3.389267 (3.461226)\n",
      "Epoch [5/50] Step [321/391]  acc1 0.281250 (0.181367)  loss 3.130768 (3.458415)\n",
      "Epoch [5/50] Step [361/391]  acc1 0.187500 (0.182046)  loss 3.343167 (3.457032)\n",
      "Epoch [6/50] Step [1/391]  acc1 0.125000 (0.125000)  loss 3.467402 (3.467402)\n",
      "Epoch [6/50] Step [41/391]  acc1 0.125000 (0.184832)  loss 3.686484 (3.394505)\n",
      "Epoch [6/50] Step [81/391]  acc1 0.203125 (0.189236)  loss 3.482171 (3.375197)\n",
      "Epoch [6/50] Step [121/391]  acc1 0.250000 (0.194861)  loss 3.269385 (3.368012)\n",
      "Epoch [6/50] Step [161/391]  acc1 0.187500 (0.198370)  loss 3.262217 (3.365788)\n",
      "Epoch [6/50] Step [201/391]  acc1 0.187500 (0.199316)  loss 3.289290 (3.359254)\n",
      "Epoch [6/50] Step [241/391]  acc1 0.203125 (0.200143)  loss 3.090186 (3.357088)\n",
      "Epoch [6/50] Step [281/391]  acc1 0.171875 (0.199566)  loss 3.262018 (3.358296)\n",
      "Epoch [6/50] Step [321/391]  acc1 0.234375 (0.199961)  loss 3.352437 (3.357089)\n",
      "Epoch [6/50] Step [361/391]  acc1 0.203125 (0.201221)  loss 3.469280 (3.351352)\n",
      "Epoch [7/50] Step [1/391]  acc1 0.218750 (0.218750)  loss 3.321064 (3.321064)\n",
      "Epoch [7/50] Step [41/391]  acc1 0.156250 (0.210366)  loss 3.132768 (3.288834)\n",
      "Epoch [7/50] Step [81/391]  acc1 0.250000 (0.214699)  loss 3.349271 (3.279778)\n",
      "Epoch [7/50] Step [121/391]  acc1 0.218750 (0.213972)  loss 3.347879 (3.281102)\n",
      "Epoch [7/50] Step [161/391]  acc1 0.265625 (0.212442)  loss 3.062703 (3.286641)\n",
      "Epoch [7/50] Step [201/391]  acc1 0.203125 (0.212220)  loss 3.442500 (3.279526)\n",
      "Epoch [7/50] Step [241/391]  acc1 0.218750 (0.210970)  loss 3.296108 (3.279841)\n",
      "Epoch [7/50] Step [281/391]  acc1 0.156250 (0.209130)  loss 3.279584 (3.281626)\n",
      "Epoch [7/50] Step [321/391]  acc1 0.171875 (0.210232)  loss 3.226036 (3.275385)\n",
      "Epoch [7/50] Step [361/391]  acc1 0.265625 (0.210050)  loss 3.277130 (3.273810)\n",
      "Epoch [8/50] Step [1/391]  acc1 0.296875 (0.296875)  loss 3.078559 (3.078559)\n",
      "Epoch [8/50] Step [41/391]  acc1 0.250000 (0.229040)  loss 3.318798 (3.176447)\n",
      "Epoch [8/50] Step [81/391]  acc1 0.250000 (0.232639)  loss 3.066817 (3.167877)\n",
      "Epoch [8/50] Step [121/391]  acc1 0.312500 (0.228564)  loss 3.322100 (3.178410)\n",
      "Epoch [8/50] Step [161/391]  acc1 0.203125 (0.229425)  loss 3.065243 (3.178133)\n",
      "Epoch [8/50] Step [201/391]  acc1 0.203125 (0.229866)  loss 3.261339 (3.180227)\n",
      "Epoch [8/50] Step [241/391]  acc1 0.250000 (0.229383)  loss 3.398305 (3.182284)\n",
      "Epoch [8/50] Step [281/391]  acc1 0.250000 (0.226535)  loss 3.245011 (3.189231)\n",
      "Epoch [8/50] Step [321/391]  acc1 0.218750 (0.227074)  loss 3.198344 (3.187025)\n",
      "Epoch [8/50] Step [361/391]  acc1 0.218750 (0.227450)  loss 2.930327 (3.182042)\n",
      "Epoch [9/50] Step [1/391]  acc1 0.203125 (0.203125)  loss 3.335498 (3.335498)\n",
      "Epoch [9/50] Step [41/391]  acc1 0.171875 (0.235518)  loss 3.192586 (3.191245)\n",
      "Epoch [9/50] Step [81/391]  acc1 0.187500 (0.242477)  loss 3.161541 (3.148096)\n",
      "Epoch [9/50] Step [121/391]  acc1 0.156250 (0.237732)  loss 3.434726 (3.144420)\n",
      "Epoch [9/50] Step [161/391]  acc1 0.234375 (0.238839)  loss 3.123065 (3.142440)\n",
      "Epoch [9/50] Step [201/391]  acc1 0.218750 (0.239894)  loss 3.173838 (3.135988)\n",
      "Epoch [9/50] Step [241/391]  acc1 0.296875 (0.238524)  loss 2.880179 (3.134648)\n",
      "Epoch [9/50] Step [281/391]  acc1 0.218750 (0.237378)  loss 3.313246 (3.133794)\n",
      "Epoch [9/50] Step [321/391]  acc1 0.312500 (0.238269)  loss 2.801349 (3.132313)\n",
      "Epoch [9/50] Step [361/391]  acc1 0.328125 (0.238573)  loss 2.775770 (3.127083)\n",
      "Epoch [10/50] Step [1/391]  acc1 0.250000 (0.250000)  loss 3.216011 (3.216011)\n",
      "Epoch [10/50] Step [41/391]  acc1 0.296875 (0.250762)  loss 2.840401 (3.051651)\n",
      "Epoch [10/50] Step [81/391]  acc1 0.312500 (0.248071)  loss 3.314085 (3.063258)\n",
      "Epoch [10/50] Step [121/391]  acc1 0.296875 (0.248967)  loss 3.045805 (3.042483)\n",
      "Epoch [10/50] Step [161/391]  acc1 0.296875 (0.251844)  loss 3.191957 (3.042295)\n",
      "Epoch [10/50] Step [201/391]  acc1 0.250000 (0.250000)  loss 3.069229 (3.055827)\n",
      "Epoch [10/50] Step [241/391]  acc1 0.187500 (0.249157)  loss 3.140117 (3.054649)\n",
      "Epoch [10/50] Step [281/391]  acc1 0.328125 (0.250667)  loss 2.730061 (3.048031)\n",
      "Epoch [10/50] Step [321/391]  acc1 0.343750 (0.250000)  loss 2.842963 (3.048329)\n",
      "Epoch [10/50] Step [361/391]  acc1 0.187500 (0.249264)  loss 3.589799 (3.054415)\n",
      "Epoch [11/50] Step [1/391]  acc1 0.250000 (0.250000)  loss 2.899292 (2.899292)\n",
      "Epoch [11/50] Step [41/391]  acc1 0.203125 (0.267530)  loss 3.333984 (3.005106)\n",
      "Epoch [11/50] Step [81/391]  acc1 0.375000 (0.256752)  loss 2.885360 (3.013704)\n",
      "Epoch [11/50] Step [121/391]  acc1 0.328125 (0.260460)  loss 2.881990 (3.001162)\n",
      "Epoch [11/50] Step [161/391]  acc1 0.265625 (0.261161)  loss 2.931102 (2.990087)\n",
      "Epoch [11/50] Step [201/391]  acc1 0.250000 (0.264303)  loss 3.087449 (2.988123)\n",
      "Epoch [11/50] Step [241/391]  acc1 0.343750 (0.262902)  loss 2.876025 (2.994328)\n",
      "Epoch [11/50] Step [281/391]  acc1 0.296875 (0.261121)  loss 2.933443 (2.998639)\n",
      "Epoch [11/50] Step [321/391]  acc1 0.250000 (0.262266)  loss 3.337638 (2.994838)\n",
      "Epoch [11/50] Step [361/391]  acc1 0.296875 (0.261773)  loss 2.825963 (2.992396)\n",
      "Epoch [12/50] Step [1/391]  acc1 0.218750 (0.218750)  loss 3.060369 (3.060369)\n",
      "Epoch [12/50] Step [41/391]  acc1 0.234375 (0.263338)  loss 2.886371 (2.980777)\n",
      "Epoch [12/50] Step [81/391]  acc1 0.250000 (0.271991)  loss 2.810646 (2.937250)\n",
      "Epoch [12/50] Step [121/391]  acc1 0.312500 (0.272082)  loss 2.666492 (2.942324)\n",
      "Epoch [12/50] Step [161/391]  acc1 0.359375 (0.271545)  loss 2.397724 (2.942162)\n",
      "Epoch [12/50] Step [201/391]  acc1 0.250000 (0.268501)  loss 2.903310 (2.946574)\n",
      "Epoch [12/50] Step [241/391]  acc1 0.296875 (0.271395)  loss 3.022372 (2.940857)\n",
      "Epoch [12/50] Step [281/391]  acc1 0.281250 (0.271074)  loss 2.804564 (2.936909)\n",
      "Epoch [12/50] Step [321/391]  acc1 0.312500 (0.273316)  loss 2.721591 (2.936765)\n",
      "Epoch [12/50] Step [361/391]  acc1 0.281250 (0.273026)  loss 2.933898 (2.940376)\n",
      "Epoch [13/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.705239 (2.705239)\n",
      "Epoch [13/50] Step [41/391]  acc1 0.312500 (0.279726)  loss 2.877759 (2.880564)\n",
      "Epoch [13/50] Step [81/391]  acc1 0.187500 (0.274113)  loss 3.023330 (2.904707)\n",
      "Epoch [13/50] Step [121/391]  acc1 0.265625 (0.274019)  loss 2.730891 (2.882113)\n",
      "Epoch [13/50] Step [161/391]  acc1 0.265625 (0.273874)  loss 2.901345 (2.892112)\n",
      "Epoch [13/50] Step [201/391]  acc1 0.203125 (0.273476)  loss 2.930506 (2.885572)\n",
      "Epoch [13/50] Step [241/391]  acc1 0.296875 (0.275545)  loss 2.943268 (2.885066)\n",
      "Epoch [13/50] Step [281/391]  acc1 0.203125 (0.275078)  loss 2.944470 (2.892434)\n",
      "Epoch [13/50] Step [321/391]  acc1 0.343750 (0.276674)  loss 2.766878 (2.887875)\n",
      "Epoch [13/50] Step [361/391]  acc1 0.296875 (0.277744)  loss 2.852709 (2.882643)\n",
      "Epoch [14/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.789577 (2.789577)\n",
      "Epoch [14/50] Step [41/391]  acc1 0.171875 (0.282393)  loss 2.970887 (2.871214)\n",
      "Epoch [14/50] Step [81/391]  acc1 0.296875 (0.283565)  loss 2.759071 (2.865807)\n",
      "Epoch [14/50] Step [121/391]  acc1 0.343750 (0.285770)  loss 2.535168 (2.841432)\n",
      "Epoch [14/50] Step [161/391]  acc1 0.328125 (0.286782)  loss 2.561231 (2.837869)\n",
      "Epoch [14/50] Step [201/391]  acc1 0.218750 (0.287080)  loss 2.888724 (2.835421)\n",
      "Epoch [14/50] Step [241/391]  acc1 0.187500 (0.287733)  loss 3.127831 (2.839708)\n",
      "Epoch [14/50] Step [281/391]  acc1 0.328125 (0.287867)  loss 2.784981 (2.845270)\n",
      "Epoch [14/50] Step [321/391]  acc1 0.359375 (0.288016)  loss 2.689588 (2.841633)\n",
      "Epoch [14/50] Step [361/391]  acc1 0.296875 (0.288868)  loss 2.837108 (2.836701)\n",
      "Epoch [15/50] Step [1/391]  acc1 0.312500 (0.312500)  loss 2.661579 (2.661579)\n",
      "Epoch [15/50] Step [41/391]  acc1 0.312500 (0.307165)  loss 2.905580 (2.759250)\n",
      "Epoch [15/50] Step [81/391]  acc1 0.312500 (0.298032)  loss 2.763477 (2.787303)\n",
      "Epoch [15/50] Step [121/391]  acc1 0.328125 (0.300749)  loss 2.771325 (2.787925)\n",
      "Epoch [15/50] Step [161/391]  acc1 0.375000 (0.302213)  loss 2.900829 (2.786571)\n",
      "Epoch [15/50] Step [201/391]  acc1 0.312500 (0.297497)  loss 2.698562 (2.789958)\n",
      "Epoch [15/50] Step [241/391]  acc1 0.250000 (0.296227)  loss 2.817429 (2.791481)\n",
      "Epoch [15/50] Step [281/391]  acc1 0.187500 (0.296152)  loss 2.923549 (2.792447)\n",
      "Epoch [15/50] Step [321/391]  acc1 0.343750 (0.296875)  loss 2.585559 (2.790813)\n",
      "Epoch [15/50] Step [361/391]  acc1 0.343750 (0.296312)  loss 2.911662 (2.787584)\n",
      "Epoch [16/50] Step [1/391]  acc1 0.234375 (0.234375)  loss 2.946286 (2.946286)\n",
      "Epoch [16/50] Step [41/391]  acc1 0.375000 (0.309070)  loss 2.768786 (2.726893)\n",
      "Epoch [16/50] Step [81/391]  acc1 0.328125 (0.313079)  loss 2.775248 (2.725427)\n",
      "Epoch [16/50] Step [121/391]  acc1 0.328125 (0.309917)  loss 2.527374 (2.726173)\n",
      "Epoch [16/50] Step [161/391]  acc1 0.328125 (0.308327)  loss 2.838778 (2.739688)\n",
      "Epoch [16/50] Step [201/391]  acc1 0.203125 (0.308380)  loss 2.871101 (2.741621)\n",
      "Epoch [16/50] Step [241/391]  acc1 0.281250 (0.309777)  loss 2.524870 (2.736383)\n",
      "Epoch [16/50] Step [281/391]  acc1 0.234375 (0.309887)  loss 3.192683 (2.738101)\n",
      "Epoch [16/50] Step [321/391]  acc1 0.312500 (0.308509)  loss 2.752849 (2.742893)\n",
      "Epoch [16/50] Step [361/391]  acc1 0.359375 (0.306830)  loss 2.620402 (2.747751)\n",
      "Epoch [17/50] Step [1/391]  acc1 0.218750 (0.218750)  loss 2.954552 (2.954552)\n",
      "Epoch [17/50] Step [41/391]  acc1 0.250000 (0.304116)  loss 2.821622 (2.705374)\n",
      "Epoch [17/50] Step [81/391]  acc1 0.343750 (0.302083)  loss 2.815761 (2.745938)\n",
      "Epoch [17/50] Step [121/391]  acc1 0.390625 (0.312113)  loss 2.545150 (2.726906)\n",
      "Epoch [17/50] Step [161/391]  acc1 0.234375 (0.311238)  loss 2.846447 (2.721296)\n",
      "Epoch [17/50] Step [201/391]  acc1 0.281250 (0.309701)  loss 2.803551 (2.721440)\n",
      "Epoch [17/50] Step [241/391]  acc1 0.375000 (0.312241)  loss 2.403440 (2.708729)\n",
      "Epoch [17/50] Step [281/391]  acc1 0.281250 (0.312444)  loss 2.880853 (2.709479)\n",
      "Epoch [17/50] Step [321/391]  acc1 0.234375 (0.314155)  loss 2.965434 (2.702129)\n",
      "Epoch [17/50] Step [361/391]  acc1 0.390625 (0.314448)  loss 2.587817 (2.706687)\n",
      "Epoch [18/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.598501 (2.598501)\n",
      "Epoch [18/50] Step [41/391]  acc1 0.281250 (0.308308)  loss 2.727365 (2.705532)\n",
      "Epoch [18/50] Step [81/391]  acc1 0.328125 (0.325424)  loss 2.725897 (2.652626)\n",
      "Epoch [18/50] Step [121/391]  acc1 0.328125 (0.328254)  loss 2.735068 (2.652541)\n",
      "Epoch [18/50] Step [161/391]  acc1 0.312500 (0.326863)  loss 2.681071 (2.660330)\n",
      "Epoch [18/50] Step [201/391]  acc1 0.265625 (0.325249)  loss 2.848479 (2.662872)\n",
      "Epoch [18/50] Step [241/391]  acc1 0.312500 (0.322225)  loss 2.697502 (2.673169)\n",
      "Epoch [18/50] Step [281/391]  acc1 0.359375 (0.320507)  loss 2.820446 (2.681213)\n",
      "Epoch [18/50] Step [321/391]  acc1 0.312500 (0.319558)  loss 2.841299 (2.678128)\n",
      "Epoch [18/50] Step [361/391]  acc1 0.328125 (0.319295)  loss 2.302416 (2.677397)\n",
      "Epoch [19/50] Step [1/391]  acc1 0.281250 (0.281250)  loss 2.965815 (2.965815)\n",
      "Epoch [19/50] Step [41/391]  acc1 0.296875 (0.355564)  loss 2.876919 (2.559784)\n",
      "Epoch [19/50] Step [81/391]  acc1 0.312500 (0.340471)  loss 2.440952 (2.594061)\n",
      "Epoch [19/50] Step [121/391]  acc1 0.375000 (0.340263)  loss 2.574481 (2.593254)\n",
      "Epoch [19/50] Step [161/391]  acc1 0.281250 (0.338315)  loss 2.863536 (2.596101)\n",
      "Epoch [19/50] Step [201/391]  acc1 0.375000 (0.336754)  loss 2.189468 (2.598504)\n",
      "Epoch [19/50] Step [241/391]  acc1 0.296875 (0.335840)  loss 2.577696 (2.599758)\n",
      "Epoch [19/50] Step [281/391]  acc1 0.359375 (0.333129)  loss 2.288789 (2.603446)\n",
      "Epoch [19/50] Step [321/391]  acc1 0.328125 (0.332214)  loss 2.669711 (2.608620)\n",
      "Epoch [19/50] Step [361/391]  acc1 0.250000 (0.330506)  loss 2.635728 (2.612836)\n",
      "Epoch [20/50] Step [1/391]  acc1 0.265625 (0.265625)  loss 2.736492 (2.736492)\n",
      "Epoch [20/50] Step [41/391]  acc1 0.437500 (0.348323)  loss 2.408421 (2.553836)\n",
      "Epoch [20/50] Step [81/391]  acc1 0.265625 (0.346836)  loss 2.596235 (2.563949)\n",
      "Epoch [20/50] Step [121/391]  acc1 0.296875 (0.345041)  loss 2.705016 (2.554666)\n",
      "Epoch [20/50] Step [161/391]  acc1 0.359375 (0.342488)  loss 2.645544 (2.556564)\n",
      "Epoch [20/50] Step [201/391]  acc1 0.328125 (0.343672)  loss 2.659108 (2.557044)\n",
      "Epoch [20/50] Step [241/391]  acc1 0.406250 (0.341935)  loss 2.230957 (2.565148)\n",
      "Epoch [20/50] Step [281/391]  acc1 0.437500 (0.341025)  loss 2.389176 (2.576593)\n",
      "Epoch [20/50] Step [321/391]  acc1 0.281250 (0.339077)  loss 2.567913 (2.582651)\n",
      "Epoch [20/50] Step [361/391]  acc1 0.359375 (0.339205)  loss 2.416362 (2.581589)\n",
      "Epoch [21/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.573393 (2.573393)\n",
      "Epoch [21/50] Step [41/391]  acc1 0.328125 (0.343369)  loss 2.439901 (2.556976)\n",
      "Epoch [21/50] Step [81/391]  acc1 0.328125 (0.342978)  loss 2.343432 (2.566232)\n",
      "Epoch [21/50] Step [121/391]  acc1 0.375000 (0.344654)  loss 2.412684 (2.559046)\n",
      "Epoch [21/50] Step [161/391]  acc1 0.296875 (0.343944)  loss 2.363098 (2.553160)\n",
      "Epoch [21/50] Step [201/391]  acc1 0.468750 (0.345460)  loss 2.256992 (2.541708)\n",
      "Epoch [21/50] Step [241/391]  acc1 0.328125 (0.348418)  loss 2.447495 (2.541061)\n",
      "Epoch [21/50] Step [281/391]  acc1 0.375000 (0.348032)  loss 2.442671 (2.542771)\n",
      "Epoch [21/50] Step [321/391]  acc1 0.375000 (0.348374)  loss 2.560783 (2.543447)\n",
      "Epoch [21/50] Step [361/391]  acc1 0.296875 (0.347559)  loss 2.770131 (2.544890)\n",
      "Epoch [22/50] Step [1/391]  acc1 0.406250 (0.406250)  loss 2.460865 (2.460865)\n",
      "Epoch [22/50] Step [41/391]  acc1 0.484375 (0.350610)  loss 2.153015 (2.506785)\n",
      "Epoch [22/50] Step [81/391]  acc1 0.328125 (0.352045)  loss 2.581373 (2.492724)\n",
      "Epoch [22/50] Step [121/391]  acc1 0.375000 (0.356276)  loss 2.224168 (2.498368)\n",
      "Epoch [22/50] Step [161/391]  acc1 0.406250 (0.355784)  loss 2.224114 (2.498794)\n",
      "Epoch [22/50] Step [201/391]  acc1 0.296875 (0.354400)  loss 2.888535 (2.499157)\n",
      "Epoch [22/50] Step [241/391]  acc1 0.296875 (0.353021)  loss 2.593546 (2.510369)\n",
      "Epoch [22/50] Step [281/391]  acc1 0.312500 (0.353370)  loss 3.006017 (2.507543)\n",
      "Epoch [22/50] Step [321/391]  acc1 0.390625 (0.353290)  loss 2.381288 (2.508735)\n",
      "Epoch [22/50] Step [361/391]  acc1 0.343750 (0.351541)  loss 2.773003 (2.512898)\n",
      "Epoch [23/50] Step [1/391]  acc1 0.312500 (0.312500)  loss 2.696962 (2.696962)\n",
      "Epoch [23/50] Step [41/391]  acc1 0.343750 (0.350991)  loss 2.269216 (2.512558)\n",
      "Epoch [23/50] Step [81/391]  acc1 0.328125 (0.351466)  loss 2.497707 (2.498725)\n",
      "Epoch [23/50] Step [121/391]  acc1 0.531250 (0.355372)  loss 1.958629 (2.485777)\n",
      "Epoch [23/50] Step [161/391]  acc1 0.453125 (0.361316)  loss 2.165279 (2.469543)\n",
      "Epoch [23/50] Step [201/391]  acc1 0.328125 (0.358520)  loss 2.671132 (2.476883)\n",
      "Epoch [23/50] Step [241/391]  acc1 0.421875 (0.360218)  loss 2.174989 (2.467908)\n",
      "Epoch [23/50] Step [281/391]  acc1 0.359375 (0.360153)  loss 2.424395 (2.467708)\n",
      "Epoch [23/50] Step [321/391]  acc1 0.343750 (0.361663)  loss 2.347255 (2.467791)\n",
      "Epoch [23/50] Step [361/391]  acc1 0.296875 (0.359462)  loss 2.540357 (2.476773)\n",
      "Epoch [24/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 2.221395 (2.221395)\n",
      "Epoch [24/50] Step [41/391]  acc1 0.312500 (0.358232)  loss 2.369941 (2.430371)\n",
      "Epoch [24/50] Step [81/391]  acc1 0.359375 (0.358218)  loss 2.459924 (2.453739)\n",
      "Epoch [24/50] Step [121/391]  acc1 0.375000 (0.362216)  loss 2.284167 (2.434026)\n",
      "Epoch [24/50] Step [161/391]  acc1 0.359375 (0.363451)  loss 2.498641 (2.440457)\n",
      "Epoch [24/50] Step [201/391]  acc1 0.312500 (0.360230)  loss 2.459795 (2.454620)\n",
      "Epoch [24/50] Step [241/391]  acc1 0.390625 (0.361515)  loss 2.444496 (2.451325)\n",
      "Epoch [24/50] Step [281/391]  acc1 0.375000 (0.358096)  loss 2.382281 (2.461727)\n",
      "Epoch [24/50] Step [321/391]  acc1 0.359375 (0.358499)  loss 2.390867 (2.461014)\n",
      "Epoch [24/50] Step [361/391]  acc1 0.343750 (0.358380)  loss 2.639714 (2.464887)\n",
      "Epoch [25/50] Step [1/391]  acc1 0.375000 (0.375000)  loss 2.265093 (2.265093)\n",
      "Epoch [25/50] Step [41/391]  acc1 0.328125 (0.387195)  loss 2.511527 (2.355929)\n",
      "Epoch [25/50] Step [81/391]  acc1 0.328125 (0.386381)  loss 2.378468 (2.372036)\n",
      "Epoch [25/50] Step [121/391]  acc1 0.312500 (0.376420)  loss 2.530470 (2.383829)\n",
      "Epoch [25/50] Step [161/391]  acc1 0.390625 (0.378688)  loss 2.425328 (2.385990)\n",
      "Epoch [25/50] Step [201/391]  acc1 0.531250 (0.375933)  loss 2.209360 (2.389262)\n",
      "Epoch [25/50] Step [241/391]  acc1 0.484375 (0.372407)  loss 2.217709 (2.396987)\n",
      "Epoch [25/50] Step [281/391]  acc1 0.406250 (0.369495)  loss 2.128714 (2.404918)\n",
      "Epoch [25/50] Step [321/391]  acc1 0.312500 (0.368234)  loss 2.580707 (2.411905)\n",
      "Epoch [25/50] Step [361/391]  acc1 0.250000 (0.367642)  loss 2.687049 (2.416754)\n",
      "Epoch [26/50] Step [1/391]  acc1 0.312500 (0.312500)  loss 2.382887 (2.382887)\n",
      "Epoch [26/50] Step [41/391]  acc1 0.406250 (0.378049)  loss 2.350193 (2.401899)\n",
      "Epoch [26/50] Step [81/391]  acc1 0.359375 (0.381752)  loss 2.350627 (2.387993)\n",
      "Epoch [26/50] Step [121/391]  acc1 0.437500 (0.379003)  loss 2.129211 (2.393024)\n",
      "Epoch [26/50] Step [161/391]  acc1 0.437500 (0.379755)  loss 2.377510 (2.386750)\n",
      "Epoch [26/50] Step [201/391]  acc1 0.312500 (0.380286)  loss 2.563399 (2.385112)\n",
      "Epoch [26/50] Step [241/391]  acc1 0.421875 (0.380511)  loss 2.285057 (2.388627)\n",
      "Epoch [26/50] Step [281/391]  acc1 0.343750 (0.380338)  loss 2.725918 (2.390449)\n",
      "Epoch [26/50] Step [321/391]  acc1 0.328125 (0.380160)  loss 2.366579 (2.390740)\n",
      "Epoch [26/50] Step [361/391]  acc1 0.390625 (0.377510)  loss 2.107853 (2.394584)\n",
      "Epoch [27/50] Step [1/391]  acc1 0.296875 (0.296875)  loss 2.614103 (2.614103)\n",
      "Epoch [27/50] Step [41/391]  acc1 0.421875 (0.391387)  loss 2.168843 (2.322900)\n",
      "Epoch [27/50] Step [81/391]  acc1 0.312500 (0.385417)  loss 2.306011 (2.332417)\n",
      "Epoch [27/50] Step [121/391]  acc1 0.390625 (0.391142)  loss 2.259964 (2.315457)\n",
      "Epoch [27/50] Step [161/391]  acc1 0.421875 (0.389655)  loss 2.195470 (2.329616)\n",
      "Epoch [27/50] Step [201/391]  acc1 0.406250 (0.389848)  loss 2.081970 (2.326921)\n",
      "Epoch [27/50] Step [241/391]  acc1 0.437500 (0.386670)  loss 2.390737 (2.337778)\n",
      "Epoch [27/50] Step [281/391]  acc1 0.328125 (0.384008)  loss 2.643094 (2.348337)\n",
      "Epoch [27/50] Step [321/391]  acc1 0.343750 (0.384054)  loss 2.492132 (2.352904)\n",
      "Epoch [27/50] Step [361/391]  acc1 0.437500 (0.384695)  loss 2.193282 (2.348982)\n",
      "Epoch [28/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.280805 (2.280805)\n",
      "Epoch [28/50] Step [41/391]  acc1 0.359375 (0.399771)  loss 2.369404 (2.306544)\n",
      "Epoch [28/50] Step [81/391]  acc1 0.375000 (0.397955)  loss 2.268084 (2.311168)\n",
      "Epoch [28/50] Step [121/391]  acc1 0.406250 (0.395661)  loss 2.289546 (2.308955)\n",
      "Epoch [28/50] Step [161/391]  acc1 0.406250 (0.394895)  loss 2.292883 (2.307066)\n",
      "Epoch [28/50] Step [201/391]  acc1 0.406250 (0.392024)  loss 2.186394 (2.317742)\n",
      "Epoch [28/50] Step [241/391]  acc1 0.328125 (0.390430)  loss 2.489339 (2.316439)\n",
      "Epoch [28/50] Step [281/391]  acc1 0.390625 (0.388846)  loss 2.317604 (2.324403)\n",
      "Epoch [28/50] Step [321/391]  acc1 0.359375 (0.387802)  loss 2.529414 (2.324639)\n",
      "Epoch [28/50] Step [361/391]  acc1 0.406250 (0.385171)  loss 1.989220 (2.335830)\n",
      "Epoch [29/50] Step [1/391]  acc1 0.343750 (0.343750)  loss 2.253608 (2.253608)\n",
      "Epoch [29/50] Step [41/391]  acc1 0.375000 (0.390625)  loss 2.411841 (2.276173)\n",
      "Epoch [29/50] Step [81/391]  acc1 0.359375 (0.390625)  loss 2.533237 (2.304373)\n",
      "Epoch [29/50] Step [121/391]  acc1 0.437500 (0.393208)  loss 2.167376 (2.309015)\n",
      "Epoch [29/50] Step [161/391]  acc1 0.390625 (0.395380)  loss 2.231028 (2.311869)\n",
      "Epoch [29/50] Step [201/391]  acc1 0.312500 (0.393346)  loss 2.525799 (2.310620)\n",
      "Epoch [29/50] Step [241/391]  acc1 0.468750 (0.395163)  loss 1.918278 (2.309976)\n",
      "Epoch [29/50] Step [281/391]  acc1 0.359375 (0.394851)  loss 2.146345 (2.313113)\n",
      "Epoch [29/50] Step [321/391]  acc1 0.500000 (0.395201)  loss 2.012798 (2.315111)\n",
      "Epoch [29/50] Step [361/391]  acc1 0.468750 (0.395386)  loss 2.146243 (2.317234)\n",
      "Epoch [30/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.447715 (2.447715)\n",
      "Epoch [30/50] Step [41/391]  acc1 0.390625 (0.403582)  loss 2.414958 (2.245649)\n",
      "Epoch [30/50] Step [81/391]  acc1 0.500000 (0.401813)  loss 2.309692 (2.258973)\n",
      "Epoch [30/50] Step [121/391]  acc1 0.281250 (0.399535)  loss 2.487502 (2.262891)\n",
      "Epoch [30/50] Step [161/391]  acc1 0.328125 (0.395963)  loss 2.303213 (2.283131)\n",
      "Epoch [30/50] Step [201/391]  acc1 0.343750 (0.398165)  loss 2.219491 (2.278201)\n",
      "Epoch [30/50] Step [241/391]  acc1 0.484375 (0.398600)  loss 2.061874 (2.275871)\n",
      "Epoch [30/50] Step [281/391]  acc1 0.265625 (0.394184)  loss 2.762189 (2.289898)\n",
      "Epoch [30/50] Step [321/391]  acc1 0.531250 (0.393935)  loss 1.933216 (2.287474)\n",
      "Epoch [30/50] Step [361/391]  acc1 0.406250 (0.393438)  loss 2.331823 (2.291333)\n",
      "Epoch [31/50] Step [1/391]  acc1 0.531250 (0.531250)  loss 1.914905 (1.914905)\n",
      "Epoch [31/50] Step [41/391]  acc1 0.343750 (0.385671)  loss 2.420763 (2.302363)\n",
      "Epoch [31/50] Step [81/391]  acc1 0.328125 (0.391782)  loss 2.433204 (2.297937)\n",
      "Epoch [31/50] Step [121/391]  acc1 0.312500 (0.389850)  loss 2.304482 (2.279772)\n",
      "Epoch [31/50] Step [161/391]  acc1 0.375000 (0.393925)  loss 2.505282 (2.284139)\n",
      "Epoch [31/50] Step [201/391]  acc1 0.390625 (0.395756)  loss 2.386219 (2.281276)\n",
      "Epoch [31/50] Step [241/391]  acc1 0.328125 (0.397044)  loss 2.626953 (2.285313)\n",
      "Epoch [31/50] Step [281/391]  acc1 0.406250 (0.396630)  loss 2.019872 (2.286012)\n",
      "Epoch [31/50] Step [321/391]  acc1 0.468750 (0.399289)  loss 2.098340 (2.274738)\n",
      "Epoch [31/50] Step [361/391]  acc1 0.343750 (0.400364)  loss 2.649643 (2.272646)\n",
      "Epoch [32/50] Step [1/391]  acc1 0.375000 (0.375000)  loss 2.310547 (2.310547)\n",
      "Epoch [32/50] Step [41/391]  acc1 0.343750 (0.404726)  loss 2.510922 (2.269383)\n",
      "Epoch [32/50] Step [81/391]  acc1 0.437500 (0.406636)  loss 2.197398 (2.249368)\n",
      "Epoch [32/50] Step [121/391]  acc1 0.343750 (0.412577)  loss 2.387411 (2.224646)\n",
      "Epoch [32/50] Step [161/391]  acc1 0.421875 (0.410229)  loss 2.233968 (2.234026)\n",
      "Epoch [32/50] Step [201/391]  acc1 0.437500 (0.411147)  loss 2.095033 (2.228130)\n",
      "Epoch [32/50] Step [241/391]  acc1 0.359375 (0.409232)  loss 2.308716 (2.234170)\n",
      "Epoch [32/50] Step [281/391]  acc1 0.437500 (0.411254)  loss 1.968595 (2.231780)\n",
      "Epoch [32/50] Step [321/391]  acc1 0.437500 (0.410339)  loss 2.170713 (2.233544)\n",
      "Epoch [32/50] Step [361/391]  acc1 0.343750 (0.409972)  loss 2.212784 (2.237525)\n",
      "Epoch [33/50] Step [1/391]  acc1 0.328125 (0.328125)  loss 2.421771 (2.421771)\n",
      "Epoch [33/50] Step [41/391]  acc1 0.515625 (0.403201)  loss 2.107332 (2.254890)\n",
      "Epoch [33/50] Step [81/391]  acc1 0.406250 (0.411844)  loss 2.203273 (2.198517)\n",
      "Epoch [33/50] Step [121/391]  acc1 0.265625 (0.411544)  loss 2.866821 (2.204086)\n",
      "Epoch [33/50] Step [161/391]  acc1 0.453125 (0.412946)  loss 2.226366 (2.200287)\n",
      "Epoch [33/50] Step [201/391]  acc1 0.312500 (0.411070)  loss 2.449377 (2.206867)\n",
      "Epoch [33/50] Step [241/391]  acc1 0.312500 (0.409946)  loss 2.514251 (2.207245)\n",
      "Epoch [33/50] Step [281/391]  acc1 0.406250 (0.410532)  loss 2.391728 (2.211648)\n",
      "Epoch [33/50] Step [321/391]  acc1 0.406250 (0.412383)  loss 2.329335 (2.212159)\n",
      "Epoch [33/50] Step [361/391]  acc1 0.468750 (0.412829)  loss 2.073828 (2.213319)\n",
      "Epoch [34/50] Step [1/391]  acc1 0.515625 (0.515625)  loss 1.965463 (1.965463)\n",
      "Epoch [34/50] Step [41/391]  acc1 0.484375 (0.407012)  loss 1.957198 (2.217645)\n",
      "Epoch [34/50] Step [81/391]  acc1 0.328125 (0.409915)  loss 2.375486 (2.196303)\n",
      "Epoch [34/50] Step [121/391]  acc1 0.453125 (0.414902)  loss 1.995831 (2.181717)\n",
      "Epoch [34/50] Step [161/391]  acc1 0.390625 (0.416052)  loss 2.262824 (2.197280)\n",
      "Epoch [34/50] Step [201/391]  acc1 0.421875 (0.414568)  loss 2.303313 (2.203441)\n",
      "Epoch [34/50] Step [241/391]  acc1 0.468750 (0.417012)  loss 2.239071 (2.197066)\n",
      "Epoch [34/50] Step [281/391]  acc1 0.437500 (0.418706)  loss 2.133649 (2.188316)\n",
      "Epoch [34/50] Step [321/391]  acc1 0.312500 (0.419198)  loss 2.282958 (2.189090)\n",
      "Epoch [34/50] Step [361/391]  acc1 0.468750 (0.418499)  loss 2.249262 (2.192368)\n",
      "Epoch [35/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.397550 (2.397550)\n",
      "Epoch [35/50] Step [41/391]  acc1 0.375000 (0.425305)  loss 2.114345 (2.173692)\n",
      "Epoch [35/50] Step [81/391]  acc1 0.437500 (0.416860)  loss 2.020182 (2.192145)\n",
      "Epoch [35/50] Step [121/391]  acc1 0.500000 (0.416193)  loss 1.939483 (2.197718)\n",
      "Epoch [35/50] Step [161/391]  acc1 0.296875 (0.420710)  loss 2.388006 (2.184833)\n",
      "Epoch [35/50] Step [201/391]  acc1 0.468750 (0.423741)  loss 2.068500 (2.181342)\n",
      "Epoch [35/50] Step [241/391]  acc1 0.390625 (0.422329)  loss 2.340780 (2.187427)\n",
      "Epoch [35/50] Step [281/391]  acc1 0.468750 (0.421319)  loss 2.043751 (2.188074)\n",
      "Epoch [35/50] Step [321/391]  acc1 0.406250 (0.423822)  loss 2.367240 (2.181167)\n",
      "Epoch [35/50] Step [361/391]  acc1 0.390625 (0.422741)  loss 2.337045 (2.183562)\n",
      "Epoch [36/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 1.866583 (1.866583)\n",
      "Epoch [36/50] Step [41/391]  acc1 0.437500 (0.436357)  loss 2.160651 (2.143137)\n",
      "Epoch [36/50] Step [81/391]  acc1 0.359375 (0.430363)  loss 2.252819 (2.158462)\n",
      "Epoch [36/50] Step [121/391]  acc1 0.453125 (0.426653)  loss 2.151103 (2.165337)\n",
      "Epoch [36/50] Step [161/391]  acc1 0.328125 (0.422166)  loss 2.301100 (2.175517)\n",
      "Epoch [36/50] Step [201/391]  acc1 0.328125 (0.423507)  loss 2.135224 (2.175566)\n",
      "Epoch [36/50] Step [241/391]  acc1 0.406250 (0.424857)  loss 2.075771 (2.170981)\n",
      "Epoch [36/50] Step [281/391]  acc1 0.453125 (0.425823)  loss 2.313590 (2.167709)\n",
      "Epoch [36/50] Step [321/391]  acc1 0.359375 (0.425380)  loss 2.331740 (2.167617)\n",
      "Epoch [36/50] Step [361/391]  acc1 0.437500 (0.425597)  loss 2.133651 (2.170588)\n",
      "Epoch [37/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.482478 (2.482478)\n",
      "Epoch [37/50] Step [41/391]  acc1 0.484375 (0.432927)  loss 1.926243 (2.155718)\n",
      "Epoch [37/50] Step [81/391]  acc1 0.421875 (0.433449)  loss 2.119453 (2.138940)\n",
      "Epoch [37/50] Step [121/391]  acc1 0.468750 (0.431043)  loss 2.095197 (2.149854)\n",
      "Epoch [37/50] Step [161/391]  acc1 0.359375 (0.428766)  loss 2.440259 (2.158511)\n",
      "Epoch [37/50] Step [201/391]  acc1 0.468750 (0.430037)  loss 2.063017 (2.155837)\n",
      "Epoch [37/50] Step [241/391]  acc1 0.359375 (0.430044)  loss 2.275905 (2.154273)\n",
      "Epoch [37/50] Step [281/391]  acc1 0.484375 (0.429382)  loss 2.012952 (2.155364)\n",
      "Epoch [37/50] Step [321/391]  acc1 0.437500 (0.429809)  loss 2.249964 (2.152310)\n",
      "Epoch [37/50] Step [361/391]  acc1 0.406250 (0.430142)  loss 2.357893 (2.152037)\n",
      "Epoch [38/50] Step [1/391]  acc1 0.406250 (0.406250)  loss 2.216784 (2.216784)\n",
      "Epoch [38/50] Step [41/391]  acc1 0.406250 (0.437881)  loss 1.956433 (2.109134)\n",
      "Epoch [38/50] Step [81/391]  acc1 0.406250 (0.438079)  loss 2.511122 (2.135459)\n",
      "Epoch [38/50] Step [121/391]  acc1 0.453125 (0.434272)  loss 2.109921 (2.127776)\n",
      "Epoch [38/50] Step [161/391]  acc1 0.421875 (0.432065)  loss 2.044903 (2.137782)\n",
      "Epoch [38/50] Step [201/391]  acc1 0.531250 (0.431592)  loss 1.941518 (2.139278)\n",
      "Epoch [38/50] Step [241/391]  acc1 0.468750 (0.435620)  loss 1.910177 (2.126346)\n",
      "Epoch [38/50] Step [281/391]  acc1 0.375000 (0.432940)  loss 2.422195 (2.135636)\n",
      "Epoch [38/50] Step [321/391]  acc1 0.515625 (0.431221)  loss 1.921854 (2.145550)\n",
      "Epoch [38/50] Step [361/391]  acc1 0.484375 (0.430532)  loss 1.829651 (2.144544)\n",
      "Epoch [39/50] Step [1/391]  acc1 0.562500 (0.562500)  loss 2.049003 (2.049003)\n",
      "Epoch [39/50] Step [41/391]  acc1 0.484375 (0.459985)  loss 2.085543 (2.041143)\n",
      "Epoch [39/50] Step [81/391]  acc1 0.421875 (0.440972)  loss 2.359126 (2.084550)\n",
      "Epoch [39/50] Step [121/391]  acc1 0.437500 (0.435563)  loss 2.098304 (2.104548)\n",
      "Epoch [39/50] Step [161/391]  acc1 0.484375 (0.429542)  loss 2.123870 (2.130485)\n",
      "Epoch [39/50] Step [201/391]  acc1 0.500000 (0.428794)  loss 2.051516 (2.134700)\n",
      "Epoch [39/50] Step [241/391]  acc1 0.296875 (0.428618)  loss 2.501368 (2.133977)\n",
      "Epoch [39/50] Step [281/391]  acc1 0.250000 (0.430049)  loss 2.380414 (2.128538)\n",
      "Epoch [39/50] Step [321/391]  acc1 0.468750 (0.430247)  loss 1.864434 (2.121414)\n",
      "Epoch [39/50] Step [361/391]  acc1 0.437500 (0.430445)  loss 2.172122 (2.123366)\n",
      "Epoch [40/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.688648 (2.688648)\n",
      "Epoch [40/50] Step [41/391]  acc1 0.328125 (0.445503)  loss 2.302095 (2.078661)\n",
      "Epoch [40/50] Step [81/391]  acc1 0.281250 (0.441165)  loss 2.494400 (2.091569)\n",
      "Epoch [40/50] Step [121/391]  acc1 0.421875 (0.440857)  loss 2.010778 (2.095899)\n",
      "Epoch [40/50] Step [161/391]  acc1 0.500000 (0.441576)  loss 2.014182 (2.103297)\n",
      "Epoch [40/50] Step [201/391]  acc1 0.375000 (0.441931)  loss 2.388147 (2.099403)\n",
      "Epoch [40/50] Step [241/391]  acc1 0.437500 (0.441066)  loss 1.872171 (2.099392)\n",
      "Epoch [40/50] Step [281/391]  acc1 0.453125 (0.439446)  loss 1.825337 (2.100285)\n",
      "Epoch [40/50] Step [321/391]  acc1 0.328125 (0.438766)  loss 2.278496 (2.099553)\n",
      "Epoch [40/50] Step [361/391]  acc1 0.343750 (0.437327)  loss 2.299017 (2.108023)\n",
      "Epoch [41/50] Step [1/391]  acc1 0.562500 (0.562500)  loss 1.770871 (1.770871)\n",
      "Epoch [41/50] Step [41/391]  acc1 0.390625 (0.448171)  loss 2.162629 (2.041584)\n",
      "Epoch [41/50] Step [81/391]  acc1 0.437500 (0.446373)  loss 2.090253 (2.062576)\n",
      "Epoch [41/50] Step [121/391]  acc1 0.468750 (0.439824)  loss 2.038598 (2.094496)\n",
      "Epoch [41/50] Step [161/391]  acc1 0.328125 (0.438568)  loss 2.060635 (2.094997)\n",
      "Epoch [41/50] Step [201/391]  acc1 0.390625 (0.439132)  loss 1.943971 (2.087044)\n",
      "Epoch [41/50] Step [241/391]  acc1 0.531250 (0.442103)  loss 2.103883 (2.082301)\n",
      "Epoch [41/50] Step [281/391]  acc1 0.453125 (0.442727)  loss 2.208879 (2.083768)\n",
      "Epoch [41/50] Step [321/391]  acc1 0.406250 (0.441783)  loss 2.230599 (2.089642)\n",
      "Epoch [41/50] Step [361/391]  acc1 0.546875 (0.440746)  loss 1.888961 (2.090653)\n",
      "Epoch [42/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.302628 (2.302628)\n",
      "Epoch [42/50] Step [41/391]  acc1 0.531250 (0.455412)  loss 1.656383 (2.070995)\n",
      "Epoch [42/50] Step [81/391]  acc1 0.406250 (0.451196)  loss 2.128575 (2.064656)\n",
      "Epoch [42/50] Step [121/391]  acc1 0.484375 (0.449251)  loss 1.811995 (2.060112)\n",
      "Epoch [42/50] Step [161/391]  acc1 0.468750 (0.449825)  loss 2.246789 (2.054997)\n",
      "Epoch [42/50] Step [201/391]  acc1 0.390625 (0.450871)  loss 1.976001 (2.057714)\n",
      "Epoch [42/50] Step [241/391]  acc1 0.437500 (0.449689)  loss 2.119724 (2.058642)\n",
      "Epoch [42/50] Step [281/391]  acc1 0.500000 (0.446619)  loss 1.927013 (2.069232)\n",
      "Epoch [42/50] Step [321/391]  acc1 0.421875 (0.446213)  loss 2.266320 (2.066038)\n",
      "Epoch [42/50] Step [361/391]  acc1 0.390625 (0.444295)  loss 2.046425 (2.071204)\n",
      "Epoch [43/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 1.971965 (1.971965)\n",
      "Epoch [43/50] Step [41/391]  acc1 0.453125 (0.447409)  loss 2.219982 (2.067386)\n",
      "Epoch [43/50] Step [81/391]  acc1 0.406250 (0.451968)  loss 2.344650 (2.059405)\n",
      "Epoch [43/50] Step [121/391]  acc1 0.437500 (0.452350)  loss 2.155686 (2.056145)\n",
      "Epoch [43/50] Step [161/391]  acc1 0.515625 (0.453513)  loss 2.014147 (2.058892)\n",
      "Epoch [43/50] Step [201/391]  acc1 0.531250 (0.451648)  loss 1.834328 (2.057875)\n",
      "Epoch [43/50] Step [241/391]  acc1 0.421875 (0.451634)  loss 2.388767 (2.056554)\n",
      "Epoch [43/50] Step [281/391]  acc1 0.406250 (0.450067)  loss 2.316464 (2.064395)\n",
      "Epoch [43/50] Step [321/391]  acc1 0.437500 (0.450448)  loss 2.086090 (2.063048)\n",
      "Epoch [43/50] Step [361/391]  acc1 0.531250 (0.450571)  loss 1.786291 (2.060969)\n",
      "Epoch [44/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.296764 (2.296764)\n",
      "Epoch [44/50] Step [41/391]  acc1 0.500000 (0.458841)  loss 1.740294 (2.037277)\n",
      "Epoch [44/50] Step [81/391]  acc1 0.390625 (0.457755)  loss 2.255004 (2.034781)\n",
      "Epoch [44/50] Step [121/391]  acc1 0.390625 (0.452221)  loss 2.230544 (2.035046)\n",
      "Epoch [44/50] Step [161/391]  acc1 0.421875 (0.449728)  loss 2.244852 (2.041749)\n",
      "Epoch [44/50] Step [201/391]  acc1 0.546875 (0.448305)  loss 1.861773 (2.044135)\n",
      "Epoch [44/50] Step [241/391]  acc1 0.375000 (0.448976)  loss 2.092090 (2.038373)\n",
      "Epoch [44/50] Step [281/391]  acc1 0.437500 (0.447509)  loss 1.837460 (2.042580)\n",
      "Epoch [44/50] Step [321/391]  acc1 0.500000 (0.446116)  loss 1.859262 (2.048300)\n",
      "Epoch [44/50] Step [361/391]  acc1 0.546875 (0.446589)  loss 1.709601 (2.046614)\n",
      "Epoch [45/50] Step [1/391]  acc1 0.625000 (0.625000)  loss 1.703101 (1.703101)\n",
      "Epoch [45/50] Step [41/391]  acc1 0.468750 (0.469512)  loss 2.117781 (1.968678)\n",
      "Epoch [45/50] Step [81/391]  acc1 0.437500 (0.448688)  loss 1.744385 (2.036572)\n",
      "Epoch [45/50] Step [121/391]  acc1 0.453125 (0.451705)  loss 2.052025 (2.025382)\n",
      "Epoch [45/50] Step [161/391]  acc1 0.421875 (0.452349)  loss 2.286723 (2.027535)\n",
      "Epoch [45/50] Step [201/391]  acc1 0.546875 (0.453825)  loss 1.566893 (2.028008)\n",
      "Epoch [45/50] Step [241/391]  acc1 0.515625 (0.454033)  loss 1.846204 (2.030959)\n",
      "Epoch [45/50] Step [281/391]  acc1 0.593750 (0.455016)  loss 1.726053 (2.024614)\n",
      "Epoch [45/50] Step [321/391]  acc1 0.437500 (0.453855)  loss 2.034108 (2.026438)\n",
      "Epoch [45/50] Step [361/391]  acc1 0.406250 (0.454553)  loss 2.162693 (2.029822)\n",
      "Epoch [46/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.157997 (2.157997)\n",
      "Epoch [46/50] Step [41/391]  acc1 0.500000 (0.460366)  loss 2.010216 (1.992809)\n",
      "Epoch [46/50] Step [81/391]  acc1 0.515625 (0.463542)  loss 1.953397 (1.999096)\n",
      "Epoch [46/50] Step [121/391]  acc1 0.546875 (0.463585)  loss 1.711623 (1.996054)\n",
      "Epoch [46/50] Step [161/391]  acc1 0.406250 (0.463412)  loss 2.128744 (2.000339)\n",
      "Epoch [46/50] Step [201/391]  acc1 0.468750 (0.459344)  loss 1.868255 (2.010160)\n",
      "Epoch [46/50] Step [241/391]  acc1 0.421875 (0.456626)  loss 1.789262 (2.017554)\n",
      "Epoch [46/50] Step [281/391]  acc1 0.515625 (0.456016)  loss 1.639225 (2.017247)\n",
      "Epoch [46/50] Step [321/391]  acc1 0.468750 (0.457311)  loss 2.085381 (2.016365)\n",
      "Epoch [46/50] Step [361/391]  acc1 0.375000 (0.456501)  loss 2.230726 (2.020379)\n",
      "Epoch [47/50] Step [1/391]  acc1 0.531250 (0.531250)  loss 1.847315 (1.847315)\n",
      "Epoch [47/50] Step [41/391]  acc1 0.500000 (0.467607)  loss 1.777926 (1.972422)\n",
      "Epoch [47/50] Step [81/391]  acc1 0.484375 (0.467207)  loss 1.844510 (1.968600)\n",
      "Epoch [47/50] Step [121/391]  acc1 0.406250 (0.461131)  loss 2.242487 (1.982044)\n",
      "Epoch [47/50] Step [161/391]  acc1 0.546875 (0.462733)  loss 1.704732 (1.985268)\n",
      "Epoch [47/50] Step [201/391]  acc1 0.593750 (0.463464)  loss 1.752568 (1.989413)\n",
      "Epoch [47/50] Step [241/391]  acc1 0.375000 (0.460516)  loss 2.093852 (2.003165)\n",
      "Epoch [47/50] Step [281/391]  acc1 0.484375 (0.458685)  loss 2.002001 (2.007057)\n",
      "Epoch [47/50] Step [321/391]  acc1 0.437500 (0.458917)  loss 2.229535 (2.007595)\n",
      "Epoch [47/50] Step [361/391]  acc1 0.468750 (0.458189)  loss 1.904627 (2.011414)\n",
      "Epoch [48/50] Step [1/391]  acc1 0.515625 (0.515625)  loss 1.917276 (1.917276)\n",
      "Epoch [48/50] Step [41/391]  acc1 0.546875 (0.482088)  loss 1.738653 (1.950285)\n",
      "Epoch [48/50] Step [81/391]  acc1 0.343750 (0.467014)  loss 2.279734 (1.954236)\n",
      "Epoch [48/50] Step [121/391]  acc1 0.515625 (0.466426)  loss 1.766477 (1.965793)\n",
      "Epoch [48/50] Step [161/391]  acc1 0.406250 (0.463800)  loss 1.917299 (1.976122)\n",
      "Epoch [48/50] Step [201/391]  acc1 0.375000 (0.458644)  loss 2.409271 (1.998190)\n",
      "Epoch [48/50] Step [241/391]  acc1 0.546875 (0.454940)  loss 1.793000 (2.008901)\n",
      "Epoch [48/50] Step [281/391]  acc1 0.468750 (0.455016)  loss 1.963124 (2.008085)\n",
      "Epoch [48/50] Step [321/391]  acc1 0.437500 (0.454245)  loss 1.895184 (2.009740)\n",
      "Epoch [48/50] Step [361/391]  acc1 0.453125 (0.456328)  loss 1.970381 (2.009872)\n",
      "Epoch [49/50] Step [1/391]  acc1 0.609375 (0.609375)  loss 1.604234 (1.604234)\n",
      "Epoch [49/50] Step [41/391]  acc1 0.546875 (0.485899)  loss 1.607818 (1.917488)\n",
      "Epoch [49/50] Step [81/391]  acc1 0.453125 (0.473380)  loss 1.902386 (1.959832)\n",
      "Epoch [49/50] Step [121/391]  acc1 0.437500 (0.466942)  loss 2.070390 (1.977723)\n",
      "Epoch [49/50] Step [161/391]  acc1 0.468750 (0.464577)  loss 1.886137 (1.980446)\n",
      "Epoch [49/50] Step [201/391]  acc1 0.453125 (0.464708)  loss 2.117001 (1.981682)\n",
      "Epoch [49/50] Step [241/391]  acc1 0.390625 (0.467388)  loss 2.300347 (1.976981)\n",
      "Epoch [49/50] Step [281/391]  acc1 0.453125 (0.464802)  loss 1.916743 (1.981287)\n",
      "Epoch [49/50] Step [321/391]  acc1 0.453125 (0.462422)  loss 2.169253 (1.985005)\n",
      "Epoch [49/50] Step [361/391]  acc1 0.437500 (0.460396)  loss 2.038710 (1.994152)\n",
      "Epoch [50/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 2.042091 (2.042091)\n",
      "Epoch [50/50] Step [41/391]  acc1 0.484375 (0.463415)  loss 1.997407 (1.990906)\n",
      "Epoch [50/50] Step [81/391]  acc1 0.468750 (0.462577)  loss 1.807420 (1.995727)\n",
      "Epoch [50/50] Step [121/391]  acc1 0.453125 (0.462681)  loss 1.891209 (1.997229)\n",
      "Epoch [50/50] Step [161/391]  acc1 0.531250 (0.466809)  loss 1.821464 (1.983800)\n",
      "Epoch [50/50] Step [201/391]  acc1 0.453125 (0.466185)  loss 2.060024 (1.984424)\n",
      "Epoch [50/50] Step [241/391]  acc1 0.562500 (0.468555)  loss 1.760713 (1.978990)\n",
      "Epoch [50/50] Step [281/391]  acc1 0.406250 (0.466581)  loss 2.136601 (1.984153)\n",
      "Epoch [50/50] Step [321/391]  acc1 0.468750 (0.464467)  loss 1.874708 (1.991346)\n",
      "Epoch [50/50] Step [361/391]  acc1 0.421875 (0.464292)  loss 2.105689 (1.995284)\n",
      "Final architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'sepconv5x5', 'reduce_n3_p0': 'sepconv5x5', 'reduce_n3_p1': 'sepconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'sepconv3x3', 'reduce_n4_p1': 'maxpool', 'reduce_n4_p2': 'sepconv3x3', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv5x5', 'reduce_n5_p1': 'maxpool', 'reduce_n5_p2': 'maxpool', 'reduce_n5_p3': 'sepconv3x3', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [1], 'reduce_n3_switch': [0], 'reduce_n4_switch': [2], 'reduce_n5_switch': [4]}\n",
      "Epoch [1/50] Step [1/391]  acc1 0.062500 (0.062500)  loss 4.595099 (4.595099)\n",
      "Epoch [1/50] Step [41/391]  acc1 0.062500 (0.023247)  loss 4.361245 (4.550597)\n",
      "Epoch [1/50] Step [81/391]  acc1 0.015625 (0.034722)  loss 4.246338 (4.439270)\n",
      "Epoch [1/50] Step [121/391]  acc1 0.015625 (0.042872)  loss 4.201808 (4.363267)\n",
      "Epoch [1/50] Step [161/391]  acc1 0.062500 (0.050078)  loss 4.072190 (4.310161)\n",
      "Epoch [1/50] Step [201/391]  acc1 0.125000 (0.056281)  loss 3.776328 (4.254182)\n",
      "Epoch [1/50] Step [241/391]  acc1 0.109375 (0.064056)  loss 3.951974 (4.207480)\n",
      "Epoch [1/50] Step [281/391]  acc1 0.109375 (0.068783)  loss 3.866901 (4.169465)\n",
      "Epoch [1/50] Step [321/391]  acc1 0.156250 (0.073355)  loss 3.771845 (4.129868)\n",
      "Epoch [1/50] Step [361/391]  acc1 0.078125 (0.077649)  loss 3.851888 (4.099960)\n",
      "Epoch [2/50] Step [1/391]  acc1 0.093750 (0.093750)  loss 3.719956 (3.719956)\n",
      "Epoch [2/50] Step [41/391]  acc1 0.140625 (0.125762)  loss 3.640859 (3.721247)\n",
      "Epoch [2/50] Step [81/391]  acc1 0.093750 (0.127701)  loss 3.630042 (3.722266)\n",
      "Epoch [2/50] Step [121/391]  acc1 0.078125 (0.129520)  loss 3.731066 (3.702230)\n",
      "Epoch [2/50] Step [161/391]  acc1 0.156250 (0.133637)  loss 3.774597 (3.691448)\n",
      "Epoch [2/50] Step [201/391]  acc1 0.140625 (0.133629)  loss 3.877980 (3.680823)\n",
      "Epoch [2/50] Step [241/391]  acc1 0.109375 (0.134336)  loss 3.656757 (3.680585)\n",
      "Epoch [2/50] Step [281/391]  acc1 0.171875 (0.136510)  loss 3.553190 (3.664507)\n",
      "Epoch [2/50] Step [321/391]  acc1 0.187500 (0.138045)  loss 3.812565 (3.652613)\n",
      "Epoch [2/50] Step [361/391]  acc1 0.187500 (0.138547)  loss 3.915035 (3.647077)\n",
      "Epoch [3/50] Step [1/391]  acc1 0.296875 (0.296875)  loss 3.313653 (3.313653)\n",
      "Epoch [3/50] Step [41/391]  acc1 0.171875 (0.162348)  loss 3.283980 (3.506563)\n",
      "Epoch [3/50] Step [81/391]  acc1 0.171875 (0.172261)  loss 3.286422 (3.480734)\n",
      "Epoch [3/50] Step [121/391]  acc1 0.203125 (0.168130)  loss 3.581806 (3.486845)\n",
      "Epoch [3/50] Step [161/391]  acc1 0.171875 (0.167605)  loss 3.428442 (3.474478)\n",
      "Epoch [3/50] Step [201/391]  acc1 0.218750 (0.168532)  loss 3.327346 (3.467683)\n",
      "Epoch [3/50] Step [241/391]  acc1 0.218750 (0.169930)  loss 3.113690 (3.454238)\n",
      "Epoch [3/50] Step [281/391]  acc1 0.218750 (0.171541)  loss 3.474217 (3.446186)\n",
      "Epoch [3/50] Step [321/391]  acc1 0.234375 (0.173433)  loss 3.236660 (3.438225)\n",
      "Epoch [3/50] Step [361/391]  acc1 0.187500 (0.173347)  loss 3.672780 (3.441802)\n",
      "Epoch [4/50] Step [1/391]  acc1 0.125000 (0.125000)  loss 3.797978 (3.797978)\n",
      "Epoch [4/50] Step [41/391]  acc1 0.234375 (0.201982)  loss 3.297940 (3.317768)\n",
      "Epoch [4/50] Step [81/391]  acc1 0.187500 (0.195602)  loss 3.317511 (3.334480)\n",
      "Epoch [4/50] Step [121/391]  acc1 0.328125 (0.200413)  loss 3.221688 (3.318526)\n",
      "Epoch [4/50] Step [161/391]  acc1 0.234375 (0.201087)  loss 3.067494 (3.302314)\n",
      "Epoch [4/50] Step [201/391]  acc1 0.140625 (0.195274)  loss 3.473186 (3.317504)\n",
      "Epoch [4/50] Step [241/391]  acc1 0.265625 (0.195993)  loss 3.017689 (3.314709)\n",
      "Epoch [4/50] Step [281/391]  acc1 0.265625 (0.196786)  loss 3.219918 (3.309941)\n",
      "Epoch [4/50] Step [321/391]  acc1 0.234375 (0.196408)  loss 3.056372 (3.311610)\n",
      "Epoch [4/50] Step [361/391]  acc1 0.234375 (0.196676)  loss 3.338834 (3.308543)\n",
      "Epoch [5/50] Step [1/391]  acc1 0.265625 (0.265625)  loss 3.122021 (3.122021)\n",
      "Epoch [5/50] Step [41/391]  acc1 0.218750 (0.217988)  loss 3.477625 (3.237596)\n",
      "Epoch [5/50] Step [81/391]  acc1 0.171875 (0.215471)  loss 3.213391 (3.224101)\n",
      "Epoch [5/50] Step [121/391]  acc1 0.187500 (0.215909)  loss 3.309816 (3.217730)\n",
      "Epoch [5/50] Step [161/391]  acc1 0.187500 (0.216033)  loss 3.197781 (3.209866)\n",
      "Epoch [5/50] Step [201/391]  acc1 0.187500 (0.215718)  loss 3.329959 (3.205055)\n",
      "Epoch [5/50] Step [241/391]  acc1 0.171875 (0.215703)  loss 3.304058 (3.203380)\n",
      "Epoch [5/50] Step [281/391]  acc1 0.187500 (0.217527)  loss 3.386013 (3.201063)\n",
      "Epoch [5/50] Step [321/391]  acc1 0.250000 (0.218896)  loss 3.055696 (3.191678)\n",
      "Epoch [5/50] Step [361/391]  acc1 0.234375 (0.217625)  loss 3.038825 (3.195394)\n",
      "Epoch [6/50] Step [1/391]  acc1 0.203125 (0.203125)  loss 3.159829 (3.159829)\n",
      "Epoch [6/50] Step [41/391]  acc1 0.218750 (0.247332)  loss 3.188016 (3.061380)\n",
      "Epoch [6/50] Step [81/391]  acc1 0.171875 (0.244792)  loss 3.126696 (3.066949)\n",
      "Epoch [6/50] Step [121/391]  acc1 0.281250 (0.236441)  loss 2.844861 (3.084377)\n",
      "Epoch [6/50] Step [161/391]  acc1 0.281250 (0.236413)  loss 3.052947 (3.089948)\n",
      "Epoch [6/50] Step [201/391]  acc1 0.156250 (0.237329)  loss 3.323850 (3.089968)\n",
      "Epoch [6/50] Step [241/391]  acc1 0.281250 (0.237682)  loss 2.890895 (3.080663)\n",
      "Epoch [6/50] Step [281/391]  acc1 0.156250 (0.234987)  loss 3.347491 (3.084495)\n",
      "Epoch [6/50] Step [321/391]  acc1 0.187500 (0.236419)  loss 3.389189 (3.088200)\n",
      "Epoch [6/50] Step [361/391]  acc1 0.265625 (0.237751)  loss 2.865597 (3.085110)\n",
      "Epoch [7/50] Step [1/391]  acc1 0.281250 (0.281250)  loss 3.017955 (3.017955)\n",
      "Epoch [7/50] Step [41/391]  acc1 0.375000 (0.262576)  loss 2.648529 (2.991336)\n",
      "Epoch [7/50] Step [81/391]  acc1 0.296875 (0.263696)  loss 2.819146 (2.974189)\n",
      "Epoch [7/50] Step [121/391]  acc1 0.250000 (0.257877)  loss 3.057065 (2.995272)\n",
      "Epoch [7/50] Step [161/391]  acc1 0.203125 (0.254950)  loss 3.243785 (3.000217)\n",
      "Epoch [7/50] Step [201/391]  acc1 0.250000 (0.252954)  loss 3.126155 (3.008752)\n",
      "Epoch [7/50] Step [241/391]  acc1 0.281250 (0.252399)  loss 2.818525 (3.003465)\n",
      "Epoch [7/50] Step [281/391]  acc1 0.203125 (0.252502)  loss 2.883203 (2.999430)\n",
      "Epoch [7/50] Step [321/391]  acc1 0.250000 (0.251898)  loss 2.751439 (2.998908)\n",
      "Epoch [7/50] Step [361/391]  acc1 0.296875 (0.252034)  loss 2.809933 (3.000447)\n",
      "Epoch [8/50] Step [1/391]  acc1 0.343750 (0.343750)  loss 2.945497 (2.945497)\n",
      "Epoch [8/50] Step [41/391]  acc1 0.375000 (0.290396)  loss 2.606933 (2.838778)\n",
      "Epoch [8/50] Step [81/391]  acc1 0.218750 (0.283179)  loss 2.631189 (2.853412)\n",
      "Epoch [8/50] Step [121/391]  acc1 0.359375 (0.278667)  loss 2.518088 (2.876991)\n",
      "Epoch [8/50] Step [161/391]  acc1 0.203125 (0.277077)  loss 2.981039 (2.879262)\n",
      "Epoch [8/50] Step [201/391]  acc1 0.250000 (0.274642)  loss 2.886727 (2.883575)\n",
      "Epoch [8/50] Step [241/391]  acc1 0.234375 (0.272238)  loss 3.172644 (2.888117)\n",
      "Epoch [8/50] Step [281/391]  acc1 0.390625 (0.273410)  loss 2.783694 (2.887263)\n",
      "Epoch [8/50] Step [321/391]  acc1 0.265625 (0.271515)  loss 2.810868 (2.892679)\n",
      "Epoch [8/50] Step [361/391]  acc1 0.343750 (0.271944)  loss 2.953356 (2.893364)\n",
      "Epoch [9/50] Step [1/391]  acc1 0.343750 (0.343750)  loss 2.599521 (2.599521)\n",
      "Epoch [9/50] Step [41/391]  acc1 0.250000 (0.297256)  loss 2.859574 (2.842741)\n",
      "Epoch [9/50] Step [81/391]  acc1 0.328125 (0.291088)  loss 2.436933 (2.821103)\n",
      "Epoch [9/50] Step [121/391]  acc1 0.250000 (0.286544)  loss 2.903247 (2.827458)\n",
      "Epoch [9/50] Step [161/391]  acc1 0.296875 (0.285035)  loss 2.783112 (2.823665)\n",
      "Epoch [9/50] Step [201/391]  acc1 0.218750 (0.283504)  loss 2.891736 (2.821950)\n",
      "Epoch [9/50] Step [241/391]  acc1 0.328125 (0.280407)  loss 2.774904 (2.829588)\n",
      "Epoch [9/50] Step [281/391]  acc1 0.203125 (0.281028)  loss 2.838805 (2.827704)\n",
      "Epoch [9/50] Step [321/391]  acc1 0.296875 (0.282467)  loss 2.657444 (2.825764)\n",
      "Epoch [9/50] Step [361/391]  acc1 0.328125 (0.283977)  loss 2.800386 (2.820818)\n",
      "Epoch [10/50] Step [1/391]  acc1 0.218750 (0.218750)  loss 2.879713 (2.879713)\n",
      "Epoch [10/50] Step [41/391]  acc1 0.390625 (0.306402)  loss 2.473841 (2.700092)\n",
      "Epoch [10/50] Step [81/391]  acc1 0.281250 (0.299383)  loss 2.796404 (2.744227)\n",
      "Epoch [10/50] Step [121/391]  acc1 0.265625 (0.300749)  loss 2.907562 (2.739668)\n",
      "Epoch [10/50] Step [161/391]  acc1 0.390625 (0.302407)  loss 2.538041 (2.746143)\n",
      "Epoch [10/50] Step [201/391]  acc1 0.281250 (0.303249)  loss 2.925521 (2.750717)\n",
      "Epoch [10/50] Step [241/391]  acc1 0.281250 (0.303812)  loss 2.740028 (2.742965)\n",
      "Epoch [10/50] Step [281/391]  acc1 0.281250 (0.304437)  loss 2.759290 (2.742089)\n",
      "Epoch [10/50] Step [321/391]  acc1 0.296875 (0.303495)  loss 2.787796 (2.744161)\n",
      "Epoch [10/50] Step [361/391]  acc1 0.250000 (0.302978)  loss 2.846894 (2.743121)\n",
      "Epoch [11/50] Step [1/391]  acc1 0.234375 (0.234375)  loss 2.648931 (2.648931)\n",
      "Epoch [11/50] Step [41/391]  acc1 0.296875 (0.323552)  loss 2.840117 (2.668359)\n",
      "Epoch [11/50] Step [81/391]  acc1 0.437500 (0.316165)  loss 2.633843 (2.688863)\n",
      "Epoch [11/50] Step [121/391]  acc1 0.343750 (0.310692)  loss 2.356358 (2.699208)\n",
      "Epoch [11/50] Step [161/391]  acc1 0.343750 (0.308036)  loss 2.786371 (2.711101)\n",
      "Epoch [11/50] Step [201/391]  acc1 0.312500 (0.311023)  loss 2.685932 (2.701288)\n",
      "Epoch [11/50] Step [241/391]  acc1 0.250000 (0.313019)  loss 2.777550 (2.690388)\n",
      "Epoch [11/50] Step [281/391]  acc1 0.390625 (0.310832)  loss 2.834170 (2.693156)\n",
      "Epoch [11/50] Step [321/391]  acc1 0.296875 (0.312695)  loss 2.605548 (2.689394)\n",
      "Epoch [11/50] Step [361/391]  acc1 0.375000 (0.311375)  loss 2.314210 (2.689050)\n",
      "Epoch [12/50] Step [1/391]  acc1 0.343750 (0.343750)  loss 2.751405 (2.751405)\n",
      "Epoch [12/50] Step [41/391]  acc1 0.328125 (0.315168)  loss 2.546165 (2.661170)\n",
      "Epoch [12/50] Step [81/391]  acc1 0.265625 (0.328511)  loss 2.678727 (2.615134)\n",
      "Epoch [12/50] Step [121/391]  acc1 0.312500 (0.326705)  loss 2.574530 (2.619660)\n",
      "Epoch [12/50] Step [161/391]  acc1 0.234375 (0.326669)  loss 2.679993 (2.614170)\n",
      "Epoch [12/50] Step [201/391]  acc1 0.296875 (0.322606)  loss 2.676716 (2.620860)\n",
      "Epoch [12/50] Step [241/391]  acc1 0.312500 (0.323522)  loss 2.786234 (2.622878)\n",
      "Epoch [12/50] Step [281/391]  acc1 0.328125 (0.326290)  loss 2.627362 (2.616209)\n",
      "Epoch [12/50] Step [321/391]  acc1 0.312500 (0.324474)  loss 2.854630 (2.630258)\n",
      "Epoch [12/50] Step [361/391]  acc1 0.328125 (0.325268)  loss 2.467431 (2.626302)\n",
      "Epoch [13/50] Step [1/391]  acc1 0.265625 (0.265625)  loss 2.920849 (2.920849)\n",
      "Epoch [13/50] Step [41/391]  acc1 0.265625 (0.333079)  loss 2.654110 (2.627312)\n",
      "Epoch [13/50] Step [81/391]  acc1 0.265625 (0.335455)  loss 2.577219 (2.595567)\n",
      "Epoch [13/50] Step [121/391]  acc1 0.328125 (0.341942)  loss 2.520508 (2.579648)\n",
      "Epoch [13/50] Step [161/391]  acc1 0.406250 (0.343168)  loss 2.503484 (2.569993)\n",
      "Epoch [13/50] Step [201/391]  acc1 0.250000 (0.338542)  loss 2.703419 (2.582350)\n",
      "Epoch [13/50] Step [241/391]  acc1 0.328125 (0.335970)  loss 2.650110 (2.581499)\n",
      "Epoch [13/50] Step [281/391]  acc1 0.218750 (0.334520)  loss 2.810064 (2.582695)\n",
      "Epoch [13/50] Step [321/391]  acc1 0.406250 (0.333528)  loss 2.589187 (2.583070)\n",
      "Epoch [13/50] Step [361/391]  acc1 0.437500 (0.334531)  loss 2.157498 (2.581584)\n",
      "Epoch [14/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.291952 (2.291952)\n",
      "Epoch [14/50] Step [41/391]  acc1 0.328125 (0.333460)  loss 2.607882 (2.534608)\n",
      "Epoch [14/50] Step [81/391]  acc1 0.296875 (0.345486)  loss 2.683713 (2.526302)\n",
      "Epoch [14/50] Step [121/391]  acc1 0.343750 (0.345429)  loss 2.211747 (2.529717)\n",
      "Epoch [14/50] Step [161/391]  acc1 0.281250 (0.345691)  loss 2.666362 (2.529825)\n",
      "Epoch [14/50] Step [201/391]  acc1 0.343750 (0.344683)  loss 2.509587 (2.523604)\n",
      "Epoch [14/50] Step [241/391]  acc1 0.562500 (0.345241)  loss 2.236089 (2.527032)\n",
      "Epoch [14/50] Step [281/391]  acc1 0.265625 (0.345029)  loss 2.915309 (2.522403)\n",
      "Epoch [14/50] Step [321/391]  acc1 0.312500 (0.343069)  loss 2.554480 (2.528249)\n",
      "Epoch [14/50] Step [361/391]  acc1 0.281250 (0.341932)  loss 2.658015 (2.531149)\n",
      "Epoch [15/50] Step [1/391]  acc1 0.484375 (0.484375)  loss 2.266145 (2.266145)\n",
      "Epoch [15/50] Step [41/391]  acc1 0.296875 (0.362805)  loss 2.555044 (2.447258)\n",
      "Epoch [15/50] Step [81/391]  acc1 0.375000 (0.366319)  loss 2.609378 (2.458627)\n",
      "Epoch [15/50] Step [121/391]  acc1 0.343750 (0.357309)  loss 2.564772 (2.480770)\n",
      "Epoch [15/50] Step [161/391]  acc1 0.390625 (0.357822)  loss 2.551031 (2.480785)\n",
      "Epoch [15/50] Step [201/391]  acc1 0.343750 (0.356188)  loss 2.607701 (2.488098)\n",
      "Epoch [15/50] Step [241/391]  acc1 0.343750 (0.354707)  loss 2.708948 (2.484821)\n",
      "Epoch [15/50] Step [281/391]  acc1 0.312500 (0.354315)  loss 2.655862 (2.484108)\n",
      "Epoch [15/50] Step [321/391]  acc1 0.296875 (0.354945)  loss 2.486635 (2.480830)\n",
      "Epoch [15/50] Step [361/391]  acc1 0.437500 (0.354700)  loss 2.400142 (2.479928)\n",
      "Epoch [16/50] Step [1/391]  acc1 0.281250 (0.281250)  loss 2.827653 (2.827653)\n",
      "Epoch [16/50] Step [41/391]  acc1 0.359375 (0.350610)  loss 2.515200 (2.479383)\n",
      "Epoch [16/50] Step [81/391]  acc1 0.265625 (0.350694)  loss 2.656425 (2.496873)\n",
      "Epoch [16/50] Step [121/391]  acc1 0.265625 (0.355888)  loss 2.838120 (2.478291)\n",
      "Epoch [16/50] Step [161/391]  acc1 0.312500 (0.358696)  loss 2.347642 (2.467852)\n",
      "Epoch [16/50] Step [201/391]  acc1 0.437500 (0.362951)  loss 2.364435 (2.452919)\n",
      "Epoch [16/50] Step [241/391]  acc1 0.437500 (0.364108)  loss 2.300058 (2.451969)\n",
      "Epoch [16/50] Step [281/391]  acc1 0.421875 (0.363045)  loss 2.246971 (2.451765)\n",
      "Epoch [16/50] Step [321/391]  acc1 0.265625 (0.362247)  loss 2.885543 (2.458533)\n",
      "Epoch [16/50] Step [361/391]  acc1 0.406250 (0.360370)  loss 2.023324 (2.460222)\n",
      "Epoch [17/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 2.170082 (2.170082)\n",
      "Epoch [17/50] Step [41/391]  acc1 0.296875 (0.376905)  loss 2.522446 (2.412546)\n",
      "Epoch [17/50] Step [81/391]  acc1 0.296875 (0.366705)  loss 2.651046 (2.417847)\n",
      "Epoch [17/50] Step [121/391]  acc1 0.343750 (0.362603)  loss 2.448331 (2.433644)\n",
      "Epoch [17/50] Step [161/391]  acc1 0.375000 (0.368012)  loss 2.501165 (2.423243)\n",
      "Epoch [17/50] Step [201/391]  acc1 0.328125 (0.369558)  loss 2.522391 (2.415043)\n",
      "Epoch [17/50] Step [241/391]  acc1 0.375000 (0.367609)  loss 2.386050 (2.429229)\n",
      "Epoch [17/50] Step [281/391]  acc1 0.328125 (0.367104)  loss 2.317387 (2.430146)\n",
      "Epoch [17/50] Step [321/391]  acc1 0.359375 (0.368867)  loss 2.237981 (2.422254)\n",
      "Epoch [17/50] Step [361/391]  acc1 0.484375 (0.370066)  loss 2.122351 (2.418072)\n",
      "Epoch [18/50] Step [1/391]  acc1 0.312500 (0.312500)  loss 2.417989 (2.417989)\n",
      "Epoch [18/50] Step [41/391]  acc1 0.421875 (0.380716)  loss 2.496698 (2.367618)\n",
      "Epoch [18/50] Step [81/391]  acc1 0.406250 (0.380208)  loss 2.357094 (2.383672)\n",
      "Epoch [18/50] Step [121/391]  acc1 0.421875 (0.372417)  loss 2.192004 (2.402694)\n",
      "Epoch [18/50] Step [161/391]  acc1 0.343750 (0.375097)  loss 2.635247 (2.390914)\n",
      "Epoch [18/50] Step [201/391]  acc1 0.312500 (0.375078)  loss 2.719486 (2.386247)\n",
      "Epoch [18/50] Step [241/391]  acc1 0.453125 (0.375065)  loss 2.187060 (2.383314)\n",
      "Epoch [18/50] Step [281/391]  acc1 0.312500 (0.375111)  loss 2.263143 (2.380247)\n",
      "Epoch [18/50] Step [321/391]  acc1 0.343750 (0.373734)  loss 2.490129 (2.382004)\n",
      "Epoch [18/50] Step [361/391]  acc1 0.453125 (0.375130)  loss 2.276076 (2.380869)\n",
      "Epoch [19/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.292648 (2.292648)\n",
      "Epoch [19/50] Step [41/391]  acc1 0.390625 (0.363948)  loss 2.260738 (2.358451)\n",
      "Epoch [19/50] Step [81/391]  acc1 0.390625 (0.375386)  loss 2.301380 (2.359950)\n",
      "Epoch [19/50] Step [121/391]  acc1 0.453125 (0.381198)  loss 2.036807 (2.351960)\n",
      "Epoch [19/50] Step [161/391]  acc1 0.453125 (0.384899)  loss 2.288010 (2.352612)\n",
      "Epoch [19/50] Step [201/391]  acc1 0.296875 (0.383085)  loss 2.633177 (2.360021)\n",
      "Epoch [19/50] Step [241/391]  acc1 0.328125 (0.382845)  loss 2.574138 (2.361625)\n",
      "Epoch [19/50] Step [281/391]  acc1 0.406250 (0.384898)  loss 2.319488 (2.355286)\n",
      "Epoch [19/50] Step [321/391]  acc1 0.406250 (0.381912)  loss 2.500772 (2.361416)\n",
      "Epoch [19/50] Step [361/391]  acc1 0.390625 (0.379545)  loss 2.455875 (2.369280)\n",
      "Epoch [20/50] Step [1/391]  acc1 0.281250 (0.281250)  loss 2.551328 (2.551328)\n",
      "Epoch [20/50] Step [41/391]  acc1 0.359375 (0.383765)  loss 2.441220 (2.315912)\n",
      "Epoch [20/50] Step [81/391]  acc1 0.359375 (0.379244)  loss 2.489398 (2.342536)\n",
      "Epoch [20/50] Step [121/391]  acc1 0.421875 (0.383264)  loss 2.484818 (2.333808)\n",
      "Epoch [20/50] Step [161/391]  acc1 0.406250 (0.384414)  loss 2.164230 (2.330594)\n",
      "Epoch [20/50] Step [201/391]  acc1 0.343750 (0.384484)  loss 2.221177 (2.328372)\n",
      "Epoch [20/50] Step [241/391]  acc1 0.453125 (0.384206)  loss 2.201010 (2.331785)\n",
      "Epoch [20/50] Step [281/391]  acc1 0.328125 (0.385120)  loss 2.402643 (2.330000)\n",
      "Epoch [20/50] Step [321/391]  acc1 0.468750 (0.386390)  loss 2.109197 (2.327552)\n",
      "Epoch [20/50] Step [361/391]  acc1 0.281250 (0.386946)  loss 2.634945 (2.325786)\n",
      "Epoch [21/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.409384 (2.409384)\n",
      "Epoch [21/50] Step [41/391]  acc1 0.515625 (0.389482)  loss 2.110997 (2.320938)\n",
      "Epoch [21/50] Step [81/391]  acc1 0.468750 (0.395448)  loss 2.047945 (2.284373)\n",
      "Epoch [21/50] Step [121/391]  acc1 0.390625 (0.392433)  loss 2.032615 (2.273134)\n",
      "Epoch [21/50] Step [161/391]  acc1 0.296875 (0.393342)  loss 2.648823 (2.290008)\n",
      "Epoch [21/50] Step [201/391]  acc1 0.359375 (0.390392)  loss 2.554254 (2.297537)\n",
      "Epoch [21/50] Step [241/391]  acc1 0.437500 (0.389717)  loss 2.508711 (2.299635)\n",
      "Epoch [21/50] Step [281/391]  acc1 0.421875 (0.388846)  loss 2.337957 (2.301976)\n",
      "Epoch [21/50] Step [321/391]  acc1 0.562500 (0.389944)  loss 1.770238 (2.301787)\n",
      "Epoch [21/50] Step [361/391]  acc1 0.250000 (0.390149)  loss 2.808811 (2.298973)\n",
      "Epoch [22/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.369347 (2.369347)\n",
      "Epoch [22/50] Step [41/391]  acc1 0.421875 (0.406250)  loss 2.080732 (2.216320)\n",
      "Epoch [22/50] Step [81/391]  acc1 0.562500 (0.394869)  loss 2.074679 (2.268743)\n",
      "Epoch [22/50] Step [121/391]  acc1 0.453125 (0.397082)  loss 2.234668 (2.265195)\n",
      "Epoch [22/50] Step [161/391]  acc1 0.421875 (0.397516)  loss 2.362032 (2.264781)\n",
      "Epoch [22/50] Step [201/391]  acc1 0.343750 (0.396922)  loss 2.296637 (2.272805)\n",
      "Epoch [22/50] Step [241/391]  acc1 0.515625 (0.398600)  loss 2.145058 (2.272819)\n",
      "Epoch [22/50] Step [281/391]  acc1 0.421875 (0.400578)  loss 2.247595 (2.272972)\n",
      "Epoch [22/50] Step [321/391]  acc1 0.281250 (0.397488)  loss 2.508147 (2.279318)\n",
      "Epoch [22/50] Step [361/391]  acc1 0.312500 (0.398632)  loss 2.359767 (2.278159)\n",
      "Epoch [23/50] Step [1/391]  acc1 0.453125 (0.453125)  loss 2.068928 (2.068928)\n",
      "Epoch [23/50] Step [41/391]  acc1 0.421875 (0.405107)  loss 2.356564 (2.229011)\n",
      "Epoch [23/50] Step [81/391]  acc1 0.359375 (0.403935)  loss 2.436144 (2.236344)\n",
      "Epoch [23/50] Step [121/391]  acc1 0.468750 (0.405863)  loss 2.052913 (2.235242)\n",
      "Epoch [23/50] Step [161/391]  acc1 0.468750 (0.405085)  loss 2.074926 (2.242595)\n",
      "Epoch [23/50] Step [201/391]  acc1 0.390625 (0.403374)  loss 2.237033 (2.239343)\n",
      "Epoch [23/50] Step [241/391]  acc1 0.421875 (0.404046)  loss 2.331697 (2.240069)\n",
      "Epoch [23/50] Step [281/391]  acc1 0.453125 (0.405082)  loss 2.233494 (2.241706)\n",
      "Epoch [23/50] Step [321/391]  acc1 0.468750 (0.405130)  loss 2.128112 (2.242220)\n",
      "Epoch [23/50] Step [361/391]  acc1 0.453125 (0.405644)  loss 2.043136 (2.239013)\n",
      "Epoch [24/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 1.961616 (1.961616)\n",
      "Epoch [24/50] Step [41/391]  acc1 0.359375 (0.415396)  loss 2.222247 (2.132404)\n",
      "Epoch [24/50] Step [81/391]  acc1 0.312500 (0.407407)  loss 2.554132 (2.198006)\n",
      "Epoch [24/50] Step [121/391]  acc1 0.375000 (0.404700)  loss 2.337623 (2.209798)\n",
      "Epoch [24/50] Step [161/391]  acc1 0.453125 (0.409356)  loss 2.261826 (2.209156)\n",
      "Epoch [24/50] Step [201/391]  acc1 0.453125 (0.410603)  loss 2.053792 (2.206249)\n",
      "Epoch [24/50] Step [241/391]  acc1 0.421875 (0.410335)  loss 2.093098 (2.207754)\n",
      "Epoch [24/50] Step [281/391]  acc1 0.390625 (0.410532)  loss 2.183690 (2.211169)\n",
      "Epoch [24/50] Step [321/391]  acc1 0.437500 (0.409998)  loss 2.079458 (2.210596)\n",
      "Epoch [24/50] Step [361/391]  acc1 0.515625 (0.409150)  loss 2.072120 (2.215507)\n",
      "Epoch [25/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.158518 (2.158518)\n",
      "Epoch [25/50] Step [41/391]  acc1 0.484375 (0.427973)  loss 2.143783 (2.159454)\n",
      "Epoch [25/50] Step [81/391]  acc1 0.406250 (0.418789)  loss 2.380172 (2.176907)\n",
      "Epoch [25/50] Step [121/391]  acc1 0.343750 (0.416064)  loss 2.347837 (2.177754)\n",
      "Epoch [25/50] Step [161/391]  acc1 0.484375 (0.417993)  loss 2.075399 (2.182224)\n",
      "Epoch [25/50] Step [201/391]  acc1 0.500000 (0.419232)  loss 2.200601 (2.180682)\n",
      "Epoch [25/50] Step [241/391]  acc1 0.343750 (0.417401)  loss 2.319417 (2.186310)\n",
      "Epoch [25/50] Step [281/391]  acc1 0.453125 (0.416259)  loss 2.205165 (2.190732)\n",
      "Epoch [25/50] Step [321/391]  acc1 0.437500 (0.415547)  loss 2.050792 (2.191810)\n",
      "Epoch [25/50] Step [361/391]  acc1 0.265625 (0.415556)  loss 2.611564 (2.195500)\n",
      "Epoch [26/50] Step [1/391]  acc1 0.546875 (0.546875)  loss 1.887505 (1.887505)\n",
      "Epoch [26/50] Step [41/391]  acc1 0.468750 (0.431402)  loss 2.102416 (2.144467)\n",
      "Epoch [26/50] Step [81/391]  acc1 0.453125 (0.430363)  loss 1.911513 (2.141663)\n",
      "Epoch [26/50] Step [121/391]  acc1 0.421875 (0.432206)  loss 2.007778 (2.121799)\n",
      "Epoch [26/50] Step [161/391]  acc1 0.375000 (0.424107)  loss 2.282676 (2.143676)\n",
      "Epoch [26/50] Step [201/391]  acc1 0.421875 (0.424518)  loss 2.092920 (2.141803)\n",
      "Epoch [26/50] Step [241/391]  acc1 0.375000 (0.422653)  loss 2.312570 (2.154084)\n",
      "Epoch [26/50] Step [281/391]  acc1 0.328125 (0.419706)  loss 2.417345 (2.164055)\n",
      "Epoch [26/50] Step [321/391]  acc1 0.343750 (0.417056)  loss 2.486292 (2.169826)\n",
      "Epoch [26/50] Step [361/391]  acc1 0.375000 (0.416075)  loss 2.033142 (2.175174)\n",
      "Epoch [27/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 2.231939 (2.231939)\n",
      "Epoch [27/50] Step [41/391]  acc1 0.187500 (0.425305)  loss 3.007979 (2.139247)\n",
      "Epoch [27/50] Step [81/391]  acc1 0.406250 (0.425154)  loss 2.044502 (2.130052)\n",
      "Epoch [27/50] Step [121/391]  acc1 0.406250 (0.428461)  loss 2.317759 (2.127001)\n",
      "Epoch [27/50] Step [161/391]  acc1 0.500000 (0.432842)  loss 1.944764 (2.117163)\n",
      "Epoch [27/50] Step [201/391]  acc1 0.390625 (0.432447)  loss 2.213453 (2.119727)\n",
      "Epoch [27/50] Step [241/391]  acc1 0.421875 (0.432054)  loss 2.032139 (2.123245)\n",
      "Epoch [27/50] Step [281/391]  acc1 0.453125 (0.429993)  loss 2.065931 (2.128456)\n",
      "Epoch [27/50] Step [321/391]  acc1 0.468750 (0.429079)  loss 2.116744 (2.130713)\n",
      "Epoch [27/50] Step [361/391]  acc1 0.421875 (0.427545)  loss 2.127469 (2.141624)\n",
      "Epoch [28/50] Step [1/391]  acc1 0.343750 (0.343750)  loss 2.361197 (2.361197)\n",
      "Epoch [28/50] Step [41/391]  acc1 0.468750 (0.426067)  loss 1.740833 (2.112804)\n",
      "Epoch [28/50] Step [81/391]  acc1 0.390625 (0.431327)  loss 2.048153 (2.111340)\n",
      "Epoch [28/50] Step [121/391]  acc1 0.328125 (0.427686)  loss 2.446970 (2.123951)\n",
      "Epoch [28/50] Step [161/391]  acc1 0.390625 (0.429348)  loss 2.441635 (2.123027)\n",
      "Epoch [28/50] Step [201/391]  acc1 0.562500 (0.427938)  loss 1.813473 (2.126001)\n",
      "Epoch [28/50] Step [241/391]  acc1 0.390625 (0.428553)  loss 2.147874 (2.131852)\n",
      "Epoch [28/50] Step [281/391]  acc1 0.406250 (0.426991)  loss 2.248688 (2.139205)\n",
      "Epoch [28/50] Step [321/391]  acc1 0.468750 (0.427083)  loss 2.027505 (2.140150)\n",
      "Epoch [28/50] Step [361/391]  acc1 0.453125 (0.428454)  loss 2.167752 (2.137132)\n",
      "Epoch [29/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.159750 (2.159750)\n",
      "Epoch [29/50] Step [41/391]  acc1 0.531250 (0.433308)  loss 1.874306 (2.130373)\n",
      "Epoch [29/50] Step [81/391]  acc1 0.468750 (0.428048)  loss 2.328922 (2.124440)\n",
      "Epoch [29/50] Step [121/391]  acc1 0.421875 (0.428332)  loss 2.102605 (2.118790)\n",
      "Epoch [29/50] Step [161/391]  acc1 0.437500 (0.431386)  loss 2.026434 (2.112038)\n",
      "Epoch [29/50] Step [201/391]  acc1 0.375000 (0.430659)  loss 2.200910 (2.116768)\n",
      "Epoch [29/50] Step [241/391]  acc1 0.421875 (0.432054)  loss 2.162717 (2.116035)\n",
      "Epoch [29/50] Step [281/391]  acc1 0.390625 (0.431272)  loss 2.269332 (2.113060)\n",
      "Epoch [29/50] Step [321/391]  acc1 0.375000 (0.430977)  loss 2.344099 (2.113336)\n",
      "Epoch [29/50] Step [361/391]  acc1 0.484375 (0.433258)  loss 1.879185 (2.105642)\n",
      "Epoch [30/50] Step [1/391]  acc1 0.421875 (0.421875)  loss 1.940276 (1.940276)\n",
      "Epoch [30/50] Step [41/391]  acc1 0.515625 (0.446646)  loss 2.095680 (2.073945)\n",
      "Epoch [30/50] Step [81/391]  acc1 0.421875 (0.440972)  loss 2.000282 (2.086154)\n",
      "Epoch [30/50] Step [121/391]  acc1 0.390625 (0.439179)  loss 2.092412 (2.083730)\n",
      "Epoch [30/50] Step [161/391]  acc1 0.484375 (0.442450)  loss 2.172418 (2.079805)\n",
      "Epoch [30/50] Step [201/391]  acc1 0.453125 (0.441931)  loss 2.039050 (2.079730)\n",
      "Epoch [30/50] Step [241/391]  acc1 0.453125 (0.439640)  loss 1.940686 (2.080835)\n",
      "Epoch [30/50] Step [281/391]  acc1 0.406250 (0.441726)  loss 2.205207 (2.081828)\n",
      "Epoch [30/50] Step [321/391]  acc1 0.437500 (0.442562)  loss 2.153448 (2.080952)\n",
      "Epoch [30/50] Step [361/391]  acc1 0.359375 (0.441266)  loss 2.342048 (2.082215)\n",
      "Epoch [31/50] Step [1/391]  acc1 0.484375 (0.484375)  loss 1.958762 (1.958762)\n",
      "Epoch [31/50] Step [41/391]  acc1 0.578125 (0.452744)  loss 1.619683 (2.003856)\n",
      "Epoch [31/50] Step [81/391]  acc1 0.421875 (0.447338)  loss 2.139807 (2.039557)\n",
      "Epoch [31/50] Step [121/391]  acc1 0.421875 (0.447701)  loss 2.101844 (2.047517)\n",
      "Epoch [31/50] Step [161/391]  acc1 0.453125 (0.447108)  loss 1.851407 (2.049087)\n",
      "Epoch [31/50] Step [201/391]  acc1 0.312500 (0.447994)  loss 2.515279 (2.057207)\n",
      "Epoch [31/50] Step [241/391]  acc1 0.343750 (0.448587)  loss 2.554502 (2.059373)\n",
      "Epoch [31/50] Step [281/391]  acc1 0.421875 (0.447787)  loss 1.882229 (2.058553)\n",
      "Epoch [31/50] Step [321/391]  acc1 0.390625 (0.447722)  loss 2.162045 (2.058978)\n",
      "Epoch [31/50] Step [361/391]  acc1 0.625000 (0.447888)  loss 1.599840 (2.060912)\n",
      "Epoch [32/50] Step [1/391]  acc1 0.453125 (0.453125)  loss 2.134909 (2.134909)\n",
      "Epoch [32/50] Step [41/391]  acc1 0.500000 (0.453506)  loss 1.842245 (1.990241)\n",
      "Epoch [32/50] Step [81/391]  acc1 0.437500 (0.456211)  loss 1.865604 (1.999905)\n",
      "Epoch [32/50] Step [121/391]  acc1 0.453125 (0.452221)  loss 2.101663 (2.012689)\n",
      "Epoch [32/50] Step [161/391]  acc1 0.406250 (0.449049)  loss 2.172616 (2.025798)\n",
      "Epoch [32/50] Step [201/391]  acc1 0.484375 (0.448850)  loss 1.911335 (2.027277)\n",
      "Epoch [32/50] Step [241/391]  acc1 0.437500 (0.447095)  loss 1.865443 (2.041719)\n",
      "Epoch [32/50] Step [281/391]  acc1 0.500000 (0.445952)  loss 1.844136 (2.039857)\n",
      "Epoch [32/50] Step [321/391]  acc1 0.468750 (0.447722)  loss 2.025646 (2.038749)\n",
      "Epoch [32/50] Step [361/391]  acc1 0.406250 (0.446762)  loss 2.166688 (2.042644)\n",
      "Epoch [33/50] Step [1/391]  acc1 0.390625 (0.390625)  loss 2.339807 (2.339807)\n",
      "Epoch [33/50] Step [41/391]  acc1 0.453125 (0.461890)  loss 2.112788 (1.990185)\n",
      "Epoch [33/50] Step [81/391]  acc1 0.437500 (0.455054)  loss 1.980313 (2.008663)\n",
      "Epoch [33/50] Step [121/391]  acc1 0.531250 (0.457515)  loss 1.803093 (1.995477)\n",
      "Epoch [33/50] Step [161/391]  acc1 0.390625 (0.450505)  loss 2.156530 (2.014039)\n",
      "Epoch [33/50] Step [201/391]  acc1 0.515625 (0.450249)  loss 1.913303 (2.016743)\n",
      "Epoch [33/50] Step [241/391]  acc1 0.437500 (0.449818)  loss 1.926906 (2.028004)\n",
      "Epoch [33/50] Step [281/391]  acc1 0.390625 (0.450234)  loss 2.392705 (2.026913)\n",
      "Epoch [33/50] Step [321/391]  acc1 0.375000 (0.448988)  loss 2.111580 (2.031027)\n",
      "Epoch [33/50] Step [361/391]  acc1 0.500000 (0.447671)  loss 2.051597 (2.037363)\n",
      "Epoch [34/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 1.907317 (1.907317)\n",
      "Epoch [34/50] Step [41/391]  acc1 0.468750 (0.460366)  loss 1.807933 (2.004790)\n",
      "Epoch [34/50] Step [81/391]  acc1 0.421875 (0.467785)  loss 1.947549 (1.962906)\n",
      "Epoch [34/50] Step [121/391]  acc1 0.421875 (0.462552)  loss 2.072059 (1.977221)\n",
      "Epoch [34/50] Step [161/391]  acc1 0.343750 (0.458851)  loss 2.275289 (1.997500)\n",
      "Epoch [34/50] Step [201/391]  acc1 0.500000 (0.460044)  loss 1.931775 (1.996804)\n",
      "Epoch [34/50] Step [241/391]  acc1 0.500000 (0.462656)  loss 2.099151 (2.001076)\n",
      "Epoch [34/50] Step [281/391]  acc1 0.484375 (0.461799)  loss 1.956852 (2.006388)\n",
      "Epoch [34/50] Step [321/391]  acc1 0.453125 (0.459745)  loss 2.014981 (2.013097)\n",
      "Epoch [34/50] Step [361/391]  acc1 0.500000 (0.459617)  loss 1.794350 (2.012488)\n",
      "Epoch [35/50] Step [1/391]  acc1 0.468750 (0.468750)  loss 2.026578 (2.026578)\n",
      "Epoch [35/50] Step [41/391]  acc1 0.515625 (0.480183)  loss 1.999943 (1.947723)\n",
      "Epoch [35/50] Step [81/391]  acc1 0.453125 (0.471451)  loss 2.257641 (1.947628)\n",
      "Epoch [35/50] Step [121/391]  acc1 0.484375 (0.467717)  loss 1.831632 (1.957752)\n",
      "Epoch [35/50] Step [161/391]  acc1 0.437500 (0.465936)  loss 2.001296 (1.964082)\n",
      "Epoch [35/50] Step [201/391]  acc1 0.453125 (0.462609)  loss 2.185188 (1.984071)\n",
      "Epoch [35/50] Step [241/391]  acc1 0.421875 (0.459997)  loss 1.970418 (1.992113)\n",
      "Epoch [35/50] Step [281/391]  acc1 0.421875 (0.459464)  loss 1.986758 (1.993876)\n",
      "Epoch [35/50] Step [321/391]  acc1 0.453125 (0.458771)  loss 1.982408 (1.999452)\n",
      "Epoch [35/50] Step [361/391]  acc1 0.531250 (0.457280)  loss 1.884717 (2.004939)\n",
      "Epoch [36/50] Step [1/391]  acc1 0.359375 (0.359375)  loss 2.248908 (2.248908)\n",
      "Epoch [36/50] Step [41/391]  acc1 0.484375 (0.470274)  loss 2.073343 (1.979004)\n",
      "Epoch [36/50] Step [81/391]  acc1 0.343750 (0.467014)  loss 2.370139 (1.981908)\n",
      "Epoch [36/50] Step [121/391]  acc1 0.468750 (0.463585)  loss 2.157129 (1.982395)\n",
      "Epoch [36/50] Step [161/391]  acc1 0.578125 (0.458851)  loss 1.796409 (1.996821)\n",
      "Epoch [36/50] Step [201/391]  acc1 0.453125 (0.461210)  loss 1.896380 (1.987644)\n",
      "Epoch [36/50] Step [241/391]  acc1 0.312500 (0.461035)  loss 2.113661 (1.990960)\n",
      "Epoch [36/50] Step [281/391]  acc1 0.593750 (0.462300)  loss 1.836442 (1.988634)\n",
      "Epoch [36/50] Step [321/391]  acc1 0.500000 (0.462276)  loss 1.791051 (1.989620)\n",
      "Epoch [36/50] Step [361/391]  acc1 0.390625 (0.461392)  loss 2.326237 (1.991916)\n",
      "Epoch [37/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.063496 (2.063496)\n",
      "Epoch [37/50] Step [41/391]  acc1 0.484375 (0.477134)  loss 1.887470 (1.960688)\n",
      "Epoch [37/50] Step [81/391]  acc1 0.421875 (0.471644)  loss 1.954979 (1.951169)\n",
      "Epoch [37/50] Step [121/391]  acc1 0.546875 (0.469137)  loss 1.800231 (1.962745)\n",
      "Epoch [37/50] Step [161/391]  acc1 0.375000 (0.469235)  loss 2.021153 (1.952049)\n",
      "Epoch [37/50] Step [201/391]  acc1 0.453125 (0.470460)  loss 2.411180 (1.948826)\n",
      "Epoch [37/50] Step [241/391]  acc1 0.484375 (0.471408)  loss 1.982168 (1.951374)\n",
      "Epoch [37/50] Step [281/391]  acc1 0.406250 (0.469806)  loss 1.900905 (1.956455)\n",
      "Epoch [37/50] Step [321/391]  acc1 0.437500 (0.468458)  loss 1.863934 (1.960182)\n",
      "Epoch [37/50] Step [361/391]  acc1 0.484375 (0.467538)  loss 2.033903 (1.967254)\n",
      "Epoch [38/50] Step [1/391]  acc1 0.578125 (0.578125)  loss 1.703887 (1.703887)\n",
      "Epoch [38/50] Step [41/391]  acc1 0.468750 (0.469131)  loss 1.946230 (1.940173)\n",
      "Epoch [38/50] Step [81/391]  acc1 0.546875 (0.475502)  loss 1.831150 (1.936085)\n",
      "Epoch [38/50] Step [121/391]  acc1 0.484375 (0.470558)  loss 2.004212 (1.951007)\n",
      "Epoch [38/50] Step [161/391]  acc1 0.484375 (0.473505)  loss 1.663900 (1.945440)\n",
      "Epoch [38/50] Step [201/391]  acc1 0.484375 (0.474969)  loss 1.930394 (1.943095)\n",
      "Epoch [38/50] Step [241/391]  acc1 0.406250 (0.471992)  loss 2.168878 (1.954843)\n",
      "Epoch [38/50] Step [281/391]  acc1 0.468750 (0.470974)  loss 1.894749 (1.952867)\n",
      "Epoch [38/50] Step [321/391]  acc1 0.500000 (0.471768)  loss 1.940578 (1.952001)\n",
      "Epoch [38/50] Step [361/391]  acc1 0.421875 (0.471563)  loss 2.007125 (1.950652)\n",
      "Epoch [39/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 1.836283 (1.836283)\n",
      "Epoch [39/50] Step [41/391]  acc1 0.562500 (0.491997)  loss 1.811366 (1.913532)\n",
      "Epoch [39/50] Step [81/391]  acc1 0.562500 (0.486883)  loss 1.867229 (1.911722)\n",
      "Epoch [39/50] Step [121/391]  acc1 0.375000 (0.481405)  loss 2.191834 (1.935800)\n",
      "Epoch [39/50] Step [161/391]  acc1 0.437500 (0.479134)  loss 1.797284 (1.928276)\n",
      "Epoch [39/50] Step [201/391]  acc1 0.500000 (0.476524)  loss 1.938344 (1.933628)\n",
      "Epoch [39/50] Step [241/391]  acc1 0.593750 (0.474650)  loss 1.746231 (1.940325)\n",
      "Epoch [39/50] Step [281/391]  acc1 0.515625 (0.473810)  loss 1.897401 (1.948582)\n",
      "Epoch [39/50] Step [321/391]  acc1 0.531250 (0.470454)  loss 1.594394 (1.955748)\n",
      "Epoch [39/50] Step [361/391]  acc1 0.531250 (0.471087)  loss 1.987393 (1.958982)\n",
      "Epoch [40/50] Step [1/391]  acc1 0.578125 (0.578125)  loss 1.822993 (1.822993)\n",
      "Epoch [40/50] Step [41/391]  acc1 0.484375 (0.480183)  loss 1.843947 (1.925634)\n",
      "Epoch [40/50] Step [81/391]  acc1 0.546875 (0.479167)  loss 1.911694 (1.940028)\n",
      "Epoch [40/50] Step [121/391]  acc1 0.437500 (0.481405)  loss 2.069101 (1.941054)\n",
      "Epoch [40/50] Step [161/391]  acc1 0.406250 (0.480978)  loss 1.995296 (1.926719)\n",
      "Epoch [40/50] Step [201/391]  acc1 0.500000 (0.478467)  loss 1.933755 (1.930862)\n",
      "Epoch [40/50] Step [241/391]  acc1 0.484375 (0.477697)  loss 2.069560 (1.934732)\n",
      "Epoch [40/50] Step [281/391]  acc1 0.296875 (0.475200)  loss 2.347157 (1.943632)\n",
      "Epoch [40/50] Step [321/391]  acc1 0.531250 (0.477998)  loss 1.916050 (1.939463)\n",
      "Epoch [40/50] Step [361/391]  acc1 0.484375 (0.478575)  loss 1.950932 (1.937371)\n",
      "Epoch [41/50] Step [1/391]  acc1 0.531250 (0.531250)  loss 1.669776 (1.669776)\n",
      "Epoch [41/50] Step [41/391]  acc1 0.531250 (0.480945)  loss 1.783717 (1.910487)\n",
      "Epoch [41/50] Step [81/391]  acc1 0.437500 (0.486304)  loss 2.028372 (1.904441)\n",
      "Epoch [41/50] Step [121/391]  acc1 0.484375 (0.482051)  loss 2.036226 (1.916136)\n",
      "Epoch [41/50] Step [161/391]  acc1 0.531250 (0.478552)  loss 1.798240 (1.919845)\n",
      "Epoch [41/50] Step [201/391]  acc1 0.500000 (0.476057)  loss 1.771430 (1.930459)\n",
      "Epoch [41/50] Step [241/391]  acc1 0.453125 (0.477373)  loss 1.926102 (1.927383)\n",
      "Epoch [41/50] Step [281/391]  acc1 0.531250 (0.476312)  loss 1.794342 (1.925848)\n",
      "Epoch [41/50] Step [321/391]  acc1 0.562500 (0.475954)  loss 1.697239 (1.929984)\n",
      "Epoch [41/50] Step [361/391]  acc1 0.406250 (0.475632)  loss 2.136179 (1.933902)\n",
      "Epoch [42/50] Step [1/391]  acc1 0.515625 (0.515625)  loss 1.729592 (1.729592)\n",
      "Epoch [42/50] Step [41/391]  acc1 0.484375 (0.482851)  loss 2.018418 (1.898765)\n",
      "Epoch [42/50] Step [81/391]  acc1 0.390625 (0.483410)  loss 2.114929 (1.903150)\n",
      "Epoch [42/50] Step [121/391]  acc1 0.421875 (0.479855)  loss 2.119808 (1.906967)\n",
      "Epoch [42/50] Step [161/391]  acc1 0.531250 (0.481561)  loss 1.677956 (1.907547)\n",
      "Epoch [42/50] Step [201/391]  acc1 0.453125 (0.481732)  loss 2.001827 (1.909461)\n",
      "Epoch [42/50] Step [241/391]  acc1 0.562500 (0.485023)  loss 1.805183 (1.902157)\n",
      "Epoch [42/50] Step [281/391]  acc1 0.375000 (0.483096)  loss 2.129497 (1.908847)\n",
      "Epoch [42/50] Step [321/391]  acc1 0.562500 (0.481308)  loss 1.901597 (1.916461)\n",
      "Epoch [42/50] Step [361/391]  acc1 0.437500 (0.482644)  loss 1.968430 (1.915170)\n",
      "Epoch [43/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.040001 (2.040001)\n",
      "Epoch [43/50] Step [41/391]  acc1 0.484375 (0.486662)  loss 1.739535 (1.895503)\n",
      "Epoch [43/50] Step [81/391]  acc1 0.515625 (0.481481)  loss 1.853701 (1.912462)\n",
      "Epoch [43/50] Step [121/391]  acc1 0.437500 (0.484633)  loss 1.917895 (1.888924)\n",
      "Epoch [43/50] Step [161/391]  acc1 0.390625 (0.481269)  loss 2.269828 (1.900082)\n",
      "Epoch [43/50] Step [201/391]  acc1 0.468750 (0.483209)  loss 1.834056 (1.893947)\n",
      "Epoch [43/50] Step [241/391]  acc1 0.546875 (0.482171)  loss 1.718679 (1.900441)\n",
      "Epoch [43/50] Step [281/391]  acc1 0.453125 (0.483430)  loss 1.860292 (1.896979)\n",
      "Epoch [43/50] Step [321/391]  acc1 0.421875 (0.481503)  loss 2.173762 (1.903566)\n",
      "Epoch [43/50] Step [361/391]  acc1 0.515625 (0.482817)  loss 1.711542 (1.898688)\n",
      "Epoch [44/50] Step [1/391]  acc1 0.578125 (0.578125)  loss 1.607665 (1.607665)\n",
      "Epoch [44/50] Step [41/391]  acc1 0.437500 (0.494284)  loss 1.931274 (1.873925)\n",
      "Epoch [44/50] Step [81/391]  acc1 0.453125 (0.497685)  loss 1.774478 (1.858411)\n",
      "Epoch [44/50] Step [121/391]  acc1 0.500000 (0.492639)  loss 1.960909 (1.865377)\n",
      "Epoch [44/50] Step [161/391]  acc1 0.593750 (0.490683)  loss 1.587485 (1.867586)\n",
      "Epoch [44/50] Step [201/391]  acc1 0.406250 (0.486085)  loss 2.112040 (1.878870)\n",
      "Epoch [44/50] Step [241/391]  acc1 0.546875 (0.483532)  loss 1.737181 (1.888212)\n",
      "Epoch [44/50] Step [281/391]  acc1 0.562500 (0.484041)  loss 1.793265 (1.889188)\n",
      "Epoch [44/50] Step [321/391]  acc1 0.531250 (0.485349)  loss 1.884801 (1.891708)\n",
      "Epoch [44/50] Step [361/391]  acc1 0.578125 (0.483942)  loss 1.652315 (1.898939)\n",
      "Epoch [45/50] Step [1/391]  acc1 0.312500 (0.312500)  loss 2.629788 (2.629788)\n",
      "Epoch [45/50] Step [41/391]  acc1 0.437500 (0.485137)  loss 2.068177 (1.896330)\n",
      "Epoch [45/50] Step [81/391]  acc1 0.625000 (0.490741)  loss 1.509516 (1.870234)\n",
      "Epoch [45/50] Step [121/391]  acc1 0.531250 (0.491348)  loss 1.732376 (1.863758)\n",
      "Epoch [45/50] Step [161/391]  acc1 0.484375 (0.489713)  loss 1.893720 (1.870748)\n",
      "Epoch [45/50] Step [201/391]  acc1 0.515625 (0.491138)  loss 1.838188 (1.866423)\n",
      "Epoch [45/50] Step [241/391]  acc1 0.500000 (0.489043)  loss 1.744155 (1.866761)\n",
      "Epoch [45/50] Step [281/391]  acc1 0.593750 (0.488434)  loss 1.688180 (1.872827)\n",
      "Epoch [45/50] Step [321/391]  acc1 0.515625 (0.486663)  loss 1.672193 (1.878077)\n",
      "Epoch [45/50] Step [361/391]  acc1 0.609375 (0.487621)  loss 1.970483 (1.880935)\n",
      "Epoch [46/50] Step [1/391]  acc1 0.437500 (0.437500)  loss 2.080661 (2.080661)\n",
      "Epoch [46/50] Step [41/391]  acc1 0.515625 (0.496570)  loss 1.818310 (1.832200)\n",
      "Epoch [46/50] Step [81/391]  acc1 0.437500 (0.490355)  loss 2.090011 (1.859997)\n",
      "Epoch [46/50] Step [121/391]  acc1 0.531250 (0.486054)  loss 1.797815 (1.879213)\n",
      "Epoch [46/50] Step [161/391]  acc1 0.484375 (0.490683)  loss 1.779460 (1.873876)\n",
      "Epoch [46/50] Step [201/391]  acc1 0.500000 (0.492537)  loss 1.881347 (1.868941)\n",
      "Epoch [46/50] Step [241/391]  acc1 0.453125 (0.491312)  loss 1.981826 (1.874883)\n",
      "Epoch [46/50] Step [281/391]  acc1 0.562500 (0.491826)  loss 1.624726 (1.870925)\n",
      "Epoch [46/50] Step [321/391]  acc1 0.468750 (0.490752)  loss 1.865067 (1.873083)\n",
      "Epoch [46/50] Step [361/391]  acc1 0.515625 (0.489482)  loss 1.700173 (1.875734)\n",
      "Epoch [47/50] Step [1/391]  acc1 0.500000 (0.500000)  loss 1.586831 (1.586831)\n",
      "Epoch [47/50] Step [41/391]  acc1 0.562500 (0.490091)  loss 1.523776 (1.842895)\n",
      "Epoch [47/50] Step [81/391]  acc1 0.500000 (0.496914)  loss 1.780033 (1.834988)\n",
      "Epoch [47/50] Step [121/391]  acc1 0.453125 (0.496901)  loss 1.812437 (1.844638)\n",
      "Epoch [47/50] Step [161/391]  acc1 0.468750 (0.491751)  loss 1.970630 (1.855004)\n",
      "Epoch [47/50] Step [201/391]  acc1 0.515625 (0.492771)  loss 1.750554 (1.855324)\n",
      "Epoch [47/50] Step [241/391]  acc1 0.406250 (0.494359)  loss 2.236263 (1.854920)\n",
      "Epoch [47/50] Step [281/391]  acc1 0.531250 (0.492160)  loss 1.629735 (1.855676)\n",
      "Epoch [47/50] Step [321/391]  acc1 0.546875 (0.490557)  loss 1.771235 (1.858034)\n",
      "Epoch [47/50] Step [361/391]  acc1 0.453125 (0.489093)  loss 2.072427 (1.866025)\n",
      "Epoch [48/50] Step [1/391]  acc1 0.546875 (0.546875)  loss 1.513736 (1.513736)\n",
      "Epoch [48/50] Step [41/391]  acc1 0.484375 (0.490854)  loss 2.127137 (1.869791)\n",
      "Epoch [48/50] Step [81/391]  acc1 0.578125 (0.493248)  loss 1.581422 (1.857024)\n",
      "Epoch [48/50] Step [121/391]  acc1 0.531250 (0.494189)  loss 1.834638 (1.855943)\n",
      "Epoch [48/50] Step [161/391]  acc1 0.593750 (0.495342)  loss 1.643128 (1.851603)\n",
      "Epoch [48/50] Step [201/391]  acc1 0.515625 (0.493004)  loss 1.713710 (1.852059)\n",
      "Epoch [48/50] Step [241/391]  acc1 0.421875 (0.490923)  loss 1.953417 (1.859106)\n",
      "Epoch [48/50] Step [281/391]  acc1 0.484375 (0.491103)  loss 1.843403 (1.857501)\n",
      "Epoch [48/50] Step [321/391]  acc1 0.484375 (0.490557)  loss 1.923078 (1.860264)\n",
      "Epoch [48/50] Step [361/391]  acc1 0.500000 (0.492382)  loss 1.879295 (1.855568)\n",
      "Epoch [49/50] Step [1/391]  acc1 0.484375 (0.484375)  loss 1.764899 (1.764899)\n",
      "Epoch [49/50] Step [41/391]  acc1 0.515625 (0.504954)  loss 1.837908 (1.831626)\n",
      "Epoch [49/50] Step [81/391]  acc1 0.546875 (0.501929)  loss 1.702457 (1.829336)\n",
      "Epoch [49/50] Step [121/391]  acc1 0.437500 (0.503487)  loss 1.993306 (1.821383)\n",
      "Epoch [49/50] Step [161/391]  acc1 0.484375 (0.502717)  loss 1.922089 (1.832695)\n",
      "Epoch [49/50] Step [201/391]  acc1 0.437500 (0.499300)  loss 2.148386 (1.837838)\n",
      "Epoch [49/50] Step [241/391]  acc1 0.468750 (0.497860)  loss 1.661898 (1.841678)\n",
      "Epoch [49/50] Step [281/391]  acc1 0.531250 (0.496219)  loss 1.630980 (1.845503)\n",
      "Epoch [49/50] Step [321/391]  acc1 0.500000 (0.493915)  loss 1.803338 (1.849389)\n",
      "Epoch [49/50] Step [361/391]  acc1 0.468750 (0.494417)  loss 1.920784 (1.852395)\n",
      "Epoch [50/50] Step [1/391]  acc1 0.546875 (0.546875)  loss 1.601208 (1.601208)\n",
      "Epoch [50/50] Step [41/391]  acc1 0.437500 (0.500762)  loss 1.961819 (1.854552)\n",
      "Epoch [50/50] Step [81/391]  acc1 0.625000 (0.501157)  loss 1.448148 (1.828456)\n",
      "Epoch [50/50] Step [121/391]  acc1 0.546875 (0.501033)  loss 1.908520 (1.836863)\n",
      "Epoch [50/50] Step [161/391]  acc1 0.578125 (0.497380)  loss 1.569226 (1.848550)\n",
      "Epoch [50/50] Step [201/391]  acc1 0.453125 (0.497590)  loss 1.880520 (1.837717)\n",
      "Epoch [50/50] Step [241/391]  acc1 0.625000 (0.494748)  loss 1.323606 (1.846145)\n",
      "Epoch [50/50] Step [281/391]  acc1 0.437500 (0.495663)  loss 1.865150 (1.839206)\n",
      "Epoch [50/50] Step [321/391]  acc1 0.625000 (0.497615)  loss 1.454277 (1.836758)\n",
      "Epoch [50/50] Step [361/391]  acc1 0.625000 (0.497922)  loss 1.455763 (1.839255)\n",
      "Final architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'maxpool', 'reduce_n3_p0': 'sepconv5x5', 'reduce_n3_p1': 'dilconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'sepconv5x5', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'sepconv5x5', 'reduce_n5_p4': 'skipconnect', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lambd = 1\n",
    "n_chosen = 1\n",
    "weight = 0.1\n",
    "wt = None\n",
    "for number in [1, 2, 3]:\n",
    "    if dataset == \"fashionmnist\":\n",
    "        model = CNN(32, 1, channels, 10, layers, n_chosen=1)\n",
    "    elif dataset == \"cifar10\":\n",
    "        model = CNN(32, 3, channels, 10, layers, n_chosen=1)\n",
    "    elif dataset == \"cifar100\":\n",
    "        model = CNN(32, 3, channels, 100, layers, n_chosen=1)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() # mycriterion()\n",
    "    optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, epochs, eta_min=0.001)\n",
    "    trainer = MyDartsTrainer(\n",
    "        model=model,\n",
    "        loss=criterion, # =mycriterion,\n",
    "        metrics=lambda output, target: utils.accuracy(output, target, topk=(1,)),\n",
    "        optimizer=optim,\n",
    "        num_epochs=epochs,\n",
    "        dataset=dataset_train,\n",
    "        batch_size=batch_size,\n",
    "        log_frequency=log_frequency,\n",
    "        unrolled=unrolled,\n",
    "        weight=weight, #  \n",
    "        lambd=lambd, #   \n",
    "        train_as_optimal=True,\n",
    "        optimalPath='checkpoints/CIFAR100/optimal/arc.json',\n",
    "        tau=1.0,\n",
    "        learning_rate=2.5E-3,\n",
    "        arc_learning_rate=3.0E-1,\n",
    "        n_chosen=1,\n",
    "        t_alpha=0.2,\n",
    "        t_beta=0.2,\n",
    "    )\n",
    "    trainer.fit()\n",
    "    final_architecture = trainer.export()\n",
    "    print('Final architecture:', final_architecture)\n",
    "    json.dump(trainer.export(), open(f'checkpoints/CIFAR100/random/{number}/arc.json', 'w+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 2\n",
    "batch_size = 96\n",
    "log_frequency = 20\n",
    "channels = 16\n",
    "unrolled = False\n",
    "visualization = False\n",
    "dataset = 'cifar100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "./checkpoints/cifar100/random/1/\n",
      "[2024-01-15 09:11:58] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv3x3', 'reduce_n2_p1': 'sepconv5x5', 'reduce_n3_p0': 'maxpool', 'reduce_n3_p1': 'sepconv5x5', 'reduce_n3_p2': 'dilconv5x5', 'reduce_n4_p0': 'dilconv3x3', 'reduce_n4_p1': 'sepconv5x5', 'reduce_n4_p2': 'sepconv5x5', 'reduce_n4_p3': 'sepconv5x5', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'skipconnect', 'reduce_n5_p2': 'dilconv3x3', 'reduce_n5_p3': 'sepconv3x3', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [0], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [4]}\u001b[0m\n",
      "[2024-01-15 09:11:58] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2024-01-15 09:12:05] \u001b[32mTrain: [  1/50] Step 000/520 Loss 6.551 Prec@(1,5) (0.0%, 4.2%)\u001b[0m\n",
      "[2024-01-15 09:12:05] \u001b[32mTrain: [  1/50] Step 020/520 Loss 6.450 Prec@(1,5) (1.1%, 6.3%)\u001b[0m\n",
      "[2024-01-15 09:12:05] \u001b[32mTrain: [  1/50] Step 040/520 Loss 6.345 Prec@(1,5) (1.9%, 9.0%)\u001b[0m\n",
      "[2024-01-15 09:12:06] \u001b[32mTrain: [  1/50] Step 060/520 Loss 6.255 Prec@(1,5) (2.8%, 11.4%)\u001b[0m\n",
      "[2024-01-15 09:12:06] \u001b[32mTrain: [  1/50] Step 080/520 Loss 6.172 Prec@(1,5) (3.2%, 13.2%)\u001b[0m\n",
      "[2024-01-15 09:12:06] \u001b[32mTrain: [  1/50] Step 100/520 Loss 6.097 Prec@(1,5) (3.8%, 14.8%)\u001b[0m\n",
      "[2024-01-15 09:12:06] \u001b[32mTrain: [  1/50] Step 120/520 Loss 6.036 Prec@(1,5) (4.2%, 16.2%)\u001b[0m\n",
      "[2024-01-15 09:12:07] \u001b[32mTrain: [  1/50] Step 140/520 Loss 5.974 Prec@(1,5) (4.7%, 17.8%)\u001b[0m\n",
      "[2024-01-15 09:12:07] \u001b[32mTrain: [  1/50] Step 160/520 Loss 5.914 Prec@(1,5) (5.4%, 19.1%)\u001b[0m\n",
      "[2024-01-15 09:12:07] \u001b[32mTrain: [  1/50] Step 180/520 Loss 5.867 Prec@(1,5) (5.8%, 20.2%)\u001b[0m\n",
      "[2024-01-15 09:12:08] \u001b[32mTrain: [  1/50] Step 200/520 Loss 5.839 Prec@(1,5) (6.0%, 20.8%)\u001b[0m\n",
      "[2024-01-15 09:12:08] \u001b[32mTrain: [  1/50] Step 220/520 Loss 5.811 Prec@(1,5) (6.3%, 21.5%)\u001b[0m\n",
      "[2024-01-15 09:12:08] \u001b[32mTrain: [  1/50] Step 240/520 Loss 5.778 Prec@(1,5) (6.5%, 22.3%)\u001b[0m\n",
      "[2024-01-15 09:12:09] \u001b[32mTrain: [  1/50] Step 260/520 Loss 5.747 Prec@(1,5) (6.7%, 23.1%)\u001b[0m\n",
      "[2024-01-15 09:12:09] \u001b[32mTrain: [  1/50] Step 280/520 Loss 5.717 Prec@(1,5) (6.9%, 23.8%)\u001b[0m\n",
      "[2024-01-15 09:12:09] \u001b[32mTrain: [  1/50] Step 300/520 Loss 5.691 Prec@(1,5) (7.2%, 24.4%)\u001b[0m\n",
      "[2024-01-15 09:12:09] \u001b[32mTrain: [  1/50] Step 320/520 Loss 5.667 Prec@(1,5) (7.5%, 25.0%)\u001b[0m\n",
      "[2024-01-15 09:12:10] \u001b[32mTrain: [  1/50] Step 340/520 Loss 5.641 Prec@(1,5) (7.6%, 25.5%)\u001b[0m\n",
      "[2024-01-15 09:12:10] \u001b[32mTrain: [  1/50] Step 360/520 Loss 5.611 Prec@(1,5) (7.9%, 26.2%)\u001b[0m\n",
      "[2024-01-15 09:12:10] \u001b[32mTrain: [  1/50] Step 380/520 Loss 5.591 Prec@(1,5) (8.1%, 26.6%)\u001b[0m\n",
      "[2024-01-15 09:12:11] \u001b[32mTrain: [  1/50] Step 400/520 Loss 5.567 Prec@(1,5) (8.4%, 27.2%)\u001b[0m\n",
      "[2024-01-15 09:12:11] \u001b[32mTrain: [  1/50] Step 420/520 Loss 5.546 Prec@(1,5) (8.6%, 27.6%)\u001b[0m\n",
      "[2024-01-15 09:12:11] \u001b[32mTrain: [  1/50] Step 440/520 Loss 5.523 Prec@(1,5) (8.9%, 28.1%)\u001b[0m\n",
      "[2024-01-15 09:12:12] \u001b[32mTrain: [  1/50] Step 460/520 Loss 5.498 Prec@(1,5) (9.1%, 28.6%)\u001b[0m\n",
      "[2024-01-15 09:12:12] \u001b[32mTrain: [  1/50] Step 480/520 Loss 5.480 Prec@(1,5) (9.4%, 29.0%)\u001b[0m\n",
      "[2024-01-15 09:12:12] \u001b[32mTrain: [  1/50] Step 500/520 Loss 5.462 Prec@(1,5) (9.6%, 29.4%)\u001b[0m\n",
      "[2024-01-15 09:12:12] \u001b[32mTrain: [  1/50] Step 520/520 Loss 5.444 Prec@(1,5) (9.8%, 29.8%)\u001b[0m\n",
      "[2024-01-15 09:12:13] \u001b[32mTrain: [  1/50] Final Prec@1 9.7520%\u001b[0m\n",
      "[2024-01-15 09:12:16] \u001b[32mValid: [  1/50] Step 000/104 Loss 4.854 Prec@(1,5) (13.5%, 34.4%)\u001b[0m\n",
      "[2024-01-15 09:12:17] \u001b[32mValid: [  1/50] Step 020/104 Loss 4.558 Prec@(1,5) (16.2%, 41.1%)\u001b[0m\n",
      "[2024-01-15 09:12:17] \u001b[32mValid: [  1/50] Step 040/104 Loss 4.574 Prec@(1,5) (15.2%, 40.7%)\u001b[0m\n",
      "[2024-01-15 09:12:17] \u001b[32mValid: [  1/50] Step 060/104 Loss 4.550 Prec@(1,5) (15.4%, 41.2%)\u001b[0m\n",
      "[2024-01-15 09:12:18] \u001b[32mValid: [  1/50] Step 080/104 Loss 4.578 Prec@(1,5) (15.1%, 40.9%)\u001b[0m\n",
      "[2024-01-15 09:12:18] \u001b[32mValid: [  1/50] Step 100/104 Loss 4.597 Prec@(1,5) (15.2%, 40.8%)\u001b[0m\n",
      "[2024-01-15 09:12:18] \u001b[32mValid: [  1/50] Step 104/104 Loss 4.599 Prec@(1,5) (15.1%, 40.8%)\u001b[0m\n",
      "[2024-01-15 09:12:18] \u001b[32mValid: [  1/50] Final Prec@1 15.1300%\u001b[0m\n",
      "[2024-01-15 09:12:18] \u001b[32mEpoch 1 LR 0.024975\u001b[0m\n",
      "[2024-01-15 09:12:25] \u001b[32mTrain: [  2/50] Step 000/520 Loss 4.825 Prec@(1,5) (15.6%, 49.0%)\u001b[0m\n",
      "[2024-01-15 09:12:26] \u001b[32mTrain: [  2/50] Step 020/520 Loss 4.915 Prec@(1,5) (14.8%, 41.2%)\u001b[0m\n",
      "[2024-01-15 09:12:27] \u001b[32mTrain: [  2/50] Step 040/520 Loss 4.959 Prec@(1,5) (14.5%, 40.2%)\u001b[0m\n",
      "[2024-01-15 09:12:27] \u001b[32mTrain: [  2/50] Step 060/520 Loss 4.956 Prec@(1,5) (14.7%, 39.9%)\u001b[0m\n",
      "[2024-01-15 09:12:28] \u001b[32mTrain: [  2/50] Step 080/520 Loss 4.923 Prec@(1,5) (15.5%, 40.7%)\u001b[0m\n",
      "[2024-01-15 09:12:29] \u001b[32mTrain: [  2/50] Step 100/520 Loss 4.919 Prec@(1,5) (15.4%, 40.8%)\u001b[0m\n",
      "[2024-01-15 09:12:29] \u001b[32mTrain: [  2/50] Step 120/520 Loss 4.908 Prec@(1,5) (15.5%, 41.2%)\u001b[0m\n",
      "[2024-01-15 09:12:30] \u001b[32mTrain: [  2/50] Step 140/520 Loss 4.890 Prec@(1,5) (15.7%, 41.7%)\u001b[0m\n",
      "[2024-01-15 09:12:31] \u001b[32mTrain: [  2/50] Step 160/520 Loss 4.883 Prec@(1,5) (15.9%, 42.1%)\u001b[0m\n",
      "[2024-01-15 09:12:31] \u001b[32mTrain: [  2/50] Step 180/520 Loss 4.874 Prec@(1,5) (16.0%, 42.3%)\u001b[0m\n",
      "[2024-01-15 09:12:32] \u001b[32mTrain: [  2/50] Step 200/520 Loss 4.862 Prec@(1,5) (16.1%, 42.6%)\u001b[0m\n",
      "[2024-01-15 09:12:33] \u001b[32mTrain: [  2/50] Step 220/520 Loss 4.857 Prec@(1,5) (16.1%, 42.6%)\u001b[0m\n",
      "[2024-01-15 09:12:34] \u001b[32mTrain: [  2/50] Step 240/520 Loss 4.845 Prec@(1,5) (16.2%, 42.8%)\u001b[0m\n",
      "[2024-01-15 09:12:34] \u001b[32mTrain: [  2/50] Step 260/520 Loss 4.834 Prec@(1,5) (16.3%, 43.0%)\u001b[0m\n",
      "[2024-01-15 09:12:35] \u001b[32mTrain: [  2/50] Step 280/520 Loss 4.826 Prec@(1,5) (16.5%, 43.2%)\u001b[0m\n",
      "[2024-01-15 09:12:36] \u001b[32mTrain: [  2/50] Step 300/520 Loss 4.814 Prec@(1,5) (16.6%, 43.4%)\u001b[0m\n",
      "[2024-01-15 09:12:37] \u001b[32mTrain: [  2/50] Step 320/520 Loss 4.803 Prec@(1,5) (16.8%, 43.6%)\u001b[0m\n",
      "[2024-01-15 09:12:37] \u001b[32mTrain: [  2/50] Step 340/520 Loss 4.795 Prec@(1,5) (16.9%, 43.8%)\u001b[0m\n",
      "[2024-01-15 09:12:38] \u001b[32mTrain: [  2/50] Step 360/520 Loss 4.782 Prec@(1,5) (17.2%, 44.1%)\u001b[0m\n",
      "[2024-01-15 09:12:39] \u001b[32mTrain: [  2/50] Step 380/520 Loss 4.772 Prec@(1,5) (17.3%, 44.3%)\u001b[0m\n",
      "[2024-01-15 09:12:39] \u001b[32mTrain: [  2/50] Step 400/520 Loss 4.761 Prec@(1,5) (17.4%, 44.5%)\u001b[0m\n",
      "[2024-01-15 09:12:40] \u001b[32mTrain: [  2/50] Step 420/520 Loss 4.751 Prec@(1,5) (17.5%, 44.7%)\u001b[0m\n",
      "[2024-01-15 09:12:41] \u001b[32mTrain: [  2/50] Step 440/520 Loss 4.740 Prec@(1,5) (17.7%, 45.0%)\u001b[0m\n",
      "[2024-01-15 09:12:42] \u001b[32mTrain: [  2/50] Step 460/520 Loss 4.733 Prec@(1,5) (17.7%, 45.2%)\u001b[0m\n",
      "[2024-01-15 09:12:43] \u001b[32mTrain: [  2/50] Step 480/520 Loss 4.724 Prec@(1,5) (17.9%, 45.4%)\u001b[0m\n",
      "[2024-01-15 09:12:44] \u001b[32mTrain: [  2/50] Step 500/520 Loss 4.716 Prec@(1,5) (18.0%, 45.5%)\u001b[0m\n",
      "[2024-01-15 09:12:44] \u001b[32mTrain: [  2/50] Step 520/520 Loss 4.710 Prec@(1,5) (18.1%, 45.6%)\u001b[0m\n",
      "[2024-01-15 09:12:45] \u001b[32mTrain: [  2/50] Final Prec@1 18.0900%\u001b[0m\n",
      "[2024-01-15 09:12:49] \u001b[32mValid: [  2/50] Step 000/104 Loss 4.057 Prec@(1,5) (25.0%, 52.1%)\u001b[0m\n",
      "[2024-01-15 09:12:49] \u001b[32mValid: [  2/50] Step 020/104 Loss 4.175 Prec@(1,5) (21.1%, 49.7%)\u001b[0m\n",
      "[2024-01-15 09:12:50] \u001b[32mValid: [  2/50] Step 040/104 Loss 4.166 Prec@(1,5) (20.8%, 50.1%)\u001b[0m\n",
      "[2024-01-15 09:12:50] \u001b[32mValid: [  2/50] Step 060/104 Loss 4.136 Prec@(1,5) (21.3%, 50.0%)\u001b[0m\n",
      "[2024-01-15 09:12:50] \u001b[32mValid: [  2/50] Step 080/104 Loss 4.158 Prec@(1,5) (20.9%, 49.8%)\u001b[0m\n",
      "[2024-01-15 09:12:50] \u001b[32mValid: [  2/50] Step 100/104 Loss 4.173 Prec@(1,5) (20.8%, 49.8%)\u001b[0m\n",
      "[2024-01-15 09:12:50] \u001b[32mValid: [  2/50] Step 104/104 Loss 4.177 Prec@(1,5) (20.8%, 49.7%)\u001b[0m\n",
      "[2024-01-15 09:12:51] \u001b[32mValid: [  2/50] Final Prec@1 20.7500%\u001b[0m\n",
      "[2024-01-15 09:12:51] \u001b[32mEpoch 2 LR 0.024901\u001b[0m\n",
      "[2024-01-15 09:12:58] \u001b[32mTrain: [  3/50] Step 000/520 Loss 4.634 Prec@(1,5) (15.6%, 50.0%)\u001b[0m\n",
      "[2024-01-15 09:12:59] \u001b[32mTrain: [  3/50] Step 020/520 Loss 4.379 Prec@(1,5) (23.5%, 51.4%)\u001b[0m\n",
      "[2024-01-15 09:12:59] \u001b[32mTrain: [  3/50] Step 040/520 Loss 4.359 Prec@(1,5) (23.2%, 52.2%)\u001b[0m\n",
      "[2024-01-15 09:13:00] \u001b[32mTrain: [  3/50] Step 060/520 Loss 4.379 Prec@(1,5) (22.7%, 51.7%)\u001b[0m\n",
      "[2024-01-15 09:13:01] \u001b[32mTrain: [  3/50] Step 080/520 Loss 4.391 Prec@(1,5) (22.3%, 51.5%)\u001b[0m\n",
      "[2024-01-15 09:13:01] \u001b[32mTrain: [  3/50] Step 100/520 Loss 4.391 Prec@(1,5) (22.0%, 51.7%)\u001b[0m\n",
      "[2024-01-15 09:13:02] \u001b[32mTrain: [  3/50] Step 120/520 Loss 4.379 Prec@(1,5) (22.1%, 52.0%)\u001b[0m\n",
      "[2024-01-15 09:13:03] \u001b[32mTrain: [  3/50] Step 140/520 Loss 4.381 Prec@(1,5) (22.2%, 52.1%)\u001b[0m\n",
      "[2024-01-15 09:13:03] \u001b[32mTrain: [  3/50] Step 160/520 Loss 4.373 Prec@(1,5) (22.3%, 52.2%)\u001b[0m\n",
      "[2024-01-15 09:13:04] \u001b[32mTrain: [  3/50] Step 180/520 Loss 4.356 Prec@(1,5) (22.6%, 52.3%)\u001b[0m\n",
      "[2024-01-15 09:13:05] \u001b[32mTrain: [  3/50] Step 200/520 Loss 4.349 Prec@(1,5) (22.6%, 52.4%)\u001b[0m\n",
      "[2024-01-15 09:13:05] \u001b[32mTrain: [  3/50] Step 220/520 Loss 4.346 Prec@(1,5) (22.6%, 52.5%)\u001b[0m\n",
      "[2024-01-15 09:13:06] \u001b[32mTrain: [  3/50] Step 240/520 Loss 4.335 Prec@(1,5) (22.8%, 52.8%)\u001b[0m\n",
      "[2024-01-15 09:13:07] \u001b[32mTrain: [  3/50] Step 260/520 Loss 4.326 Prec@(1,5) (22.9%, 52.9%)\u001b[0m\n",
      "[2024-01-15 09:13:08] \u001b[32mTrain: [  3/50] Step 280/520 Loss 4.312 Prec@(1,5) (23.1%, 53.2%)\u001b[0m\n",
      "[2024-01-15 09:13:08] \u001b[32mTrain: [  3/50] Step 300/520 Loss 4.313 Prec@(1,5) (23.0%, 53.2%)\u001b[0m\n",
      "[2024-01-15 09:13:09] \u001b[32mTrain: [  3/50] Step 320/520 Loss 4.308 Prec@(1,5) (23.1%, 53.3%)\u001b[0m\n",
      "[2024-01-15 09:13:10] \u001b[32mTrain: [  3/50] Step 340/520 Loss 4.299 Prec@(1,5) (23.2%, 53.4%)\u001b[0m\n",
      "[2024-01-15 09:13:11] \u001b[32mTrain: [  3/50] Step 360/520 Loss 4.291 Prec@(1,5) (23.4%, 53.5%)\u001b[0m\n",
      "[2024-01-15 09:13:11] \u001b[32mTrain: [  3/50] Step 380/520 Loss 4.287 Prec@(1,5) (23.5%, 53.6%)\u001b[0m\n",
      "[2024-01-15 09:13:12] \u001b[32mTrain: [  3/50] Step 400/520 Loss 4.283 Prec@(1,5) (23.6%, 53.7%)\u001b[0m\n",
      "[2024-01-15 09:13:13] \u001b[32mTrain: [  3/50] Step 420/520 Loss 4.279 Prec@(1,5) (23.6%, 53.8%)\u001b[0m\n",
      "[2024-01-15 09:13:14] \u001b[32mTrain: [  3/50] Step 440/520 Loss 4.272 Prec@(1,5) (23.7%, 53.9%)\u001b[0m\n",
      "[2024-01-15 09:13:14] \u001b[32mTrain: [  3/50] Step 460/520 Loss 4.273 Prec@(1,5) (23.7%, 53.8%)\u001b[0m\n",
      "[2024-01-15 09:13:15] \u001b[32mTrain: [  3/50] Step 480/520 Loss 4.266 Prec@(1,5) (23.8%, 54.0%)\u001b[0m\n",
      "[2024-01-15 09:13:16] \u001b[32mTrain: [  3/50] Step 500/520 Loss 4.256 Prec@(1,5) (24.0%, 54.2%)\u001b[0m\n",
      "[2024-01-15 09:13:17] \u001b[32mTrain: [  3/50] Step 520/520 Loss 4.252 Prec@(1,5) (24.1%, 54.3%)\u001b[0m\n",
      "[2024-01-15 09:13:17] \u001b[32mTrain: [  3/50] Final Prec@1 24.0560%\u001b[0m\n",
      "[2024-01-15 09:13:21] \u001b[32mValid: [  3/50] Step 000/104 Loss 4.015 Prec@(1,5) (22.9%, 53.1%)\u001b[0m\n",
      "[2024-01-15 09:13:21] \u001b[32mValid: [  3/50] Step 020/104 Loss 4.148 Prec@(1,5) (23.5%, 54.0%)\u001b[0m\n",
      "[2024-01-15 09:13:22] \u001b[32mValid: [  3/50] Step 040/104 Loss 4.067 Prec@(1,5) (24.0%, 54.5%)\u001b[0m\n",
      "[2024-01-15 09:13:22] \u001b[32mValid: [  3/50] Step 060/104 Loss 4.035 Prec@(1,5) (24.6%, 54.2%)\u001b[0m\n",
      "[2024-01-15 09:13:22] \u001b[32mValid: [  3/50] Step 080/104 Loss 4.060 Prec@(1,5) (24.2%, 54.0%)\u001b[0m\n",
      "[2024-01-15 09:13:23] \u001b[32mValid: [  3/50] Step 100/104 Loss 4.083 Prec@(1,5) (24.0%, 54.0%)\u001b[0m\n",
      "[2024-01-15 09:13:23] \u001b[32mValid: [  3/50] Step 104/104 Loss 4.081 Prec@(1,5) (24.0%, 54.0%)\u001b[0m\n",
      "[2024-01-15 09:13:23] \u001b[32mValid: [  3/50] Final Prec@1 23.9700%\u001b[0m\n",
      "[2024-01-15 09:13:23] \u001b[32mEpoch 3 LR 0.024779\u001b[0m\n",
      "[2024-01-15 09:13:30] \u001b[32mTrain: [  4/50] Step 000/520 Loss 4.098 Prec@(1,5) (30.2%, 57.3%)\u001b[0m\n",
      "[2024-01-15 09:13:31] \u001b[32mTrain: [  4/50] Step 020/520 Loss 4.063 Prec@(1,5) (26.9%, 56.6%)\u001b[0m\n",
      "[2024-01-15 09:13:31] \u001b[32mTrain: [  4/50] Step 040/520 Loss 4.082 Prec@(1,5) (26.6%, 56.8%)\u001b[0m\n",
      "[2024-01-15 09:13:32] \u001b[32mTrain: [  4/50] Step 060/520 Loss 4.059 Prec@(1,5) (27.0%, 57.3%)\u001b[0m\n",
      "[2024-01-15 09:13:33] \u001b[32mTrain: [  4/50] Step 080/520 Loss 4.059 Prec@(1,5) (26.9%, 57.5%)\u001b[0m\n",
      "[2024-01-15 09:13:33] \u001b[32mTrain: [  4/50] Step 100/520 Loss 4.064 Prec@(1,5) (26.7%, 57.6%)\u001b[0m\n",
      "[2024-01-15 09:13:34] \u001b[32mTrain: [  4/50] Step 120/520 Loss 4.062 Prec@(1,5) (26.7%, 57.6%)\u001b[0m\n",
      "[2024-01-15 09:13:35] \u001b[32mTrain: [  4/50] Step 140/520 Loss 4.070 Prec@(1,5) (26.4%, 57.3%)\u001b[0m\n",
      "[2024-01-15 09:13:35] \u001b[32mTrain: [  4/50] Step 160/520 Loss 4.057 Prec@(1,5) (26.8%, 57.7%)\u001b[0m\n",
      "[2024-01-15 09:13:36] \u001b[32mTrain: [  4/50] Step 180/520 Loss 4.041 Prec@(1,5) (26.8%, 57.9%)\u001b[0m\n",
      "[2024-01-15 09:13:37] \u001b[32mTrain: [  4/50] Step 200/520 Loss 4.046 Prec@(1,5) (26.8%, 57.9%)\u001b[0m\n",
      "[2024-01-15 09:13:37] \u001b[32mTrain: [  4/50] Step 220/520 Loss 4.050 Prec@(1,5) (26.8%, 57.9%)\u001b[0m\n",
      "[2024-01-15 09:13:38] \u001b[32mTrain: [  4/50] Step 240/520 Loss 4.050 Prec@(1,5) (26.8%, 58.0%)\u001b[0m\n",
      "[2024-01-15 09:13:39] \u001b[32mTrain: [  4/50] Step 260/520 Loss 4.035 Prec@(1,5) (27.1%, 58.1%)\u001b[0m\n",
      "[2024-01-15 09:13:40] \u001b[32mTrain: [  4/50] Step 280/520 Loss 4.032 Prec@(1,5) (27.1%, 58.2%)\u001b[0m\n",
      "[2024-01-15 09:13:40] \u001b[32mTrain: [  4/50] Step 300/520 Loss 4.024 Prec@(1,5) (27.2%, 58.2%)\u001b[0m\n",
      "[2024-01-15 09:13:41] \u001b[32mTrain: [  4/50] Step 320/520 Loss 4.021 Prec@(1,5) (27.3%, 58.3%)\u001b[0m\n",
      "[2024-01-15 09:13:42] \u001b[32mTrain: [  4/50] Step 340/520 Loss 4.010 Prec@(1,5) (27.4%, 58.5%)\u001b[0m\n",
      "[2024-01-15 09:13:42] \u001b[32mTrain: [  4/50] Step 360/520 Loss 4.011 Prec@(1,5) (27.3%, 58.5%)\u001b[0m\n",
      "[2024-01-15 09:13:43] \u001b[32mTrain: [  4/50] Step 380/520 Loss 4.006 Prec@(1,5) (27.4%, 58.5%)\u001b[0m\n",
      "[2024-01-15 09:13:44] \u001b[32mTrain: [  4/50] Step 400/520 Loss 4.008 Prec@(1,5) (27.3%, 58.5%)\u001b[0m\n",
      "[2024-01-15 09:13:45] \u001b[32mTrain: [  4/50] Step 420/520 Loss 3.997 Prec@(1,5) (27.5%, 58.7%)\u001b[0m\n",
      "[2024-01-15 09:13:45] \u001b[32mTrain: [  4/50] Step 440/520 Loss 3.991 Prec@(1,5) (27.6%, 58.8%)\u001b[0m\n",
      "[2024-01-15 09:13:46] \u001b[32mTrain: [  4/50] Step 460/520 Loss 3.987 Prec@(1,5) (27.6%, 58.9%)\u001b[0m\n",
      "[2024-01-15 09:13:47] \u001b[32mTrain: [  4/50] Step 480/520 Loss 3.978 Prec@(1,5) (27.7%, 59.1%)\u001b[0m\n",
      "[2024-01-15 09:13:48] \u001b[32mTrain: [  4/50] Step 500/520 Loss 3.973 Prec@(1,5) (27.8%, 59.1%)\u001b[0m\n",
      "[2024-01-15 09:13:49] \u001b[32mTrain: [  4/50] Step 520/520 Loss 3.969 Prec@(1,5) (27.9%, 59.2%)\u001b[0m\n",
      "[2024-01-15 09:13:49] \u001b[32mTrain: [  4/50] Final Prec@1 27.9280%\u001b[0m\n",
      "[2024-01-15 09:13:53] \u001b[32mValid: [  4/50] Step 000/104 Loss 4.392 Prec@(1,5) (28.1%, 51.0%)\u001b[0m\n",
      "[2024-01-15 09:13:54] \u001b[32mValid: [  4/50] Step 020/104 Loss 4.291 Prec@(1,5) (25.7%, 52.4%)\u001b[0m\n",
      "[2024-01-15 09:13:54] \u001b[32mValid: [  4/50] Step 040/104 Loss 4.228 Prec@(1,5) (25.1%, 52.7%)\u001b[0m\n",
      "[2024-01-15 09:13:55] \u001b[32mValid: [  4/50] Step 060/104 Loss 4.208 Prec@(1,5) (25.7%, 52.6%)\u001b[0m\n",
      "[2024-01-15 09:13:55] \u001b[32mValid: [  4/50] Step 080/104 Loss 4.225 Prec@(1,5) (25.0%, 52.6%)\u001b[0m\n",
      "[2024-01-15 09:13:56] \u001b[32mValid: [  4/50] Step 100/104 Loss 4.245 Prec@(1,5) (24.8%, 52.6%)\u001b[0m\n",
      "[2024-01-15 09:13:56] \u001b[32mValid: [  4/50] Step 104/104 Loss 4.242 Prec@(1,5) (24.8%, 52.6%)\u001b[0m\n",
      "[2024-01-15 09:13:56] \u001b[32mValid: [  4/50] Final Prec@1 24.7700%\u001b[0m\n",
      "[2024-01-15 09:13:56] \u001b[32mEpoch 4 LR 0.024607\u001b[0m\n",
      "[2024-01-15 09:14:04] \u001b[32mTrain: [  5/50] Step 000/520 Loss 3.764 Prec@(1,5) (34.4%, 58.3%)\u001b[0m\n",
      "[2024-01-15 09:14:04] \u001b[32mTrain: [  5/50] Step 020/520 Loss 3.769 Prec@(1,5) (30.9%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:14:05] \u001b[32mTrain: [  5/50] Step 040/520 Loss 3.839 Prec@(1,5) (29.7%, 60.9%)\u001b[0m\n",
      "[2024-01-15 09:14:06] \u001b[32mTrain: [  5/50] Step 060/520 Loss 3.829 Prec@(1,5) (29.8%, 61.0%)\u001b[0m\n",
      "[2024-01-15 09:14:06] \u001b[32mTrain: [  5/50] Step 080/520 Loss 3.797 Prec@(1,5) (30.2%, 61.2%)\u001b[0m\n",
      "[2024-01-15 09:14:07] \u001b[32mTrain: [  5/50] Step 100/520 Loss 3.802 Prec@(1,5) (30.0%, 61.5%)\u001b[0m\n",
      "[2024-01-15 09:14:08] \u001b[32mTrain: [  5/50] Step 120/520 Loss 3.808 Prec@(1,5) (29.8%, 61.5%)\u001b[0m\n",
      "[2024-01-15 09:14:09] \u001b[32mTrain: [  5/50] Step 140/520 Loss 3.811 Prec@(1,5) (29.7%, 61.5%)\u001b[0m\n",
      "[2024-01-15 09:14:10] \u001b[32mTrain: [  5/50] Step 160/520 Loss 3.807 Prec@(1,5) (29.9%, 61.7%)\u001b[0m\n",
      "[2024-01-15 09:14:11] \u001b[32mTrain: [  5/50] Step 180/520 Loss 3.805 Prec@(1,5) (29.9%, 61.8%)\u001b[0m\n",
      "[2024-01-15 09:14:11] \u001b[32mTrain: [  5/50] Step 200/520 Loss 3.810 Prec@(1,5) (29.8%, 61.7%)\u001b[0m\n",
      "[2024-01-15 09:14:12] \u001b[32mTrain: [  5/50] Step 220/520 Loss 3.810 Prec@(1,5) (29.8%, 61.8%)\u001b[0m\n",
      "[2024-01-15 09:14:13] \u001b[32mTrain: [  5/50] Step 240/520 Loss 3.809 Prec@(1,5) (29.9%, 61.8%)\u001b[0m\n",
      "[2024-01-15 09:14:14] \u001b[32mTrain: [  5/50] Step 260/520 Loss 3.796 Prec@(1,5) (30.1%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:14:15] \u001b[32mTrain: [  5/50] Step 280/520 Loss 3.795 Prec@(1,5) (30.3%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:14:16] \u001b[32mTrain: [  5/50] Step 300/520 Loss 3.796 Prec@(1,5) (30.3%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:14:17] \u001b[32mTrain: [  5/50] Step 320/520 Loss 3.795 Prec@(1,5) (30.3%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:14:17] \u001b[32mTrain: [  5/50] Step 340/520 Loss 3.791 Prec@(1,5) (30.3%, 62.2%)\u001b[0m\n",
      "[2024-01-15 09:14:18] \u001b[32mTrain: [  5/50] Step 360/520 Loss 3.786 Prec@(1,5) (30.3%, 62.3%)\u001b[0m\n",
      "[2024-01-15 09:14:19] \u001b[32mTrain: [  5/50] Step 380/520 Loss 3.783 Prec@(1,5) (30.4%, 62.3%)\u001b[0m\n",
      "[2024-01-15 09:14:20] \u001b[32mTrain: [  5/50] Step 400/520 Loss 3.783 Prec@(1,5) (30.4%, 62.3%)\u001b[0m\n",
      "[2024-01-15 09:14:21] \u001b[32mTrain: [  5/50] Step 420/520 Loss 3.782 Prec@(1,5) (30.4%, 62.3%)\u001b[0m\n",
      "[2024-01-15 09:14:22] \u001b[32mTrain: [  5/50] Step 440/520 Loss 3.777 Prec@(1,5) (30.6%, 62.4%)\u001b[0m\n",
      "[2024-01-15 09:14:22] \u001b[32mTrain: [  5/50] Step 460/520 Loss 3.774 Prec@(1,5) (30.6%, 62.4%)\u001b[0m\n",
      "[2024-01-15 09:14:23] \u001b[32mTrain: [  5/50] Step 480/520 Loss 3.767 Prec@(1,5) (30.7%, 62.6%)\u001b[0m\n",
      "[2024-01-15 09:14:24] \u001b[32mTrain: [  5/50] Step 500/520 Loss 3.767 Prec@(1,5) (30.7%, 62.5%)\u001b[0m\n",
      "[2024-01-15 09:14:25] \u001b[32mTrain: [  5/50] Step 520/520 Loss 3.764 Prec@(1,5) (30.7%, 62.6%)\u001b[0m\n",
      "[2024-01-15 09:14:25] \u001b[32mTrain: [  5/50] Final Prec@1 30.6800%\u001b[0m\n",
      "[2024-01-15 09:14:30] \u001b[32mValid: [  5/50] Step 000/104 Loss 3.450 Prec@(1,5) (31.2%, 64.6%)\u001b[0m\n",
      "[2024-01-15 09:14:30] \u001b[32mValid: [  5/50] Step 020/104 Loss 3.464 Prec@(1,5) (30.6%, 62.5%)\u001b[0m\n",
      "[2024-01-15 09:14:30] \u001b[32mValid: [  5/50] Step 040/104 Loss 3.439 Prec@(1,5) (30.3%, 62.7%)\u001b[0m\n",
      "[2024-01-15 09:14:30] \u001b[32mValid: [  5/50] Step 060/104 Loss 3.434 Prec@(1,5) (30.4%, 62.5%)\u001b[0m\n",
      "[2024-01-15 09:14:31] \u001b[32mValid: [  5/50] Step 080/104 Loss 3.462 Prec@(1,5) (30.4%, 62.2%)\u001b[0m\n",
      "[2024-01-15 09:14:31] \u001b[32mValid: [  5/50] Step 100/104 Loss 3.477 Prec@(1,5) (30.3%, 62.0%)\u001b[0m\n",
      "[2024-01-15 09:14:31] \u001b[32mValid: [  5/50] Step 104/104 Loss 3.478 Prec@(1,5) (30.4%, 62.0%)\u001b[0m\n",
      "[2024-01-15 09:14:31] \u001b[32mValid: [  5/50] Final Prec@1 30.4000%\u001b[0m\n",
      "[2024-01-15 09:14:31] \u001b[32mEpoch 5 LR 0.024388\u001b[0m\n",
      "[2024-01-15 09:14:39] \u001b[32mTrain: [  6/50] Step 000/520 Loss 3.985 Prec@(1,5) (27.1%, 63.5%)\u001b[0m\n",
      "[2024-01-15 09:14:40] \u001b[32mTrain: [  6/50] Step 020/520 Loss 3.626 Prec@(1,5) (32.9%, 65.6%)\u001b[0m\n",
      "[2024-01-15 09:14:40] \u001b[32mTrain: [  6/50] Step 040/520 Loss 3.627 Prec@(1,5) (32.7%, 65.9%)\u001b[0m\n",
      "[2024-01-15 09:14:41] \u001b[32mTrain: [  6/50] Step 060/520 Loss 3.641 Prec@(1,5) (32.9%, 65.5%)\u001b[0m\n",
      "[2024-01-15 09:14:42] \u001b[32mTrain: [  6/50] Step 080/520 Loss 3.629 Prec@(1,5) (33.2%, 65.3%)\u001b[0m\n",
      "[2024-01-15 09:14:43] \u001b[32mTrain: [  6/50] Step 100/520 Loss 3.630 Prec@(1,5) (33.1%, 64.8%)\u001b[0m\n",
      "[2024-01-15 09:14:43] \u001b[32mTrain: [  6/50] Step 120/520 Loss 3.633 Prec@(1,5) (33.0%, 64.7%)\u001b[0m\n",
      "[2024-01-15 09:14:44] \u001b[32mTrain: [  6/50] Step 140/520 Loss 3.618 Prec@(1,5) (33.3%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:14:45] \u001b[32mTrain: [  6/50] Step 160/520 Loss 3.617 Prec@(1,5) (33.3%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:14:46] \u001b[32mTrain: [  6/50] Step 180/520 Loss 3.619 Prec@(1,5) (33.3%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:14:47] \u001b[32mTrain: [  6/50] Step 200/520 Loss 3.625 Prec@(1,5) (33.1%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:14:47] \u001b[32mTrain: [  6/50] Step 220/520 Loss 3.620 Prec@(1,5) (33.2%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:14:48] \u001b[32mTrain: [  6/50] Step 240/520 Loss 3.610 Prec@(1,5) (33.3%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:14:49] \u001b[32mTrain: [  6/50] Step 260/520 Loss 3.618 Prec@(1,5) (33.2%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:14:50] \u001b[32mTrain: [  6/50] Step 280/520 Loss 3.621 Prec@(1,5) (33.2%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:14:51] \u001b[32mTrain: [  6/50] Step 300/520 Loss 3.616 Prec@(1,5) (33.3%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:14:52] \u001b[32mTrain: [  6/50] Step 320/520 Loss 3.618 Prec@(1,5) (33.2%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:14:53] \u001b[32mTrain: [  6/50] Step 340/520 Loss 3.612 Prec@(1,5) (33.2%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:14:54] \u001b[32mTrain: [  6/50] Step 360/520 Loss 3.615 Prec@(1,5) (33.2%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:14:55] \u001b[32mTrain: [  6/50] Step 380/520 Loss 3.614 Prec@(1,5) (33.2%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:14:56] \u001b[32mTrain: [  6/50] Step 400/520 Loss 3.612 Prec@(1,5) (33.2%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:14:56] \u001b[32mTrain: [  6/50] Step 420/520 Loss 3.610 Prec@(1,5) (33.2%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:14:57] \u001b[32mTrain: [  6/50] Step 440/520 Loss 3.607 Prec@(1,5) (33.3%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:14:58] \u001b[32mTrain: [  6/50] Step 460/520 Loss 3.604 Prec@(1,5) (33.3%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:14:59] \u001b[32mTrain: [  6/50] Step 480/520 Loss 3.602 Prec@(1,5) (33.4%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:14:59] \u001b[32mTrain: [  6/50] Step 500/520 Loss 3.598 Prec@(1,5) (33.4%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:15:00] \u001b[32mTrain: [  6/50] Step 520/520 Loss 3.599 Prec@(1,5) (33.4%, 65.2%)\u001b[0m\n",
      "[2024-01-15 09:15:00] \u001b[32mTrain: [  6/50] Final Prec@1 33.3720%\u001b[0m\n",
      "[2024-01-15 09:15:05] \u001b[32mValid: [  6/50] Step 000/104 Loss 3.372 Prec@(1,5) (36.5%, 67.7%)\u001b[0m\n",
      "[2024-01-15 09:15:05] \u001b[32mValid: [  6/50] Step 020/104 Loss 3.359 Prec@(1,5) (33.6%, 64.8%)\u001b[0m\n",
      "[2024-01-15 09:15:05] \u001b[32mValid: [  6/50] Step 040/104 Loss 3.339 Prec@(1,5) (32.4%, 64.8%)\u001b[0m\n",
      "[2024-01-15 09:15:06] \u001b[32mValid: [  6/50] Step 060/104 Loss 3.324 Prec@(1,5) (32.8%, 64.5%)\u001b[0m\n",
      "[2024-01-15 09:15:06] \u001b[32mValid: [  6/50] Step 080/104 Loss 3.335 Prec@(1,5) (32.5%, 64.3%)\u001b[0m\n",
      "[2024-01-15 09:15:06] \u001b[32mValid: [  6/50] Step 100/104 Loss 3.337 Prec@(1,5) (32.5%, 64.2%)\u001b[0m\n",
      "[2024-01-15 09:15:06] \u001b[32mValid: [  6/50] Step 104/104 Loss 3.339 Prec@(1,5) (32.5%, 64.3%)\u001b[0m\n",
      "[2024-01-15 09:15:06] \u001b[32mValid: [  6/50] Final Prec@1 32.4700%\u001b[0m\n",
      "[2024-01-15 09:15:06] \u001b[32mEpoch 6 LR 0.024122\u001b[0m\n",
      "[2024-01-15 09:15:14] \u001b[32mTrain: [  7/50] Step 000/520 Loss 3.553 Prec@(1,5) (32.3%, 65.6%)\u001b[0m\n",
      "[2024-01-15 09:15:15] \u001b[32mTrain: [  7/50] Step 020/520 Loss 3.401 Prec@(1,5) (36.8%, 67.6%)\u001b[0m\n",
      "[2024-01-15 09:15:15] \u001b[32mTrain: [  7/50] Step 040/520 Loss 3.427 Prec@(1,5) (36.1%, 67.6%)\u001b[0m\n",
      "[2024-01-15 09:15:16] \u001b[32mTrain: [  7/50] Step 060/520 Loss 3.448 Prec@(1,5) (35.3%, 67.8%)\u001b[0m\n",
      "[2024-01-15 09:15:17] \u001b[32mTrain: [  7/50] Step 080/520 Loss 3.471 Prec@(1,5) (35.4%, 67.2%)\u001b[0m\n",
      "[2024-01-15 09:15:18] \u001b[32mTrain: [  7/50] Step 100/520 Loss 3.468 Prec@(1,5) (35.6%, 67.2%)\u001b[0m\n",
      "[2024-01-15 09:15:18] \u001b[32mTrain: [  7/50] Step 120/520 Loss 3.484 Prec@(1,5) (35.5%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:19] \u001b[32mTrain: [  7/50] Step 140/520 Loss 3.483 Prec@(1,5) (35.5%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:20] \u001b[32mTrain: [  7/50] Step 160/520 Loss 3.483 Prec@(1,5) (35.4%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:21] \u001b[32mTrain: [  7/50] Step 180/520 Loss 3.478 Prec@(1,5) (35.4%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:21] \u001b[32mTrain: [  7/50] Step 200/520 Loss 3.474 Prec@(1,5) (35.5%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:22] \u001b[32mTrain: [  7/50] Step 220/520 Loss 3.470 Prec@(1,5) (35.6%, 67.2%)\u001b[0m\n",
      "[2024-01-15 09:15:23] \u001b[32mTrain: [  7/50] Step 240/520 Loss 3.474 Prec@(1,5) (35.6%, 67.2%)\u001b[0m\n",
      "[2024-01-15 09:15:24] \u001b[32mTrain: [  7/50] Step 260/520 Loss 3.481 Prec@(1,5) (35.5%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:25] \u001b[32mTrain: [  7/50] Step 280/520 Loss 3.481 Prec@(1,5) (35.5%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:26] \u001b[32mTrain: [  7/50] Step 300/520 Loss 3.480 Prec@(1,5) (35.4%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:26] \u001b[32mTrain: [  7/50] Step 320/520 Loss 3.479 Prec@(1,5) (35.5%, 67.0%)\u001b[0m\n",
      "[2024-01-15 09:15:27] \u001b[32mTrain: [  7/50] Step 340/520 Loss 3.474 Prec@(1,5) (35.5%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:28] \u001b[32mTrain: [  7/50] Step 360/520 Loss 3.478 Prec@(1,5) (35.4%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:29] \u001b[32mTrain: [  7/50] Step 380/520 Loss 3.477 Prec@(1,5) (35.5%, 67.2%)\u001b[0m\n",
      "[2024-01-15 09:15:30] \u001b[32mTrain: [  7/50] Step 400/520 Loss 3.476 Prec@(1,5) (35.5%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:31] \u001b[32mTrain: [  7/50] Step 420/520 Loss 3.479 Prec@(1,5) (35.4%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:32] \u001b[32mTrain: [  7/50] Step 440/520 Loss 3.476 Prec@(1,5) (35.5%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:33] \u001b[32mTrain: [  7/50] Step 460/520 Loss 3.477 Prec@(1,5) (35.6%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:33] \u001b[32mTrain: [  7/50] Step 480/520 Loss 3.475 Prec@(1,5) (35.6%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:34] \u001b[32mTrain: [  7/50] Step 500/520 Loss 3.474 Prec@(1,5) (35.6%, 67.2%)\u001b[0m\n",
      "[2024-01-15 09:15:35] \u001b[32mTrain: [  7/50] Step 520/520 Loss 3.475 Prec@(1,5) (35.6%, 67.1%)\u001b[0m\n",
      "[2024-01-15 09:15:36] \u001b[32mTrain: [  7/50] Final Prec@1 35.5780%\u001b[0m\n",
      "[2024-01-15 09:15:42] \u001b[32mValid: [  7/50] Step 000/104 Loss 3.246 Prec@(1,5) (33.3%, 67.7%)\u001b[0m\n",
      "[2024-01-15 09:15:43] \u001b[32mValid: [  7/50] Step 020/104 Loss 3.343 Prec@(1,5) (33.7%, 64.5%)\u001b[0m\n",
      "[2024-01-15 09:15:43] \u001b[32mValid: [  7/50] Step 040/104 Loss 3.281 Prec@(1,5) (33.9%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:15:43] \u001b[32mValid: [  7/50] Step 060/104 Loss 3.237 Prec@(1,5) (34.5%, 65.4%)\u001b[0m\n",
      "[2024-01-15 09:15:44] \u001b[32mValid: [  7/50] Step 080/104 Loss 3.251 Prec@(1,5) (34.3%, 65.5%)\u001b[0m\n",
      "[2024-01-15 09:15:44] \u001b[32mValid: [  7/50] Step 100/104 Loss 3.267 Prec@(1,5) (34.1%, 65.3%)\u001b[0m\n",
      "[2024-01-15 09:15:44] \u001b[32mValid: [  7/50] Step 104/104 Loss 3.270 Prec@(1,5) (34.1%, 65.3%)\u001b[0m\n",
      "[2024-01-15 09:15:44] \u001b[32mValid: [  7/50] Final Prec@1 34.0700%\u001b[0m\n",
      "[2024-01-15 09:15:44] \u001b[32mEpoch 7 LR 0.023810\u001b[0m\n",
      "[2024-01-15 09:15:52] \u001b[32mTrain: [  8/50] Step 000/520 Loss 3.509 Prec@(1,5) (39.6%, 66.7%)\u001b[0m\n",
      "[2024-01-15 09:15:53] \u001b[32mTrain: [  8/50] Step 020/520 Loss 3.438 Prec@(1,5) (35.9%, 67.9%)\u001b[0m\n",
      "[2024-01-15 09:15:53] \u001b[32mTrain: [  8/50] Step 040/520 Loss 3.449 Prec@(1,5) (35.3%, 67.9%)\u001b[0m\n",
      "[2024-01-15 09:15:54] \u001b[32mTrain: [  8/50] Step 060/520 Loss 3.410 Prec@(1,5) (36.0%, 68.7%)\u001b[0m\n",
      "[2024-01-15 09:15:55] \u001b[32mTrain: [  8/50] Step 080/520 Loss 3.401 Prec@(1,5) (36.5%, 68.6%)\u001b[0m\n",
      "[2024-01-15 09:15:56] \u001b[32mTrain: [  8/50] Step 100/520 Loss 3.404 Prec@(1,5) (36.4%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:15:56] \u001b[32mTrain: [  8/50] Step 120/520 Loss 3.390 Prec@(1,5) (36.7%, 68.7%)\u001b[0m\n",
      "[2024-01-15 09:15:57] \u001b[32mTrain: [  8/50] Step 140/520 Loss 3.379 Prec@(1,5) (36.8%, 68.8%)\u001b[0m\n",
      "[2024-01-15 09:15:58] \u001b[32mTrain: [  8/50] Step 160/520 Loss 3.371 Prec@(1,5) (36.8%, 68.8%)\u001b[0m\n",
      "[2024-01-15 09:15:59] \u001b[32mTrain: [  8/50] Step 180/520 Loss 3.382 Prec@(1,5) (36.6%, 68.7%)\u001b[0m\n",
      "[2024-01-15 09:15:59] \u001b[32mTrain: [  8/50] Step 200/520 Loss 3.390 Prec@(1,5) (36.5%, 68.6%)\u001b[0m\n",
      "[2024-01-15 09:16:00] \u001b[32mTrain: [  8/50] Step 220/520 Loss 3.396 Prec@(1,5) (36.5%, 68.3%)\u001b[0m\n",
      "[2024-01-15 09:16:01] \u001b[32mTrain: [  8/50] Step 240/520 Loss 3.393 Prec@(1,5) (36.5%, 68.4%)\u001b[0m\n",
      "[2024-01-15 09:16:02] \u001b[32mTrain: [  8/50] Step 260/520 Loss 3.396 Prec@(1,5) (36.5%, 68.4%)\u001b[0m\n",
      "[2024-01-15 09:16:02] \u001b[32mTrain: [  8/50] Step 280/520 Loss 3.394 Prec@(1,5) (36.6%, 68.4%)\u001b[0m\n",
      "[2024-01-15 09:16:03] \u001b[32mTrain: [  8/50] Step 300/520 Loss 3.399 Prec@(1,5) (36.7%, 68.3%)\u001b[0m\n",
      "[2024-01-15 09:16:04] \u001b[32mTrain: [  8/50] Step 320/520 Loss 3.393 Prec@(1,5) (36.8%, 68.3%)\u001b[0m\n",
      "[2024-01-15 09:16:05] \u001b[32mTrain: [  8/50] Step 340/520 Loss 3.391 Prec@(1,5) (36.9%, 68.3%)\u001b[0m\n",
      "[2024-01-15 09:16:05] \u001b[32mTrain: [  8/50] Step 360/520 Loss 3.389 Prec@(1,5) (36.9%, 68.4%)\u001b[0m\n",
      "[2024-01-15 09:16:06] \u001b[32mTrain: [  8/50] Step 380/520 Loss 3.384 Prec@(1,5) (37.0%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:07] \u001b[32mTrain: [  8/50] Step 400/520 Loss 3.384 Prec@(1,5) (36.9%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:08] \u001b[32mTrain: [  8/50] Step 420/520 Loss 3.380 Prec@(1,5) (37.1%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:08] \u001b[32mTrain: [  8/50] Step 440/520 Loss 3.378 Prec@(1,5) (37.1%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:09] \u001b[32mTrain: [  8/50] Step 460/520 Loss 3.374 Prec@(1,5) (37.2%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:10] \u001b[32mTrain: [  8/50] Step 480/520 Loss 3.374 Prec@(1,5) (37.2%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:11] \u001b[32mTrain: [  8/50] Step 500/520 Loss 3.374 Prec@(1,5) (37.2%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:11] \u001b[32mTrain: [  8/50] Step 520/520 Loss 3.371 Prec@(1,5) (37.2%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:16:12] \u001b[32mTrain: [  8/50] Final Prec@1 37.2140%\u001b[0m\n",
      "[2024-01-15 09:16:16] \u001b[32mValid: [  8/50] Step 000/104 Loss 2.939 Prec@(1,5) (42.7%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:16:16] \u001b[32mValid: [  8/50] Step 020/104 Loss 2.912 Prec@(1,5) (38.0%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:16:17] \u001b[32mValid: [  8/50] Step 040/104 Loss 2.846 Prec@(1,5) (38.4%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:16:17] \u001b[32mValid: [  8/50] Step 060/104 Loss 2.831 Prec@(1,5) (38.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:16:17] \u001b[32mValid: [  8/50] Step 080/104 Loss 2.852 Prec@(1,5) (38.4%, 71.0%)\u001b[0m\n",
      "[2024-01-15 09:16:18] \u001b[32mValid: [  8/50] Step 100/104 Loss 2.857 Prec@(1,5) (38.4%, 71.1%)\u001b[0m\n",
      "[2024-01-15 09:16:18] \u001b[32mValid: [  8/50] Step 104/104 Loss 2.854 Prec@(1,5) (38.4%, 71.1%)\u001b[0m\n",
      "[2024-01-15 09:16:18] \u001b[32mValid: [  8/50] Final Prec@1 38.4100%\u001b[0m\n",
      "[2024-01-15 09:16:18] \u001b[32mEpoch 8 LR 0.023454\u001b[0m\n",
      "[2024-01-15 09:16:26] \u001b[32mTrain: [  9/50] Step 000/520 Loss 3.080 Prec@(1,5) (39.6%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:16:27] \u001b[32mTrain: [  9/50] Step 020/520 Loss 3.302 Prec@(1,5) (39.0%, 69.4%)\u001b[0m\n",
      "[2024-01-15 09:16:27] \u001b[32mTrain: [  9/50] Step 040/520 Loss 3.274 Prec@(1,5) (39.5%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:16:28] \u001b[32mTrain: [  9/50] Step 060/520 Loss 3.277 Prec@(1,5) (39.5%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:16:29] \u001b[32mTrain: [  9/50] Step 080/520 Loss 3.259 Prec@(1,5) (39.5%, 70.4%)\u001b[0m\n",
      "[2024-01-15 09:16:29] \u001b[32mTrain: [  9/50] Step 100/520 Loss 3.258 Prec@(1,5) (39.2%, 70.4%)\u001b[0m\n",
      "[2024-01-15 09:16:30] \u001b[32mTrain: [  9/50] Step 120/520 Loss 3.270 Prec@(1,5) (39.1%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:16:31] \u001b[32mTrain: [  9/50] Step 140/520 Loss 3.278 Prec@(1,5) (38.9%, 69.9%)\u001b[0m\n",
      "[2024-01-15 09:16:32] \u001b[32mTrain: [  9/50] Step 160/520 Loss 3.283 Prec@(1,5) (38.7%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:16:32] \u001b[32mTrain: [  9/50] Step 180/520 Loss 3.281 Prec@(1,5) (38.9%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:16:33] \u001b[32mTrain: [  9/50] Step 200/520 Loss 3.281 Prec@(1,5) (38.8%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:16:34] \u001b[32mTrain: [  9/50] Step 220/520 Loss 3.281 Prec@(1,5) (38.7%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:16:35] \u001b[32mTrain: [  9/50] Step 240/520 Loss 3.289 Prec@(1,5) (38.5%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:35] \u001b[32mTrain: [  9/50] Step 260/520 Loss 3.288 Prec@(1,5) (38.5%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:36] \u001b[32mTrain: [  9/50] Step 280/520 Loss 3.291 Prec@(1,5) (38.5%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:37] \u001b[32mTrain: [  9/50] Step 300/520 Loss 3.290 Prec@(1,5) (38.5%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:38] \u001b[32mTrain: [  9/50] Step 320/520 Loss 3.291 Prec@(1,5) (38.5%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:38] \u001b[32mTrain: [  9/50] Step 340/520 Loss 3.290 Prec@(1,5) (38.5%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:39] \u001b[32mTrain: [  9/50] Step 360/520 Loss 3.283 Prec@(1,5) (38.6%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:40] \u001b[32mTrain: [  9/50] Step 380/520 Loss 3.280 Prec@(1,5) (38.7%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:41] \u001b[32mTrain: [  9/50] Step 400/520 Loss 3.279 Prec@(1,5) (38.7%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:41] \u001b[32mTrain: [  9/50] Step 420/520 Loss 3.282 Prec@(1,5) (38.7%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:42] \u001b[32mTrain: [  9/50] Step 440/520 Loss 3.285 Prec@(1,5) (38.6%, 69.7%)\u001b[0m\n",
      "[2024-01-15 09:16:43] \u001b[32mTrain: [  9/50] Step 460/520 Loss 3.287 Prec@(1,5) (38.6%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:43] \u001b[32mTrain: [  9/50] Step 480/520 Loss 3.289 Prec@(1,5) (38.6%, 69.5%)\u001b[0m\n",
      "[2024-01-15 09:16:44] \u001b[32mTrain: [  9/50] Step 500/520 Loss 3.289 Prec@(1,5) (38.6%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:45] \u001b[32mTrain: [  9/50] Step 520/520 Loss 3.291 Prec@(1,5) (38.5%, 69.6%)\u001b[0m\n",
      "[2024-01-15 09:16:45] \u001b[32mTrain: [  9/50] Final Prec@1 38.5280%\u001b[0m\n",
      "[2024-01-15 09:16:49] \u001b[32mValid: [  9/50] Step 000/104 Loss 3.083 Prec@(1,5) (40.6%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:16:50] \u001b[32mValid: [  9/50] Step 020/104 Loss 3.186 Prec@(1,5) (37.2%, 70.3%)\u001b[0m\n",
      "[2024-01-15 09:16:50] \u001b[32mValid: [  9/50] Step 040/104 Loss 3.128 Prec@(1,5) (37.9%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:16:50] \u001b[32mValid: [  9/50] Step 060/104 Loss 3.115 Prec@(1,5) (38.2%, 70.2%)\u001b[0m\n",
      "[2024-01-15 09:16:51] \u001b[32mValid: [  9/50] Step 080/104 Loss 3.138 Prec@(1,5) (37.7%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:16:51] \u001b[32mValid: [  9/50] Step 100/104 Loss 3.148 Prec@(1,5) (37.5%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:16:51] \u001b[32mValid: [  9/50] Step 104/104 Loss 3.152 Prec@(1,5) (37.5%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:16:51] \u001b[32mValid: [  9/50] Final Prec@1 37.4700%\u001b[0m\n",
      "[2024-01-15 09:16:51] \u001b[32mEpoch 9 LR 0.023054\u001b[0m\n",
      "[2024-01-15 09:16:59] \u001b[32mTrain: [ 10/50] Step 000/520 Loss 3.529 Prec@(1,5) (36.5%, 66.7%)\u001b[0m\n",
      "[2024-01-15 09:16:59] \u001b[32mTrain: [ 10/50] Step 020/520 Loss 3.174 Prec@(1,5) (40.3%, 71.1%)\u001b[0m\n",
      "[2024-01-15 09:17:00] \u001b[32mTrain: [ 10/50] Step 040/520 Loss 3.176 Prec@(1,5) (39.0%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:17:00] \u001b[32mTrain: [ 10/50] Step 060/520 Loss 3.173 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:17:01] \u001b[32mTrain: [ 10/50] Step 080/520 Loss 3.175 Prec@(1,5) (39.8%, 71.1%)\u001b[0m\n",
      "[2024-01-15 09:17:02] \u001b[32mTrain: [ 10/50] Step 100/520 Loss 3.194 Prec@(1,5) (39.6%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:02] \u001b[32mTrain: [ 10/50] Step 120/520 Loss 3.212 Prec@(1,5) (39.3%, 70.7%)\u001b[0m\n",
      "[2024-01-15 09:17:03] \u001b[32mTrain: [ 10/50] Step 140/520 Loss 3.204 Prec@(1,5) (39.4%, 71.0%)\u001b[0m\n",
      "[2024-01-15 09:17:04] \u001b[32mTrain: [ 10/50] Step 160/520 Loss 3.226 Prec@(1,5) (39.2%, 70.6%)\u001b[0m\n",
      "[2024-01-15 09:17:05] \u001b[32mTrain: [ 10/50] Step 180/520 Loss 3.213 Prec@(1,5) (39.4%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:05] \u001b[32mTrain: [ 10/50] Step 200/520 Loss 3.216 Prec@(1,5) (39.3%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:06] \u001b[32mTrain: [ 10/50] Step 220/520 Loss 3.211 Prec@(1,5) (39.3%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:07] \u001b[32mTrain: [ 10/50] Step 240/520 Loss 3.213 Prec@(1,5) (39.4%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:07] \u001b[32mTrain: [ 10/50] Step 260/520 Loss 3.215 Prec@(1,5) (39.4%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:08] \u001b[32mTrain: [ 10/50] Step 280/520 Loss 3.217 Prec@(1,5) (39.3%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:09] \u001b[32mTrain: [ 10/50] Step 300/520 Loss 3.225 Prec@(1,5) (39.2%, 70.7%)\u001b[0m\n",
      "[2024-01-15 09:17:10] \u001b[32mTrain: [ 10/50] Step 320/520 Loss 3.225 Prec@(1,5) (39.2%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:10] \u001b[32mTrain: [ 10/50] Step 340/520 Loss 3.225 Prec@(1,5) (39.2%, 70.7%)\u001b[0m\n",
      "[2024-01-15 09:17:11] \u001b[32mTrain: [ 10/50] Step 360/520 Loss 3.218 Prec@(1,5) (39.3%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:12] \u001b[32mTrain: [ 10/50] Step 380/520 Loss 3.218 Prec@(1,5) (39.4%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:13] \u001b[32mTrain: [ 10/50] Step 400/520 Loss 3.216 Prec@(1,5) (39.4%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:13] \u001b[32mTrain: [ 10/50] Step 420/520 Loss 3.217 Prec@(1,5) (39.4%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:14] \u001b[32mTrain: [ 10/50] Step 440/520 Loss 3.217 Prec@(1,5) (39.5%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:17:15] \u001b[32mTrain: [ 10/50] Step 460/520 Loss 3.212 Prec@(1,5) (39.6%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:16] \u001b[32mTrain: [ 10/50] Step 480/520 Loss 3.212 Prec@(1,5) (39.7%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:16] \u001b[32mTrain: [ 10/50] Step 500/520 Loss 3.212 Prec@(1,5) (39.7%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:17] \u001b[32mTrain: [ 10/50] Step 520/520 Loss 3.211 Prec@(1,5) (39.8%, 70.9%)\u001b[0m\n",
      "[2024-01-15 09:17:17] \u001b[32mTrain: [ 10/50] Final Prec@1 39.7700%\u001b[0m\n",
      "[2024-01-15 09:17:21] \u001b[32mValid: [ 10/50] Step 000/104 Loss 2.798 Prec@(1,5) (41.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:17:21] \u001b[32mValid: [ 10/50] Step 020/104 Loss 2.860 Prec@(1,5) (38.6%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:17:22] \u001b[32mValid: [ 10/50] Step 040/104 Loss 2.807 Prec@(1,5) (39.3%, 72.4%)\u001b[0m\n",
      "[2024-01-15 09:17:22] \u001b[32mValid: [ 10/50] Step 060/104 Loss 2.786 Prec@(1,5) (39.4%, 72.6%)\u001b[0m\n",
      "[2024-01-15 09:17:22] \u001b[32mValid: [ 10/50] Step 080/104 Loss 2.788 Prec@(1,5) (39.4%, 72.5%)\u001b[0m\n",
      "[2024-01-15 09:17:23] \u001b[32mValid: [ 10/50] Step 100/104 Loss 2.776 Prec@(1,5) (39.3%, 72.3%)\u001b[0m\n",
      "[2024-01-15 09:17:23] \u001b[32mValid: [ 10/50] Step 104/104 Loss 2.780 Prec@(1,5) (39.3%, 72.4%)\u001b[0m\n",
      "[2024-01-15 09:17:23] \u001b[32mValid: [ 10/50] Final Prec@1 39.3400%\u001b[0m\n",
      "[2024-01-15 09:17:23] \u001b[32mEpoch 10 LR 0.022613\u001b[0m\n",
      "[2024-01-15 09:17:30] \u001b[32mTrain: [ 11/50] Step 000/520 Loss 3.259 Prec@(1,5) (36.5%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:17:31] \u001b[32mTrain: [ 11/50] Step 020/520 Loss 3.072 Prec@(1,5) (41.9%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:17:31] \u001b[32mTrain: [ 11/50] Step 040/520 Loss 3.139 Prec@(1,5) (41.2%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:17:32] \u001b[32mTrain: [ 11/50] Step 060/520 Loss 3.106 Prec@(1,5) (41.4%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:17:33] \u001b[32mTrain: [ 11/50] Step 080/520 Loss 3.087 Prec@(1,5) (41.7%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:17:33] \u001b[32mTrain: [ 11/50] Step 100/520 Loss 3.086 Prec@(1,5) (41.7%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:17:34] \u001b[32mTrain: [ 11/50] Step 120/520 Loss 3.077 Prec@(1,5) (42.0%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:17:35] \u001b[32mTrain: [ 11/50] Step 140/520 Loss 3.082 Prec@(1,5) (41.7%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:17:35] \u001b[32mTrain: [ 11/50] Step 160/520 Loss 3.079 Prec@(1,5) (41.9%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:17:36] \u001b[32mTrain: [ 11/50] Step 180/520 Loss 3.088 Prec@(1,5) (41.9%, 72.6%)\u001b[0m\n",
      "[2024-01-15 09:17:37] \u001b[32mTrain: [ 11/50] Step 200/520 Loss 3.098 Prec@(1,5) (41.7%, 72.5%)\u001b[0m\n",
      "[2024-01-15 09:17:37] \u001b[32mTrain: [ 11/50] Step 220/520 Loss 3.101 Prec@(1,5) (41.7%, 72.5%)\u001b[0m\n",
      "[2024-01-15 09:17:38] \u001b[32mTrain: [ 11/50] Step 240/520 Loss 3.101 Prec@(1,5) (41.7%, 72.4%)\u001b[0m\n",
      "[2024-01-15 09:17:39] \u001b[32mTrain: [ 11/50] Step 260/520 Loss 3.112 Prec@(1,5) (41.5%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:17:40] \u001b[32mTrain: [ 11/50] Step 280/520 Loss 3.115 Prec@(1,5) (41.4%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:17:40] \u001b[32mTrain: [ 11/50] Step 300/520 Loss 3.118 Prec@(1,5) (41.4%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:17:41] \u001b[32mTrain: [ 11/50] Step 320/520 Loss 3.125 Prec@(1,5) (41.3%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:17:42] \u001b[32mTrain: [ 11/50] Step 340/520 Loss 3.125 Prec@(1,5) (41.3%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:17:42] \u001b[32mTrain: [ 11/50] Step 360/520 Loss 3.130 Prec@(1,5) (41.2%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:17:43] \u001b[32mTrain: [ 11/50] Step 380/520 Loss 3.130 Prec@(1,5) (41.1%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:17:44] \u001b[32mTrain: [ 11/50] Step 400/520 Loss 3.133 Prec@(1,5) (41.1%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:17:45] \u001b[32mTrain: [ 11/50] Step 420/520 Loss 3.135 Prec@(1,5) (41.1%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:17:45] \u001b[32mTrain: [ 11/50] Step 440/520 Loss 3.138 Prec@(1,5) (41.0%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:17:46] \u001b[32mTrain: [ 11/50] Step 460/520 Loss 3.137 Prec@(1,5) (41.0%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:17:47] \u001b[32mTrain: [ 11/50] Step 480/520 Loss 3.137 Prec@(1,5) (41.0%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:17:48] \u001b[32mTrain: [ 11/50] Step 500/520 Loss 3.138 Prec@(1,5) (40.9%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:17:48] \u001b[32mTrain: [ 11/50] Step 520/520 Loss 3.138 Prec@(1,5) (40.9%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:17:49] \u001b[32mTrain: [ 11/50] Final Prec@1 40.9020%\u001b[0m\n",
      "[2024-01-15 09:17:53] \u001b[32mValid: [ 11/50] Step 000/104 Loss 3.286 Prec@(1,5) (40.6%, 66.7%)\u001b[0m\n",
      "[2024-01-15 09:17:53] \u001b[32mValid: [ 11/50] Step 020/104 Loss 3.075 Prec@(1,5) (38.4%, 70.6%)\u001b[0m\n",
      "[2024-01-15 09:17:54] \u001b[32mValid: [ 11/50] Step 040/104 Loss 3.016 Prec@(1,5) (38.4%, 70.7%)\u001b[0m\n",
      "[2024-01-15 09:17:54] \u001b[32mValid: [ 11/50] Step 060/104 Loss 2.994 Prec@(1,5) (38.7%, 70.5%)\u001b[0m\n",
      "[2024-01-15 09:17:54] \u001b[32mValid: [ 11/50] Step 080/104 Loss 3.022 Prec@(1,5) (38.3%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:17:55] \u001b[32mValid: [ 11/50] Step 100/104 Loss 3.022 Prec@(1,5) (38.3%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:17:55] \u001b[32mValid: [ 11/50] Step 104/104 Loss 3.018 Prec@(1,5) (38.2%, 70.1%)\u001b[0m\n",
      "[2024-01-15 09:17:55] \u001b[32mValid: [ 11/50] Final Prec@1 38.2000%\u001b[0m\n",
      "[2024-01-15 09:17:55] \u001b[32mEpoch 11 LR 0.022132\u001b[0m\n",
      "[2024-01-15 09:18:03] \u001b[32mTrain: [ 12/50] Step 000/520 Loss 3.099 Prec@(1,5) (35.4%, 68.8%)\u001b[0m\n",
      "[2024-01-15 09:18:03] \u001b[32mTrain: [ 12/50] Step 020/520 Loss 3.073 Prec@(1,5) (42.1%, 72.4%)\u001b[0m\n",
      "[2024-01-15 09:18:04] \u001b[32mTrain: [ 12/50] Step 040/520 Loss 3.087 Prec@(1,5) (41.7%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:05] \u001b[32mTrain: [ 12/50] Step 060/520 Loss 3.081 Prec@(1,5) (42.0%, 72.3%)\u001b[0m\n",
      "[2024-01-15 09:18:05] \u001b[32mTrain: [ 12/50] Step 080/520 Loss 3.095 Prec@(1,5) (42.1%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:18:06] \u001b[32mTrain: [ 12/50] Step 100/520 Loss 3.116 Prec@(1,5) (41.8%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:18:07] \u001b[32mTrain: [ 12/50] Step 120/520 Loss 3.116 Prec@(1,5) (41.9%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:18:08] \u001b[32mTrain: [ 12/50] Step 140/520 Loss 3.108 Prec@(1,5) (41.8%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:18:08] \u001b[32mTrain: [ 12/50] Step 160/520 Loss 3.123 Prec@(1,5) (41.6%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:18:09] \u001b[32mTrain: [ 12/50] Step 180/520 Loss 3.120 Prec@(1,5) (41.5%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:18:10] \u001b[32mTrain: [ 12/50] Step 200/520 Loss 3.116 Prec@(1,5) (41.5%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:18:11] \u001b[32mTrain: [ 12/50] Step 220/520 Loss 3.114 Prec@(1,5) (41.4%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:18:12] \u001b[32mTrain: [ 12/50] Step 240/520 Loss 3.120 Prec@(1,5) (41.2%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:18:12] \u001b[32mTrain: [ 12/50] Step 260/520 Loss 3.119 Prec@(1,5) (41.3%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:18:13] \u001b[32mTrain: [ 12/50] Step 280/520 Loss 3.122 Prec@(1,5) (41.3%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:18:14] \u001b[32mTrain: [ 12/50] Step 300/520 Loss 3.118 Prec@(1,5) (41.3%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:18:15] \u001b[32mTrain: [ 12/50] Step 320/520 Loss 3.116 Prec@(1,5) (41.3%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:18:15] \u001b[32mTrain: [ 12/50] Step 340/520 Loss 3.111 Prec@(1,5) (41.4%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:18:16] \u001b[32mTrain: [ 12/50] Step 360/520 Loss 3.105 Prec@(1,5) (41.5%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:18:17] \u001b[32mTrain: [ 12/50] Step 380/520 Loss 3.103 Prec@(1,5) (41.5%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:18:18] \u001b[32mTrain: [ 12/50] Step 400/520 Loss 3.102 Prec@(1,5) (41.4%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:19] \u001b[32mTrain: [ 12/50] Step 420/520 Loss 3.100 Prec@(1,5) (41.5%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:19] \u001b[32mTrain: [ 12/50] Step 440/520 Loss 3.102 Prec@(1,5) (41.4%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:20] \u001b[32mTrain: [ 12/50] Step 460/520 Loss 3.101 Prec@(1,5) (41.4%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:21] \u001b[32mTrain: [ 12/50] Step 480/520 Loss 3.100 Prec@(1,5) (41.5%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:22] \u001b[32mTrain: [ 12/50] Step 500/520 Loss 3.101 Prec@(1,5) (41.4%, 72.2%)\u001b[0m\n",
      "[2024-01-15 09:18:22] \u001b[32mTrain: [ 12/50] Step 520/520 Loss 3.097 Prec@(1,5) (41.5%, 72.3%)\u001b[0m\n",
      "[2024-01-15 09:18:23] \u001b[32mTrain: [ 12/50] Final Prec@1 41.5300%\u001b[0m\n",
      "[2024-01-15 09:18:27] \u001b[32mValid: [ 12/50] Step 000/104 Loss 3.037 Prec@(1,5) (39.6%, 68.8%)\u001b[0m\n",
      "[2024-01-15 09:18:28] \u001b[32mValid: [ 12/50] Step 020/104 Loss 2.899 Prec@(1,5) (39.4%, 70.7%)\u001b[0m\n",
      "[2024-01-15 09:18:28] \u001b[32mValid: [ 12/50] Step 040/104 Loss 2.838 Prec@(1,5) (40.1%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:18:28] \u001b[32mValid: [ 12/50] Step 060/104 Loss 2.813 Prec@(1,5) (40.1%, 71.6%)\u001b[0m\n",
      "[2024-01-15 09:18:29] \u001b[32mValid: [ 12/50] Step 080/104 Loss 2.843 Prec@(1,5) (39.9%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:18:29] \u001b[32mValid: [ 12/50] Step 100/104 Loss 2.847 Prec@(1,5) (39.8%, 71.0%)\u001b[0m\n",
      "[2024-01-15 09:18:29] \u001b[32mValid: [ 12/50] Step 104/104 Loss 2.847 Prec@(1,5) (39.9%, 71.0%)\u001b[0m\n",
      "[2024-01-15 09:18:29] \u001b[32mValid: [ 12/50] Final Prec@1 39.8800%\u001b[0m\n",
      "[2024-01-15 09:18:29] \u001b[32mEpoch 12 LR 0.021612\u001b[0m\n",
      "[2024-01-15 09:18:37] \u001b[32mTrain: [ 13/50] Step 000/520 Loss 3.064 Prec@(1,5) (42.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:18:38] \u001b[32mTrain: [ 13/50] Step 020/520 Loss 2.991 Prec@(1,5) (44.2%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:18:39] \u001b[32mTrain: [ 13/50] Step 040/520 Loss 3.034 Prec@(1,5) (42.7%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:18:39] \u001b[32mTrain: [ 13/50] Step 060/520 Loss 3.083 Prec@(1,5) (42.2%, 72.3%)\u001b[0m\n",
      "[2024-01-15 09:18:40] \u001b[32mTrain: [ 13/50] Step 080/520 Loss 3.071 Prec@(1,5) (42.4%, 72.5%)\u001b[0m\n",
      "[2024-01-15 09:18:41] \u001b[32mTrain: [ 13/50] Step 100/520 Loss 3.061 Prec@(1,5) (42.5%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:42] \u001b[32mTrain: [ 13/50] Step 120/520 Loss 3.032 Prec@(1,5) (42.9%, 73.1%)\u001b[0m\n",
      "[2024-01-15 09:18:42] \u001b[32mTrain: [ 13/50] Step 140/520 Loss 3.027 Prec@(1,5) (42.9%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:18:43] \u001b[32mTrain: [ 13/50] Step 160/520 Loss 3.038 Prec@(1,5) (42.7%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:18:44] \u001b[32mTrain: [ 13/50] Step 180/520 Loss 3.044 Prec@(1,5) (42.7%, 73.0%)\u001b[0m\n",
      "[2024-01-15 09:18:45] \u001b[32mTrain: [ 13/50] Step 200/520 Loss 3.039 Prec@(1,5) (42.7%, 73.0%)\u001b[0m\n",
      "[2024-01-15 09:18:45] \u001b[32mTrain: [ 13/50] Step 220/520 Loss 3.047 Prec@(1,5) (42.7%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:46] \u001b[32mTrain: [ 13/50] Step 240/520 Loss 3.049 Prec@(1,5) (42.5%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:47] \u001b[32mTrain: [ 13/50] Step 260/520 Loss 3.056 Prec@(1,5) (42.3%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:48] \u001b[32mTrain: [ 13/50] Step 280/520 Loss 3.051 Prec@(1,5) (42.4%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:49] \u001b[32mTrain: [ 13/50] Step 300/520 Loss 3.048 Prec@(1,5) (42.4%, 73.0%)\u001b[0m\n",
      "[2024-01-15 09:18:49] \u001b[32mTrain: [ 13/50] Step 320/520 Loss 3.053 Prec@(1,5) (42.3%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:50] \u001b[32mTrain: [ 13/50] Step 340/520 Loss 3.055 Prec@(1,5) (42.2%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:51] \u001b[32mTrain: [ 13/50] Step 360/520 Loss 3.061 Prec@(1,5) (42.1%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:52] \u001b[32mTrain: [ 13/50] Step 380/520 Loss 3.060 Prec@(1,5) (42.1%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:52] \u001b[32mTrain: [ 13/50] Step 400/520 Loss 3.060 Prec@(1,5) (42.1%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:53] \u001b[32mTrain: [ 13/50] Step 420/520 Loss 3.058 Prec@(1,5) (42.2%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:54] \u001b[32mTrain: [ 13/50] Step 440/520 Loss 3.056 Prec@(1,5) (42.2%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:55] \u001b[32mTrain: [ 13/50] Step 460/520 Loss 3.054 Prec@(1,5) (42.2%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:18:55] \u001b[32mTrain: [ 13/50] Step 480/520 Loss 3.054 Prec@(1,5) (42.2%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:56] \u001b[32mTrain: [ 13/50] Step 500/520 Loss 3.052 Prec@(1,5) (42.2%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:57] \u001b[32mTrain: [ 13/50] Step 520/520 Loss 3.050 Prec@(1,5) (42.3%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:18:57] \u001b[32mTrain: [ 13/50] Final Prec@1 42.2540%\u001b[0m\n",
      "[2024-01-15 09:19:02] \u001b[32mValid: [ 13/50] Step 000/104 Loss 2.680 Prec@(1,5) (45.8%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:19:02] \u001b[32mValid: [ 13/50] Step 020/104 Loss 2.648 Prec@(1,5) (42.9%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:19:02] \u001b[32mValid: [ 13/50] Step 040/104 Loss 2.586 Prec@(1,5) (42.7%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:19:03] \u001b[32mValid: [ 13/50] Step 060/104 Loss 2.562 Prec@(1,5) (42.7%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:19:03] \u001b[32mValid: [ 13/50] Step 080/104 Loss 2.573 Prec@(1,5) (42.6%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:19:03] \u001b[32mValid: [ 13/50] Step 100/104 Loss 2.572 Prec@(1,5) (42.7%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:19:04] \u001b[32mValid: [ 13/50] Step 104/104 Loss 2.575 Prec@(1,5) (42.6%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:19:04] \u001b[32mValid: [ 13/50] Final Prec@1 42.6400%\u001b[0m\n",
      "[2024-01-15 09:19:04] \u001b[32mEpoch 13 LR 0.021057\u001b[0m\n",
      "[2024-01-15 09:19:12] \u001b[32mTrain: [ 14/50] Step 000/520 Loss 2.984 Prec@(1,5) (41.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:19:12] \u001b[32mTrain: [ 14/50] Step 020/520 Loss 3.031 Prec@(1,5) (42.3%, 72.5%)\u001b[0m\n",
      "[2024-01-15 09:19:13] \u001b[32mTrain: [ 14/50] Step 040/520 Loss 3.011 Prec@(1,5) (43.9%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:19:14] \u001b[32mTrain: [ 14/50] Step 060/520 Loss 3.044 Prec@(1,5) (43.1%, 72.7%)\u001b[0m\n",
      "[2024-01-15 09:19:14] \u001b[32mTrain: [ 14/50] Step 080/520 Loss 3.033 Prec@(1,5) (43.0%, 73.1%)\u001b[0m\n",
      "[2024-01-15 09:19:15] \u001b[32mTrain: [ 14/50] Step 100/520 Loss 3.019 Prec@(1,5) (42.8%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:16] \u001b[32mTrain: [ 14/50] Step 120/520 Loss 3.001 Prec@(1,5) (43.0%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:17] \u001b[32mTrain: [ 14/50] Step 140/520 Loss 3.003 Prec@(1,5) (42.8%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:17] \u001b[32mTrain: [ 14/50] Step 160/520 Loss 2.992 Prec@(1,5) (43.1%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:19:18] \u001b[32mTrain: [ 14/50] Step 180/520 Loss 2.988 Prec@(1,5) (43.1%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:19] \u001b[32mTrain: [ 14/50] Step 200/520 Loss 2.992 Prec@(1,5) (43.0%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:20] \u001b[32mTrain: [ 14/50] Step 220/520 Loss 2.984 Prec@(1,5) (43.3%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:20] \u001b[32mTrain: [ 14/50] Step 240/520 Loss 2.990 Prec@(1,5) (43.2%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:19:21] \u001b[32mTrain: [ 14/50] Step 260/520 Loss 2.991 Prec@(1,5) (43.2%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:19:22] \u001b[32mTrain: [ 14/50] Step 280/520 Loss 3.001 Prec@(1,5) (43.0%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:19:23] \u001b[32mTrain: [ 14/50] Step 300/520 Loss 3.005 Prec@(1,5) (43.0%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:23] \u001b[32mTrain: [ 14/50] Step 320/520 Loss 3.010 Prec@(1,5) (43.0%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:19:24] \u001b[32mTrain: [ 14/50] Step 340/520 Loss 3.013 Prec@(1,5) (42.9%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:25] \u001b[32mTrain: [ 14/50] Step 360/520 Loss 3.011 Prec@(1,5) (42.9%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:19:26] \u001b[32mTrain: [ 14/50] Step 380/520 Loss 3.015 Prec@(1,5) (42.9%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:26] \u001b[32mTrain: [ 14/50] Step 400/520 Loss 3.015 Prec@(1,5) (42.9%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:27] \u001b[32mTrain: [ 14/50] Step 420/520 Loss 3.013 Prec@(1,5) (42.9%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:28] \u001b[32mTrain: [ 14/50] Step 440/520 Loss 3.009 Prec@(1,5) (43.0%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:29] \u001b[32mTrain: [ 14/50] Step 460/520 Loss 3.010 Prec@(1,5) (43.0%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:30] \u001b[32mTrain: [ 14/50] Step 480/520 Loss 3.011 Prec@(1,5) (43.0%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:19:30] \u001b[32mTrain: [ 14/50] Step 500/520 Loss 3.012 Prec@(1,5) (42.9%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:19:31] \u001b[32mTrain: [ 14/50] Step 520/520 Loss 3.013 Prec@(1,5) (43.0%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:19:31] \u001b[32mTrain: [ 14/50] Final Prec@1 42.9800%\u001b[0m\n",
      "[2024-01-15 09:19:36] \u001b[32mValid: [ 14/50] Step 000/104 Loss 2.943 Prec@(1,5) (39.6%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:19:36] \u001b[32mValid: [ 14/50] Step 020/104 Loss 2.737 Prec@(1,5) (41.7%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:37] \u001b[32mValid: [ 14/50] Step 040/104 Loss 2.667 Prec@(1,5) (42.1%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:19:37] \u001b[32mValid: [ 14/50] Step 060/104 Loss 2.640 Prec@(1,5) (42.5%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:19:37] \u001b[32mValid: [ 14/50] Step 080/104 Loss 2.669 Prec@(1,5) (42.1%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:19:38] \u001b[32mValid: [ 14/50] Step 100/104 Loss 2.671 Prec@(1,5) (42.2%, 73.9%)\u001b[0m\n",
      "[2024-01-15 09:19:38] \u001b[32mValid: [ 14/50] Step 104/104 Loss 2.671 Prec@(1,5) (42.2%, 73.8%)\u001b[0m\n",
      "[2024-01-15 09:19:38] \u001b[32mValid: [ 14/50] Final Prec@1 42.1900%\u001b[0m\n",
      "[2024-01-15 09:19:38] \u001b[32mEpoch 14 LR 0.020468\u001b[0m\n",
      "[2024-01-15 09:19:46] \u001b[32mTrain: [ 15/50] Step 000/520 Loss 3.035 Prec@(1,5) (45.8%, 64.6%)\u001b[0m\n",
      "[2024-01-15 09:19:47] \u001b[32mTrain: [ 15/50] Step 020/520 Loss 3.018 Prec@(1,5) (44.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:19:47] \u001b[32mTrain: [ 15/50] Step 040/520 Loss 3.016 Prec@(1,5) (43.9%, 72.8%)\u001b[0m\n",
      "[2024-01-15 09:19:48] \u001b[32mTrain: [ 15/50] Step 060/520 Loss 3.032 Prec@(1,5) (43.7%, 72.4%)\u001b[0m\n",
      "[2024-01-15 09:19:49] \u001b[32mTrain: [ 15/50] Step 080/520 Loss 3.010 Prec@(1,5) (43.7%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:19:50] \u001b[32mTrain: [ 15/50] Step 100/520 Loss 2.983 Prec@(1,5) (43.8%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:19:50] \u001b[32mTrain: [ 15/50] Step 120/520 Loss 2.972 Prec@(1,5) (44.0%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:19:51] \u001b[32mTrain: [ 15/50] Step 140/520 Loss 2.960 Prec@(1,5) (44.2%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:52] \u001b[32mTrain: [ 15/50] Step 160/520 Loss 2.965 Prec@(1,5) (44.1%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:53] \u001b[32mTrain: [ 15/50] Step 180/520 Loss 2.965 Prec@(1,5) (44.1%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:19:54] \u001b[32mTrain: [ 15/50] Step 200/520 Loss 2.971 Prec@(1,5) (43.9%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:19:54] \u001b[32mTrain: [ 15/50] Step 220/520 Loss 2.972 Prec@(1,5) (43.8%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:19:55] \u001b[32mTrain: [ 15/50] Step 240/520 Loss 2.982 Prec@(1,5) (43.7%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:19:56] \u001b[32mTrain: [ 15/50] Step 260/520 Loss 2.974 Prec@(1,5) (43.8%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:57] \u001b[32mTrain: [ 15/50] Step 280/520 Loss 2.972 Prec@(1,5) (43.9%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:57] \u001b[32mTrain: [ 15/50] Step 300/520 Loss 2.968 Prec@(1,5) (43.9%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:58] \u001b[32mTrain: [ 15/50] Step 320/520 Loss 2.980 Prec@(1,5) (43.7%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:19:59] \u001b[32mTrain: [ 15/50] Step 340/520 Loss 2.980 Prec@(1,5) (43.7%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:00] \u001b[32mTrain: [ 15/50] Step 360/520 Loss 2.980 Prec@(1,5) (43.6%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:01] \u001b[32mTrain: [ 15/50] Step 380/520 Loss 2.986 Prec@(1,5) (43.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:01] \u001b[32mTrain: [ 15/50] Step 400/520 Loss 2.984 Prec@(1,5) (43.6%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:02] \u001b[32mTrain: [ 15/50] Step 420/520 Loss 2.982 Prec@(1,5) (43.6%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:20:03] \u001b[32mTrain: [ 15/50] Step 440/520 Loss 2.986 Prec@(1,5) (43.6%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:04] \u001b[32mTrain: [ 15/50] Step 460/520 Loss 2.985 Prec@(1,5) (43.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:05] \u001b[32mTrain: [ 15/50] Step 480/520 Loss 2.990 Prec@(1,5) (43.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:20:05] \u001b[32mTrain: [ 15/50] Step 500/520 Loss 2.988 Prec@(1,5) (43.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:20:06] \u001b[32mTrain: [ 15/50] Step 520/520 Loss 2.988 Prec@(1,5) (43.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:20:06] \u001b[32mTrain: [ 15/50] Final Prec@1 43.5420%\u001b[0m\n",
      "[2024-01-15 09:20:11] \u001b[32mValid: [ 15/50] Step 000/104 Loss 2.778 Prec@(1,5) (46.9%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:20:11] \u001b[32mValid: [ 15/50] Step 020/104 Loss 2.626 Prec@(1,5) (44.8%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:20:12] \u001b[32mValid: [ 15/50] Step 040/104 Loss 2.578 Prec@(1,5) (44.3%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:20:12] \u001b[32mValid: [ 15/50] Step 060/104 Loss 2.553 Prec@(1,5) (44.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:20:12] \u001b[32mValid: [ 15/50] Step 080/104 Loss 2.573 Prec@(1,5) (43.7%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:20:13] \u001b[32mValid: [ 15/50] Step 100/104 Loss 2.564 Prec@(1,5) (44.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:20:13] \u001b[32mValid: [ 15/50] Step 104/104 Loss 2.567 Prec@(1,5) (43.9%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:20:13] \u001b[32mValid: [ 15/50] Final Prec@1 43.9000%\u001b[0m\n",
      "[2024-01-15 09:20:13] \u001b[32mEpoch 15 LR 0.019848\u001b[0m\n",
      "[2024-01-15 09:20:21] \u001b[32mTrain: [ 16/50] Step 000/520 Loss 3.126 Prec@(1,5) (43.8%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:20:22] \u001b[32mTrain: [ 16/50] Step 020/520 Loss 2.916 Prec@(1,5) (45.3%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:20:23] \u001b[32mTrain: [ 16/50] Step 040/520 Loss 2.911 Prec@(1,5) (44.8%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:20:24] \u001b[32mTrain: [ 16/50] Step 060/520 Loss 2.900 Prec@(1,5) (45.0%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:20:24] \u001b[32mTrain: [ 16/50] Step 080/520 Loss 2.918 Prec@(1,5) (44.6%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:20:25] \u001b[32mTrain: [ 16/50] Step 100/520 Loss 2.918 Prec@(1,5) (44.8%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:20:26] \u001b[32mTrain: [ 16/50] Step 120/520 Loss 2.914 Prec@(1,5) (44.8%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:20:27] \u001b[32mTrain: [ 16/50] Step 140/520 Loss 2.910 Prec@(1,5) (44.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 09:20:27] \u001b[32mTrain: [ 16/50] Step 160/520 Loss 2.918 Prec@(1,5) (44.9%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:20:28] \u001b[32mTrain: [ 16/50] Step 180/520 Loss 2.924 Prec@(1,5) (44.7%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:20:29] \u001b[32mTrain: [ 16/50] Step 200/520 Loss 2.918 Prec@(1,5) (44.8%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:20:30] \u001b[32mTrain: [ 16/50] Step 220/520 Loss 2.921 Prec@(1,5) (44.9%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:20:30] \u001b[32mTrain: [ 16/50] Step 240/520 Loss 2.918 Prec@(1,5) (44.9%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:20:31] \u001b[32mTrain: [ 16/50] Step 260/520 Loss 2.926 Prec@(1,5) (44.8%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:20:32] \u001b[32mTrain: [ 16/50] Step 280/520 Loss 2.930 Prec@(1,5) (44.7%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:20:33] \u001b[32mTrain: [ 16/50] Step 300/520 Loss 2.929 Prec@(1,5) (44.7%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:34] \u001b[32mTrain: [ 16/50] Step 320/520 Loss 2.930 Prec@(1,5) (44.7%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:34] \u001b[32mTrain: [ 16/50] Step 340/520 Loss 2.934 Prec@(1,5) (44.6%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:35] \u001b[32mTrain: [ 16/50] Step 360/520 Loss 2.928 Prec@(1,5) (44.6%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:20:36] \u001b[32mTrain: [ 16/50] Step 380/520 Loss 2.932 Prec@(1,5) (44.5%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:37] \u001b[32mTrain: [ 16/50] Step 400/520 Loss 2.934 Prec@(1,5) (44.5%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:38] \u001b[32mTrain: [ 16/50] Step 420/520 Loss 2.933 Prec@(1,5) (44.5%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:38] \u001b[32mTrain: [ 16/50] Step 440/520 Loss 2.935 Prec@(1,5) (44.5%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:39] \u001b[32mTrain: [ 16/50] Step 460/520 Loss 2.935 Prec@(1,5) (44.5%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:40] \u001b[32mTrain: [ 16/50] Step 480/520 Loss 2.934 Prec@(1,5) (44.6%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:20:41] \u001b[32mTrain: [ 16/50] Step 500/520 Loss 2.937 Prec@(1,5) (44.5%, 74.2%)\u001b[0m\n",
      "[2024-01-15 09:20:41] \u001b[32mTrain: [ 16/50] Step 520/520 Loss 2.941 Prec@(1,5) (44.4%, 74.2%)\u001b[0m\n",
      "[2024-01-15 09:20:42] \u001b[32mTrain: [ 16/50] Final Prec@1 44.3620%\u001b[0m\n",
      "[2024-01-15 09:20:46] \u001b[32mValid: [ 16/50] Step 000/104 Loss 2.765 Prec@(1,5) (52.1%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:20:47] \u001b[32mValid: [ 16/50] Step 020/104 Loss 2.661 Prec@(1,5) (44.9%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:20:47] \u001b[32mValid: [ 16/50] Step 040/104 Loss 2.619 Prec@(1,5) (43.9%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:20:47] \u001b[32mValid: [ 16/50] Step 060/104 Loss 2.599 Prec@(1,5) (44.2%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:20:48] \u001b[32mValid: [ 16/50] Step 080/104 Loss 2.611 Prec@(1,5) (43.8%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:20:48] \u001b[32mValid: [ 16/50] Step 100/104 Loss 2.603 Prec@(1,5) (43.8%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:20:48] \u001b[32mValid: [ 16/50] Step 104/104 Loss 2.609 Prec@(1,5) (43.8%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:20:48] \u001b[32mValid: [ 16/50] Final Prec@1 43.7700%\u001b[0m\n",
      "[2024-01-15 09:20:48] \u001b[32mEpoch 16 LR 0.019198\u001b[0m\n",
      "[2024-01-15 09:20:57] \u001b[32mTrain: [ 17/50] Step 000/520 Loss 2.393 Prec@(1,5) (55.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:20:57] \u001b[32mTrain: [ 17/50] Step 020/520 Loss 2.877 Prec@(1,5) (44.9%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:20:58] \u001b[32mTrain: [ 17/50] Step 040/520 Loss 2.911 Prec@(1,5) (44.4%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:20:59] \u001b[32mTrain: [ 17/50] Step 060/520 Loss 2.885 Prec@(1,5) (44.7%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:20:59] \u001b[32mTrain: [ 17/50] Step 080/520 Loss 2.917 Prec@(1,5) (44.3%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:21:00] \u001b[32mTrain: [ 17/50] Step 100/520 Loss 2.922 Prec@(1,5) (44.3%, 73.9%)\u001b[0m\n",
      "[2024-01-15 09:21:01] \u001b[32mTrain: [ 17/50] Step 120/520 Loss 2.905 Prec@(1,5) (44.4%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:21:02] \u001b[32mTrain: [ 17/50] Step 140/520 Loss 2.914 Prec@(1,5) (44.3%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:02] \u001b[32mTrain: [ 17/50] Step 160/520 Loss 2.905 Prec@(1,5) (44.6%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:03] \u001b[32mTrain: [ 17/50] Step 180/520 Loss 2.904 Prec@(1,5) (44.7%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:04] \u001b[32mTrain: [ 17/50] Step 200/520 Loss 2.901 Prec@(1,5) (44.7%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:05] \u001b[32mTrain: [ 17/50] Step 220/520 Loss 2.899 Prec@(1,5) (44.8%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:06] \u001b[32mTrain: [ 17/50] Step 240/520 Loss 2.904 Prec@(1,5) (44.8%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:06] \u001b[32mTrain: [ 17/50] Step 260/520 Loss 2.910 Prec@(1,5) (44.7%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:07] \u001b[32mTrain: [ 17/50] Step 280/520 Loss 2.913 Prec@(1,5) (44.6%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:21:08] \u001b[32mTrain: [ 17/50] Step 300/520 Loss 2.920 Prec@(1,5) (44.4%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:09] \u001b[32mTrain: [ 17/50] Step 320/520 Loss 2.919 Prec@(1,5) (44.4%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:09] \u001b[32mTrain: [ 17/50] Step 340/520 Loss 2.915 Prec@(1,5) (44.4%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:21:10] \u001b[32mTrain: [ 17/50] Step 360/520 Loss 2.915 Prec@(1,5) (44.4%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:21:11] \u001b[32mTrain: [ 17/50] Step 380/520 Loss 2.923 Prec@(1,5) (44.3%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:12] \u001b[32mTrain: [ 17/50] Step 400/520 Loss 2.925 Prec@(1,5) (44.3%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:12] \u001b[32mTrain: [ 17/50] Step 420/520 Loss 2.925 Prec@(1,5) (44.3%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:13] \u001b[32mTrain: [ 17/50] Step 440/520 Loss 2.924 Prec@(1,5) (44.4%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:21:14] \u001b[32mTrain: [ 17/50] Step 460/520 Loss 2.926 Prec@(1,5) (44.3%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:15] \u001b[32mTrain: [ 17/50] Step 480/520 Loss 2.926 Prec@(1,5) (44.4%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:16] \u001b[32mTrain: [ 17/50] Step 500/520 Loss 2.927 Prec@(1,5) (44.4%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:21:16] \u001b[32mTrain: [ 17/50] Step 520/520 Loss 2.930 Prec@(1,5) (44.4%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:21:17] \u001b[32mTrain: [ 17/50] Final Prec@1 44.3820%\u001b[0m\n",
      "[2024-01-15 09:21:21] \u001b[32mValid: [ 17/50] Step 000/104 Loss 2.782 Prec@(1,5) (45.8%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:21:22] \u001b[32mValid: [ 17/50] Step 020/104 Loss 2.534 Prec@(1,5) (45.4%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:21:22] \u001b[32mValid: [ 17/50] Step 040/104 Loss 2.486 Prec@(1,5) (44.8%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:21:22] \u001b[32mValid: [ 17/50] Step 060/104 Loss 2.466 Prec@(1,5) (45.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:21:22] \u001b[32mValid: [ 17/50] Step 080/104 Loss 2.491 Prec@(1,5) (44.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:21:23] \u001b[32mValid: [ 17/50] Step 100/104 Loss 2.487 Prec@(1,5) (44.8%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:21:23] \u001b[32mValid: [ 17/50] Step 104/104 Loss 2.486 Prec@(1,5) (44.7%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:21:23] \u001b[32mValid: [ 17/50] Final Prec@1 44.7200%\u001b[0m\n",
      "[2024-01-15 09:21:23] \u001b[32mEpoch 17 LR 0.018522\u001b[0m\n",
      "[2024-01-15 09:21:31] \u001b[32mTrain: [ 18/50] Step 000/520 Loss 2.750 Prec@(1,5) (56.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:21:32] \u001b[32mTrain: [ 18/50] Step 020/520 Loss 2.833 Prec@(1,5) (47.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:21:33] \u001b[32mTrain: [ 18/50] Step 040/520 Loss 2.902 Prec@(1,5) (44.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 09:21:33] \u001b[32mTrain: [ 18/50] Step 060/520 Loss 2.896 Prec@(1,5) (45.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:21:34] \u001b[32mTrain: [ 18/50] Step 080/520 Loss 2.882 Prec@(1,5) (45.3%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:21:35] \u001b[32mTrain: [ 18/50] Step 100/520 Loss 2.881 Prec@(1,5) (45.3%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:21:36] \u001b[32mTrain: [ 18/50] Step 120/520 Loss 2.869 Prec@(1,5) (45.5%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:21:36] \u001b[32mTrain: [ 18/50] Step 140/520 Loss 2.861 Prec@(1,5) (45.6%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:21:37] \u001b[32mTrain: [ 18/50] Step 160/520 Loss 2.862 Prec@(1,5) (45.6%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:21:38] \u001b[32mTrain: [ 18/50] Step 180/520 Loss 2.873 Prec@(1,5) (45.7%, 74.9%)\u001b[0m\n",
      "[2024-01-15 09:21:39] \u001b[32mTrain: [ 18/50] Step 200/520 Loss 2.873 Prec@(1,5) (45.6%, 74.9%)\u001b[0m\n",
      "[2024-01-15 09:21:40] \u001b[32mTrain: [ 18/50] Step 220/520 Loss 2.882 Prec@(1,5) (45.5%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:21:40] \u001b[32mTrain: [ 18/50] Step 240/520 Loss 2.885 Prec@(1,5) (45.5%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:21:41] \u001b[32mTrain: [ 18/50] Step 260/520 Loss 2.880 Prec@(1,5) (45.5%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:21:42] \u001b[32mTrain: [ 18/50] Step 280/520 Loss 2.883 Prec@(1,5) (45.4%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:21:43] \u001b[32mTrain: [ 18/50] Step 300/520 Loss 2.886 Prec@(1,5) (45.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:43] \u001b[32mTrain: [ 18/50] Step 320/520 Loss 2.887 Prec@(1,5) (45.3%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:44] \u001b[32mTrain: [ 18/50] Step 340/520 Loss 2.888 Prec@(1,5) (45.2%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:45] \u001b[32mTrain: [ 18/50] Step 360/520 Loss 2.892 Prec@(1,5) (45.2%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:46] \u001b[32mTrain: [ 18/50] Step 380/520 Loss 2.892 Prec@(1,5) (45.2%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:46] \u001b[32mTrain: [ 18/50] Step 400/520 Loss 2.893 Prec@(1,5) (45.1%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:47] \u001b[32mTrain: [ 18/50] Step 420/520 Loss 2.893 Prec@(1,5) (45.1%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:48] \u001b[32mTrain: [ 18/50] Step 440/520 Loss 2.897 Prec@(1,5) (45.0%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:49] \u001b[32mTrain: [ 18/50] Step 460/520 Loss 2.898 Prec@(1,5) (45.0%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:49] \u001b[32mTrain: [ 18/50] Step 480/520 Loss 2.897 Prec@(1,5) (45.0%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:50] \u001b[32mTrain: [ 18/50] Step 500/520 Loss 2.897 Prec@(1,5) (45.0%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:51] \u001b[32mTrain: [ 18/50] Step 520/520 Loss 2.893 Prec@(1,5) (45.1%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:21:51] \u001b[32mTrain: [ 18/50] Final Prec@1 45.1000%\u001b[0m\n",
      "[2024-01-15 09:21:56] \u001b[32mValid: [ 18/50] Step 000/104 Loss 2.964 Prec@(1,5) (49.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:21:56] \u001b[32mValid: [ 18/50] Step 020/104 Loss 2.817 Prec@(1,5) (44.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:21:57] \u001b[32mValid: [ 18/50] Step 040/104 Loss 2.718 Prec@(1,5) (44.2%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:21:57] \u001b[32mValid: [ 18/50] Step 060/104 Loss 2.680 Prec@(1,5) (44.3%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:21:57] \u001b[32mValid: [ 18/50] Step 080/104 Loss 2.690 Prec@(1,5) (43.8%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:21:58] \u001b[32mValid: [ 18/50] Step 100/104 Loss 2.689 Prec@(1,5) (43.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:21:58] \u001b[32mValid: [ 18/50] Step 104/104 Loss 2.691 Prec@(1,5) (43.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:21:58] \u001b[32mValid: [ 18/50] Final Prec@1 43.4600%\u001b[0m\n",
      "[2024-01-15 09:21:58] \u001b[32mEpoch 18 LR 0.017823\u001b[0m\n",
      "[2024-01-15 09:22:06] \u001b[32mTrain: [ 19/50] Step 000/520 Loss 2.780 Prec@(1,5) (45.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:22:07] \u001b[32mTrain: [ 19/50] Step 020/520 Loss 2.856 Prec@(1,5) (45.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:08] \u001b[32mTrain: [ 19/50] Step 040/520 Loss 2.787 Prec@(1,5) (46.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:22:08] \u001b[32mTrain: [ 19/50] Step 060/520 Loss 2.812 Prec@(1,5) (46.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:22:09] \u001b[32mTrain: [ 19/50] Step 080/520 Loss 2.826 Prec@(1,5) (46.2%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:22:10] \u001b[32mTrain: [ 19/50] Step 100/520 Loss 2.814 Prec@(1,5) (46.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:22:11] \u001b[32mTrain: [ 19/50] Step 120/520 Loss 2.836 Prec@(1,5) (46.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:22:11] \u001b[32mTrain: [ 19/50] Step 140/520 Loss 2.835 Prec@(1,5) (46.2%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:22:12] \u001b[32mTrain: [ 19/50] Step 160/520 Loss 2.843 Prec@(1,5) (46.2%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:22:13] \u001b[32mTrain: [ 19/50] Step 180/520 Loss 2.851 Prec@(1,5) (46.0%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:14] \u001b[32mTrain: [ 19/50] Step 200/520 Loss 2.855 Prec@(1,5) (45.9%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:22:14] \u001b[32mTrain: [ 19/50] Step 220/520 Loss 2.861 Prec@(1,5) (45.8%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:15] \u001b[32mTrain: [ 19/50] Step 240/520 Loss 2.865 Prec@(1,5) (45.7%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:16] \u001b[32mTrain: [ 19/50] Step 260/520 Loss 2.869 Prec@(1,5) (45.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:17] \u001b[32mTrain: [ 19/50] Step 280/520 Loss 2.868 Prec@(1,5) (45.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:18] \u001b[32mTrain: [ 19/50] Step 300/520 Loss 2.868 Prec@(1,5) (45.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:18] \u001b[32mTrain: [ 19/50] Step 320/520 Loss 2.871 Prec@(1,5) (45.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:19] \u001b[32mTrain: [ 19/50] Step 340/520 Loss 2.872 Prec@(1,5) (45.6%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:20] \u001b[32mTrain: [ 19/50] Step 360/520 Loss 2.875 Prec@(1,5) (45.5%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:22:21] \u001b[32mTrain: [ 19/50] Step 380/520 Loss 2.882 Prec@(1,5) (45.3%, 74.9%)\u001b[0m\n",
      "[2024-01-15 09:22:22] \u001b[32mTrain: [ 19/50] Step 400/520 Loss 2.887 Prec@(1,5) (45.3%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:22] \u001b[32mTrain: [ 19/50] Step 420/520 Loss 2.885 Prec@(1,5) (45.3%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:23] \u001b[32mTrain: [ 19/50] Step 440/520 Loss 2.885 Prec@(1,5) (45.4%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:24] \u001b[32mTrain: [ 19/50] Step 460/520 Loss 2.884 Prec@(1,5) (45.4%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:25] \u001b[32mTrain: [ 19/50] Step 480/520 Loss 2.882 Prec@(1,5) (45.4%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:25] \u001b[32mTrain: [ 19/50] Step 500/520 Loss 2.886 Prec@(1,5) (45.4%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:26] \u001b[32mTrain: [ 19/50] Step 520/520 Loss 2.889 Prec@(1,5) (45.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:22:26] \u001b[32mTrain: [ 19/50] Final Prec@1 45.4120%\u001b[0m\n",
      "[2024-01-15 09:22:31] \u001b[32mValid: [ 19/50] Step 000/104 Loss 2.660 Prec@(1,5) (50.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:22:32] \u001b[32mValid: [ 19/50] Step 020/104 Loss 2.533 Prec@(1,5) (46.2%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:22:32] \u001b[32mValid: [ 19/50] Step 040/104 Loss 2.471 Prec@(1,5) (45.3%, 78.4%)\u001b[0m\n",
      "[2024-01-15 09:22:32] \u001b[32mValid: [ 19/50] Step 060/104 Loss 2.435 Prec@(1,5) (45.7%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:22:33] \u001b[32mValid: [ 19/50] Step 080/104 Loss 2.464 Prec@(1,5) (45.1%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:22:33] \u001b[32mValid: [ 19/50] Step 100/104 Loss 2.450 Prec@(1,5) (45.5%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:22:33] \u001b[32mValid: [ 19/50] Step 104/104 Loss 2.451 Prec@(1,5) (45.5%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:22:33] \u001b[32mValid: [ 19/50] Final Prec@1 45.5200%\u001b[0m\n",
      "[2024-01-15 09:22:33] \u001b[32mEpoch 19 LR 0.017102\u001b[0m\n",
      "[2024-01-15 09:22:41] \u001b[32mTrain: [ 20/50] Step 000/520 Loss 3.343 Prec@(1,5) (39.6%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:22:42] \u001b[32mTrain: [ 20/50] Step 020/520 Loss 2.904 Prec@(1,5) (45.2%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:22:43] \u001b[32mTrain: [ 20/50] Step 040/520 Loss 2.899 Prec@(1,5) (45.0%, 74.8%)\u001b[0m\n",
      "[2024-01-15 09:22:44] \u001b[32mTrain: [ 20/50] Step 060/520 Loss 2.892 Prec@(1,5) (45.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:22:44] \u001b[32mTrain: [ 20/50] Step 080/520 Loss 2.871 Prec@(1,5) (45.6%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:45] \u001b[32mTrain: [ 20/50] Step 100/520 Loss 2.873 Prec@(1,5) (45.8%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:46] \u001b[32mTrain: [ 20/50] Step 120/520 Loss 2.875 Prec@(1,5) (45.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 09:22:47] \u001b[32mTrain: [ 20/50] Step 140/520 Loss 2.855 Prec@(1,5) (46.2%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:47] \u001b[32mTrain: [ 20/50] Step 160/520 Loss 2.848 Prec@(1,5) (46.3%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:48] \u001b[32mTrain: [ 20/50] Step 180/520 Loss 2.838 Prec@(1,5) (46.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:22:49] \u001b[32mTrain: [ 20/50] Step 200/520 Loss 2.832 Prec@(1,5) (46.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:22:50] \u001b[32mTrain: [ 20/50] Step 220/520 Loss 2.837 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:22:50] \u001b[32mTrain: [ 20/50] Step 240/520 Loss 2.837 Prec@(1,5) (46.6%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:22:51] \u001b[32mTrain: [ 20/50] Step 260/520 Loss 2.836 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:22:52] \u001b[32mTrain: [ 20/50] Step 280/520 Loss 2.841 Prec@(1,5) (46.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:22:53] \u001b[32mTrain: [ 20/50] Step 300/520 Loss 2.838 Prec@(1,5) (46.5%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:54] \u001b[32mTrain: [ 20/50] Step 320/520 Loss 2.834 Prec@(1,5) (46.5%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:22:54] \u001b[32mTrain: [ 20/50] Step 340/520 Loss 2.839 Prec@(1,5) (46.5%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:55] \u001b[32mTrain: [ 20/50] Step 360/520 Loss 2.843 Prec@(1,5) (46.4%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:56] \u001b[32mTrain: [ 20/50] Step 380/520 Loss 2.844 Prec@(1,5) (46.3%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:57] \u001b[32mTrain: [ 20/50] Step 400/520 Loss 2.847 Prec@(1,5) (46.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:57] \u001b[32mTrain: [ 20/50] Step 420/520 Loss 2.847 Prec@(1,5) (46.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:22:58] \u001b[32mTrain: [ 20/50] Step 440/520 Loss 2.845 Prec@(1,5) (46.3%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:22:59] \u001b[32mTrain: [ 20/50] Step 460/520 Loss 2.843 Prec@(1,5) (46.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:23:00] \u001b[32mTrain: [ 20/50] Step 480/520 Loss 2.843 Prec@(1,5) (46.3%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:23:01] \u001b[32mTrain: [ 20/50] Step 500/520 Loss 2.842 Prec@(1,5) (46.3%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:23:01] \u001b[32mTrain: [ 20/50] Step 520/520 Loss 2.845 Prec@(1,5) (46.3%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:23:02] \u001b[32mTrain: [ 20/50] Final Prec@1 46.3020%\u001b[0m\n",
      "[2024-01-15 09:23:06] \u001b[32mValid: [ 20/50] Step 000/104 Loss 2.743 Prec@(1,5) (51.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:23:07] \u001b[32mValid: [ 20/50] Step 020/104 Loss 2.410 Prec@(1,5) (48.8%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:23:07] \u001b[32mValid: [ 20/50] Step 040/104 Loss 2.371 Prec@(1,5) (47.5%, 78.4%)\u001b[0m\n",
      "[2024-01-15 09:23:07] \u001b[32mValid: [ 20/50] Step 060/104 Loss 2.354 Prec@(1,5) (47.4%, 78.4%)\u001b[0m\n",
      "[2024-01-15 09:23:08] \u001b[32mValid: [ 20/50] Step 080/104 Loss 2.363 Prec@(1,5) (47.1%, 78.4%)\u001b[0m\n",
      "[2024-01-15 09:23:08] \u001b[32mValid: [ 20/50] Step 100/104 Loss 2.352 Prec@(1,5) (47.3%, 78.5%)\u001b[0m\n",
      "[2024-01-15 09:23:08] \u001b[32mValid: [ 20/50] Step 104/104 Loss 2.356 Prec@(1,5) (47.2%, 78.5%)\u001b[0m\n",
      "[2024-01-15 09:23:08] \u001b[32mValid: [ 20/50] Final Prec@1 47.1900%\u001b[0m\n",
      "[2024-01-15 09:23:08] \u001b[32mEpoch 20 LR 0.016363\u001b[0m\n",
      "[2024-01-15 09:23:17] \u001b[32mTrain: [ 21/50] Step 000/520 Loss 2.859 Prec@(1,5) (44.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:23:17] \u001b[32mTrain: [ 21/50] Step 020/520 Loss 2.740 Prec@(1,5) (47.5%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:23:18] \u001b[32mTrain: [ 21/50] Step 040/520 Loss 2.751 Prec@(1,5) (47.4%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:23:19] \u001b[32mTrain: [ 21/50] Step 060/520 Loss 2.774 Prec@(1,5) (47.1%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:23:20] \u001b[32mTrain: [ 21/50] Step 080/520 Loss 2.795 Prec@(1,5) (46.8%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:23:20] \u001b[32mTrain: [ 21/50] Step 100/520 Loss 2.819 Prec@(1,5) (46.4%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:23:21] \u001b[32mTrain: [ 21/50] Step 120/520 Loss 2.805 Prec@(1,5) (46.5%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:23:22] \u001b[32mTrain: [ 21/50] Step 140/520 Loss 2.807 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:23] \u001b[32mTrain: [ 21/50] Step 160/520 Loss 2.812 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:23] \u001b[32mTrain: [ 21/50] Step 180/520 Loss 2.812 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:24] \u001b[32mTrain: [ 21/50] Step 200/520 Loss 2.811 Prec@(1,5) (46.6%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:25] \u001b[32mTrain: [ 21/50] Step 220/520 Loss 2.818 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:26] \u001b[32mTrain: [ 21/50] Step 240/520 Loss 2.821 Prec@(1,5) (46.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:23:26] \u001b[32mTrain: [ 21/50] Step 260/520 Loss 2.819 Prec@(1,5) (46.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:23:27] \u001b[32mTrain: [ 21/50] Step 280/520 Loss 2.822 Prec@(1,5) (46.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:23:28] \u001b[32mTrain: [ 21/50] Step 300/520 Loss 2.829 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:29] \u001b[32mTrain: [ 21/50] Step 320/520 Loss 2.833 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:29] \u001b[32mTrain: [ 21/50] Step 340/520 Loss 2.834 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:30] \u001b[32mTrain: [ 21/50] Step 360/520 Loss 2.832 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:31] \u001b[32mTrain: [ 21/50] Step 380/520 Loss 2.831 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:32] \u001b[32mTrain: [ 21/50] Step 400/520 Loss 2.831 Prec@(1,5) (46.4%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:23:33] \u001b[32mTrain: [ 21/50] Step 420/520 Loss 2.830 Prec@(1,5) (46.4%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:23:33] \u001b[32mTrain: [ 21/50] Step 440/520 Loss 2.828 Prec@(1,5) (46.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:23:34] \u001b[32mTrain: [ 21/50] Step 460/520 Loss 2.831 Prec@(1,5) (46.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:35] \u001b[32mTrain: [ 21/50] Step 480/520 Loss 2.834 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:36] \u001b[32mTrain: [ 21/50] Step 500/520 Loss 2.833 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:36] \u001b[32mTrain: [ 21/50] Step 520/520 Loss 2.831 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:23:37] \u001b[32mTrain: [ 21/50] Final Prec@1 46.4060%\u001b[0m\n",
      "[2024-01-15 09:23:41] \u001b[32mValid: [ 21/50] Step 000/104 Loss 2.604 Prec@(1,5) (51.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:23:42] \u001b[32mValid: [ 21/50] Step 020/104 Loss 2.561 Prec@(1,5) (46.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:23:42] \u001b[32mValid: [ 21/50] Step 040/104 Loss 2.519 Prec@(1,5) (46.2%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:23:42] \u001b[32mValid: [ 21/50] Step 060/104 Loss 2.484 Prec@(1,5) (46.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:23:43] \u001b[32mValid: [ 21/50] Step 080/104 Loss 2.492 Prec@(1,5) (46.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:23:43] \u001b[32mValid: [ 21/50] Step 100/104 Loss 2.484 Prec@(1,5) (46.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:23:43] \u001b[32mValid: [ 21/50] Step 104/104 Loss 2.484 Prec@(1,5) (46.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:23:43] \u001b[32mValid: [ 21/50] Final Prec@1 46.1800%\u001b[0m\n",
      "[2024-01-15 09:23:43] \u001b[32mEpoch 21 LR 0.015609\u001b[0m\n",
      "[2024-01-15 09:23:51] \u001b[32mTrain: [ 22/50] Step 000/520 Loss 2.430 Prec@(1,5) (50.0%, 86.5%)\u001b[0m\n",
      "[2024-01-15 09:23:52] \u001b[32mTrain: [ 22/50] Step 020/520 Loss 2.786 Prec@(1,5) (46.3%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:23:53] \u001b[32mTrain: [ 22/50] Step 040/520 Loss 2.768 Prec@(1,5) (46.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:23:54] \u001b[32mTrain: [ 22/50] Step 060/520 Loss 2.783 Prec@(1,5) (46.8%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:23:54] \u001b[32mTrain: [ 22/50] Step 080/520 Loss 2.796 Prec@(1,5) (46.5%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:23:55] \u001b[32mTrain: [ 22/50] Step 100/520 Loss 2.793 Prec@(1,5) (46.6%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:23:56] \u001b[32mTrain: [ 22/50] Step 120/520 Loss 2.795 Prec@(1,5) (46.6%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:23:56] \u001b[32mTrain: [ 22/50] Step 140/520 Loss 2.788 Prec@(1,5) (46.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:23:57] \u001b[32mTrain: [ 22/50] Step 160/520 Loss 2.793 Prec@(1,5) (46.8%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:23:58] \u001b[32mTrain: [ 22/50] Step 180/520 Loss 2.801 Prec@(1,5) (46.7%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:23:59] \u001b[32mTrain: [ 22/50] Step 200/520 Loss 2.799 Prec@(1,5) (46.7%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:24:00] \u001b[32mTrain: [ 22/50] Step 220/520 Loss 2.806 Prec@(1,5) (46.6%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:24:00] \u001b[32mTrain: [ 22/50] Step 240/520 Loss 2.811 Prec@(1,5) (46.5%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:01] \u001b[32mTrain: [ 22/50] Step 260/520 Loss 2.813 Prec@(1,5) (46.5%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:02] \u001b[32mTrain: [ 22/50] Step 280/520 Loss 2.817 Prec@(1,5) (46.4%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:03] \u001b[32mTrain: [ 22/50] Step 300/520 Loss 2.816 Prec@(1,5) (46.5%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:03] \u001b[32mTrain: [ 22/50] Step 320/520 Loss 2.819 Prec@(1,5) (46.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:04] \u001b[32mTrain: [ 22/50] Step 340/520 Loss 2.821 Prec@(1,5) (46.4%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:05] \u001b[32mTrain: [ 22/50] Step 360/520 Loss 2.829 Prec@(1,5) (46.3%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:24:06] \u001b[32mTrain: [ 22/50] Step 380/520 Loss 2.833 Prec@(1,5) (46.2%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:24:06] \u001b[32mTrain: [ 22/50] Step 400/520 Loss 2.829 Prec@(1,5) (46.2%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:24:07] \u001b[32mTrain: [ 22/50] Step 420/520 Loss 2.826 Prec@(1,5) (46.3%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:24:08] \u001b[32mTrain: [ 22/50] Step 440/520 Loss 2.820 Prec@(1,5) (46.4%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:09] \u001b[32mTrain: [ 22/50] Step 460/520 Loss 2.823 Prec@(1,5) (46.3%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:10] \u001b[32mTrain: [ 22/50] Step 480/520 Loss 2.819 Prec@(1,5) (46.3%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:10] \u001b[32mTrain: [ 22/50] Step 500/520 Loss 2.814 Prec@(1,5) (46.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:11] \u001b[32mTrain: [ 22/50] Step 520/520 Loss 2.814 Prec@(1,5) (46.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:11] \u001b[32mTrain: [ 22/50] Final Prec@1 46.4640%\u001b[0m\n",
      "[2024-01-15 09:24:16] \u001b[32mValid: [ 22/50] Step 000/104 Loss 2.583 Prec@(1,5) (47.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:24:16] \u001b[32mValid: [ 22/50] Step 020/104 Loss 2.477 Prec@(1,5) (47.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:24:17] \u001b[32mValid: [ 22/50] Step 040/104 Loss 2.443 Prec@(1,5) (46.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:24:17] \u001b[32mValid: [ 22/50] Step 060/104 Loss 2.407 Prec@(1,5) (46.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:24:17] \u001b[32mValid: [ 22/50] Step 080/104 Loss 2.413 Prec@(1,5) (46.3%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:24:18] \u001b[32mValid: [ 22/50] Step 100/104 Loss 2.412 Prec@(1,5) (46.3%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:24:18] \u001b[32mValid: [ 22/50] Step 104/104 Loss 2.415 Prec@(1,5) (46.3%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:24:18] \u001b[32mValid: [ 22/50] Final Prec@1 46.2600%\u001b[0m\n",
      "[2024-01-15 09:24:18] \u001b[32mEpoch 22 LR 0.014843\u001b[0m\n",
      "[2024-01-15 09:24:26] \u001b[32mTrain: [ 23/50] Step 000/520 Loss 2.391 Prec@(1,5) (53.1%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:24:27] \u001b[32mTrain: [ 23/50] Step 020/520 Loss 2.775 Prec@(1,5) (47.6%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:24:28] \u001b[32mTrain: [ 23/50] Step 040/520 Loss 2.776 Prec@(1,5) (47.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:24:28] \u001b[32mTrain: [ 23/50] Step 060/520 Loss 2.781 Prec@(1,5) (47.4%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:24:29] \u001b[32mTrain: [ 23/50] Step 080/520 Loss 2.797 Prec@(1,5) (47.1%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:30] \u001b[32mTrain: [ 23/50] Step 100/520 Loss 2.799 Prec@(1,5) (46.9%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:31] \u001b[32mTrain: [ 23/50] Step 120/520 Loss 2.796 Prec@(1,5) (46.9%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:31] \u001b[32mTrain: [ 23/50] Step 140/520 Loss 2.803 Prec@(1,5) (46.8%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:24:32] \u001b[32mTrain: [ 23/50] Step 160/520 Loss 2.800 Prec@(1,5) (46.8%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:24:33] \u001b[32mTrain: [ 23/50] Step 180/520 Loss 2.795 Prec@(1,5) (47.0%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:34] \u001b[32mTrain: [ 23/50] Step 200/520 Loss 2.788 Prec@(1,5) (47.2%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:24:34] \u001b[32mTrain: [ 23/50] Step 220/520 Loss 2.788 Prec@(1,5) (47.2%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:35] \u001b[32mTrain: [ 23/50] Step 240/520 Loss 2.784 Prec@(1,5) (47.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:24:36] \u001b[32mTrain: [ 23/50] Step 260/520 Loss 2.790 Prec@(1,5) (47.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:24:37] \u001b[32mTrain: [ 23/50] Step 280/520 Loss 2.786 Prec@(1,5) (47.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:24:37] \u001b[32mTrain: [ 23/50] Step 300/520 Loss 2.788 Prec@(1,5) (47.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:24:38] \u001b[32mTrain: [ 23/50] Step 320/520 Loss 2.796 Prec@(1,5) (47.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:24:39] \u001b[32mTrain: [ 23/50] Step 340/520 Loss 2.795 Prec@(1,5) (47.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:24:40] \u001b[32mTrain: [ 23/50] Step 360/520 Loss 2.795 Prec@(1,5) (47.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:24:41] \u001b[32mTrain: [ 23/50] Step 380/520 Loss 2.797 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:41] \u001b[32mTrain: [ 23/50] Step 400/520 Loss 2.796 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:42] \u001b[32mTrain: [ 23/50] Step 420/520 Loss 2.796 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:43] \u001b[32mTrain: [ 23/50] Step 440/520 Loss 2.794 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:44] \u001b[32mTrain: [ 23/50] Step 460/520 Loss 2.795 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:44] \u001b[32mTrain: [ 23/50] Step 480/520 Loss 2.795 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:45] \u001b[32mTrain: [ 23/50] Step 500/520 Loss 2.796 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:46] \u001b[32mTrain: [ 23/50] Step 520/520 Loss 2.795 Prec@(1,5) (47.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:24:46] \u001b[32mTrain: [ 23/50] Final Prec@1 46.9760%\u001b[0m\n",
      "[2024-01-15 09:24:51] \u001b[32mValid: [ 23/50] Step 000/104 Loss 2.821 Prec@(1,5) (51.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:24:51] \u001b[32mValid: [ 23/50] Step 020/104 Loss 2.620 Prec@(1,5) (45.2%, 75.2%)\u001b[0m\n",
      "[2024-01-15 09:24:52] \u001b[32mValid: [ 23/50] Step 040/104 Loss 2.589 Prec@(1,5) (44.8%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:52] \u001b[32mValid: [ 23/50] Step 060/104 Loss 2.559 Prec@(1,5) (45.0%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:52] \u001b[32mValid: [ 23/50] Step 080/104 Loss 2.578 Prec@(1,5) (44.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:24:53] \u001b[32mValid: [ 23/50] Step 100/104 Loss 2.576 Prec@(1,5) (45.0%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:53] \u001b[32mValid: [ 23/50] Step 104/104 Loss 2.579 Prec@(1,5) (44.9%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:24:53] \u001b[32mValid: [ 23/50] Final Prec@1 44.8900%\u001b[0m\n",
      "[2024-01-15 09:24:53] \u001b[32mEpoch 23 LR 0.014067\u001b[0m\n",
      "[2024-01-15 09:25:01] \u001b[32mTrain: [ 24/50] Step 000/520 Loss 2.806 Prec@(1,5) (46.9%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:25:02] \u001b[32mTrain: [ 24/50] Step 020/520 Loss 2.840 Prec@(1,5) (46.8%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:25:03] \u001b[32mTrain: [ 24/50] Step 040/520 Loss 2.821 Prec@(1,5) (47.0%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:25:03] \u001b[32mTrain: [ 24/50] Step 060/520 Loss 2.800 Prec@(1,5) (46.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:25:04] \u001b[32mTrain: [ 24/50] Step 080/520 Loss 2.788 Prec@(1,5) (46.9%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:05] \u001b[32mTrain: [ 24/50] Step 100/520 Loss 2.789 Prec@(1,5) (47.0%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:06] \u001b[32mTrain: [ 24/50] Step 120/520 Loss 2.799 Prec@(1,5) (46.7%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:06] \u001b[32mTrain: [ 24/50] Step 140/520 Loss 2.805 Prec@(1,5) (46.7%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:07] \u001b[32mTrain: [ 24/50] Step 160/520 Loss 2.814 Prec@(1,5) (46.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:08] \u001b[32mTrain: [ 24/50] Step 180/520 Loss 2.811 Prec@(1,5) (46.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:09] \u001b[32mTrain: [ 24/50] Step 200/520 Loss 2.801 Prec@(1,5) (46.9%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:25:09] \u001b[32mTrain: [ 24/50] Step 220/520 Loss 2.801 Prec@(1,5) (46.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:25:10] \u001b[32mTrain: [ 24/50] Step 240/520 Loss 2.797 Prec@(1,5) (46.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:25:11] \u001b[32mTrain: [ 24/50] Step 260/520 Loss 2.792 Prec@(1,5) (47.0%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:25:12] \u001b[32mTrain: [ 24/50] Step 280/520 Loss 2.782 Prec@(1,5) (47.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:25:13] \u001b[32mTrain: [ 24/50] Step 300/520 Loss 2.782 Prec@(1,5) (47.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:25:13] \u001b[32mTrain: [ 24/50] Step 320/520 Loss 2.780 Prec@(1,5) (47.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:25:14] \u001b[32mTrain: [ 24/50] Step 340/520 Loss 2.772 Prec@(1,5) (47.2%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:15] \u001b[32mTrain: [ 24/50] Step 360/520 Loss 2.769 Prec@(1,5) (47.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:16] \u001b[32mTrain: [ 24/50] Step 380/520 Loss 2.771 Prec@(1,5) (47.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:16] \u001b[32mTrain: [ 24/50] Step 400/520 Loss 2.770 Prec@(1,5) (47.4%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:17] \u001b[32mTrain: [ 24/50] Step 420/520 Loss 2.769 Prec@(1,5) (47.4%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:18] \u001b[32mTrain: [ 24/50] Step 440/520 Loss 2.767 Prec@(1,5) (47.5%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:19] \u001b[32mTrain: [ 24/50] Step 460/520 Loss 2.767 Prec@(1,5) (47.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:20] \u001b[32mTrain: [ 24/50] Step 480/520 Loss 2.771 Prec@(1,5) (47.4%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:20] \u001b[32mTrain: [ 24/50] Step 500/520 Loss 2.769 Prec@(1,5) (47.4%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:21] \u001b[32mTrain: [ 24/50] Step 520/520 Loss 2.771 Prec@(1,5) (47.4%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:21] \u001b[32mTrain: [ 24/50] Final Prec@1 47.4060%\u001b[0m\n",
      "[2024-01-15 09:25:26] \u001b[32mValid: [ 24/50] Step 000/104 Loss 2.485 Prec@(1,5) (47.9%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:25:27] \u001b[32mValid: [ 24/50] Step 020/104 Loss 2.505 Prec@(1,5) (46.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:25:27] \u001b[32mValid: [ 24/50] Step 040/104 Loss 2.438 Prec@(1,5) (46.6%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:25:27] \u001b[32mValid: [ 24/50] Step 060/104 Loss 2.410 Prec@(1,5) (46.8%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:25:27] \u001b[32mValid: [ 24/50] Step 080/104 Loss 2.418 Prec@(1,5) (46.5%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:25:28] \u001b[32mValid: [ 24/50] Step 100/104 Loss 2.408 Prec@(1,5) (46.4%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:25:28] \u001b[32mValid: [ 24/50] Step 104/104 Loss 2.409 Prec@(1,5) (46.4%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:25:28] \u001b[32mValid: [ 24/50] Final Prec@1 46.3800%\u001b[0m\n",
      "[2024-01-15 09:25:28] \u001b[32mEpoch 24 LR 0.013285\u001b[0m\n",
      "[2024-01-15 09:25:36] \u001b[32mTrain: [ 25/50] Step 000/520 Loss 2.832 Prec@(1,5) (40.6%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:25:37] \u001b[32mTrain: [ 25/50] Step 020/520 Loss 2.662 Prec@(1,5) (50.1%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:25:38] \u001b[32mTrain: [ 25/50] Step 040/520 Loss 2.700 Prec@(1,5) (49.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:25:39] \u001b[32mTrain: [ 25/50] Step 060/520 Loss 2.721 Prec@(1,5) (48.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:25:39] \u001b[32mTrain: [ 25/50] Step 080/520 Loss 2.712 Prec@(1,5) (48.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:25:40] \u001b[32mTrain: [ 25/50] Step 100/520 Loss 2.729 Prec@(1,5) (48.1%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:25:41] \u001b[32mTrain: [ 25/50] Step 120/520 Loss 2.732 Prec@(1,5) (48.0%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:25:42] \u001b[32mTrain: [ 25/50] Step 140/520 Loss 2.736 Prec@(1,5) (48.1%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:25:42] \u001b[32mTrain: [ 25/50] Step 160/520 Loss 2.733 Prec@(1,5) (48.0%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:25:43] \u001b[32mTrain: [ 25/50] Step 180/520 Loss 2.744 Prec@(1,5) (47.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:25:44] \u001b[32mTrain: [ 25/50] Step 200/520 Loss 2.746 Prec@(1,5) (47.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:25:45] \u001b[32mTrain: [ 25/50] Step 220/520 Loss 2.745 Prec@(1,5) (48.0%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:25:45] \u001b[32mTrain: [ 25/50] Step 240/520 Loss 2.751 Prec@(1,5) (47.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:25:46] \u001b[32mTrain: [ 25/50] Step 260/520 Loss 2.755 Prec@(1,5) (47.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:47] \u001b[32mTrain: [ 25/50] Step 280/520 Loss 2.754 Prec@(1,5) (47.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:25:48] \u001b[32mTrain: [ 25/50] Step 300/520 Loss 2.752 Prec@(1,5) (48.1%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:25:49] \u001b[32mTrain: [ 25/50] Step 320/520 Loss 2.759 Prec@(1,5) (48.1%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:49] \u001b[32mTrain: [ 25/50] Step 340/520 Loss 2.759 Prec@(1,5) (48.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:50] \u001b[32mTrain: [ 25/50] Step 360/520 Loss 2.762 Prec@(1,5) (47.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:51] \u001b[32mTrain: [ 25/50] Step 380/520 Loss 2.761 Prec@(1,5) (47.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:52] \u001b[32mTrain: [ 25/50] Step 400/520 Loss 2.759 Prec@(1,5) (48.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:53] \u001b[32mTrain: [ 25/50] Step 420/520 Loss 2.760 Prec@(1,5) (48.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:53] \u001b[32mTrain: [ 25/50] Step 440/520 Loss 2.761 Prec@(1,5) (47.9%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:25:54] \u001b[32mTrain: [ 25/50] Step 460/520 Loss 2.760 Prec@(1,5) (47.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:55] \u001b[32mTrain: [ 25/50] Step 480/520 Loss 2.758 Prec@(1,5) (48.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:56] \u001b[32mTrain: [ 25/50] Step 500/520 Loss 2.755 Prec@(1,5) (48.0%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:25:56] \u001b[32mTrain: [ 25/50] Step 520/520 Loss 2.756 Prec@(1,5) (48.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:25:57] \u001b[32mTrain: [ 25/50] Final Prec@1 47.9880%\u001b[0m\n",
      "[2024-01-15 09:26:02] \u001b[32mValid: [ 25/50] Step 000/104 Loss 2.415 Prec@(1,5) (53.1%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:26:02] \u001b[32mValid: [ 25/50] Step 020/104 Loss 2.374 Prec@(1,5) (49.7%, 79.1%)\u001b[0m\n",
      "[2024-01-15 09:26:02] \u001b[32mValid: [ 25/50] Step 040/104 Loss 2.354 Prec@(1,5) (48.6%, 78.6%)\u001b[0m\n",
      "[2024-01-15 09:26:03] \u001b[32mValid: [ 25/50] Step 060/104 Loss 2.320 Prec@(1,5) (48.3%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:26:03] \u001b[32mValid: [ 25/50] Step 080/104 Loss 2.332 Prec@(1,5) (48.1%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:26:03] \u001b[32mValid: [ 25/50] Step 100/104 Loss 2.336 Prec@(1,5) (48.2%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:26:03] \u001b[32mValid: [ 25/50] Step 104/104 Loss 2.339 Prec@(1,5) (48.1%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:26:04] \u001b[32mValid: [ 25/50] Final Prec@1 48.0700%\u001b[0m\n",
      "[2024-01-15 09:26:04] \u001b[32mEpoch 25 LR 0.012500\u001b[0m\n",
      "[2024-01-15 09:26:12] \u001b[32mTrain: [ 26/50] Step 000/520 Loss 2.722 Prec@(1,5) (43.8%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:26:13] \u001b[32mTrain: [ 26/50] Step 020/520 Loss 2.675 Prec@(1,5) (48.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:26:13] \u001b[32mTrain: [ 26/50] Step 040/520 Loss 2.675 Prec@(1,5) (48.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:26:14] \u001b[32mTrain: [ 26/50] Step 060/520 Loss 2.698 Prec@(1,5) (48.8%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:26:15] \u001b[32mTrain: [ 26/50] Step 080/520 Loss 2.691 Prec@(1,5) (48.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:26:16] \u001b[32mTrain: [ 26/50] Step 100/520 Loss 2.703 Prec@(1,5) (48.7%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:26:16] \u001b[32mTrain: [ 26/50] Step 120/520 Loss 2.706 Prec@(1,5) (48.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:26:17] \u001b[32mTrain: [ 26/50] Step 140/520 Loss 2.719 Prec@(1,5) (48.7%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:18] \u001b[32mTrain: [ 26/50] Step 160/520 Loss 2.721 Prec@(1,5) (48.5%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:19] \u001b[32mTrain: [ 26/50] Step 180/520 Loss 2.718 Prec@(1,5) (48.6%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:26:19] \u001b[32mTrain: [ 26/50] Step 200/520 Loss 2.733 Prec@(1,5) (48.3%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:26:20] \u001b[32mTrain: [ 26/50] Step 220/520 Loss 2.738 Prec@(1,5) (48.4%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:26:21] \u001b[32mTrain: [ 26/50] Step 240/520 Loss 2.731 Prec@(1,5) (48.4%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:22] \u001b[32mTrain: [ 26/50] Step 260/520 Loss 2.732 Prec@(1,5) (48.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:23] \u001b[32mTrain: [ 26/50] Step 280/520 Loss 2.730 Prec@(1,5) (48.4%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:23] \u001b[32mTrain: [ 26/50] Step 300/520 Loss 2.729 Prec@(1,5) (48.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:24] \u001b[32mTrain: [ 26/50] Step 320/520 Loss 2.729 Prec@(1,5) (48.3%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:25] \u001b[32mTrain: [ 26/50] Step 340/520 Loss 2.728 Prec@(1,5) (48.3%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:26] \u001b[32mTrain: [ 26/50] Step 360/520 Loss 2.730 Prec@(1,5) (48.3%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:26] \u001b[32mTrain: [ 26/50] Step 380/520 Loss 2.735 Prec@(1,5) (48.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:27] \u001b[32mTrain: [ 26/50] Step 400/520 Loss 2.737 Prec@(1,5) (48.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:28] \u001b[32mTrain: [ 26/50] Step 420/520 Loss 2.739 Prec@(1,5) (48.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:29] \u001b[32mTrain: [ 26/50] Step 440/520 Loss 2.738 Prec@(1,5) (48.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:26:30] \u001b[32mTrain: [ 26/50] Step 460/520 Loss 2.738 Prec@(1,5) (48.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:30] \u001b[32mTrain: [ 26/50] Step 480/520 Loss 2.740 Prec@(1,5) (48.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:31] \u001b[32mTrain: [ 26/50] Step 500/520 Loss 2.740 Prec@(1,5) (48.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:32] \u001b[32mTrain: [ 26/50] Step 520/520 Loss 2.740 Prec@(1,5) (48.1%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:32] \u001b[32mTrain: [ 26/50] Final Prec@1 48.1460%\u001b[0m\n",
      "[2024-01-15 09:26:37] \u001b[32mValid: [ 26/50] Step 000/104 Loss 2.400 Prec@(1,5) (49.0%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:26:37] \u001b[32mValid: [ 26/50] Step 020/104 Loss 2.380 Prec@(1,5) (48.5%, 78.8%)\u001b[0m\n",
      "[2024-01-15 09:26:38] \u001b[32mValid: [ 26/50] Step 040/104 Loss 2.307 Prec@(1,5) (48.7%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:26:38] \u001b[32mValid: [ 26/50] Step 060/104 Loss 2.272 Prec@(1,5) (49.0%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:26:38] \u001b[32mValid: [ 26/50] Step 080/104 Loss 2.283 Prec@(1,5) (48.7%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:26:39] \u001b[32mValid: [ 26/50] Step 100/104 Loss 2.277 Prec@(1,5) (48.5%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:26:39] \u001b[32mValid: [ 26/50] Step 104/104 Loss 2.278 Prec@(1,5) (48.5%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:26:39] \u001b[32mValid: [ 26/50] Final Prec@1 48.4800%\u001b[0m\n",
      "[2024-01-15 09:26:39] \u001b[32mEpoch 26 LR 0.011716\u001b[0m\n",
      "[2024-01-15 09:26:47] \u001b[32mTrain: [ 27/50] Step 000/520 Loss 2.604 Prec@(1,5) (49.0%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:26:48] \u001b[32mTrain: [ 27/50] Step 020/520 Loss 2.627 Prec@(1,5) (50.7%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:26:49] \u001b[32mTrain: [ 27/50] Step 040/520 Loss 2.699 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:26:50] \u001b[32mTrain: [ 27/50] Step 060/520 Loss 2.708 Prec@(1,5) (49.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:26:50] \u001b[32mTrain: [ 27/50] Step 080/520 Loss 2.707 Prec@(1,5) (49.2%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:26:51] \u001b[32mTrain: [ 27/50] Step 100/520 Loss 2.691 Prec@(1,5) (49.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:26:52] \u001b[32mTrain: [ 27/50] Step 120/520 Loss 2.697 Prec@(1,5) (49.4%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:26:53] \u001b[32mTrain: [ 27/50] Step 140/520 Loss 2.710 Prec@(1,5) (49.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:53] \u001b[32mTrain: [ 27/50] Step 160/520 Loss 2.716 Prec@(1,5) (49.3%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:26:54] \u001b[32mTrain: [ 27/50] Step 180/520 Loss 2.722 Prec@(1,5) (49.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:26:55] \u001b[32mTrain: [ 27/50] Step 200/520 Loss 2.720 Prec@(1,5) (49.1%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:56] \u001b[32mTrain: [ 27/50] Step 220/520 Loss 2.717 Prec@(1,5) (49.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:56] \u001b[32mTrain: [ 27/50] Step 240/520 Loss 2.711 Prec@(1,5) (49.2%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:26:57] \u001b[32mTrain: [ 27/50] Step 260/520 Loss 2.715 Prec@(1,5) (49.0%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:26:58] \u001b[32mTrain: [ 27/50] Step 280/520 Loss 2.720 Prec@(1,5) (48.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:26:59] \u001b[32mTrain: [ 27/50] Step 300/520 Loss 2.709 Prec@(1,5) (49.1%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:00] \u001b[32mTrain: [ 27/50] Step 320/520 Loss 2.716 Prec@(1,5) (49.0%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:27:00] \u001b[32mTrain: [ 27/50] Step 340/520 Loss 2.711 Prec@(1,5) (49.0%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:01] \u001b[32mTrain: [ 27/50] Step 360/520 Loss 2.715 Prec@(1,5) (49.0%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:27:02] \u001b[32mTrain: [ 27/50] Step 380/520 Loss 2.716 Prec@(1,5) (49.0%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:27:03] \u001b[32mTrain: [ 27/50] Step 400/520 Loss 2.719 Prec@(1,5) (48.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:27:04] \u001b[32mTrain: [ 27/50] Step 420/520 Loss 2.717 Prec@(1,5) (48.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:04] \u001b[32mTrain: [ 27/50] Step 440/520 Loss 2.716 Prec@(1,5) (48.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:27:05] \u001b[32mTrain: [ 27/50] Step 460/520 Loss 2.720 Prec@(1,5) (48.8%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:06] \u001b[32mTrain: [ 27/50] Step 480/520 Loss 2.720 Prec@(1,5) (48.8%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:07] \u001b[32mTrain: [ 27/50] Step 500/520 Loss 2.719 Prec@(1,5) (48.8%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:07] \u001b[32mTrain: [ 27/50] Step 520/520 Loss 2.721 Prec@(1,5) (48.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:27:08] \u001b[32mTrain: [ 27/50] Final Prec@1 48.7400%\u001b[0m\n",
      "[2024-01-15 09:27:13] \u001b[32mValid: [ 27/50] Step 000/104 Loss 2.587 Prec@(1,5) (52.1%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:27:13] \u001b[32mValid: [ 27/50] Step 020/104 Loss 2.357 Prec@(1,5) (48.8%, 79.0%)\u001b[0m\n",
      "[2024-01-15 09:27:13] \u001b[32mValid: [ 27/50] Step 040/104 Loss 2.307 Prec@(1,5) (48.6%, 79.5%)\u001b[0m\n",
      "[2024-01-15 09:27:14] \u001b[32mValid: [ 27/50] Step 060/104 Loss 2.264 Prec@(1,5) (48.8%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:27:14] \u001b[32mValid: [ 27/50] Step 080/104 Loss 2.274 Prec@(1,5) (48.2%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:27:14] \u001b[32mValid: [ 27/50] Step 100/104 Loss 2.256 Prec@(1,5) (48.5%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:27:14] \u001b[32mValid: [ 27/50] Step 104/104 Loss 2.256 Prec@(1,5) (48.6%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:27:14] \u001b[32mValid: [ 27/50] Final Prec@1 48.5500%\u001b[0m\n",
      "[2024-01-15 09:27:14] \u001b[32mEpoch 27 LR 0.010934\u001b[0m\n",
      "[2024-01-15 09:27:23] \u001b[32mTrain: [ 28/50] Step 000/520 Loss 2.533 Prec@(1,5) (50.0%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:27:23] \u001b[32mTrain: [ 28/50] Step 020/520 Loss 2.681 Prec@(1,5) (48.8%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:27:24] \u001b[32mTrain: [ 28/50] Step 040/520 Loss 2.650 Prec@(1,5) (48.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:27:25] \u001b[32mTrain: [ 28/50] Step 060/520 Loss 2.648 Prec@(1,5) (49.4%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:27:26] \u001b[32mTrain: [ 28/50] Step 080/520 Loss 2.658 Prec@(1,5) (49.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:27:26] \u001b[32mTrain: [ 28/50] Step 100/520 Loss 2.668 Prec@(1,5) (49.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:27:27] \u001b[32mTrain: [ 28/50] Step 120/520 Loss 2.657 Prec@(1,5) (49.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:27:28] \u001b[32mTrain: [ 28/50] Step 140/520 Loss 2.653 Prec@(1,5) (49.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:27:29] \u001b[32mTrain: [ 28/50] Step 160/520 Loss 2.666 Prec@(1,5) (49.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:27:29] \u001b[32mTrain: [ 28/50] Step 180/520 Loss 2.668 Prec@(1,5) (49.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:27:30] \u001b[32mTrain: [ 28/50] Step 200/520 Loss 2.670 Prec@(1,5) (49.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:27:31] \u001b[32mTrain: [ 28/50] Step 220/520 Loss 2.670 Prec@(1,5) (49.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:27:32] \u001b[32mTrain: [ 28/50] Step 240/520 Loss 2.670 Prec@(1,5) (49.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:27:32] \u001b[32mTrain: [ 28/50] Step 260/520 Loss 2.677 Prec@(1,5) (49.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:27:33] \u001b[32mTrain: [ 28/50] Step 280/520 Loss 2.682 Prec@(1,5) (49.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:27:34] \u001b[32mTrain: [ 28/50] Step 300/520 Loss 2.685 Prec@(1,5) (49.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:27:35] \u001b[32mTrain: [ 28/50] Step 320/520 Loss 2.687 Prec@(1,5) (49.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:27:36] \u001b[32mTrain: [ 28/50] Step 340/520 Loss 2.687 Prec@(1,5) (49.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:27:36] \u001b[32mTrain: [ 28/50] Step 360/520 Loss 2.691 Prec@(1,5) (49.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:27:37] \u001b[32mTrain: [ 28/50] Step 380/520 Loss 2.693 Prec@(1,5) (49.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:27:38] \u001b[32mTrain: [ 28/50] Step 400/520 Loss 2.699 Prec@(1,5) (49.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:27:39] \u001b[32mTrain: [ 28/50] Step 420/520 Loss 2.704 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:27:40] \u001b[32mTrain: [ 28/50] Step 440/520 Loss 2.705 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:27:40] \u001b[32mTrain: [ 28/50] Step 460/520 Loss 2.704 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:27:41] \u001b[32mTrain: [ 28/50] Step 480/520 Loss 2.705 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:27:42] \u001b[32mTrain: [ 28/50] Step 500/520 Loss 2.706 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:27:43] \u001b[32mTrain: [ 28/50] Step 520/520 Loss 2.708 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:27:43] \u001b[32mTrain: [ 28/50] Final Prec@1 48.8840%\u001b[0m\n",
      "[2024-01-15 09:27:48] \u001b[32mValid: [ 28/50] Step 000/104 Loss 2.438 Prec@(1,5) (52.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:27:48] \u001b[32mValid: [ 28/50] Step 020/104 Loss 2.362 Prec@(1,5) (47.8%, 79.1%)\u001b[0m\n",
      "[2024-01-15 09:27:48] \u001b[32mValid: [ 28/50] Step 040/104 Loss 2.340 Prec@(1,5) (47.8%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:27:49] \u001b[32mValid: [ 28/50] Step 060/104 Loss 2.317 Prec@(1,5) (47.9%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:27:49] \u001b[32mValid: [ 28/50] Step 080/104 Loss 2.329 Prec@(1,5) (47.6%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:27:49] \u001b[32mValid: [ 28/50] Step 100/104 Loss 2.327 Prec@(1,5) (47.9%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:27:49] \u001b[32mValid: [ 28/50] Step 104/104 Loss 2.330 Prec@(1,5) (47.8%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:27:50] \u001b[32mValid: [ 28/50] Final Prec@1 47.8000%\u001b[0m\n",
      "[2024-01-15 09:27:50] \u001b[32mEpoch 28 LR 0.010158\u001b[0m\n",
      "[2024-01-15 09:27:58] \u001b[32mTrain: [ 29/50] Step 000/520 Loss 2.583 Prec@(1,5) (52.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:27:58] \u001b[32mTrain: [ 29/50] Step 020/520 Loss 2.690 Prec@(1,5) (49.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:27:59] \u001b[32mTrain: [ 29/50] Step 040/520 Loss 2.674 Prec@(1,5) (49.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:28:00] \u001b[32mTrain: [ 29/50] Step 060/520 Loss 2.670 Prec@(1,5) (49.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:28:01] \u001b[32mTrain: [ 29/50] Step 080/520 Loss 2.657 Prec@(1,5) (49.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:28:01] \u001b[32mTrain: [ 29/50] Step 100/520 Loss 2.677 Prec@(1,5) (49.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:28:02] \u001b[32mTrain: [ 29/50] Step 120/520 Loss 2.680 Prec@(1,5) (48.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:28:03] \u001b[32mTrain: [ 29/50] Step 140/520 Loss 2.683 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:28:04] \u001b[32mTrain: [ 29/50] Step 160/520 Loss 2.669 Prec@(1,5) (49.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:28:04] \u001b[32mTrain: [ 29/50] Step 180/520 Loss 2.672 Prec@(1,5) (49.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:28:05] \u001b[32mTrain: [ 29/50] Step 200/520 Loss 2.679 Prec@(1,5) (49.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:28:06] \u001b[32mTrain: [ 29/50] Step 220/520 Loss 2.680 Prec@(1,5) (49.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:28:07] \u001b[32mTrain: [ 29/50] Step 240/520 Loss 2.673 Prec@(1,5) (49.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:28:08] \u001b[32mTrain: [ 29/50] Step 260/520 Loss 2.673 Prec@(1,5) (49.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:28:08] \u001b[32mTrain: [ 29/50] Step 280/520 Loss 2.674 Prec@(1,5) (49.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:28:09] \u001b[32mTrain: [ 29/50] Step 300/520 Loss 2.671 Prec@(1,5) (49.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:28:10] \u001b[32mTrain: [ 29/50] Step 320/520 Loss 2.674 Prec@(1,5) (49.2%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:28:11] \u001b[32mTrain: [ 29/50] Step 340/520 Loss 2.678 Prec@(1,5) (49.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:28:11] \u001b[32mTrain: [ 29/50] Step 360/520 Loss 2.687 Prec@(1,5) (49.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:28:12] \u001b[32mTrain: [ 29/50] Step 380/520 Loss 2.688 Prec@(1,5) (48.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:28:13] \u001b[32mTrain: [ 29/50] Step 400/520 Loss 2.694 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:28:14] \u001b[32mTrain: [ 29/50] Step 420/520 Loss 2.693 Prec@(1,5) (48.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:28:14] \u001b[32mTrain: [ 29/50] Step 440/520 Loss 2.695 Prec@(1,5) (48.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:28:15] \u001b[32mTrain: [ 29/50] Step 460/520 Loss 2.699 Prec@(1,5) (48.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:28:16] \u001b[32mTrain: [ 29/50] Step 480/520 Loss 2.700 Prec@(1,5) (48.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:28:17] \u001b[32mTrain: [ 29/50] Step 500/520 Loss 2.703 Prec@(1,5) (48.8%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:28:17] \u001b[32mTrain: [ 29/50] Step 520/520 Loss 2.707 Prec@(1,5) (48.8%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:28:18] \u001b[32mTrain: [ 29/50] Final Prec@1 48.7520%\u001b[0m\n",
      "[2024-01-15 09:28:23] \u001b[32mValid: [ 29/50] Step 000/104 Loss 2.593 Prec@(1,5) (50.0%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:28:23] \u001b[32mValid: [ 29/50] Step 020/104 Loss 2.319 Prec@(1,5) (49.6%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:28:23] \u001b[32mValid: [ 29/50] Step 040/104 Loss 2.281 Prec@(1,5) (49.4%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:28:24] \u001b[32mValid: [ 29/50] Step 060/104 Loss 2.238 Prec@(1,5) (49.7%, 80.3%)\u001b[0m\n",
      "[2024-01-15 09:28:24] \u001b[32mValid: [ 29/50] Step 080/104 Loss 2.252 Prec@(1,5) (49.2%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:28:24] \u001b[32mValid: [ 29/50] Step 100/104 Loss 2.243 Prec@(1,5) (49.3%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:28:24] \u001b[32mValid: [ 29/50] Step 104/104 Loss 2.242 Prec@(1,5) (49.4%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:28:25] \u001b[32mValid: [ 29/50] Final Prec@1 49.3900%\u001b[0m\n",
      "[2024-01-15 09:28:25] \u001b[32mEpoch 29 LR 0.009392\u001b[0m\n",
      "[2024-01-15 09:28:33] \u001b[32mTrain: [ 30/50] Step 000/520 Loss 2.921 Prec@(1,5) (41.7%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:28:33] \u001b[32mTrain: [ 30/50] Step 020/520 Loss 2.653 Prec@(1,5) (48.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:28:34] \u001b[32mTrain: [ 30/50] Step 040/520 Loss 2.664 Prec@(1,5) (48.8%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:28:35] \u001b[32mTrain: [ 30/50] Step 060/520 Loss 2.645 Prec@(1,5) (49.5%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:28:36] \u001b[32mTrain: [ 30/50] Step 080/520 Loss 2.654 Prec@(1,5) (49.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:28:36] \u001b[32mTrain: [ 30/50] Step 100/520 Loss 2.659 Prec@(1,5) (49.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:28:37] \u001b[32mTrain: [ 30/50] Step 120/520 Loss 2.650 Prec@(1,5) (49.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:28:38] \u001b[32mTrain: [ 30/50] Step 140/520 Loss 2.648 Prec@(1,5) (49.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:28:39] \u001b[32mTrain: [ 30/50] Step 160/520 Loss 2.654 Prec@(1,5) (49.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:28:39] \u001b[32mTrain: [ 30/50] Step 180/520 Loss 2.655 Prec@(1,5) (49.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:28:40] \u001b[32mTrain: [ 30/50] Step 200/520 Loss 2.649 Prec@(1,5) (50.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:28:41] \u001b[32mTrain: [ 30/50] Step 220/520 Loss 2.654 Prec@(1,5) (49.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:28:42] \u001b[32mTrain: [ 30/50] Step 240/520 Loss 2.653 Prec@(1,5) (50.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:28:42] \u001b[32mTrain: [ 30/50] Step 260/520 Loss 2.657 Prec@(1,5) (49.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:28:43] \u001b[32mTrain: [ 30/50] Step 280/520 Loss 2.653 Prec@(1,5) (49.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:28:44] \u001b[32mTrain: [ 30/50] Step 300/520 Loss 2.658 Prec@(1,5) (49.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:28:45] \u001b[32mTrain: [ 30/50] Step 320/520 Loss 2.661 Prec@(1,5) (49.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:28:45] \u001b[32mTrain: [ 30/50] Step 340/520 Loss 2.659 Prec@(1,5) (49.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:28:46] \u001b[32mTrain: [ 30/50] Step 360/520 Loss 2.663 Prec@(1,5) (49.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:28:47] \u001b[32mTrain: [ 30/50] Step 380/520 Loss 2.664 Prec@(1,5) (49.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:28:48] \u001b[32mTrain: [ 30/50] Step 400/520 Loss 2.665 Prec@(1,5) (49.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:28:49] \u001b[32mTrain: [ 30/50] Step 420/520 Loss 2.671 Prec@(1,5) (49.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:28:49] \u001b[32mTrain: [ 30/50] Step 440/520 Loss 2.671 Prec@(1,5) (49.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:28:50] \u001b[32mTrain: [ 30/50] Step 460/520 Loss 2.671 Prec@(1,5) (49.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:28:51] \u001b[32mTrain: [ 30/50] Step 480/520 Loss 2.671 Prec@(1,5) (49.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:28:52] \u001b[32mTrain: [ 30/50] Step 500/520 Loss 2.668 Prec@(1,5) (49.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:28:52] \u001b[32mTrain: [ 30/50] Step 520/520 Loss 2.667 Prec@(1,5) (49.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:28:53] \u001b[32mTrain: [ 30/50] Final Prec@1 49.7340%\u001b[0m\n",
      "[2024-01-15 09:28:57] \u001b[32mValid: [ 30/50] Step 000/104 Loss 2.521 Prec@(1,5) (52.1%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:28:58] \u001b[32mValid: [ 30/50] Step 020/104 Loss 2.292 Prec@(1,5) (48.8%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:28:58] \u001b[32mValid: [ 30/50] Step 040/104 Loss 2.272 Prec@(1,5) (48.8%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:28:58] \u001b[32mValid: [ 30/50] Step 060/104 Loss 2.233 Prec@(1,5) (49.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:28:59] \u001b[32mValid: [ 30/50] Step 080/104 Loss 2.260 Prec@(1,5) (48.8%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:28:59] \u001b[32mValid: [ 30/50] Step 100/104 Loss 2.246 Prec@(1,5) (48.9%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:28:59] \u001b[32mValid: [ 30/50] Step 104/104 Loss 2.248 Prec@(1,5) (48.9%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:28:59] \u001b[32mValid: [ 30/50] Final Prec@1 48.9300%\u001b[0m\n",
      "[2024-01-15 09:28:59] \u001b[32mEpoch 30 LR 0.008638\u001b[0m\n",
      "[2024-01-15 09:29:07] \u001b[32mTrain: [ 31/50] Step 000/520 Loss 2.776 Prec@(1,5) (53.1%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:29:08] \u001b[32mTrain: [ 31/50] Step 020/520 Loss 2.693 Prec@(1,5) (48.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:29:09] \u001b[32mTrain: [ 31/50] Step 040/520 Loss 2.668 Prec@(1,5) (49.3%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:29:10] \u001b[32mTrain: [ 31/50] Step 060/520 Loss 2.608 Prec@(1,5) (50.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:10] \u001b[32mTrain: [ 31/50] Step 080/520 Loss 2.616 Prec@(1,5) (50.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:29:11] \u001b[32mTrain: [ 31/50] Step 100/520 Loss 2.608 Prec@(1,5) (50.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:29:12] \u001b[32mTrain: [ 31/50] Step 120/520 Loss 2.612 Prec@(1,5) (50.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:29:13] \u001b[32mTrain: [ 31/50] Step 140/520 Loss 2.629 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:29:13] \u001b[32mTrain: [ 31/50] Step 160/520 Loss 2.639 Prec@(1,5) (50.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:14] \u001b[32mTrain: [ 31/50] Step 180/520 Loss 2.652 Prec@(1,5) (49.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:29:15] \u001b[32mTrain: [ 31/50] Step 200/520 Loss 2.654 Prec@(1,5) (49.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:16] \u001b[32mTrain: [ 31/50] Step 220/520 Loss 2.656 Prec@(1,5) (49.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:16] \u001b[32mTrain: [ 31/50] Step 240/520 Loss 2.654 Prec@(1,5) (49.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:17] \u001b[32mTrain: [ 31/50] Step 260/520 Loss 2.655 Prec@(1,5) (49.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:18] \u001b[32mTrain: [ 31/50] Step 280/520 Loss 2.655 Prec@(1,5) (49.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:19] \u001b[32mTrain: [ 31/50] Step 300/520 Loss 2.650 Prec@(1,5) (49.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:19] \u001b[32mTrain: [ 31/50] Step 320/520 Loss 2.650 Prec@(1,5) (49.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:29:20] \u001b[32mTrain: [ 31/50] Step 340/520 Loss 2.653 Prec@(1,5) (49.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:29:21] \u001b[32mTrain: [ 31/50] Step 360/520 Loss 2.661 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:22] \u001b[32mTrain: [ 31/50] Step 380/520 Loss 2.658 Prec@(1,5) (49.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:23] \u001b[32mTrain: [ 31/50] Step 400/520 Loss 2.659 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:23] \u001b[32mTrain: [ 31/50] Step 420/520 Loss 2.662 Prec@(1,5) (49.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:24] \u001b[32mTrain: [ 31/50] Step 440/520 Loss 2.665 Prec@(1,5) (49.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:25] \u001b[32mTrain: [ 31/50] Step 460/520 Loss 2.662 Prec@(1,5) (49.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:26] \u001b[32mTrain: [ 31/50] Step 480/520 Loss 2.661 Prec@(1,5) (49.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:26] \u001b[32mTrain: [ 31/50] Step 500/520 Loss 2.667 Prec@(1,5) (49.5%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:27] \u001b[32mTrain: [ 31/50] Step 520/520 Loss 2.669 Prec@(1,5) (49.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:29:27] \u001b[32mTrain: [ 31/50] Final Prec@1 49.4820%\u001b[0m\n",
      "[2024-01-15 09:29:32] \u001b[32mValid: [ 31/50] Step 000/104 Loss 2.240 Prec@(1,5) (52.1%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:29:33] \u001b[32mValid: [ 31/50] Step 020/104 Loss 2.224 Prec@(1,5) (51.2%, 80.3%)\u001b[0m\n",
      "[2024-01-15 09:29:33] \u001b[32mValid: [ 31/50] Step 040/104 Loss 2.184 Prec@(1,5) (51.0%, 80.8%)\u001b[0m\n",
      "[2024-01-15 09:29:33] \u001b[32mValid: [ 31/50] Step 060/104 Loss 2.154 Prec@(1,5) (51.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:29:33] \u001b[32mValid: [ 31/50] Step 080/104 Loss 2.170 Prec@(1,5) (50.5%, 81.3%)\u001b[0m\n",
      "[2024-01-15 09:29:34] \u001b[32mValid: [ 31/50] Step 100/104 Loss 2.177 Prec@(1,5) (50.4%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:29:34] \u001b[32mValid: [ 31/50] Step 104/104 Loss 2.174 Prec@(1,5) (50.4%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:29:34] \u001b[32mValid: [ 31/50] Final Prec@1 50.4200%\u001b[0m\n",
      "[2024-01-15 09:29:34] \u001b[32mEpoch 31 LR 0.007899\u001b[0m\n",
      "[2024-01-15 09:29:42] \u001b[32mTrain: [ 32/50] Step 000/520 Loss 2.391 Prec@(1,5) (58.3%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:29:43] \u001b[32mTrain: [ 32/50] Step 020/520 Loss 2.604 Prec@(1,5) (51.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:29:44] \u001b[32mTrain: [ 32/50] Step 040/520 Loss 2.617 Prec@(1,5) (51.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:29:44] \u001b[32mTrain: [ 32/50] Step 060/520 Loss 2.619 Prec@(1,5) (51.1%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:29:45] \u001b[32mTrain: [ 32/50] Step 080/520 Loss 2.615 Prec@(1,5) (51.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:29:46] \u001b[32mTrain: [ 32/50] Step 100/520 Loss 2.617 Prec@(1,5) (51.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:29:47] \u001b[32mTrain: [ 32/50] Step 120/520 Loss 2.619 Prec@(1,5) (50.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:29:47] \u001b[32mTrain: [ 32/50] Step 140/520 Loss 2.615 Prec@(1,5) (50.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:29:48] \u001b[32mTrain: [ 32/50] Step 160/520 Loss 2.634 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:29:49] \u001b[32mTrain: [ 32/50] Step 180/520 Loss 2.650 Prec@(1,5) (50.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:50] \u001b[32mTrain: [ 32/50] Step 200/520 Loss 2.648 Prec@(1,5) (50.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:50] \u001b[32mTrain: [ 32/50] Step 220/520 Loss 2.651 Prec@(1,5) (50.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:51] \u001b[32mTrain: [ 32/50] Step 240/520 Loss 2.656 Prec@(1,5) (49.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:52] \u001b[32mTrain: [ 32/50] Step 260/520 Loss 2.660 Prec@(1,5) (49.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:29:53] \u001b[32mTrain: [ 32/50] Step 280/520 Loss 2.657 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:53] \u001b[32mTrain: [ 32/50] Step 300/520 Loss 2.658 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:54] \u001b[32mTrain: [ 32/50] Step 320/520 Loss 2.660 Prec@(1,5) (49.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:55] \u001b[32mTrain: [ 32/50] Step 340/520 Loss 2.663 Prec@(1,5) (49.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:56] \u001b[32mTrain: [ 32/50] Step 360/520 Loss 2.663 Prec@(1,5) (49.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:57] \u001b[32mTrain: [ 32/50] Step 380/520 Loss 2.658 Prec@(1,5) (49.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:57] \u001b[32mTrain: [ 32/50] Step 400/520 Loss 2.659 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:58] \u001b[32mTrain: [ 32/50] Step 420/520 Loss 2.663 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:29:59] \u001b[32mTrain: [ 32/50] Step 440/520 Loss 2.667 Prec@(1,5) (49.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:30:00] \u001b[32mTrain: [ 32/50] Step 460/520 Loss 2.670 Prec@(1,5) (49.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:30:00] \u001b[32mTrain: [ 32/50] Step 480/520 Loss 2.668 Prec@(1,5) (49.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:30:01] \u001b[32mTrain: [ 32/50] Step 500/520 Loss 2.667 Prec@(1,5) (49.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:30:02] \u001b[32mTrain: [ 32/50] Step 520/520 Loss 2.666 Prec@(1,5) (49.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:02] \u001b[32mTrain: [ 32/50] Final Prec@1 49.6940%\u001b[0m\n",
      "[2024-01-15 09:30:07] \u001b[32mValid: [ 32/50] Step 000/104 Loss 2.399 Prec@(1,5) (53.1%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:30:07] \u001b[32mValid: [ 32/50] Step 020/104 Loss 2.275 Prec@(1,5) (49.7%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:30:08] \u001b[32mValid: [ 32/50] Step 040/104 Loss 2.251 Prec@(1,5) (49.5%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:30:08] \u001b[32mValid: [ 32/50] Step 060/104 Loss 2.207 Prec@(1,5) (49.9%, 80.7%)\u001b[0m\n",
      "[2024-01-15 09:30:08] \u001b[32mValid: [ 32/50] Step 080/104 Loss 2.214 Prec@(1,5) (49.7%, 80.9%)\u001b[0m\n",
      "[2024-01-15 09:30:09] \u001b[32mValid: [ 32/50] Step 100/104 Loss 2.199 Prec@(1,5) (49.9%, 80.9%)\u001b[0m\n",
      "[2024-01-15 09:30:09] \u001b[32mValid: [ 32/50] Step 104/104 Loss 2.198 Prec@(1,5) (49.8%, 81.0%)\u001b[0m\n",
      "[2024-01-15 09:30:09] \u001b[32mValid: [ 32/50] Final Prec@1 49.8000%\u001b[0m\n",
      "[2024-01-15 09:30:09] \u001b[32mEpoch 32 LR 0.007178\u001b[0m\n",
      "[2024-01-15 09:30:17] \u001b[32mTrain: [ 33/50] Step 000/520 Loss 2.577 Prec@(1,5) (51.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:30:18] \u001b[32mTrain: [ 33/50] Step 020/520 Loss 2.686 Prec@(1,5) (48.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:30:18] \u001b[32mTrain: [ 33/50] Step 040/520 Loss 2.649 Prec@(1,5) (50.0%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:30:19] \u001b[32mTrain: [ 33/50] Step 060/520 Loss 2.605 Prec@(1,5) (51.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:20] \u001b[32mTrain: [ 33/50] Step 080/520 Loss 2.606 Prec@(1,5) (51.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:30:21] \u001b[32mTrain: [ 33/50] Step 100/520 Loss 2.612 Prec@(1,5) (51.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:21] \u001b[32mTrain: [ 33/50] Step 120/520 Loss 2.625 Prec@(1,5) (51.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:30:22] \u001b[32mTrain: [ 33/50] Step 140/520 Loss 2.618 Prec@(1,5) (51.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:23] \u001b[32mTrain: [ 33/50] Step 160/520 Loss 2.617 Prec@(1,5) (51.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:24] \u001b[32mTrain: [ 33/50] Step 180/520 Loss 2.613 Prec@(1,5) (51.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:30:24] \u001b[32mTrain: [ 33/50] Step 200/520 Loss 2.622 Prec@(1,5) (51.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:25] \u001b[32mTrain: [ 33/50] Step 220/520 Loss 2.626 Prec@(1,5) (51.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:26] \u001b[32mTrain: [ 33/50] Step 240/520 Loss 2.627 Prec@(1,5) (51.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:30:27] \u001b[32mTrain: [ 33/50] Step 260/520 Loss 2.622 Prec@(1,5) (51.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:28] \u001b[32mTrain: [ 33/50] Step 280/520 Loss 2.622 Prec@(1,5) (50.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:28] \u001b[32mTrain: [ 33/50] Step 300/520 Loss 2.629 Prec@(1,5) (50.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:29] \u001b[32mTrain: [ 33/50] Step 320/520 Loss 2.629 Prec@(1,5) (50.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:30] \u001b[32mTrain: [ 33/50] Step 340/520 Loss 2.631 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:31] \u001b[32mTrain: [ 33/50] Step 360/520 Loss 2.630 Prec@(1,5) (50.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:31] \u001b[32mTrain: [ 33/50] Step 380/520 Loss 2.633 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:32] \u001b[32mTrain: [ 33/50] Step 400/520 Loss 2.636 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:33] \u001b[32mTrain: [ 33/50] Step 420/520 Loss 2.635 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:34] \u001b[32mTrain: [ 33/50] Step 440/520 Loss 2.637 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:35] \u001b[32mTrain: [ 33/50] Step 460/520 Loss 2.636 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:35] \u001b[32mTrain: [ 33/50] Step 480/520 Loss 2.637 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:36] \u001b[32mTrain: [ 33/50] Step 500/520 Loss 2.637 Prec@(1,5) (50.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:37] \u001b[32mTrain: [ 33/50] Step 520/520 Loss 2.636 Prec@(1,5) (50.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:37] \u001b[32mTrain: [ 33/50] Final Prec@1 50.6440%\u001b[0m\n",
      "[2024-01-15 09:30:42] \u001b[32mValid: [ 33/50] Step 000/104 Loss 2.441 Prec@(1,5) (54.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:30:42] \u001b[32mValid: [ 33/50] Step 020/104 Loss 2.209 Prec@(1,5) (52.1%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:30:43] \u001b[32mValid: [ 33/50] Step 040/104 Loss 2.195 Prec@(1,5) (50.7%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:30:43] \u001b[32mValid: [ 33/50] Step 060/104 Loss 2.176 Prec@(1,5) (50.8%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:30:43] \u001b[32mValid: [ 33/50] Step 080/104 Loss 2.191 Prec@(1,5) (50.6%, 81.3%)\u001b[0m\n",
      "[2024-01-15 09:30:44] \u001b[32mValid: [ 33/50] Step 100/104 Loss 2.179 Prec@(1,5) (50.7%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:30:44] \u001b[32mValid: [ 33/50] Step 104/104 Loss 2.182 Prec@(1,5) (50.6%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:30:44] \u001b[32mValid: [ 33/50] Final Prec@1 50.5900%\u001b[0m\n",
      "[2024-01-15 09:30:44] \u001b[32mEpoch 33 LR 0.006479\u001b[0m\n",
      "[2024-01-15 09:30:52] \u001b[32mTrain: [ 34/50] Step 000/520 Loss 2.296 Prec@(1,5) (52.1%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:30:53] \u001b[32mTrain: [ 34/50] Step 020/520 Loss 2.529 Prec@(1,5) (52.5%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:30:54] \u001b[32mTrain: [ 34/50] Step 040/520 Loss 2.599 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:54] \u001b[32mTrain: [ 34/50] Step 060/520 Loss 2.630 Prec@(1,5) (50.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:30:55] \u001b[32mTrain: [ 34/50] Step 080/520 Loss 2.635 Prec@(1,5) (50.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:56] \u001b[32mTrain: [ 34/50] Step 100/520 Loss 2.634 Prec@(1,5) (50.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:30:57] \u001b[32mTrain: [ 34/50] Step 120/520 Loss 2.628 Prec@(1,5) (50.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:30:57] \u001b[32mTrain: [ 34/50] Step 140/520 Loss 2.623 Prec@(1,5) (50.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:30:58] \u001b[32mTrain: [ 34/50] Step 160/520 Loss 2.621 Prec@(1,5) (50.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:30:59] \u001b[32mTrain: [ 34/50] Step 180/520 Loss 2.620 Prec@(1,5) (50.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:31:00] \u001b[32mTrain: [ 34/50] Step 200/520 Loss 2.625 Prec@(1,5) (50.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:00] \u001b[32mTrain: [ 34/50] Step 220/520 Loss 2.629 Prec@(1,5) (50.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:01] \u001b[32mTrain: [ 34/50] Step 240/520 Loss 2.636 Prec@(1,5) (50.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:02] \u001b[32mTrain: [ 34/50] Step 260/520 Loss 2.628 Prec@(1,5) (50.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:31:03] \u001b[32mTrain: [ 34/50] Step 280/520 Loss 2.621 Prec@(1,5) (50.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:31:04] \u001b[32mTrain: [ 34/50] Step 300/520 Loss 2.627 Prec@(1,5) (50.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:31:04] \u001b[32mTrain: [ 34/50] Step 320/520 Loss 2.627 Prec@(1,5) (50.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:31:05] \u001b[32mTrain: [ 34/50] Step 340/520 Loss 2.626 Prec@(1,5) (50.4%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:31:06] \u001b[32mTrain: [ 34/50] Step 360/520 Loss 2.630 Prec@(1,5) (50.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:07] \u001b[32mTrain: [ 34/50] Step 380/520 Loss 2.630 Prec@(1,5) (50.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:08] \u001b[32mTrain: [ 34/50] Step 400/520 Loss 2.629 Prec@(1,5) (50.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:08] \u001b[32mTrain: [ 34/50] Step 420/520 Loss 2.631 Prec@(1,5) (50.3%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:09] \u001b[32mTrain: [ 34/50] Step 440/520 Loss 2.636 Prec@(1,5) (50.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:10] \u001b[32mTrain: [ 34/50] Step 460/520 Loss 2.642 Prec@(1,5) (50.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:11] \u001b[32mTrain: [ 34/50] Step 480/520 Loss 2.643 Prec@(1,5) (50.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:12] \u001b[32mTrain: [ 34/50] Step 500/520 Loss 2.641 Prec@(1,5) (50.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:12] \u001b[32mTrain: [ 34/50] Step 520/520 Loss 2.638 Prec@(1,5) (50.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:13] \u001b[32mTrain: [ 34/50] Final Prec@1 50.2280%\u001b[0m\n",
      "[2024-01-15 09:31:17] \u001b[32mValid: [ 34/50] Step 000/104 Loss 2.354 Prec@(1,5) (47.9%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:31:18] \u001b[32mValid: [ 34/50] Step 020/104 Loss 2.261 Prec@(1,5) (49.7%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:31:18] \u001b[32mValid: [ 34/50] Step 040/104 Loss 2.218 Prec@(1,5) (49.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:31:18] \u001b[32mValid: [ 34/50] Step 060/104 Loss 2.186 Prec@(1,5) (50.6%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:31:19] \u001b[32mValid: [ 34/50] Step 080/104 Loss 2.195 Prec@(1,5) (50.2%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:31:19] \u001b[32mValid: [ 34/50] Step 100/104 Loss 2.184 Prec@(1,5) (50.3%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:31:19] \u001b[32mValid: [ 34/50] Step 104/104 Loss 2.187 Prec@(1,5) (50.3%, 80.7%)\u001b[0m\n",
      "[2024-01-15 09:31:19] \u001b[32mValid: [ 34/50] Final Prec@1 50.3400%\u001b[0m\n",
      "[2024-01-15 09:31:19] \u001b[32mEpoch 34 LR 0.005803\u001b[0m\n",
      "[2024-01-15 09:31:28] \u001b[32mTrain: [ 35/50] Step 000/520 Loss 3.125 Prec@(1,5) (40.6%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:31:28] \u001b[32mTrain: [ 35/50] Step 020/520 Loss 2.590 Prec@(1,5) (50.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:31:29] \u001b[32mTrain: [ 35/50] Step 040/520 Loss 2.593 Prec@(1,5) (50.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:31:30] \u001b[32mTrain: [ 35/50] Step 060/520 Loss 2.582 Prec@(1,5) (50.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:31:31] \u001b[32mTrain: [ 35/50] Step 080/520 Loss 2.578 Prec@(1,5) (50.6%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:31:31] \u001b[32mTrain: [ 35/50] Step 100/520 Loss 2.577 Prec@(1,5) (50.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:31:32] \u001b[32mTrain: [ 35/50] Step 120/520 Loss 2.602 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:33] \u001b[32mTrain: [ 35/50] Step 140/520 Loss 2.614 Prec@(1,5) (50.3%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:31:34] \u001b[32mTrain: [ 35/50] Step 160/520 Loss 2.616 Prec@(1,5) (50.4%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:31:34] \u001b[32mTrain: [ 35/50] Step 180/520 Loss 2.624 Prec@(1,5) (50.2%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:31:35] \u001b[32mTrain: [ 35/50] Step 200/520 Loss 2.623 Prec@(1,5) (50.4%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:31:36] \u001b[32mTrain: [ 35/50] Step 220/520 Loss 2.616 Prec@(1,5) (50.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:31:37] \u001b[32mTrain: [ 35/50] Step 240/520 Loss 2.615 Prec@(1,5) (50.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:31:38] \u001b[32mTrain: [ 35/50] Step 260/520 Loss 2.613 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:38] \u001b[32mTrain: [ 35/50] Step 280/520 Loss 2.610 Prec@(1,5) (50.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:39] \u001b[32mTrain: [ 35/50] Step 300/520 Loss 2.617 Prec@(1,5) (50.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:40] \u001b[32mTrain: [ 35/50] Step 320/520 Loss 2.619 Prec@(1,5) (50.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:41] \u001b[32mTrain: [ 35/50] Step 340/520 Loss 2.617 Prec@(1,5) (50.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:41] \u001b[32mTrain: [ 35/50] Step 360/520 Loss 2.618 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:42] \u001b[32mTrain: [ 35/50] Step 380/520 Loss 2.620 Prec@(1,5) (50.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:43] \u001b[32mTrain: [ 35/50] Step 400/520 Loss 2.619 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:44] \u001b[32mTrain: [ 35/50] Step 420/520 Loss 2.620 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:45] \u001b[32mTrain: [ 35/50] Step 440/520 Loss 2.622 Prec@(1,5) (50.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:45] \u001b[32mTrain: [ 35/50] Step 460/520 Loss 2.624 Prec@(1,5) (50.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:46] \u001b[32mTrain: [ 35/50] Step 480/520 Loss 2.624 Prec@(1,5) (50.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:47] \u001b[32mTrain: [ 35/50] Step 500/520 Loss 2.625 Prec@(1,5) (50.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:31:48] \u001b[32mTrain: [ 35/50] Step 520/520 Loss 2.624 Prec@(1,5) (50.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:31:48] \u001b[32mTrain: [ 35/50] Final Prec@1 50.3360%\u001b[0m\n",
      "[2024-01-15 09:31:53] \u001b[32mValid: [ 35/50] Step 000/104 Loss 2.284 Prec@(1,5) (53.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:31:53] \u001b[32mValid: [ 35/50] Step 020/104 Loss 2.190 Prec@(1,5) (51.7%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:31:53] \u001b[32mValid: [ 35/50] Step 040/104 Loss 2.146 Prec@(1,5) (51.5%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:31:54] \u001b[32mValid: [ 35/50] Step 060/104 Loss 2.114 Prec@(1,5) (51.8%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:31:54] \u001b[32mValid: [ 35/50] Step 080/104 Loss 2.123 Prec@(1,5) (51.6%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:31:54] \u001b[32mValid: [ 35/50] Step 100/104 Loss 2.107 Prec@(1,5) (51.7%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:31:54] \u001b[32mValid: [ 35/50] Step 104/104 Loss 2.109 Prec@(1,5) (51.7%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:31:55] \u001b[32mValid: [ 35/50] Final Prec@1 51.6600%\u001b[0m\n",
      "[2024-01-15 09:31:55] \u001b[32mEpoch 35 LR 0.005153\u001b[0m\n",
      "[2024-01-15 09:32:03] \u001b[32mTrain: [ 36/50] Step 000/520 Loss 2.687 Prec@(1,5) (52.1%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:32:03] \u001b[32mTrain: [ 36/50] Step 020/520 Loss 2.587 Prec@(1,5) (50.7%, 78.5%)\u001b[0m\n",
      "[2024-01-15 09:32:04] \u001b[32mTrain: [ 36/50] Step 040/520 Loss 2.566 Prec@(1,5) (51.8%, 77.8%)\u001b[0m\n",
      "[2024-01-15 09:32:05] \u001b[32mTrain: [ 36/50] Step 060/520 Loss 2.562 Prec@(1,5) (51.9%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:32:06] \u001b[32mTrain: [ 36/50] Step 080/520 Loss 2.561 Prec@(1,5) (51.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:32:06] \u001b[32mTrain: [ 36/50] Step 100/520 Loss 2.563 Prec@(1,5) (52.0%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:32:07] \u001b[32mTrain: [ 36/50] Step 120/520 Loss 2.574 Prec@(1,5) (51.8%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:32:08] \u001b[32mTrain: [ 36/50] Step 140/520 Loss 2.587 Prec@(1,5) (51.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:32:09] \u001b[32mTrain: [ 36/50] Step 160/520 Loss 2.583 Prec@(1,5) (51.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:32:09] \u001b[32mTrain: [ 36/50] Step 180/520 Loss 2.592 Prec@(1,5) (51.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:32:10] \u001b[32mTrain: [ 36/50] Step 200/520 Loss 2.594 Prec@(1,5) (51.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:32:11] \u001b[32mTrain: [ 36/50] Step 220/520 Loss 2.597 Prec@(1,5) (51.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:32:12] \u001b[32mTrain: [ 36/50] Step 240/520 Loss 2.596 Prec@(1,5) (51.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:32:12] \u001b[32mTrain: [ 36/50] Step 260/520 Loss 2.602 Prec@(1,5) (51.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:13] \u001b[32mTrain: [ 36/50] Step 280/520 Loss 2.596 Prec@(1,5) (51.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:32:14] \u001b[32mTrain: [ 36/50] Step 300/520 Loss 2.599 Prec@(1,5) (51.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:32:15] \u001b[32mTrain: [ 36/50] Step 320/520 Loss 2.607 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:16] \u001b[32mTrain: [ 36/50] Step 340/520 Loss 2.608 Prec@(1,5) (50.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:16] \u001b[32mTrain: [ 36/50] Step 360/520 Loss 2.610 Prec@(1,5) (50.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:17] \u001b[32mTrain: [ 36/50] Step 380/520 Loss 2.610 Prec@(1,5) (50.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:18] \u001b[32mTrain: [ 36/50] Step 400/520 Loss 2.607 Prec@(1,5) (50.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:19] \u001b[32mTrain: [ 36/50] Step 420/520 Loss 2.612 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:20] \u001b[32mTrain: [ 36/50] Step 440/520 Loss 2.615 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:20] \u001b[32mTrain: [ 36/50] Step 460/520 Loss 2.616 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:21] \u001b[32mTrain: [ 36/50] Step 480/520 Loss 2.616 Prec@(1,5) (50.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:32:22] \u001b[32mTrain: [ 36/50] Step 500/520 Loss 2.615 Prec@(1,5) (50.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:32:23] \u001b[32mTrain: [ 36/50] Step 520/520 Loss 2.615 Prec@(1,5) (50.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:23] \u001b[32mTrain: [ 36/50] Final Prec@1 50.8540%\u001b[0m\n",
      "[2024-01-15 09:32:28] \u001b[32mValid: [ 36/50] Step 000/104 Loss 2.335 Prec@(1,5) (57.3%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:28] \u001b[32mValid: [ 36/50] Step 020/104 Loss 2.184 Prec@(1,5) (51.8%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:32:28] \u001b[32mValid: [ 36/50] Step 040/104 Loss 2.140 Prec@(1,5) (51.6%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:32:29] \u001b[32mValid: [ 36/50] Step 060/104 Loss 2.108 Prec@(1,5) (52.1%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:32:29] \u001b[32mValid: [ 36/50] Step 080/104 Loss 2.118 Prec@(1,5) (51.8%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:32:29] \u001b[32mValid: [ 36/50] Step 100/104 Loss 2.109 Prec@(1,5) (52.0%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:32:30] \u001b[32mValid: [ 36/50] Step 104/104 Loss 2.109 Prec@(1,5) (52.0%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:32:30] \u001b[32mValid: [ 36/50] Final Prec@1 52.0300%\u001b[0m\n",
      "[2024-01-15 09:32:30] \u001b[32mEpoch 36 LR 0.004533\u001b[0m\n",
      "[2024-01-15 09:32:38] \u001b[32mTrain: [ 37/50] Step 000/520 Loss 2.432 Prec@(1,5) (52.1%, 85.4%)\u001b[0m\n",
      "[2024-01-15 09:32:39] \u001b[32mTrain: [ 37/50] Step 020/520 Loss 2.612 Prec@(1,5) (50.3%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:32:39] \u001b[32mTrain: [ 37/50] Step 040/520 Loss 2.598 Prec@(1,5) (50.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:32:40] \u001b[32mTrain: [ 37/50] Step 060/520 Loss 2.617 Prec@(1,5) (50.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:32:41] \u001b[32mTrain: [ 37/50] Step 080/520 Loss 2.588 Prec@(1,5) (50.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:32:42] \u001b[32mTrain: [ 37/50] Step 100/520 Loss 2.575 Prec@(1,5) (51.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 09:32:42] \u001b[32mTrain: [ 37/50] Step 120/520 Loss 2.568 Prec@(1,5) (51.3%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:32:43] \u001b[32mTrain: [ 37/50] Step 140/520 Loss 2.582 Prec@(1,5) (51.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:32:44] \u001b[32mTrain: [ 37/50] Step 160/520 Loss 2.588 Prec@(1,5) (51.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:32:45] \u001b[32mTrain: [ 37/50] Step 180/520 Loss 2.594 Prec@(1,5) (51.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:32:45] \u001b[32mTrain: [ 37/50] Step 200/520 Loss 2.591 Prec@(1,5) (51.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:32:46] \u001b[32mTrain: [ 37/50] Step 220/520 Loss 2.593 Prec@(1,5) (51.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:32:47] \u001b[32mTrain: [ 37/50] Step 240/520 Loss 2.594 Prec@(1,5) (51.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:32:48] \u001b[32mTrain: [ 37/50] Step 260/520 Loss 2.600 Prec@(1,5) (50.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:49] \u001b[32mTrain: [ 37/50] Step 280/520 Loss 2.603 Prec@(1,5) (50.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:49] \u001b[32mTrain: [ 37/50] Step 300/520 Loss 2.600 Prec@(1,5) (50.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:32:50] \u001b[32mTrain: [ 37/50] Step 320/520 Loss 2.605 Prec@(1,5) (50.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:51] \u001b[32mTrain: [ 37/50] Step 340/520 Loss 2.610 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:52] \u001b[32mTrain: [ 37/50] Step 360/520 Loss 2.612 Prec@(1,5) (50.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:53] \u001b[32mTrain: [ 37/50] Step 380/520 Loss 2.610 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:53] \u001b[32mTrain: [ 37/50] Step 400/520 Loss 2.607 Prec@(1,5) (50.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:54] \u001b[32mTrain: [ 37/50] Step 420/520 Loss 2.605 Prec@(1,5) (50.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:32:55] \u001b[32mTrain: [ 37/50] Step 440/520 Loss 2.608 Prec@(1,5) (50.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:56] \u001b[32mTrain: [ 37/50] Step 460/520 Loss 2.610 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:56] \u001b[32mTrain: [ 37/50] Step 480/520 Loss 2.610 Prec@(1,5) (50.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:32:57] \u001b[32mTrain: [ 37/50] Step 500/520 Loss 2.611 Prec@(1,5) (50.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:32:58] \u001b[32mTrain: [ 37/50] Step 520/520 Loss 2.611 Prec@(1,5) (50.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:32:58] \u001b[32mTrain: [ 37/50] Final Prec@1 50.7860%\u001b[0m\n",
      "[2024-01-15 09:33:03] \u001b[32mValid: [ 37/50] Step 000/104 Loss 2.369 Prec@(1,5) (53.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:33:03] \u001b[32mValid: [ 37/50] Step 020/104 Loss 2.188 Prec@(1,5) (51.7%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:33:04] \u001b[32mValid: [ 37/50] Step 040/104 Loss 2.123 Prec@(1,5) (51.7%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:33:04] \u001b[32mValid: [ 37/50] Step 060/104 Loss 2.096 Prec@(1,5) (52.0%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:33:04] \u001b[32mValid: [ 37/50] Step 080/104 Loss 2.106 Prec@(1,5) (51.7%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:33:05] \u001b[32mValid: [ 37/50] Step 100/104 Loss 2.094 Prec@(1,5) (51.8%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:33:05] \u001b[32mValid: [ 37/50] Step 104/104 Loss 2.095 Prec@(1,5) (51.8%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:33:05] \u001b[32mValid: [ 37/50] Final Prec@1 51.8200%\u001b[0m\n",
      "[2024-01-15 09:33:05] \u001b[32mEpoch 37 LR 0.003944\u001b[0m\n",
      "[2024-01-15 09:33:13] \u001b[32mTrain: [ 38/50] Step 000/520 Loss 2.672 Prec@(1,5) (47.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:33:14] \u001b[32mTrain: [ 38/50] Step 020/520 Loss 2.620 Prec@(1,5) (50.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 09:33:15] \u001b[32mTrain: [ 38/50] Step 040/520 Loss 2.566 Prec@(1,5) (51.3%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:33:15] \u001b[32mTrain: [ 38/50] Step 060/520 Loss 2.590 Prec@(1,5) (50.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:16] \u001b[32mTrain: [ 38/50] Step 080/520 Loss 2.574 Prec@(1,5) (51.3%, 77.8%)\u001b[0m\n",
      "[2024-01-15 09:33:17] \u001b[32mTrain: [ 38/50] Step 100/520 Loss 2.560 Prec@(1,5) (51.6%, 77.8%)\u001b[0m\n",
      "[2024-01-15 09:33:18] \u001b[32mTrain: [ 38/50] Step 120/520 Loss 2.559 Prec@(1,5) (51.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:18] \u001b[32mTrain: [ 38/50] Step 140/520 Loss 2.558 Prec@(1,5) (51.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:19] \u001b[32mTrain: [ 38/50] Step 160/520 Loss 2.568 Prec@(1,5) (51.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:20] \u001b[32mTrain: [ 38/50] Step 180/520 Loss 2.564 Prec@(1,5) (51.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:21] \u001b[32mTrain: [ 38/50] Step 200/520 Loss 2.564 Prec@(1,5) (51.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:21] \u001b[32mTrain: [ 38/50] Step 220/520 Loss 2.569 Prec@(1,5) (51.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:22] \u001b[32mTrain: [ 38/50] Step 240/520 Loss 2.573 Prec@(1,5) (51.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:23] \u001b[32mTrain: [ 38/50] Step 260/520 Loss 2.572 Prec@(1,5) (51.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:24] \u001b[32mTrain: [ 38/50] Step 280/520 Loss 2.571 Prec@(1,5) (51.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:25] \u001b[32mTrain: [ 38/50] Step 300/520 Loss 2.566 Prec@(1,5) (51.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:25] \u001b[32mTrain: [ 38/50] Step 320/520 Loss 2.572 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:26] \u001b[32mTrain: [ 38/50] Step 340/520 Loss 2.571 Prec@(1,5) (51.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:27] \u001b[32mTrain: [ 38/50] Step 360/520 Loss 2.575 Prec@(1,5) (51.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:28] \u001b[32mTrain: [ 38/50] Step 380/520 Loss 2.580 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:28] \u001b[32mTrain: [ 38/50] Step 400/520 Loss 2.579 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:29] \u001b[32mTrain: [ 38/50] Step 420/520 Loss 2.580 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:30] \u001b[32mTrain: [ 38/50] Step 440/520 Loss 2.577 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:31] \u001b[32mTrain: [ 38/50] Step 460/520 Loss 2.584 Prec@(1,5) (51.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:32] \u001b[32mTrain: [ 38/50] Step 480/520 Loss 2.584 Prec@(1,5) (51.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:32] \u001b[32mTrain: [ 38/50] Step 500/520 Loss 2.587 Prec@(1,5) (51.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:33] \u001b[32mTrain: [ 38/50] Step 520/520 Loss 2.589 Prec@(1,5) (51.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:33] \u001b[32mTrain: [ 38/50] Final Prec@1 51.3080%\u001b[0m\n",
      "[2024-01-15 09:33:38] \u001b[32mValid: [ 38/50] Step 000/104 Loss 2.397 Prec@(1,5) (51.0%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:33:38] \u001b[32mValid: [ 38/50] Step 020/104 Loss 2.181 Prec@(1,5) (52.4%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:33:39] \u001b[32mValid: [ 38/50] Step 040/104 Loss 2.152 Prec@(1,5) (51.7%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:33:39] \u001b[32mValid: [ 38/50] Step 060/104 Loss 2.123 Prec@(1,5) (52.3%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:33:39] \u001b[32mValid: [ 38/50] Step 080/104 Loss 2.129 Prec@(1,5) (51.9%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:33:40] \u001b[32mValid: [ 38/50] Step 100/104 Loss 2.114 Prec@(1,5) (52.0%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:33:40] \u001b[32mValid: [ 38/50] Step 104/104 Loss 2.114 Prec@(1,5) (52.0%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:33:40] \u001b[32mValid: [ 38/50] Final Prec@1 52.0100%\u001b[0m\n",
      "[2024-01-15 09:33:40] \u001b[32mEpoch 38 LR 0.003389\u001b[0m\n",
      "[2024-01-15 09:33:48] \u001b[32mTrain: [ 39/50] Step 000/520 Loss 2.669 Prec@(1,5) (53.1%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:33:49] \u001b[32mTrain: [ 39/50] Step 020/520 Loss 2.468 Prec@(1,5) (53.7%, 78.5%)\u001b[0m\n",
      "[2024-01-15 09:33:50] \u001b[32mTrain: [ 39/50] Step 040/520 Loss 2.552 Prec@(1,5) (52.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:33:50] \u001b[32mTrain: [ 39/50] Step 060/520 Loss 2.553 Prec@(1,5) (52.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:33:51] \u001b[32mTrain: [ 39/50] Step 080/520 Loss 2.557 Prec@(1,5) (52.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:52] \u001b[32mTrain: [ 39/50] Step 100/520 Loss 2.545 Prec@(1,5) (52.5%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:53] \u001b[32mTrain: [ 39/50] Step 120/520 Loss 2.550 Prec@(1,5) (52.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:53] \u001b[32mTrain: [ 39/50] Step 140/520 Loss 2.558 Prec@(1,5) (51.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:54] \u001b[32mTrain: [ 39/50] Step 160/520 Loss 2.556 Prec@(1,5) (51.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:55] \u001b[32mTrain: [ 39/50] Step 180/520 Loss 2.554 Prec@(1,5) (51.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:56] \u001b[32mTrain: [ 39/50] Step 200/520 Loss 2.560 Prec@(1,5) (51.6%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:33:56] \u001b[32mTrain: [ 39/50] Step 220/520 Loss 2.573 Prec@(1,5) (51.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:33:57] \u001b[32mTrain: [ 39/50] Step 240/520 Loss 2.579 Prec@(1,5) (51.3%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:33:58] \u001b[32mTrain: [ 39/50] Step 260/520 Loss 2.581 Prec@(1,5) (51.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:33:59] \u001b[32mTrain: [ 39/50] Step 280/520 Loss 2.581 Prec@(1,5) (51.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:34:00] \u001b[32mTrain: [ 39/50] Step 300/520 Loss 2.576 Prec@(1,5) (51.5%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:00] \u001b[32mTrain: [ 39/50] Step 320/520 Loss 2.573 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:01] \u001b[32mTrain: [ 39/50] Step 340/520 Loss 2.575 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:02] \u001b[32mTrain: [ 39/50] Step 360/520 Loss 2.574 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:03] \u001b[32mTrain: [ 39/50] Step 380/520 Loss 2.567 Prec@(1,5) (51.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:34:03] \u001b[32mTrain: [ 39/50] Step 400/520 Loss 2.570 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:04] \u001b[32mTrain: [ 39/50] Step 420/520 Loss 2.572 Prec@(1,5) (51.5%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:05] \u001b[32mTrain: [ 39/50] Step 440/520 Loss 2.575 Prec@(1,5) (51.5%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:06] \u001b[32mTrain: [ 39/50] Step 460/520 Loss 2.573 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:07] \u001b[32mTrain: [ 39/50] Step 480/520 Loss 2.574 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:34:07] \u001b[32mTrain: [ 39/50] Step 500/520 Loss 2.575 Prec@(1,5) (51.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:34:08] \u001b[32mTrain: [ 39/50] Step 520/520 Loss 2.577 Prec@(1,5) (51.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:34:08] \u001b[32mTrain: [ 39/50] Final Prec@1 51.4540%\u001b[0m\n",
      "[2024-01-15 09:34:13] \u001b[32mValid: [ 39/50] Step 000/104 Loss 2.351 Prec@(1,5) (53.1%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:34:14] \u001b[32mValid: [ 39/50] Step 020/104 Loss 2.179 Prec@(1,5) (52.1%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:34:14] \u001b[32mValid: [ 39/50] Step 040/104 Loss 2.130 Prec@(1,5) (51.8%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:34:14] \u001b[32mValid: [ 39/50] Step 060/104 Loss 2.094 Prec@(1,5) (52.4%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:34:15] \u001b[32mValid: [ 39/50] Step 080/104 Loss 2.100 Prec@(1,5) (52.0%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:34:15] \u001b[32mValid: [ 39/50] Step 100/104 Loss 2.094 Prec@(1,5) (52.0%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:34:15] \u001b[32mValid: [ 39/50] Step 104/104 Loss 2.094 Prec@(1,5) (52.0%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:34:15] \u001b[32mValid: [ 39/50] Final Prec@1 52.0000%\u001b[0m\n",
      "[2024-01-15 09:34:15] \u001b[32mEpoch 39 LR 0.002869\u001b[0m\n",
      "[2024-01-15 09:34:23] \u001b[32mTrain: [ 40/50] Step 000/520 Loss 2.659 Prec@(1,5) (49.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:34:24] \u001b[32mTrain: [ 40/50] Step 020/520 Loss 2.537 Prec@(1,5) (52.4%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:34:25] \u001b[32mTrain: [ 40/50] Step 040/520 Loss 2.584 Prec@(1,5) (51.7%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:34:25] \u001b[32mTrain: [ 40/50] Step 060/520 Loss 2.586 Prec@(1,5) (51.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:34:26] \u001b[32mTrain: [ 40/50] Step 080/520 Loss 2.579 Prec@(1,5) (51.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:34:27] \u001b[32mTrain: [ 40/50] Step 100/520 Loss 2.567 Prec@(1,5) (51.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:28] \u001b[32mTrain: [ 40/50] Step 120/520 Loss 2.567 Prec@(1,5) (51.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:28] \u001b[32mTrain: [ 40/50] Step 140/520 Loss 2.579 Prec@(1,5) (51.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:34:29] \u001b[32mTrain: [ 40/50] Step 160/520 Loss 2.583 Prec@(1,5) (51.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:30] \u001b[32mTrain: [ 40/50] Step 180/520 Loss 2.588 Prec@(1,5) (51.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:34:31] \u001b[32mTrain: [ 40/50] Step 200/520 Loss 2.599 Prec@(1,5) (51.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:34:31] \u001b[32mTrain: [ 40/50] Step 220/520 Loss 2.598 Prec@(1,5) (51.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:34:32] \u001b[32mTrain: [ 40/50] Step 240/520 Loss 2.596 Prec@(1,5) (51.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:34:33] \u001b[32mTrain: [ 40/50] Step 260/520 Loss 2.595 Prec@(1,5) (51.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:34] \u001b[32mTrain: [ 40/50] Step 280/520 Loss 2.590 Prec@(1,5) (51.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:35] \u001b[32mTrain: [ 40/50] Step 300/520 Loss 2.586 Prec@(1,5) (51.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:34:35] \u001b[32mTrain: [ 40/50] Step 320/520 Loss 2.585 Prec@(1,5) (51.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:36] \u001b[32mTrain: [ 40/50] Step 340/520 Loss 2.588 Prec@(1,5) (51.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:37] \u001b[32mTrain: [ 40/50] Step 360/520 Loss 2.587 Prec@(1,5) (51.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:34:38] \u001b[32mTrain: [ 40/50] Step 380/520 Loss 2.589 Prec@(1,5) (51.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:38] \u001b[32mTrain: [ 40/50] Step 400/520 Loss 2.585 Prec@(1,5) (51.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:39] \u001b[32mTrain: [ 40/50] Step 420/520 Loss 2.587 Prec@(1,5) (51.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:40] \u001b[32mTrain: [ 40/50] Step 440/520 Loss 2.584 Prec@(1,5) (51.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:41] \u001b[32mTrain: [ 40/50] Step 460/520 Loss 2.580 Prec@(1,5) (51.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:41] \u001b[32mTrain: [ 40/50] Step 480/520 Loss 2.578 Prec@(1,5) (51.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:34:42] \u001b[32mTrain: [ 40/50] Step 500/520 Loss 2.580 Prec@(1,5) (51.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:34:43] \u001b[32mTrain: [ 40/50] Step 520/520 Loss 2.579 Prec@(1,5) (51.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:34:43] \u001b[32mTrain: [ 40/50] Final Prec@1 51.5480%\u001b[0m\n",
      "[2024-01-15 09:34:48] \u001b[32mValid: [ 40/50] Step 000/104 Loss 2.314 Prec@(1,5) (57.3%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:34:48] \u001b[32mValid: [ 40/50] Step 020/104 Loss 2.152 Prec@(1,5) (52.9%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:34:49] \u001b[32mValid: [ 40/50] Step 040/104 Loss 2.109 Prec@(1,5) (52.4%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:34:49] \u001b[32mValid: [ 40/50] Step 060/104 Loss 2.073 Prec@(1,5) (52.5%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:34:49] \u001b[32mValid: [ 40/50] Step 080/104 Loss 2.074 Prec@(1,5) (52.3%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:34:50] \u001b[32mValid: [ 40/50] Step 100/104 Loss 2.062 Prec@(1,5) (52.5%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:34:50] \u001b[32mValid: [ 40/50] Step 104/104 Loss 2.062 Prec@(1,5) (52.5%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:34:50] \u001b[32mValid: [ 40/50] Final Prec@1 52.4500%\u001b[0m\n",
      "[2024-01-15 09:34:50] \u001b[32mEpoch 40 LR 0.002388\u001b[0m\n",
      "[2024-01-15 09:34:58] \u001b[32mTrain: [ 41/50] Step 000/520 Loss 2.545 Prec@(1,5) (50.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:34:59] \u001b[32mTrain: [ 41/50] Step 020/520 Loss 2.518 Prec@(1,5) (52.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:35:00] \u001b[32mTrain: [ 41/50] Step 040/520 Loss 2.545 Prec@(1,5) (52.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:35:00] \u001b[32mTrain: [ 41/50] Step 060/520 Loss 2.531 Prec@(1,5) (52.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:01] \u001b[32mTrain: [ 41/50] Step 080/520 Loss 2.525 Prec@(1,5) (53.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:02] \u001b[32mTrain: [ 41/50] Step 100/520 Loss 2.540 Prec@(1,5) (52.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:03] \u001b[32mTrain: [ 41/50] Step 120/520 Loss 2.549 Prec@(1,5) (52.4%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:03] \u001b[32mTrain: [ 41/50] Step 140/520 Loss 2.554 Prec@(1,5) (52.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:04] \u001b[32mTrain: [ 41/50] Step 160/520 Loss 2.556 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:05] \u001b[32mTrain: [ 41/50] Step 180/520 Loss 2.558 Prec@(1,5) (52.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:06] \u001b[32mTrain: [ 41/50] Step 200/520 Loss 2.553 Prec@(1,5) (52.4%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:07] \u001b[32mTrain: [ 41/50] Step 220/520 Loss 2.561 Prec@(1,5) (52.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:07] \u001b[32mTrain: [ 41/50] Step 240/520 Loss 2.557 Prec@(1,5) (52.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:08] \u001b[32mTrain: [ 41/50] Step 260/520 Loss 2.560 Prec@(1,5) (52.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:09] \u001b[32mTrain: [ 41/50] Step 280/520 Loss 2.558 Prec@(1,5) (52.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:10] \u001b[32mTrain: [ 41/50] Step 300/520 Loss 2.558 Prec@(1,5) (52.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:10] \u001b[32mTrain: [ 41/50] Step 320/520 Loss 2.556 Prec@(1,5) (52.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:11] \u001b[32mTrain: [ 41/50] Step 340/520 Loss 2.557 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:12] \u001b[32mTrain: [ 41/50] Step 360/520 Loss 2.562 Prec@(1,5) (52.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:13] \u001b[32mTrain: [ 41/50] Step 380/520 Loss 2.564 Prec@(1,5) (52.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:14] \u001b[32mTrain: [ 41/50] Step 400/520 Loss 2.569 Prec@(1,5) (52.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:14] \u001b[32mTrain: [ 41/50] Step 420/520 Loss 2.568 Prec@(1,5) (52.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:15] \u001b[32mTrain: [ 41/50] Step 440/520 Loss 2.564 Prec@(1,5) (52.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:16] \u001b[32mTrain: [ 41/50] Step 460/520 Loss 2.563 Prec@(1,5) (52.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:17] \u001b[32mTrain: [ 41/50] Step 480/520 Loss 2.564 Prec@(1,5) (52.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:17] \u001b[32mTrain: [ 41/50] Step 500/520 Loss 2.564 Prec@(1,5) (52.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:18] \u001b[32mTrain: [ 41/50] Step 520/520 Loss 2.567 Prec@(1,5) (51.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:18] \u001b[32mTrain: [ 41/50] Final Prec@1 51.9180%\u001b[0m\n",
      "[2024-01-15 09:35:23] \u001b[32mValid: [ 41/50] Step 000/104 Loss 2.237 Prec@(1,5) (54.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:35:24] \u001b[32mValid: [ 41/50] Step 020/104 Loss 2.118 Prec@(1,5) (52.8%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:35:24] \u001b[32mValid: [ 41/50] Step 040/104 Loss 2.073 Prec@(1,5) (52.9%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:35:24] \u001b[32mValid: [ 41/50] Step 060/104 Loss 2.039 Prec@(1,5) (53.4%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:35:25] \u001b[32mValid: [ 41/50] Step 080/104 Loss 2.044 Prec@(1,5) (53.2%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:35:25] \u001b[32mValid: [ 41/50] Step 100/104 Loss 2.029 Prec@(1,5) (53.5%, 82.9%)\u001b[0m\n",
      "[2024-01-15 09:35:25] \u001b[32mValid: [ 41/50] Step 104/104 Loss 2.029 Prec@(1,5) (53.5%, 82.9%)\u001b[0m\n",
      "[2024-01-15 09:35:25] \u001b[32mValid: [ 41/50] Final Prec@1 53.4900%\u001b[0m\n",
      "[2024-01-15 09:35:25] \u001b[32mEpoch 41 LR 0.001947\u001b[0m\n",
      "[2024-01-15 09:35:33] \u001b[32mTrain: [ 42/50] Step 000/520 Loss 2.333 Prec@(1,5) (53.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:34] \u001b[32mTrain: [ 42/50] Step 020/520 Loss 2.639 Prec@(1,5) (52.1%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:35:35] \u001b[32mTrain: [ 42/50] Step 040/520 Loss 2.594 Prec@(1,5) (52.4%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:35:36] \u001b[32mTrain: [ 42/50] Step 060/520 Loss 2.555 Prec@(1,5) (52.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:36] \u001b[32mTrain: [ 42/50] Step 080/520 Loss 2.560 Prec@(1,5) (52.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:37] \u001b[32mTrain: [ 42/50] Step 100/520 Loss 2.564 Prec@(1,5) (52.4%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:38] \u001b[32mTrain: [ 42/50] Step 120/520 Loss 2.568 Prec@(1,5) (52.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:35:39] \u001b[32mTrain: [ 42/50] Step 140/520 Loss 2.566 Prec@(1,5) (52.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:39] \u001b[32mTrain: [ 42/50] Step 160/520 Loss 2.565 Prec@(1,5) (52.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:35:40] \u001b[32mTrain: [ 42/50] Step 180/520 Loss 2.567 Prec@(1,5) (52.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:41] \u001b[32mTrain: [ 42/50] Step 200/520 Loss 2.562 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:42] \u001b[32mTrain: [ 42/50] Step 220/520 Loss 2.558 Prec@(1,5) (52.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:42] \u001b[32mTrain: [ 42/50] Step 240/520 Loss 2.556 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:43] \u001b[32mTrain: [ 42/50] Step 260/520 Loss 2.552 Prec@(1,5) (52.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:44] \u001b[32mTrain: [ 42/50] Step 280/520 Loss 2.554 Prec@(1,5) (52.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:45] \u001b[32mTrain: [ 42/50] Step 300/520 Loss 2.554 Prec@(1,5) (52.2%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:46] \u001b[32mTrain: [ 42/50] Step 320/520 Loss 2.553 Prec@(1,5) (52.2%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:46] \u001b[32mTrain: [ 42/50] Step 340/520 Loss 2.555 Prec@(1,5) (52.1%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:47] \u001b[32mTrain: [ 42/50] Step 360/520 Loss 2.558 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:48] \u001b[32mTrain: [ 42/50] Step 380/520 Loss 2.558 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:49] \u001b[32mTrain: [ 42/50] Step 400/520 Loss 2.554 Prec@(1,5) (52.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:49] \u001b[32mTrain: [ 42/50] Step 420/520 Loss 2.555 Prec@(1,5) (52.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:35:50] \u001b[32mTrain: [ 42/50] Step 440/520 Loss 2.559 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:51] \u001b[32mTrain: [ 42/50] Step 460/520 Loss 2.561 Prec@(1,5) (52.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:52] \u001b[32mTrain: [ 42/50] Step 480/520 Loss 2.562 Prec@(1,5) (52.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:53] \u001b[32mTrain: [ 42/50] Step 500/520 Loss 2.561 Prec@(1,5) (52.1%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:35:53] \u001b[32mTrain: [ 42/50] Step 520/520 Loss 2.562 Prec@(1,5) (52.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:35:54] \u001b[32mTrain: [ 42/50] Final Prec@1 52.0560%\u001b[0m\n",
      "[2024-01-15 09:35:58] \u001b[32mValid: [ 42/50] Step 000/104 Loss 2.350 Prec@(1,5) (54.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:35:59] \u001b[32mValid: [ 42/50] Step 020/104 Loss 2.132 Prec@(1,5) (52.8%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:35:59] \u001b[32mValid: [ 42/50] Step 040/104 Loss 2.089 Prec@(1,5) (52.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:35:59] \u001b[32mValid: [ 42/50] Step 060/104 Loss 2.055 Prec@(1,5) (52.6%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:36:00] \u001b[32mValid: [ 42/50] Step 080/104 Loss 2.063 Prec@(1,5) (52.6%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:36:00] \u001b[32mValid: [ 42/50] Step 100/104 Loss 2.048 Prec@(1,5) (52.9%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:36:00] \u001b[32mValid: [ 42/50] Step 104/104 Loss 2.046 Prec@(1,5) (52.9%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:36:00] \u001b[32mValid: [ 42/50] Final Prec@1 52.8700%\u001b[0m\n",
      "[2024-01-15 09:36:00] \u001b[32mEpoch 42 LR 0.001547\u001b[0m\n",
      "[2024-01-15 09:36:09] \u001b[32mTrain: [ 43/50] Step 000/520 Loss 2.971 Prec@(1,5) (46.9%, 68.8%)\u001b[0m\n",
      "[2024-01-15 09:36:09] \u001b[32mTrain: [ 43/50] Step 020/520 Loss 2.552 Prec@(1,5) (52.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:36:10] \u001b[32mTrain: [ 43/50] Step 040/520 Loss 2.541 Prec@(1,5) (52.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:36:11] \u001b[32mTrain: [ 43/50] Step 060/520 Loss 2.560 Prec@(1,5) (51.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:12] \u001b[32mTrain: [ 43/50] Step 080/520 Loss 2.553 Prec@(1,5) (52.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:12] \u001b[32mTrain: [ 43/50] Step 100/520 Loss 2.570 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:13] \u001b[32mTrain: [ 43/50] Step 120/520 Loss 2.565 Prec@(1,5) (52.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:14] \u001b[32mTrain: [ 43/50] Step 140/520 Loss 2.565 Prec@(1,5) (52.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:15] \u001b[32mTrain: [ 43/50] Step 160/520 Loss 2.570 Prec@(1,5) (51.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:15] \u001b[32mTrain: [ 43/50] Step 180/520 Loss 2.566 Prec@(1,5) (51.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:16] \u001b[32mTrain: [ 43/50] Step 200/520 Loss 2.574 Prec@(1,5) (51.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:17] \u001b[32mTrain: [ 43/50] Step 220/520 Loss 2.573 Prec@(1,5) (51.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:18] \u001b[32mTrain: [ 43/50] Step 240/520 Loss 2.568 Prec@(1,5) (52.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:19] \u001b[32mTrain: [ 43/50] Step 260/520 Loss 2.563 Prec@(1,5) (52.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:19] \u001b[32mTrain: [ 43/50] Step 280/520 Loss 2.561 Prec@(1,5) (52.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:20] \u001b[32mTrain: [ 43/50] Step 300/520 Loss 2.563 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:21] \u001b[32mTrain: [ 43/50] Step 320/520 Loss 2.568 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:22] \u001b[32mTrain: [ 43/50] Step 340/520 Loss 2.568 Prec@(1,5) (51.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:23] \u001b[32mTrain: [ 43/50] Step 360/520 Loss 2.572 Prec@(1,5) (51.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:23] \u001b[32mTrain: [ 43/50] Step 380/520 Loss 2.568 Prec@(1,5) (51.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:24] \u001b[32mTrain: [ 43/50] Step 400/520 Loss 2.570 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:25] \u001b[32mTrain: [ 43/50] Step 420/520 Loss 2.572 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:26] \u001b[32mTrain: [ 43/50] Step 440/520 Loss 2.573 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:26] \u001b[32mTrain: [ 43/50] Step 460/520 Loss 2.574 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:27] \u001b[32mTrain: [ 43/50] Step 480/520 Loss 2.572 Prec@(1,5) (51.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:28] \u001b[32mTrain: [ 43/50] Step 500/520 Loss 2.574 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:29] \u001b[32mTrain: [ 43/50] Step 520/520 Loss 2.574 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:29] \u001b[32mTrain: [ 43/50] Final Prec@1 51.7900%\u001b[0m\n",
      "[2024-01-15 09:36:34] \u001b[32mValid: [ 43/50] Step 000/104 Loss 2.223 Prec@(1,5) (56.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:36:34] \u001b[32mValid: [ 43/50] Step 020/104 Loss 2.068 Prec@(1,5) (53.6%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:36:35] \u001b[32mValid: [ 43/50] Step 040/104 Loss 2.038 Prec@(1,5) (53.1%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:36:35] \u001b[32mValid: [ 43/50] Step 060/104 Loss 2.013 Prec@(1,5) (53.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:36:35] \u001b[32mValid: [ 43/50] Step 080/104 Loss 2.021 Prec@(1,5) (53.2%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:36:36] \u001b[32mValid: [ 43/50] Step 100/104 Loss 2.010 Prec@(1,5) (53.5%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:36:36] \u001b[32mValid: [ 43/50] Step 104/104 Loss 2.010 Prec@(1,5) (53.5%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:36:36] \u001b[32mValid: [ 43/50] Final Prec@1 53.4900%\u001b[0m\n",
      "[2024-01-15 09:36:36] \u001b[32mEpoch 43 LR 0.001191\u001b[0m\n",
      "[2024-01-15 09:36:45] \u001b[32mTrain: [ 44/50] Step 000/520 Loss 2.568 Prec@(1,5) (55.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:36:45] \u001b[32mTrain: [ 44/50] Step 020/520 Loss 2.601 Prec@(1,5) (51.0%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:36:46] \u001b[32mTrain: [ 44/50] Step 040/520 Loss 2.540 Prec@(1,5) (51.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:36:47] \u001b[32mTrain: [ 44/50] Step 060/520 Loss 2.547 Prec@(1,5) (51.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:36:48] \u001b[32mTrain: [ 44/50] Step 080/520 Loss 2.577 Prec@(1,5) (51.7%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:36:48] \u001b[32mTrain: [ 44/50] Step 100/520 Loss 2.581 Prec@(1,5) (51.5%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:36:49] \u001b[32mTrain: [ 44/50] Step 120/520 Loss 2.563 Prec@(1,5) (52.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:36:50] \u001b[32mTrain: [ 44/50] Step 140/520 Loss 2.587 Prec@(1,5) (51.6%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:36:51] \u001b[32mTrain: [ 44/50] Step 160/520 Loss 2.579 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:52] \u001b[32mTrain: [ 44/50] Step 180/520 Loss 2.577 Prec@(1,5) (51.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:52] \u001b[32mTrain: [ 44/50] Step 200/520 Loss 2.583 Prec@(1,5) (51.5%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:36:53] \u001b[32mTrain: [ 44/50] Step 220/520 Loss 2.583 Prec@(1,5) (51.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:54] \u001b[32mTrain: [ 44/50] Step 240/520 Loss 2.578 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:55] \u001b[32mTrain: [ 44/50] Step 260/520 Loss 2.580 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:56] \u001b[32mTrain: [ 44/50] Step 280/520 Loss 2.579 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:56] \u001b[32mTrain: [ 44/50] Step 300/520 Loss 2.576 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:57] \u001b[32mTrain: [ 44/50] Step 320/520 Loss 2.576 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:58] \u001b[32mTrain: [ 44/50] Step 340/520 Loss 2.577 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:36:59] \u001b[32mTrain: [ 44/50] Step 360/520 Loss 2.578 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:37:00] \u001b[32mTrain: [ 44/50] Step 380/520 Loss 2.577 Prec@(1,5) (51.7%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:37:00] \u001b[32mTrain: [ 44/50] Step 400/520 Loss 2.576 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:37:01] \u001b[32mTrain: [ 44/50] Step 420/520 Loss 2.577 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:37:02] \u001b[32mTrain: [ 44/50] Step 440/520 Loss 2.575 Prec@(1,5) (51.8%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:37:03] \u001b[32mTrain: [ 44/50] Step 460/520 Loss 2.575 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:37:04] \u001b[32mTrain: [ 44/50] Step 480/520 Loss 2.572 Prec@(1,5) (51.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:37:04] \u001b[32mTrain: [ 44/50] Step 500/520 Loss 2.570 Prec@(1,5) (51.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:37:05] \u001b[32mTrain: [ 44/50] Step 520/520 Loss 2.571 Prec@(1,5) (51.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:37:06] \u001b[32mTrain: [ 44/50] Final Prec@1 51.8800%\u001b[0m\n",
      "[2024-01-15 09:37:11] \u001b[32mValid: [ 44/50] Step 000/104 Loss 2.231 Prec@(1,5) (55.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:37:11] \u001b[32mValid: [ 44/50] Step 020/104 Loss 2.089 Prec@(1,5) (53.9%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:37:11] \u001b[32mValid: [ 44/50] Step 040/104 Loss 2.048 Prec@(1,5) (53.3%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:37:12] \u001b[32mValid: [ 44/50] Step 060/104 Loss 2.020 Prec@(1,5) (53.6%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:37:12] \u001b[32mValid: [ 44/50] Step 080/104 Loss 2.031 Prec@(1,5) (53.4%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:37:12] \u001b[32mValid: [ 44/50] Step 100/104 Loss 2.017 Prec@(1,5) (53.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:37:12] \u001b[32mValid: [ 44/50] Step 104/104 Loss 2.017 Prec@(1,5) (53.3%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:37:13] \u001b[32mValid: [ 44/50] Final Prec@1 53.3100%\u001b[0m\n",
      "[2024-01-15 09:37:13] \u001b[32mEpoch 44 LR 0.000879\u001b[0m\n",
      "[2024-01-15 09:37:21] \u001b[32mTrain: [ 45/50] Step 000/520 Loss 2.149 Prec@(1,5) (53.1%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:37:22] \u001b[32mTrain: [ 45/50] Step 020/520 Loss 2.493 Prec@(1,5) (53.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:37:23] \u001b[32mTrain: [ 45/50] Step 040/520 Loss 2.489 Prec@(1,5) (53.7%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:37:24] \u001b[32mTrain: [ 45/50] Step 060/520 Loss 2.493 Prec@(1,5) (53.4%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:37:24] \u001b[32mTrain: [ 45/50] Step 080/520 Loss 2.490 Prec@(1,5) (53.6%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:37:25] \u001b[32mTrain: [ 45/50] Step 100/520 Loss 2.503 Prec@(1,5) (53.4%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:37:26] \u001b[32mTrain: [ 45/50] Step 120/520 Loss 2.517 Prec@(1,5) (53.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:37:27] \u001b[32mTrain: [ 45/50] Step 140/520 Loss 2.529 Prec@(1,5) (52.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:37:27] \u001b[32mTrain: [ 45/50] Step 160/520 Loss 2.542 Prec@(1,5) (52.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:37:28] \u001b[32mTrain: [ 45/50] Step 180/520 Loss 2.540 Prec@(1,5) (52.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:37:29] \u001b[32mTrain: [ 45/50] Step 200/520 Loss 2.547 Prec@(1,5) (52.4%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:30] \u001b[32mTrain: [ 45/50] Step 220/520 Loss 2.552 Prec@(1,5) (52.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:31] \u001b[32mTrain: [ 45/50] Step 240/520 Loss 2.556 Prec@(1,5) (52.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:31] \u001b[32mTrain: [ 45/50] Step 260/520 Loss 2.557 Prec@(1,5) (52.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:32] \u001b[32mTrain: [ 45/50] Step 280/520 Loss 2.558 Prec@(1,5) (52.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:33] \u001b[32mTrain: [ 45/50] Step 300/520 Loss 2.559 Prec@(1,5) (52.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:37:34] \u001b[32mTrain: [ 45/50] Step 320/520 Loss 2.557 Prec@(1,5) (52.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:34] \u001b[32mTrain: [ 45/50] Step 340/520 Loss 2.556 Prec@(1,5) (52.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:37:35] \u001b[32mTrain: [ 45/50] Step 360/520 Loss 2.557 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:36] \u001b[32mTrain: [ 45/50] Step 380/520 Loss 2.557 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:37] \u001b[32mTrain: [ 45/50] Step 400/520 Loss 2.562 Prec@(1,5) (51.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:37:37] \u001b[32mTrain: [ 45/50] Step 420/520 Loss 2.563 Prec@(1,5) (51.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:37:38] \u001b[32mTrain: [ 45/50] Step 440/520 Loss 2.565 Prec@(1,5) (51.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:37:39] \u001b[32mTrain: [ 45/50] Step 460/520 Loss 2.560 Prec@(1,5) (51.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:40] \u001b[32mTrain: [ 45/50] Step 480/520 Loss 2.558 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:41] \u001b[32mTrain: [ 45/50] Step 500/520 Loss 2.558 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:41] \u001b[32mTrain: [ 45/50] Step 520/520 Loss 2.556 Prec@(1,5) (52.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:37:42] \u001b[32mTrain: [ 45/50] Final Prec@1 51.9980%\u001b[0m\n",
      "[2024-01-15 09:37:46] \u001b[32mValid: [ 45/50] Step 000/104 Loss 2.246 Prec@(1,5) (56.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:37:47] \u001b[32mValid: [ 45/50] Step 020/104 Loss 2.101 Prec@(1,5) (53.3%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:37:47] \u001b[32mValid: [ 45/50] Step 040/104 Loss 2.058 Prec@(1,5) (53.2%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:37:47] \u001b[32mValid: [ 45/50] Step 060/104 Loss 2.022 Prec@(1,5) (53.5%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:37:48] \u001b[32mValid: [ 45/50] Step 080/104 Loss 2.030 Prec@(1,5) (53.2%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:37:48] \u001b[32mValid: [ 45/50] Step 100/104 Loss 2.015 Prec@(1,5) (53.3%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:37:48] \u001b[32mValid: [ 45/50] Step 104/104 Loss 2.015 Prec@(1,5) (53.3%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:37:48] \u001b[32mValid: [ 45/50] Final Prec@1 53.3300%\u001b[0m\n",
      "[2024-01-15 09:37:48] \u001b[32mEpoch 45 LR 0.000613\u001b[0m\n",
      "[2024-01-15 09:37:57] \u001b[32mTrain: [ 46/50] Step 000/520 Loss 2.861 Prec@(1,5) (50.0%, 69.8%)\u001b[0m\n",
      "[2024-01-15 09:37:57] \u001b[32mTrain: [ 46/50] Step 020/520 Loss 2.550 Prec@(1,5) (53.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:37:58] \u001b[32mTrain: [ 46/50] Step 040/520 Loss 2.552 Prec@(1,5) (52.2%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:37:59] \u001b[32mTrain: [ 46/50] Step 060/520 Loss 2.566 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:00] \u001b[32mTrain: [ 46/50] Step 080/520 Loss 2.568 Prec@(1,5) (51.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:00] \u001b[32mTrain: [ 46/50] Step 100/520 Loss 2.568 Prec@(1,5) (51.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:38:01] \u001b[32mTrain: [ 46/50] Step 120/520 Loss 2.568 Prec@(1,5) (51.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:02] \u001b[32mTrain: [ 46/50] Step 140/520 Loss 2.559 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:03] \u001b[32mTrain: [ 46/50] Step 160/520 Loss 2.568 Prec@(1,5) (51.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:03] \u001b[32mTrain: [ 46/50] Step 180/520 Loss 2.569 Prec@(1,5) (52.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:04] \u001b[32mTrain: [ 46/50] Step 200/520 Loss 2.574 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:05] \u001b[32mTrain: [ 46/50] Step 220/520 Loss 2.583 Prec@(1,5) (51.7%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:06] \u001b[32mTrain: [ 46/50] Step 240/520 Loss 2.584 Prec@(1,5) (51.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:38:07] \u001b[32mTrain: [ 46/50] Step 260/520 Loss 2.589 Prec@(1,5) (51.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:38:07] \u001b[32mTrain: [ 46/50] Step 280/520 Loss 2.587 Prec@(1,5) (51.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:38:08] \u001b[32mTrain: [ 46/50] Step 300/520 Loss 2.584 Prec@(1,5) (51.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:38:09] \u001b[32mTrain: [ 46/50] Step 320/520 Loss 2.583 Prec@(1,5) (51.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:38:10] \u001b[32mTrain: [ 46/50] Step 340/520 Loss 2.579 Prec@(1,5) (51.7%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:11] \u001b[32mTrain: [ 46/50] Step 360/520 Loss 2.578 Prec@(1,5) (51.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:11] \u001b[32mTrain: [ 46/50] Step 380/520 Loss 2.575 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:12] \u001b[32mTrain: [ 46/50] Step 400/520 Loss 2.574 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:13] \u001b[32mTrain: [ 46/50] Step 420/520 Loss 2.574 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:14] \u001b[32mTrain: [ 46/50] Step 440/520 Loss 2.575 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:14] \u001b[32mTrain: [ 46/50] Step 460/520 Loss 2.577 Prec@(1,5) (51.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:15] \u001b[32mTrain: [ 46/50] Step 480/520 Loss 2.577 Prec@(1,5) (51.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:16] \u001b[32mTrain: [ 46/50] Step 500/520 Loss 2.577 Prec@(1,5) (51.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:17] \u001b[32mTrain: [ 46/50] Step 520/520 Loss 2.576 Prec@(1,5) (51.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:17] \u001b[32mTrain: [ 46/50] Final Prec@1 51.8320%\u001b[0m\n",
      "[2024-01-15 09:38:22] \u001b[32mValid: [ 46/50] Step 000/104 Loss 2.299 Prec@(1,5) (56.2%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:38:22] \u001b[32mValid: [ 46/50] Step 020/104 Loss 2.090 Prec@(1,5) (54.1%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:38:23] \u001b[32mValid: [ 46/50] Step 040/104 Loss 2.048 Prec@(1,5) (53.5%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:38:23] \u001b[32mValid: [ 46/50] Step 060/104 Loss 2.011 Prec@(1,5) (54.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:38:23] \u001b[32mValid: [ 46/50] Step 080/104 Loss 2.017 Prec@(1,5) (53.7%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:38:24] \u001b[32mValid: [ 46/50] Step 100/104 Loss 2.002 Prec@(1,5) (53.8%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:38:24] \u001b[32mValid: [ 46/50] Step 104/104 Loss 2.001 Prec@(1,5) (53.8%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:38:24] \u001b[32mValid: [ 46/50] Final Prec@1 53.7700%\u001b[0m\n",
      "[2024-01-15 09:38:24] \u001b[32mEpoch 46 LR 0.000394\u001b[0m\n",
      "[2024-01-15 09:38:33] \u001b[32mTrain: [ 47/50] Step 000/520 Loss 2.573 Prec@(1,5) (49.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 09:38:33] \u001b[32mTrain: [ 47/50] Step 020/520 Loss 2.574 Prec@(1,5) (51.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:38:34] \u001b[32mTrain: [ 47/50] Step 040/520 Loss 2.565 Prec@(1,5) (51.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:38:35] \u001b[32mTrain: [ 47/50] Step 060/520 Loss 2.561 Prec@(1,5) (51.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:38:36] \u001b[32mTrain: [ 47/50] Step 080/520 Loss 2.568 Prec@(1,5) (51.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:38:36] \u001b[32mTrain: [ 47/50] Step 100/520 Loss 2.579 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:37] \u001b[32mTrain: [ 47/50] Step 120/520 Loss 2.567 Prec@(1,5) (51.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:38] \u001b[32mTrain: [ 47/50] Step 140/520 Loss 2.577 Prec@(1,5) (51.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:39] \u001b[32mTrain: [ 47/50] Step 160/520 Loss 2.578 Prec@(1,5) (51.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:40] \u001b[32mTrain: [ 47/50] Step 180/520 Loss 2.576 Prec@(1,5) (51.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:40] \u001b[32mTrain: [ 47/50] Step 200/520 Loss 2.583 Prec@(1,5) (51.6%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:38:41] \u001b[32mTrain: [ 47/50] Step 220/520 Loss 2.583 Prec@(1,5) (51.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:42] \u001b[32mTrain: [ 47/50] Step 240/520 Loss 2.582 Prec@(1,5) (51.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:43] \u001b[32mTrain: [ 47/50] Step 260/520 Loss 2.575 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:44] \u001b[32mTrain: [ 47/50] Step 280/520 Loss 2.582 Prec@(1,5) (51.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:44] \u001b[32mTrain: [ 47/50] Step 300/520 Loss 2.583 Prec@(1,5) (51.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:45] \u001b[32mTrain: [ 47/50] Step 320/520 Loss 2.577 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:46] \u001b[32mTrain: [ 47/50] Step 340/520 Loss 2.574 Prec@(1,5) (51.8%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:47] \u001b[32mTrain: [ 47/50] Step 360/520 Loss 2.578 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:47] \u001b[32mTrain: [ 47/50] Step 380/520 Loss 2.577 Prec@(1,5) (51.7%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:48] \u001b[32mTrain: [ 47/50] Step 400/520 Loss 2.575 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:49] \u001b[32mTrain: [ 47/50] Step 420/520 Loss 2.582 Prec@(1,5) (51.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:50] \u001b[32mTrain: [ 47/50] Step 440/520 Loss 2.578 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:51] \u001b[32mTrain: [ 47/50] Step 460/520 Loss 2.580 Prec@(1,5) (51.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:51] \u001b[32mTrain: [ 47/50] Step 480/520 Loss 2.577 Prec@(1,5) (51.7%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:38:52] \u001b[32mTrain: [ 47/50] Step 500/520 Loss 2.576 Prec@(1,5) (51.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:38:53] \u001b[32mTrain: [ 47/50] Step 520/520 Loss 2.573 Prec@(1,5) (51.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:38:53] \u001b[32mTrain: [ 47/50] Final Prec@1 51.7760%\u001b[0m\n",
      "[2024-01-15 09:38:58] \u001b[32mValid: [ 47/50] Step 000/104 Loss 2.262 Prec@(1,5) (58.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:38:58] \u001b[32mValid: [ 47/50] Step 020/104 Loss 2.080 Prec@(1,5) (53.7%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:38:59] \u001b[32mValid: [ 47/50] Step 040/104 Loss 2.039 Prec@(1,5) (53.5%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:38:59] \u001b[32mValid: [ 47/50] Step 060/104 Loss 2.006 Prec@(1,5) (53.9%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:38:59] \u001b[32mValid: [ 47/50] Step 080/104 Loss 2.014 Prec@(1,5) (53.5%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:39:00] \u001b[32mValid: [ 47/50] Step 100/104 Loss 2.001 Prec@(1,5) (53.5%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:39:00] \u001b[32mValid: [ 47/50] Step 104/104 Loss 2.002 Prec@(1,5) (53.5%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:39:00] \u001b[32mValid: [ 47/50] Final Prec@1 53.5000%\u001b[0m\n",
      "[2024-01-15 09:39:00] \u001b[32mEpoch 47 LR 0.000222\u001b[0m\n",
      "[2024-01-15 09:39:09] \u001b[32mTrain: [ 48/50] Step 000/520 Loss 2.666 Prec@(1,5) (52.1%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:39:09] \u001b[32mTrain: [ 48/50] Step 020/520 Loss 2.556 Prec@(1,5) (52.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:39:10] \u001b[32mTrain: [ 48/50] Step 040/520 Loss 2.561 Prec@(1,5) (52.5%, 76.4%)\u001b[0m\n",
      "[2024-01-15 09:39:11] \u001b[32mTrain: [ 48/50] Step 060/520 Loss 2.549 Prec@(1,5) (52.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:39:12] \u001b[32mTrain: [ 48/50] Step 080/520 Loss 2.571 Prec@(1,5) (52.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:39:12] \u001b[32mTrain: [ 48/50] Step 100/520 Loss 2.564 Prec@(1,5) (52.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:13] \u001b[32mTrain: [ 48/50] Step 120/520 Loss 2.550 Prec@(1,5) (52.6%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:39:14] \u001b[32mTrain: [ 48/50] Step 140/520 Loss 2.558 Prec@(1,5) (52.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:39:15] \u001b[32mTrain: [ 48/50] Step 160/520 Loss 2.570 Prec@(1,5) (52.2%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:16] \u001b[32mTrain: [ 48/50] Step 180/520 Loss 2.572 Prec@(1,5) (52.1%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:16] \u001b[32mTrain: [ 48/50] Step 200/520 Loss 2.577 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:17] \u001b[32mTrain: [ 48/50] Step 220/520 Loss 2.574 Prec@(1,5) (52.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:18] \u001b[32mTrain: [ 48/50] Step 240/520 Loss 2.574 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:19] \u001b[32mTrain: [ 48/50] Step 260/520 Loss 2.574 Prec@(1,5) (52.1%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:19] \u001b[32mTrain: [ 48/50] Step 280/520 Loss 2.580 Prec@(1,5) (52.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:20] \u001b[32mTrain: [ 48/50] Step 300/520 Loss 2.583 Prec@(1,5) (51.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:21] \u001b[32mTrain: [ 48/50] Step 320/520 Loss 2.576 Prec@(1,5) (52.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:22] \u001b[32mTrain: [ 48/50] Step 340/520 Loss 2.572 Prec@(1,5) (52.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:39:23] \u001b[32mTrain: [ 48/50] Step 360/520 Loss 2.573 Prec@(1,5) (52.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:39:23] \u001b[32mTrain: [ 48/50] Step 380/520 Loss 2.573 Prec@(1,5) (52.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:39:24] \u001b[32mTrain: [ 48/50] Step 400/520 Loss 2.575 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:25] \u001b[32mTrain: [ 48/50] Step 420/520 Loss 2.576 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:26] \u001b[32mTrain: [ 48/50] Step 440/520 Loss 2.576 Prec@(1,5) (52.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:27] \u001b[32mTrain: [ 48/50] Step 460/520 Loss 2.575 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:27] \u001b[32mTrain: [ 48/50] Step 480/520 Loss 2.575 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:28] \u001b[32mTrain: [ 48/50] Step 500/520 Loss 2.577 Prec@(1,5) (52.1%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:29] \u001b[32mTrain: [ 48/50] Step 520/520 Loss 2.574 Prec@(1,5) (52.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 09:39:29] \u001b[32mTrain: [ 48/50] Final Prec@1 52.1200%\u001b[0m\n",
      "[2024-01-15 09:39:34] \u001b[32mValid: [ 48/50] Step 000/104 Loss 2.225 Prec@(1,5) (56.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:39:34] \u001b[32mValid: [ 48/50] Step 020/104 Loss 2.051 Prec@(1,5) (54.3%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:39:35] \u001b[32mValid: [ 48/50] Step 040/104 Loss 2.017 Prec@(1,5) (53.9%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:39:35] \u001b[32mValid: [ 48/50] Step 060/104 Loss 1.983 Prec@(1,5) (54.2%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:39:35] \u001b[32mValid: [ 48/50] Step 080/104 Loss 1.990 Prec@(1,5) (54.0%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:39:36] \u001b[32mValid: [ 48/50] Step 100/104 Loss 1.977 Prec@(1,5) (54.1%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:39:36] \u001b[32mValid: [ 48/50] Step 104/104 Loss 1.977 Prec@(1,5) (54.1%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:39:36] \u001b[32mValid: [ 48/50] Final Prec@1 54.1000%\u001b[0m\n",
      "[2024-01-15 09:39:36] \u001b[32mEpoch 48 LR 0.000100\u001b[0m\n",
      "[2024-01-15 09:39:44] \u001b[32mTrain: [ 49/50] Step 000/520 Loss 2.751 Prec@(1,5) (45.8%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:39:45] \u001b[32mTrain: [ 49/50] Step 020/520 Loss 2.622 Prec@(1,5) (50.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 09:39:46] \u001b[32mTrain: [ 49/50] Step 040/520 Loss 2.624 Prec@(1,5) (51.3%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:39:46] \u001b[32mTrain: [ 49/50] Step 060/520 Loss 2.615 Prec@(1,5) (51.2%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:39:47] \u001b[32mTrain: [ 49/50] Step 080/520 Loss 2.585 Prec@(1,5) (51.9%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:39:48] \u001b[32mTrain: [ 49/50] Step 100/520 Loss 2.586 Prec@(1,5) (52.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:39:49] \u001b[32mTrain: [ 49/50] Step 120/520 Loss 2.571 Prec@(1,5) (52.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:39:50] \u001b[32mTrain: [ 49/50] Step 140/520 Loss 2.588 Prec@(1,5) (52.0%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:39:50] \u001b[32mTrain: [ 49/50] Step 160/520 Loss 2.589 Prec@(1,5) (52.0%, 76.3%)\u001b[0m\n",
      "[2024-01-15 09:39:51] \u001b[32mTrain: [ 49/50] Step 180/520 Loss 2.589 Prec@(1,5) (52.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:39:52] \u001b[32mTrain: [ 49/50] Step 200/520 Loss 2.586 Prec@(1,5) (52.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:39:53] \u001b[32mTrain: [ 49/50] Step 220/520 Loss 2.599 Prec@(1,5) (51.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:39:53] \u001b[32mTrain: [ 49/50] Step 240/520 Loss 2.596 Prec@(1,5) (51.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:39:54] \u001b[32mTrain: [ 49/50] Step 260/520 Loss 2.594 Prec@(1,5) (51.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:39:55] \u001b[32mTrain: [ 49/50] Step 280/520 Loss 2.592 Prec@(1,5) (51.9%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:39:56] \u001b[32mTrain: [ 49/50] Step 300/520 Loss 2.589 Prec@(1,5) (51.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:39:57] \u001b[32mTrain: [ 49/50] Step 320/520 Loss 2.594 Prec@(1,5) (51.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:39:57] \u001b[32mTrain: [ 49/50] Step 340/520 Loss 2.593 Prec@(1,5) (51.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:39:58] \u001b[32mTrain: [ 49/50] Step 360/520 Loss 2.597 Prec@(1,5) (51.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:39:59] \u001b[32mTrain: [ 49/50] Step 380/520 Loss 2.597 Prec@(1,5) (51.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:00] \u001b[32mTrain: [ 49/50] Step 400/520 Loss 2.597 Prec@(1,5) (51.9%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:01] \u001b[32mTrain: [ 49/50] Step 420/520 Loss 2.598 Prec@(1,5) (51.9%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:01] \u001b[32mTrain: [ 49/50] Step 440/520 Loss 2.598 Prec@(1,5) (51.9%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:02] \u001b[32mTrain: [ 49/50] Step 460/520 Loss 2.595 Prec@(1,5) (51.9%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:03] \u001b[32mTrain: [ 49/50] Step 480/520 Loss 2.591 Prec@(1,5) (52.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:40:04] \u001b[32mTrain: [ 49/50] Step 500/520 Loss 2.591 Prec@(1,5) (51.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:40:04] \u001b[32mTrain: [ 49/50] Step 520/520 Loss 2.591 Prec@(1,5) (51.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:40:05] \u001b[32mTrain: [ 49/50] Final Prec@1 51.9000%\u001b[0m\n",
      "[2024-01-15 09:40:10] \u001b[32mValid: [ 49/50] Step 000/104 Loss 2.221 Prec@(1,5) (56.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:40:10] \u001b[32mValid: [ 49/50] Step 020/104 Loss 2.056 Prec@(1,5) (54.4%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:40:11] \u001b[32mValid: [ 49/50] Step 040/104 Loss 2.023 Prec@(1,5) (53.9%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:40:11] \u001b[32mValid: [ 49/50] Step 060/104 Loss 1.991 Prec@(1,5) (54.3%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:40:11] \u001b[32mValid: [ 49/50] Step 080/104 Loss 1.999 Prec@(1,5) (54.1%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:40:12] \u001b[32mValid: [ 49/50] Step 100/104 Loss 1.985 Prec@(1,5) (54.1%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:40:12] \u001b[32mValid: [ 49/50] Step 104/104 Loss 1.985 Prec@(1,5) (54.1%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:40:12] \u001b[32mValid: [ 49/50] Final Prec@1 54.1300%\u001b[0m\n",
      "[2024-01-15 09:40:12] \u001b[32mEpoch 49 LR 0.000026\u001b[0m\n",
      "[2024-01-15 09:40:20] \u001b[32mTrain: [ 50/50] Step 000/520 Loss 2.720 Prec@(1,5) (52.1%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:40:21] \u001b[32mTrain: [ 50/50] Step 020/520 Loss 2.621 Prec@(1,5) (50.3%, 75.4%)\u001b[0m\n",
      "[2024-01-15 09:40:22] \u001b[32mTrain: [ 50/50] Step 040/520 Loss 2.620 Prec@(1,5) (50.9%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:40:22] \u001b[32mTrain: [ 50/50] Step 060/520 Loss 2.619 Prec@(1,5) (51.5%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:40:23] \u001b[32mTrain: [ 50/50] Step 080/520 Loss 2.606 Prec@(1,5) (51.4%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:24] \u001b[32mTrain: [ 50/50] Step 100/520 Loss 2.613 Prec@(1,5) (51.2%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:40:25] \u001b[32mTrain: [ 50/50] Step 120/520 Loss 2.604 Prec@(1,5) (51.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:40:26] \u001b[32mTrain: [ 50/50] Step 140/520 Loss 2.600 Prec@(1,5) (51.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:26] \u001b[32mTrain: [ 50/50] Step 160/520 Loss 2.603 Prec@(1,5) (51.4%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:27] \u001b[32mTrain: [ 50/50] Step 180/520 Loss 2.606 Prec@(1,5) (51.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:40:28] \u001b[32mTrain: [ 50/50] Step 200/520 Loss 2.601 Prec@(1,5) (51.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:29] \u001b[32mTrain: [ 50/50] Step 220/520 Loss 2.605 Prec@(1,5) (51.4%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:29] \u001b[32mTrain: [ 50/50] Step 240/520 Loss 2.617 Prec@(1,5) (51.2%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:40:30] \u001b[32mTrain: [ 50/50] Step 260/520 Loss 2.615 Prec@(1,5) (51.3%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:40:31] \u001b[32mTrain: [ 50/50] Step 280/520 Loss 2.604 Prec@(1,5) (51.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:32] \u001b[32mTrain: [ 50/50] Step 300/520 Loss 2.604 Prec@(1,5) (51.4%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:33] \u001b[32mTrain: [ 50/50] Step 320/520 Loss 2.600 Prec@(1,5) (51.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:33] \u001b[32mTrain: [ 50/50] Step 340/520 Loss 2.598 Prec@(1,5) (51.6%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:34] \u001b[32mTrain: [ 50/50] Step 360/520 Loss 2.600 Prec@(1,5) (51.6%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:35] \u001b[32mTrain: [ 50/50] Step 380/520 Loss 2.600 Prec@(1,5) (51.6%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:36] \u001b[32mTrain: [ 50/50] Step 400/520 Loss 2.599 Prec@(1,5) (51.6%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:37] \u001b[32mTrain: [ 50/50] Step 420/520 Loss 2.603 Prec@(1,5) (51.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:40:37] \u001b[32mTrain: [ 50/50] Step 440/520 Loss 2.599 Prec@(1,5) (51.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:38] \u001b[32mTrain: [ 50/50] Step 460/520 Loss 2.596 Prec@(1,5) (51.6%, 76.2%)\u001b[0m\n",
      "[2024-01-15 09:40:39] \u001b[32mTrain: [ 50/50] Step 480/520 Loss 2.601 Prec@(1,5) (51.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:40] \u001b[32mTrain: [ 50/50] Step 500/520 Loss 2.603 Prec@(1,5) (51.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:40] \u001b[32mTrain: [ 50/50] Step 520/520 Loss 2.605 Prec@(1,5) (51.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:40:41] \u001b[32mTrain: [ 50/50] Final Prec@1 51.4640%\u001b[0m\n",
      "[2024-01-15 09:40:46] \u001b[32mValid: [ 50/50] Step 000/104 Loss 2.236 Prec@(1,5) (57.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:40:46] \u001b[32mValid: [ 50/50] Step 020/104 Loss 2.057 Prec@(1,5) (54.0%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:40:46] \u001b[32mValid: [ 50/50] Step 040/104 Loss 2.022 Prec@(1,5) (53.7%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:40:47] \u001b[32mValid: [ 50/50] Step 060/104 Loss 1.988 Prec@(1,5) (54.1%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:40:47] \u001b[32mValid: [ 50/50] Step 080/104 Loss 1.995 Prec@(1,5) (53.9%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:40:47] \u001b[32mValid: [ 50/50] Step 100/104 Loss 1.982 Prec@(1,5) (54.0%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:40:47] \u001b[32mValid: [ 50/50] Step 104/104 Loss 1.982 Prec@(1,5) (53.9%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:40:48] \u001b[32mValid: [ 50/50] Final Prec@1 53.9200%\u001b[0m\n",
      "Final best Prec@1 = 54.1300%\n",
      "./checkpoints/cifar100/random/2/\n",
      "[2024-01-15 09:40:48] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'sepconv5x5', 'reduce_n3_p0': 'sepconv5x5', 'reduce_n3_p1': 'sepconv5x5', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'sepconv3x3', 'reduce_n4_p1': 'maxpool', 'reduce_n4_p2': 'sepconv3x3', 'reduce_n4_p3': 'sepconv3x3', 'reduce_n5_p0': 'sepconv5x5', 'reduce_n5_p1': 'maxpool', 'reduce_n5_p2': 'maxpool', 'reduce_n5_p3': 'sepconv3x3', 'reduce_n5_p4': 'maxpool', 'reduce_n2_switch': [1], 'reduce_n3_switch': [0], 'reduce_n4_switch': [2], 'reduce_n5_switch': [4]}\u001b[0m\n",
      "[2024-01-15 09:40:48] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2024-01-15 09:40:56] \u001b[32mTrain: [  1/50] Step 000/520 Loss 6.558 Prec@(1,5) (0.0%, 4.2%)\u001b[0m\n",
      "[2024-01-15 09:40:57] \u001b[32mTrain: [  1/50] Step 020/520 Loss 6.473 Prec@(1,5) (1.3%, 6.0%)\u001b[0m\n",
      "[2024-01-15 09:40:58] \u001b[32mTrain: [  1/50] Step 040/520 Loss 6.360 Prec@(1,5) (2.4%, 9.5%)\u001b[0m\n",
      "[2024-01-15 09:40:59] \u001b[32mTrain: [  1/50] Step 060/520 Loss 6.260 Prec@(1,5) (3.1%, 11.9%)\u001b[0m\n",
      "[2024-01-15 09:40:59] \u001b[32mTrain: [  1/50] Step 080/520 Loss 6.158 Prec@(1,5) (3.8%, 14.3%)\u001b[0m\n",
      "[2024-01-15 09:41:00] \u001b[32mTrain: [  1/50] Step 100/520 Loss 6.073 Prec@(1,5) (4.5%, 16.1%)\u001b[0m\n",
      "[2024-01-15 09:41:01] \u001b[32mTrain: [  1/50] Step 120/520 Loss 6.001 Prec@(1,5) (5.1%, 17.7%)\u001b[0m\n",
      "[2024-01-15 09:41:02] \u001b[32mTrain: [  1/50] Step 140/520 Loss 5.939 Prec@(1,5) (5.5%, 18.8%)\u001b[0m\n",
      "[2024-01-15 09:41:03] \u001b[32mTrain: [  1/50] Step 160/520 Loss 5.883 Prec@(1,5) (5.8%, 20.0%)\u001b[0m\n",
      "[2024-01-15 09:41:04] \u001b[32mTrain: [  1/50] Step 180/520 Loss 5.844 Prec@(1,5) (6.1%, 20.9%)\u001b[0m\n",
      "[2024-01-15 09:41:05] \u001b[32mTrain: [  1/50] Step 200/520 Loss 5.791 Prec@(1,5) (6.5%, 22.1%)\u001b[0m\n",
      "[2024-01-15 09:41:05] \u001b[32mTrain: [  1/50] Step 220/520 Loss 5.747 Prec@(1,5) (6.9%, 23.0%)\u001b[0m\n",
      "[2024-01-15 09:41:06] \u001b[32mTrain: [  1/50] Step 240/520 Loss 5.715 Prec@(1,5) (7.1%, 23.7%)\u001b[0m\n",
      "[2024-01-15 09:41:07] \u001b[32mTrain: [  1/50] Step 260/520 Loss 5.688 Prec@(1,5) (7.3%, 24.3%)\u001b[0m\n",
      "[2024-01-15 09:41:08] \u001b[32mTrain: [  1/50] Step 280/520 Loss 5.661 Prec@(1,5) (7.5%, 24.9%)\u001b[0m\n",
      "[2024-01-15 09:41:09] \u001b[32mTrain: [  1/50] Step 300/520 Loss 5.634 Prec@(1,5) (7.7%, 25.4%)\u001b[0m\n",
      "[2024-01-15 09:41:10] \u001b[32mTrain: [  1/50] Step 320/520 Loss 5.608 Prec@(1,5) (8.0%, 26.0%)\u001b[0m\n",
      "[2024-01-15 09:41:11] \u001b[32mTrain: [  1/50] Step 340/520 Loss 5.583 Prec@(1,5) (8.2%, 26.5%)\u001b[0m\n",
      "[2024-01-15 09:41:12] \u001b[32mTrain: [  1/50] Step 360/520 Loss 5.559 Prec@(1,5) (8.5%, 27.1%)\u001b[0m\n",
      "[2024-01-15 09:41:12] \u001b[32mTrain: [  1/50] Step 380/520 Loss 5.538 Prec@(1,5) (8.7%, 27.5%)\u001b[0m\n",
      "[2024-01-15 09:41:13] \u001b[32mTrain: [  1/50] Step 400/520 Loss 5.517 Prec@(1,5) (9.0%, 28.0%)\u001b[0m\n",
      "[2024-01-15 09:41:14] \u001b[32mTrain: [  1/50] Step 420/520 Loss 5.499 Prec@(1,5) (9.1%, 28.4%)\u001b[0m\n",
      "[2024-01-15 09:41:15] \u001b[32mTrain: [  1/50] Step 440/520 Loss 5.479 Prec@(1,5) (9.3%, 28.8%)\u001b[0m\n",
      "[2024-01-15 09:41:16] \u001b[32mTrain: [  1/50] Step 460/520 Loss 5.458 Prec@(1,5) (9.6%, 29.2%)\u001b[0m\n",
      "[2024-01-15 09:41:17] \u001b[32mTrain: [  1/50] Step 480/520 Loss 5.440 Prec@(1,5) (9.7%, 29.6%)\u001b[0m\n",
      "[2024-01-15 09:41:18] \u001b[32mTrain: [  1/50] Step 500/520 Loss 5.425 Prec@(1,5) (9.9%, 30.0%)\u001b[0m\n",
      "[2024-01-15 09:41:19] \u001b[32mTrain: [  1/50] Step 520/520 Loss 5.409 Prec@(1,5) (10.1%, 30.4%)\u001b[0m\n",
      "[2024-01-15 09:41:19] \u001b[32mTrain: [  1/50] Final Prec@1 10.0820%\u001b[0m\n",
      "[2024-01-15 09:41:24] \u001b[32mValid: [  1/50] Step 000/104 Loss 5.153 Prec@(1,5) (15.6%, 37.5%)\u001b[0m\n",
      "[2024-01-15 09:41:24] \u001b[32mValid: [  1/50] Step 020/104 Loss 4.920 Prec@(1,5) (13.9%, 37.8%)\u001b[0m\n",
      "[2024-01-15 09:41:24] \u001b[32mValid: [  1/50] Step 040/104 Loss 4.853 Prec@(1,5) (14.3%, 38.3%)\u001b[0m\n",
      "[2024-01-15 09:41:25] \u001b[32mValid: [  1/50] Step 060/104 Loss 4.821 Prec@(1,5) (14.5%, 38.2%)\u001b[0m\n",
      "[2024-01-15 09:41:25] \u001b[32mValid: [  1/50] Step 080/104 Loss 4.850 Prec@(1,5) (14.2%, 38.1%)\u001b[0m\n",
      "[2024-01-15 09:41:25] \u001b[32mValid: [  1/50] Step 100/104 Loss 4.875 Prec@(1,5) (14.0%, 38.0%)\u001b[0m\n",
      "[2024-01-15 09:41:25] \u001b[32mValid: [  1/50] Step 104/104 Loss 4.876 Prec@(1,5) (13.9%, 38.0%)\u001b[0m\n",
      "[2024-01-15 09:41:26] \u001b[32mValid: [  1/50] Final Prec@1 13.9200%\u001b[0m\n",
      "[2024-01-15 09:41:26] \u001b[32mEpoch 1 LR 0.024975\u001b[0m\n",
      "[2024-01-15 09:41:34] \u001b[32mTrain: [  2/50] Step 000/520 Loss 4.762 Prec@(1,5) (15.6%, 43.8%)\u001b[0m\n",
      "[2024-01-15 09:41:35] \u001b[32mTrain: [  2/50] Step 020/520 Loss 4.878 Prec@(1,5) (15.2%, 41.0%)\u001b[0m\n",
      "[2024-01-15 09:41:36] \u001b[32mTrain: [  2/50] Step 040/520 Loss 4.896 Prec@(1,5) (15.3%, 41.9%)\u001b[0m\n",
      "[2024-01-15 09:41:37] \u001b[32mTrain: [  2/50] Step 060/520 Loss 4.906 Prec@(1,5) (15.3%, 41.4%)\u001b[0m\n",
      "[2024-01-15 09:41:38] \u001b[32mTrain: [  2/50] Step 080/520 Loss 4.899 Prec@(1,5) (15.6%, 41.3%)\u001b[0m\n",
      "[2024-01-15 09:41:38] \u001b[32mTrain: [  2/50] Step 100/520 Loss 4.888 Prec@(1,5) (15.5%, 41.7%)\u001b[0m\n",
      "[2024-01-15 09:41:39] \u001b[32mTrain: [  2/50] Step 120/520 Loss 4.899 Prec@(1,5) (15.5%, 41.6%)\u001b[0m\n",
      "[2024-01-15 09:41:40] \u001b[32mTrain: [  2/50] Step 140/520 Loss 4.881 Prec@(1,5) (15.8%, 42.0%)\u001b[0m\n",
      "[2024-01-15 09:41:41] \u001b[32mTrain: [  2/50] Step 160/520 Loss 4.866 Prec@(1,5) (16.1%, 42.3%)\u001b[0m\n",
      "[2024-01-15 09:41:42] \u001b[32mTrain: [  2/50] Step 180/520 Loss 4.853 Prec@(1,5) (16.2%, 42.6%)\u001b[0m\n",
      "[2024-01-15 09:41:43] \u001b[32mTrain: [  2/50] Step 200/520 Loss 4.836 Prec@(1,5) (16.5%, 42.9%)\u001b[0m\n",
      "[2024-01-15 09:41:44] \u001b[32mTrain: [  2/50] Step 220/520 Loss 4.823 Prec@(1,5) (16.7%, 43.1%)\u001b[0m\n",
      "[2024-01-15 09:41:45] \u001b[32mTrain: [  2/50] Step 240/520 Loss 4.813 Prec@(1,5) (16.8%, 43.4%)\u001b[0m\n",
      "[2024-01-15 09:41:46] \u001b[32mTrain: [  2/50] Step 260/520 Loss 4.800 Prec@(1,5) (16.9%, 43.8%)\u001b[0m\n",
      "[2024-01-15 09:41:47] \u001b[32mTrain: [  2/50] Step 280/520 Loss 4.787 Prec@(1,5) (17.0%, 44.0%)\u001b[0m\n",
      "[2024-01-15 09:41:48] \u001b[32mTrain: [  2/50] Step 300/520 Loss 4.776 Prec@(1,5) (17.2%, 44.2%)\u001b[0m\n",
      "[2024-01-15 09:41:49] \u001b[32mTrain: [  2/50] Step 320/520 Loss 4.765 Prec@(1,5) (17.4%, 44.4%)\u001b[0m\n",
      "[2024-01-15 09:41:50] \u001b[32mTrain: [  2/50] Step 340/520 Loss 4.753 Prec@(1,5) (17.5%, 44.7%)\u001b[0m\n",
      "[2024-01-15 09:41:51] \u001b[32mTrain: [  2/50] Step 360/520 Loss 4.740 Prec@(1,5) (17.7%, 44.9%)\u001b[0m\n",
      "[2024-01-15 09:41:52] \u001b[32mTrain: [  2/50] Step 380/520 Loss 4.729 Prec@(1,5) (17.8%, 45.2%)\u001b[0m\n",
      "[2024-01-15 09:41:53] \u001b[32mTrain: [  2/50] Step 400/520 Loss 4.721 Prec@(1,5) (17.9%, 45.3%)\u001b[0m\n",
      "[2024-01-15 09:41:53] \u001b[32mTrain: [  2/50] Step 420/520 Loss 4.711 Prec@(1,5) (18.0%, 45.5%)\u001b[0m\n",
      "[2024-01-15 09:41:54] \u001b[32mTrain: [  2/50] Step 440/520 Loss 4.699 Prec@(1,5) (18.2%, 45.7%)\u001b[0m\n",
      "[2024-01-15 09:41:55] \u001b[32mTrain: [  2/50] Step 460/520 Loss 4.685 Prec@(1,5) (18.3%, 46.0%)\u001b[0m\n",
      "[2024-01-15 09:41:56] \u001b[32mTrain: [  2/50] Step 480/520 Loss 4.677 Prec@(1,5) (18.5%, 46.2%)\u001b[0m\n",
      "[2024-01-15 09:41:57] \u001b[32mTrain: [  2/50] Step 500/520 Loss 4.669 Prec@(1,5) (18.6%, 46.3%)\u001b[0m\n",
      "[2024-01-15 09:41:58] \u001b[32mTrain: [  2/50] Step 520/520 Loss 4.655 Prec@(1,5) (18.8%, 46.6%)\u001b[0m\n",
      "[2024-01-15 09:41:58] \u001b[32mTrain: [  2/50] Final Prec@1 18.7520%\u001b[0m\n",
      "[2024-01-15 09:42:03] \u001b[32mValid: [  2/50] Step 000/104 Loss 4.711 Prec@(1,5) (19.8%, 51.0%)\u001b[0m\n",
      "[2024-01-15 09:42:04] \u001b[32mValid: [  2/50] Step 020/104 Loss 4.598 Prec@(1,5) (19.5%, 48.5%)\u001b[0m\n",
      "[2024-01-15 09:42:04] \u001b[32mValid: [  2/50] Step 040/104 Loss 4.534 Prec@(1,5) (19.4%, 49.0%)\u001b[0m\n",
      "[2024-01-15 09:42:04] \u001b[32mValid: [  2/50] Step 060/104 Loss 4.525 Prec@(1,5) (19.5%, 49.2%)\u001b[0m\n",
      "[2024-01-15 09:42:05] \u001b[32mValid: [  2/50] Step 080/104 Loss 4.561 Prec@(1,5) (19.4%, 49.1%)\u001b[0m\n",
      "[2024-01-15 09:42:05] \u001b[32mValid: [  2/50] Step 100/104 Loss 4.577 Prec@(1,5) (19.0%, 49.1%)\u001b[0m\n",
      "[2024-01-15 09:42:05] \u001b[32mValid: [  2/50] Step 104/104 Loss 4.572 Prec@(1,5) (19.1%, 49.2%)\u001b[0m\n",
      "[2024-01-15 09:42:05] \u001b[32mValid: [  2/50] Final Prec@1 19.0700%\u001b[0m\n",
      "[2024-01-15 09:42:05] \u001b[32mEpoch 2 LR 0.024901\u001b[0m\n",
      "[2024-01-15 09:42:14] \u001b[32mTrain: [  3/50] Step 000/520 Loss 4.681 Prec@(1,5) (25.0%, 50.0%)\u001b[0m\n",
      "[2024-01-15 09:42:15] \u001b[32mTrain: [  3/50] Step 020/520 Loss 4.361 Prec@(1,5) (23.6%, 52.5%)\u001b[0m\n",
      "[2024-01-15 09:42:16] \u001b[32mTrain: [  3/50] Step 040/520 Loss 4.325 Prec@(1,5) (23.3%, 53.7%)\u001b[0m\n",
      "[2024-01-15 09:42:17] \u001b[32mTrain: [  3/50] Step 060/520 Loss 4.333 Prec@(1,5) (23.2%, 53.5%)\u001b[0m\n",
      "[2024-01-15 09:42:17] \u001b[32mTrain: [  3/50] Step 080/520 Loss 4.338 Prec@(1,5) (23.5%, 53.2%)\u001b[0m\n",
      "[2024-01-15 09:42:18] \u001b[32mTrain: [  3/50] Step 100/520 Loss 4.324 Prec@(1,5) (23.6%, 53.1%)\u001b[0m\n",
      "[2024-01-15 09:42:19] \u001b[32mTrain: [  3/50] Step 120/520 Loss 4.314 Prec@(1,5) (23.7%, 53.2%)\u001b[0m\n",
      "[2024-01-15 09:42:20] \u001b[32mTrain: [  3/50] Step 140/520 Loss 4.304 Prec@(1,5) (23.7%, 53.3%)\u001b[0m\n",
      "[2024-01-15 09:42:21] \u001b[32mTrain: [  3/50] Step 160/520 Loss 4.295 Prec@(1,5) (23.7%, 53.5%)\u001b[0m\n",
      "[2024-01-15 09:42:22] \u001b[32mTrain: [  3/50] Step 180/520 Loss 4.294 Prec@(1,5) (23.7%, 53.5%)\u001b[0m\n",
      "[2024-01-15 09:42:23] \u001b[32mTrain: [  3/50] Step 200/520 Loss 4.286 Prec@(1,5) (23.8%, 53.6%)\u001b[0m\n",
      "[2024-01-15 09:42:24] \u001b[32mTrain: [  3/50] Step 220/520 Loss 4.279 Prec@(1,5) (23.9%, 53.8%)\u001b[0m\n",
      "[2024-01-15 09:42:25] \u001b[32mTrain: [  3/50] Step 240/520 Loss 4.266 Prec@(1,5) (23.9%, 54.0%)\u001b[0m\n",
      "[2024-01-15 09:42:26] \u001b[32mTrain: [  3/50] Step 260/520 Loss 4.263 Prec@(1,5) (24.0%, 54.1%)\u001b[0m\n",
      "[2024-01-15 09:42:27] \u001b[32mTrain: [  3/50] Step 280/520 Loss 4.254 Prec@(1,5) (24.1%, 54.3%)\u001b[0m\n",
      "[2024-01-15 09:42:28] \u001b[32mTrain: [  3/50] Step 300/520 Loss 4.247 Prec@(1,5) (24.2%, 54.3%)\u001b[0m\n",
      "[2024-01-15 09:42:29] \u001b[32mTrain: [  3/50] Step 320/520 Loss 4.236 Prec@(1,5) (24.2%, 54.5%)\u001b[0m\n",
      "[2024-01-15 09:42:30] \u001b[32mTrain: [  3/50] Step 340/520 Loss 4.222 Prec@(1,5) (24.5%, 54.7%)\u001b[0m\n",
      "[2024-01-15 09:42:31] \u001b[32mTrain: [  3/50] Step 360/520 Loss 4.213 Prec@(1,5) (24.6%, 54.8%)\u001b[0m\n",
      "[2024-01-15 09:42:32] \u001b[32mTrain: [  3/50] Step 380/520 Loss 4.205 Prec@(1,5) (24.7%, 54.9%)\u001b[0m\n",
      "[2024-01-15 09:42:33] \u001b[32mTrain: [  3/50] Step 400/520 Loss 4.201 Prec@(1,5) (24.8%, 54.9%)\u001b[0m\n",
      "[2024-01-15 09:42:34] \u001b[32mTrain: [  3/50] Step 420/520 Loss 4.197 Prec@(1,5) (24.9%, 55.0%)\u001b[0m\n",
      "[2024-01-15 09:42:35] \u001b[32mTrain: [  3/50] Step 440/520 Loss 4.195 Prec@(1,5) (24.8%, 55.1%)\u001b[0m\n",
      "[2024-01-15 09:42:36] \u001b[32mTrain: [  3/50] Step 460/520 Loss 4.189 Prec@(1,5) (24.9%, 55.2%)\u001b[0m\n",
      "[2024-01-15 09:42:37] \u001b[32mTrain: [  3/50] Step 480/520 Loss 4.186 Prec@(1,5) (25.0%, 55.2%)\u001b[0m\n",
      "[2024-01-15 09:42:38] \u001b[32mTrain: [  3/50] Step 500/520 Loss 4.182 Prec@(1,5) (25.0%, 55.3%)\u001b[0m\n",
      "[2024-01-15 09:42:39] \u001b[32mTrain: [  3/50] Step 520/520 Loss 4.176 Prec@(1,5) (25.1%, 55.4%)\u001b[0m\n",
      "[2024-01-15 09:42:39] \u001b[32mTrain: [  3/50] Final Prec@1 25.1220%\u001b[0m\n",
      "[2024-01-15 09:42:44] \u001b[32mValid: [  3/50] Step 000/104 Loss 3.992 Prec@(1,5) (28.1%, 56.2%)\u001b[0m\n",
      "[2024-01-15 09:42:44] \u001b[32mValid: [  3/50] Step 020/104 Loss 3.943 Prec@(1,5) (27.1%, 57.4%)\u001b[0m\n",
      "[2024-01-15 09:42:45] \u001b[32mValid: [  3/50] Step 040/104 Loss 3.904 Prec@(1,5) (27.1%, 57.3%)\u001b[0m\n",
      "[2024-01-15 09:42:45] \u001b[32mValid: [  3/50] Step 060/104 Loss 3.885 Prec@(1,5) (26.8%, 57.6%)\u001b[0m\n",
      "[2024-01-15 09:42:45] \u001b[32mValid: [  3/50] Step 080/104 Loss 3.902 Prec@(1,5) (26.5%, 57.4%)\u001b[0m\n",
      "[2024-01-15 09:42:46] \u001b[32mValid: [  3/50] Step 100/104 Loss 3.913 Prec@(1,5) (26.1%, 57.4%)\u001b[0m\n",
      "[2024-01-15 09:42:46] \u001b[32mValid: [  3/50] Step 104/104 Loss 3.914 Prec@(1,5) (26.1%, 57.4%)\u001b[0m\n",
      "[2024-01-15 09:42:46] \u001b[32mValid: [  3/50] Final Prec@1 26.1100%\u001b[0m\n",
      "[2024-01-15 09:42:46] \u001b[32mEpoch 3 LR 0.024779\u001b[0m\n",
      "[2024-01-15 09:42:54] \u001b[32mTrain: [  4/50] Step 000/520 Loss 4.364 Prec@(1,5) (17.7%, 58.3%)\u001b[0m\n",
      "[2024-01-15 09:42:55] \u001b[32mTrain: [  4/50] Step 020/520 Loss 3.927 Prec@(1,5) (27.8%, 59.0%)\u001b[0m\n",
      "[2024-01-15 09:42:56] \u001b[32mTrain: [  4/50] Step 040/520 Loss 3.950 Prec@(1,5) (27.9%, 59.3%)\u001b[0m\n",
      "[2024-01-15 09:42:57] \u001b[32mTrain: [  4/50] Step 060/520 Loss 3.957 Prec@(1,5) (27.6%, 59.3%)\u001b[0m\n",
      "[2024-01-15 09:42:58] \u001b[32mTrain: [  4/50] Step 080/520 Loss 3.964 Prec@(1,5) (27.7%, 59.2%)\u001b[0m\n",
      "[2024-01-15 09:42:59] \u001b[32mTrain: [  4/50] Step 100/520 Loss 3.946 Prec@(1,5) (28.1%, 59.5%)\u001b[0m\n",
      "[2024-01-15 09:43:00] \u001b[32mTrain: [  4/50] Step 120/520 Loss 3.928 Prec@(1,5) (28.4%, 59.7%)\u001b[0m\n",
      "[2024-01-15 09:43:01] \u001b[32mTrain: [  4/50] Step 140/520 Loss 3.932 Prec@(1,5) (28.2%, 59.8%)\u001b[0m\n",
      "[2024-01-15 09:43:02] \u001b[32mTrain: [  4/50] Step 160/520 Loss 3.923 Prec@(1,5) (28.5%, 59.8%)\u001b[0m\n",
      "[2024-01-15 09:43:03] \u001b[32mTrain: [  4/50] Step 180/520 Loss 3.918 Prec@(1,5) (28.6%, 60.0%)\u001b[0m\n",
      "[2024-01-15 09:43:04] \u001b[32mTrain: [  4/50] Step 200/520 Loss 3.918 Prec@(1,5) (28.7%, 60.0%)\u001b[0m\n",
      "[2024-01-15 09:43:05] \u001b[32mTrain: [  4/50] Step 220/520 Loss 3.903 Prec@(1,5) (28.9%, 60.3%)\u001b[0m\n",
      "[2024-01-15 09:43:06] \u001b[32mTrain: [  4/50] Step 240/520 Loss 3.890 Prec@(1,5) (29.2%, 60.5%)\u001b[0m\n",
      "[2024-01-15 09:43:07] \u001b[32mTrain: [  4/50] Step 260/520 Loss 3.888 Prec@(1,5) (29.1%, 60.6%)\u001b[0m\n",
      "[2024-01-15 09:43:08] \u001b[32mTrain: [  4/50] Step 280/520 Loss 3.886 Prec@(1,5) (29.2%, 60.7%)\u001b[0m\n",
      "[2024-01-15 09:43:08] \u001b[32mTrain: [  4/50] Step 300/520 Loss 3.879 Prec@(1,5) (29.2%, 60.7%)\u001b[0m\n",
      "[2024-01-15 09:43:09] \u001b[32mTrain: [  4/50] Step 320/520 Loss 3.874 Prec@(1,5) (29.2%, 60.7%)\u001b[0m\n",
      "[2024-01-15 09:43:10] \u001b[32mTrain: [  4/50] Step 340/520 Loss 3.864 Prec@(1,5) (29.4%, 60.8%)\u001b[0m\n",
      "[2024-01-15 09:43:11] \u001b[32mTrain: [  4/50] Step 360/520 Loss 3.864 Prec@(1,5) (29.3%, 60.8%)\u001b[0m\n",
      "[2024-01-15 09:43:12] \u001b[32mTrain: [  4/50] Step 380/520 Loss 3.861 Prec@(1,5) (29.5%, 60.8%)\u001b[0m\n",
      "[2024-01-15 09:43:13] \u001b[32mTrain: [  4/50] Step 400/520 Loss 3.855 Prec@(1,5) (29.5%, 60.9%)\u001b[0m\n",
      "[2024-01-15 09:43:14] \u001b[32mTrain: [  4/50] Step 420/520 Loss 3.848 Prec@(1,5) (29.6%, 61.0%)\u001b[0m\n",
      "[2024-01-15 09:43:15] \u001b[32mTrain: [  4/50] Step 440/520 Loss 3.846 Prec@(1,5) (29.7%, 61.0%)\u001b[0m\n",
      "[2024-01-15 09:43:16] \u001b[32mTrain: [  4/50] Step 460/520 Loss 3.838 Prec@(1,5) (29.8%, 61.1%)\u001b[0m\n",
      "[2024-01-15 09:43:17] \u001b[32mTrain: [  4/50] Step 480/520 Loss 3.832 Prec@(1,5) (29.9%, 61.2%)\u001b[0m\n",
      "[2024-01-15 09:43:18] \u001b[32mTrain: [  4/50] Step 500/520 Loss 3.828 Prec@(1,5) (30.0%, 61.3%)\u001b[0m\n",
      "[2024-01-15 09:43:19] \u001b[32mTrain: [  4/50] Step 520/520 Loss 3.819 Prec@(1,5) (30.2%, 61.4%)\u001b[0m\n",
      "[2024-01-15 09:43:19] \u001b[32mTrain: [  4/50] Final Prec@1 30.1540%\u001b[0m\n",
      "[2024-01-15 09:43:24] \u001b[32mValid: [  4/50] Step 000/104 Loss 3.595 Prec@(1,5) (29.2%, 62.5%)\u001b[0m\n",
      "[2024-01-15 09:43:24] \u001b[32mValid: [  4/50] Step 020/104 Loss 3.651 Prec@(1,5) (29.6%, 61.7%)\u001b[0m\n",
      "[2024-01-15 09:43:25] \u001b[32mValid: [  4/50] Step 040/104 Loss 3.614 Prec@(1,5) (29.9%, 62.0%)\u001b[0m\n",
      "[2024-01-15 09:43:25] \u001b[32mValid: [  4/50] Step 060/104 Loss 3.580 Prec@(1,5) (30.3%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:43:25] \u001b[32mValid: [  4/50] Step 080/104 Loss 3.575 Prec@(1,5) (29.9%, 62.1%)\u001b[0m\n",
      "[2024-01-15 09:43:26] \u001b[32mValid: [  4/50] Step 100/104 Loss 3.574 Prec@(1,5) (29.9%, 62.3%)\u001b[0m\n",
      "[2024-01-15 09:43:26] \u001b[32mValid: [  4/50] Step 104/104 Loss 3.575 Prec@(1,5) (29.9%, 62.3%)\u001b[0m\n",
      "[2024-01-15 09:43:26] \u001b[32mValid: [  4/50] Final Prec@1 29.9200%\u001b[0m\n",
      "[2024-01-15 09:43:26] \u001b[32mEpoch 4 LR 0.024607\u001b[0m\n",
      "[2024-01-15 09:43:34] \u001b[32mTrain: [  5/50] Step 000/520 Loss 4.027 Prec@(1,5) (28.1%, 56.2%)\u001b[0m\n",
      "[2024-01-15 09:43:35] \u001b[32mTrain: [  5/50] Step 020/520 Loss 3.652 Prec@(1,5) (32.7%, 63.0%)\u001b[0m\n",
      "[2024-01-15 09:43:36] \u001b[32mTrain: [  5/50] Step 040/520 Loss 3.682 Prec@(1,5) (31.9%, 62.4%)\u001b[0m\n",
      "[2024-01-15 09:43:37] \u001b[32mTrain: [  5/50] Step 060/520 Loss 3.641 Prec@(1,5) (32.7%, 63.8%)\u001b[0m\n",
      "[2024-01-15 09:43:38] \u001b[32mTrain: [  5/50] Step 080/520 Loss 3.649 Prec@(1,5) (32.8%, 63.8%)\u001b[0m\n",
      "[2024-01-15 09:43:39] \u001b[32mTrain: [  5/50] Step 100/520 Loss 3.660 Prec@(1,5) (32.4%, 63.7%)\u001b[0m\n",
      "[2024-01-15 09:43:40] \u001b[32mTrain: [  5/50] Step 120/520 Loss 3.661 Prec@(1,5) (32.4%, 63.7%)\u001b[0m\n",
      "[2024-01-15 09:43:41] \u001b[32mTrain: [  5/50] Step 140/520 Loss 3.668 Prec@(1,5) (32.4%, 63.8%)\u001b[0m\n",
      "[2024-01-15 09:43:41] \u001b[32mTrain: [  5/50] Step 160/520 Loss 3.654 Prec@(1,5) (32.5%, 64.0%)\u001b[0m\n",
      "[2024-01-15 09:43:42] \u001b[32mTrain: [  5/50] Step 180/520 Loss 3.644 Prec@(1,5) (32.7%, 64.3%)\u001b[0m\n",
      "[2024-01-15 09:43:43] \u001b[32mTrain: [  5/50] Step 200/520 Loss 3.636 Prec@(1,5) (32.7%, 64.5%)\u001b[0m\n",
      "[2024-01-15 09:43:44] \u001b[32mTrain: [  5/50] Step 220/520 Loss 3.625 Prec@(1,5) (32.7%, 64.7%)\u001b[0m\n",
      "[2024-01-15 09:43:45] \u001b[32mTrain: [  5/50] Step 240/520 Loss 3.623 Prec@(1,5) (32.8%, 64.8%)\u001b[0m\n",
      "[2024-01-15 09:43:46] \u001b[32mTrain: [  5/50] Step 260/520 Loss 3.619 Prec@(1,5) (33.0%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:43:47] \u001b[32mTrain: [  5/50] Step 280/520 Loss 3.611 Prec@(1,5) (33.1%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:43:48] \u001b[32mTrain: [  5/50] Step 300/520 Loss 3.608 Prec@(1,5) (33.0%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:43:49] \u001b[32mTrain: [  5/50] Step 320/520 Loss 3.597 Prec@(1,5) (33.1%, 65.3%)\u001b[0m\n",
      "[2024-01-15 09:43:50] \u001b[32mTrain: [  5/50] Step 340/520 Loss 3.597 Prec@(1,5) (33.2%, 65.3%)\u001b[0m\n",
      "[2024-01-15 09:43:51] \u001b[32mTrain: [  5/50] Step 360/520 Loss 3.592 Prec@(1,5) (33.2%, 65.3%)\u001b[0m\n",
      "[2024-01-15 09:43:52] \u001b[32mTrain: [  5/50] Step 380/520 Loss 3.586 Prec@(1,5) (33.4%, 65.5%)\u001b[0m\n",
      "[2024-01-15 09:43:53] \u001b[32mTrain: [  5/50] Step 400/520 Loss 3.581 Prec@(1,5) (33.5%, 65.6%)\u001b[0m\n",
      "[2024-01-15 09:43:54] \u001b[32mTrain: [  5/50] Step 420/520 Loss 3.573 Prec@(1,5) (33.7%, 65.7%)\u001b[0m\n",
      "[2024-01-15 09:43:55] \u001b[32mTrain: [  5/50] Step 440/520 Loss 3.568 Prec@(1,5) (33.7%, 65.8%)\u001b[0m\n",
      "[2024-01-15 09:43:56] \u001b[32mTrain: [  5/50] Step 460/520 Loss 3.564 Prec@(1,5) (33.8%, 65.8%)\u001b[0m\n",
      "[2024-01-15 09:43:56] \u001b[32mTrain: [  5/50] Step 480/520 Loss 3.564 Prec@(1,5) (33.9%, 65.9%)\u001b[0m\n",
      "[2024-01-15 09:43:57] \u001b[32mTrain: [  5/50] Step 500/520 Loss 3.560 Prec@(1,5) (34.0%, 65.9%)\u001b[0m\n",
      "[2024-01-15 09:43:58] \u001b[32mTrain: [  5/50] Step 520/520 Loss 3.556 Prec@(1,5) (34.0%, 66.0%)\u001b[0m\n",
      "[2024-01-15 09:43:59] \u001b[32mTrain: [  5/50] Final Prec@1 34.0140%\u001b[0m\n",
      "[2024-01-15 09:44:03] \u001b[32mValid: [  5/50] Step 000/104 Loss 3.447 Prec@(1,5) (36.5%, 64.6%)\u001b[0m\n",
      "[2024-01-15 09:44:04] \u001b[32mValid: [  5/50] Step 020/104 Loss 3.488 Prec@(1,5) (33.5%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:44:04] \u001b[32mValid: [  5/50] Step 040/104 Loss 3.457 Prec@(1,5) (33.5%, 65.1%)\u001b[0m\n",
      "[2024-01-15 09:44:04] \u001b[32mValid: [  5/50] Step 060/104 Loss 3.439 Prec@(1,5) (33.8%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:44:05] \u001b[32mValid: [  5/50] Step 080/104 Loss 3.443 Prec@(1,5) (33.5%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:44:05] \u001b[32mValid: [  5/50] Step 100/104 Loss 3.439 Prec@(1,5) (33.5%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:44:05] \u001b[32mValid: [  5/50] Step 104/104 Loss 3.444 Prec@(1,5) (33.5%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:44:05] \u001b[32mValid: [  5/50] Final Prec@1 33.5300%\u001b[0m\n",
      "[2024-01-15 09:44:05] \u001b[32mEpoch 5 LR 0.024388\u001b[0m\n",
      "[2024-01-15 09:44:13] \u001b[32mTrain: [  6/50] Step 000/520 Loss 3.675 Prec@(1,5) (35.4%, 66.7%)\u001b[0m\n",
      "[2024-01-15 09:44:14] \u001b[32mTrain: [  6/50] Step 020/520 Loss 3.428 Prec@(1,5) (35.9%, 67.9%)\u001b[0m\n",
      "[2024-01-15 09:44:15] \u001b[32mTrain: [  6/50] Step 040/520 Loss 3.448 Prec@(1,5) (35.4%, 68.0%)\u001b[0m\n",
      "[2024-01-15 09:44:16] \u001b[32mTrain: [  6/50] Step 060/520 Loss 3.441 Prec@(1,5) (35.7%, 67.8%)\u001b[0m\n",
      "[2024-01-15 09:44:17] \u001b[32mTrain: [  6/50] Step 080/520 Loss 3.441 Prec@(1,5) (35.5%, 67.7%)\u001b[0m\n",
      "[2024-01-15 09:44:18] \u001b[32mTrain: [  6/50] Step 100/520 Loss 3.436 Prec@(1,5) (35.7%, 67.8%)\u001b[0m\n",
      "[2024-01-15 09:44:19] \u001b[32mTrain: [  6/50] Step 120/520 Loss 3.429 Prec@(1,5) (35.9%, 67.9%)\u001b[0m\n",
      "[2024-01-15 09:44:20] \u001b[32mTrain: [  6/50] Step 140/520 Loss 3.414 Prec@(1,5) (36.1%, 68.3%)\u001b[0m\n",
      "[2024-01-15 09:44:21] \u001b[32mTrain: [  6/50] Step 160/520 Loss 3.402 Prec@(1,5) (36.3%, 68.4%)\u001b[0m\n",
      "[2024-01-15 09:44:22] \u001b[32mTrain: [  6/50] Step 180/520 Loss 3.392 Prec@(1,5) (36.6%, 68.6%)\u001b[0m\n",
      "[2024-01-15 09:44:23] \u001b[32mTrain: [  6/50] Step 200/520 Loss 3.396 Prec@(1,5) (36.6%, 68.5%)\u001b[0m\n",
      "[2024-01-15 09:44:24] \u001b[32mTrain: [  6/50] Step 220/520 Loss 3.394 Prec@(1,5) (36.6%, 68.7%)\u001b[0m\n",
      "[2024-01-15 09:44:25] \u001b[32mTrain: [  6/50] Step 240/520 Loss 3.390 Prec@(1,5) (36.6%, 68.7%)\u001b[0m\n",
      "[2024-01-15 09:44:26] \u001b[32mTrain: [  6/50] Step 260/520 Loss 3.387 Prec@(1,5) (36.6%, 68.9%)\u001b[0m\n",
      "[2024-01-15 09:44:27] \u001b[32mTrain: [  6/50] Step 280/520 Loss 3.384 Prec@(1,5) (36.6%, 68.9%)\u001b[0m\n",
      "[2024-01-15 09:44:28] \u001b[32mTrain: [  6/50] Step 300/520 Loss 3.382 Prec@(1,5) (36.5%, 69.0%)\u001b[0m\n",
      "[2024-01-15 09:44:29] \u001b[32mTrain: [  6/50] Step 320/520 Loss 3.378 Prec@(1,5) (36.6%, 69.0%)\u001b[0m\n",
      "[2024-01-15 09:44:30] \u001b[32mTrain: [  6/50] Step 340/520 Loss 3.368 Prec@(1,5) (36.8%, 69.1%)\u001b[0m\n",
      "[2024-01-15 09:44:30] \u001b[32mTrain: [  6/50] Step 360/520 Loss 3.372 Prec@(1,5) (36.7%, 69.1%)\u001b[0m\n",
      "[2024-01-15 09:44:31] \u001b[32mTrain: [  6/50] Step 380/520 Loss 3.366 Prec@(1,5) (36.9%, 69.2%)\u001b[0m\n",
      "[2024-01-15 09:44:32] \u001b[32mTrain: [  6/50] Step 400/520 Loss 3.366 Prec@(1,5) (36.9%, 69.2%)\u001b[0m\n",
      "[2024-01-15 09:44:33] \u001b[32mTrain: [  6/50] Step 420/520 Loss 3.363 Prec@(1,5) (36.9%, 69.2%)\u001b[0m\n",
      "[2024-01-15 09:44:34] \u001b[32mTrain: [  6/50] Step 440/520 Loss 3.361 Prec@(1,5) (37.0%, 69.2%)\u001b[0m\n",
      "[2024-01-15 09:44:35] \u001b[32mTrain: [  6/50] Step 460/520 Loss 3.357 Prec@(1,5) (37.1%, 69.2%)\u001b[0m\n",
      "[2024-01-15 09:44:36] \u001b[32mTrain: [  6/50] Step 480/520 Loss 3.351 Prec@(1,5) (37.2%, 69.3%)\u001b[0m\n",
      "[2024-01-15 09:44:37] \u001b[32mTrain: [  6/50] Step 500/520 Loss 3.351 Prec@(1,5) (37.2%, 69.3%)\u001b[0m\n",
      "[2024-01-15 09:44:38] \u001b[32mTrain: [  6/50] Step 520/520 Loss 3.350 Prec@(1,5) (37.2%, 69.4%)\u001b[0m\n",
      "[2024-01-15 09:44:38] \u001b[32mTrain: [  6/50] Final Prec@1 37.1920%\u001b[0m\n",
      "[2024-01-15 09:44:43] \u001b[32mValid: [  6/50] Step 000/104 Loss 3.925 Prec@(1,5) (30.2%, 61.5%)\u001b[0m\n",
      "[2024-01-15 09:44:43] \u001b[32mValid: [  6/50] Step 020/104 Loss 3.670 Prec@(1,5) (33.7%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:44:44] \u001b[32mValid: [  6/50] Step 040/104 Loss 3.674 Prec@(1,5) (33.2%, 64.4%)\u001b[0m\n",
      "[2024-01-15 09:44:44] \u001b[32mValid: [  6/50] Step 060/104 Loss 3.618 Prec@(1,5) (33.4%, 65.0%)\u001b[0m\n",
      "[2024-01-15 09:44:44] \u001b[32mValid: [  6/50] Step 080/104 Loss 3.644 Prec@(1,5) (32.9%, 64.9%)\u001b[0m\n",
      "[2024-01-15 09:44:45] \u001b[32mValid: [  6/50] Step 100/104 Loss 3.644 Prec@(1,5) (32.8%, 64.6%)\u001b[0m\n",
      "[2024-01-15 09:44:45] \u001b[32mValid: [  6/50] Step 104/104 Loss 3.643 Prec@(1,5) (32.7%, 64.6%)\u001b[0m\n",
      "[2024-01-15 09:44:45] \u001b[32mValid: [  6/50] Final Prec@1 32.7400%\u001b[0m\n",
      "[2024-01-15 09:44:45] \u001b[32mEpoch 6 LR 0.024122\u001b[0m\n",
      "[2024-01-15 09:44:53] \u001b[32mTrain: [  7/50] Step 000/520 Loss 3.260 Prec@(1,5) (36.5%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:44:54] \u001b[32mTrain: [  7/50] Step 020/520 Loss 3.204 Prec@(1,5) (39.8%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:44:55] \u001b[32mTrain: [  7/50] Step 040/520 Loss 3.234 Prec@(1,5) (39.4%, 70.4%)\u001b[0m\n",
      "[2024-01-15 09:44:56] \u001b[32mTrain: [  7/50] Step 060/520 Loss 3.226 Prec@(1,5) (39.4%, 70.7%)\u001b[0m\n",
      "[2024-01-15 09:44:57] \u001b[32mTrain: [  7/50] Step 080/520 Loss 3.226 Prec@(1,5) (39.4%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:44:58] \u001b[32mTrain: [  7/50] Step 100/520 Loss 3.213 Prec@(1,5) (39.5%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:44:59] \u001b[32mTrain: [  7/50] Step 120/520 Loss 3.207 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:00] \u001b[32mTrain: [  7/50] Step 140/520 Loss 3.207 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:01] \u001b[32mTrain: [  7/50] Step 160/520 Loss 3.216 Prec@(1,5) (39.5%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:02] \u001b[32mTrain: [  7/50] Step 180/520 Loss 3.208 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:03] \u001b[32mTrain: [  7/50] Step 200/520 Loss 3.206 Prec@(1,5) (39.7%, 71.1%)\u001b[0m\n",
      "[2024-01-15 09:45:04] \u001b[32mTrain: [  7/50] Step 220/520 Loss 3.203 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:05] \u001b[32mTrain: [  7/50] Step 240/520 Loss 3.206 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:06] \u001b[32mTrain: [  7/50] Step 260/520 Loss 3.202 Prec@(1,5) (39.7%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:45:07] \u001b[32mTrain: [  7/50] Step 280/520 Loss 3.201 Prec@(1,5) (39.8%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:45:08] \u001b[32mTrain: [  7/50] Step 300/520 Loss 3.204 Prec@(1,5) (39.6%, 71.2%)\u001b[0m\n",
      "[2024-01-15 09:45:09] \u001b[32mTrain: [  7/50] Step 320/520 Loss 3.205 Prec@(1,5) (39.5%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:45:10] \u001b[32mTrain: [  7/50] Step 340/520 Loss 3.199 Prec@(1,5) (39.6%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:11] \u001b[32mTrain: [  7/50] Step 360/520 Loss 3.198 Prec@(1,5) (39.6%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:12] \u001b[32mTrain: [  7/50] Step 380/520 Loss 3.197 Prec@(1,5) (39.5%, 71.5%)\u001b[0m\n",
      "[2024-01-15 09:45:13] \u001b[32mTrain: [  7/50] Step 400/520 Loss 3.200 Prec@(1,5) (39.5%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:13] \u001b[32mTrain: [  7/50] Step 420/520 Loss 3.202 Prec@(1,5) (39.4%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:14] \u001b[32mTrain: [  7/50] Step 440/520 Loss 3.203 Prec@(1,5) (39.3%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:15] \u001b[32mTrain: [  7/50] Step 460/520 Loss 3.202 Prec@(1,5) (39.4%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:16] \u001b[32mTrain: [  7/50] Step 480/520 Loss 3.202 Prec@(1,5) (39.3%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:45:17] \u001b[32mTrain: [  7/50] Step 500/520 Loss 3.200 Prec@(1,5) (39.4%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:18] \u001b[32mTrain: [  7/50] Step 520/520 Loss 3.200 Prec@(1,5) (39.4%, 71.4%)\u001b[0m\n",
      "[2024-01-15 09:45:19] \u001b[32mTrain: [  7/50] Final Prec@1 39.3880%\u001b[0m\n",
      "[2024-01-15 09:45:24] \u001b[32mValid: [  7/50] Step 000/104 Loss 3.466 Prec@(1,5) (39.6%, 70.8%)\u001b[0m\n",
      "[2024-01-15 09:45:24] \u001b[32mValid: [  7/50] Step 020/104 Loss 3.386 Prec@(1,5) (37.5%, 70.6%)\u001b[0m\n",
      "[2024-01-15 09:45:24] \u001b[32mValid: [  7/50] Step 040/104 Loss 3.370 Prec@(1,5) (37.1%, 70.2%)\u001b[0m\n",
      "[2024-01-15 09:45:25] \u001b[32mValid: [  7/50] Step 060/104 Loss 3.357 Prec@(1,5) (37.2%, 70.1%)\u001b[0m\n",
      "[2024-01-15 09:45:25] \u001b[32mValid: [  7/50] Step 080/104 Loss 3.379 Prec@(1,5) (36.9%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:45:25] \u001b[32mValid: [  7/50] Step 100/104 Loss 3.378 Prec@(1,5) (36.6%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:45:25] \u001b[32mValid: [  7/50] Step 104/104 Loss 3.374 Prec@(1,5) (36.7%, 70.0%)\u001b[0m\n",
      "[2024-01-15 09:45:26] \u001b[32mValid: [  7/50] Final Prec@1 36.7100%\u001b[0m\n",
      "[2024-01-15 09:45:26] \u001b[32mEpoch 7 LR 0.023810\u001b[0m\n",
      "[2024-01-15 09:45:34] \u001b[32mTrain: [  8/50] Step 000/520 Loss 2.890 Prec@(1,5) (44.8%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:45:35] \u001b[32mTrain: [  8/50] Step 020/520 Loss 3.057 Prec@(1,5) (43.3%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:36] \u001b[32mTrain: [  8/50] Step 040/520 Loss 3.079 Prec@(1,5) (42.6%, 72.6%)\u001b[0m\n",
      "[2024-01-15 09:45:37] \u001b[32mTrain: [  8/50] Step 060/520 Loss 3.091 Prec@(1,5) (41.9%, 72.7%)\u001b[0m\n",
      "[2024-01-15 09:45:38] \u001b[32mTrain: [  8/50] Step 080/520 Loss 3.071 Prec@(1,5) (41.9%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:45:39] \u001b[32mTrain: [  8/50] Step 100/520 Loss 3.061 Prec@(1,5) (41.9%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:39] \u001b[32mTrain: [  8/50] Step 120/520 Loss 3.065 Prec@(1,5) (41.8%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:45:40] \u001b[32mTrain: [  8/50] Step 140/520 Loss 3.056 Prec@(1,5) (41.9%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:41] \u001b[32mTrain: [  8/50] Step 160/520 Loss 3.061 Prec@(1,5) (41.8%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:42] \u001b[32mTrain: [  8/50] Step 180/520 Loss 3.068 Prec@(1,5) (41.8%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:45:43] \u001b[32mTrain: [  8/50] Step 200/520 Loss 3.066 Prec@(1,5) (41.8%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:44] \u001b[32mTrain: [  8/50] Step 220/520 Loss 3.070 Prec@(1,5) (41.6%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:45] \u001b[32mTrain: [  8/50] Step 240/520 Loss 3.070 Prec@(1,5) (41.6%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:46] \u001b[32mTrain: [  8/50] Step 260/520 Loss 3.068 Prec@(1,5) (41.7%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:47] \u001b[32mTrain: [  8/50] Step 280/520 Loss 3.069 Prec@(1,5) (41.6%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:48] \u001b[32mTrain: [  8/50] Step 300/520 Loss 3.064 Prec@(1,5) (41.7%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:49] \u001b[32mTrain: [  8/50] Step 320/520 Loss 3.064 Prec@(1,5) (41.6%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:50] \u001b[32mTrain: [  8/50] Step 340/520 Loss 3.066 Prec@(1,5) (41.6%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:51] \u001b[32mTrain: [  8/50] Step 360/520 Loss 3.068 Prec@(1,5) (41.7%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:52] \u001b[32mTrain: [  8/50] Step 380/520 Loss 3.065 Prec@(1,5) (41.7%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:53] \u001b[32mTrain: [  8/50] Step 400/520 Loss 3.067 Prec@(1,5) (41.7%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:54] \u001b[32mTrain: [  8/50] Step 420/520 Loss 3.069 Prec@(1,5) (41.6%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:54] \u001b[32mTrain: [  8/50] Step 440/520 Loss 3.068 Prec@(1,5) (41.6%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:55] \u001b[32mTrain: [  8/50] Step 460/520 Loss 3.071 Prec@(1,5) (41.6%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:56] \u001b[32mTrain: [  8/50] Step 480/520 Loss 3.065 Prec@(1,5) (41.7%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:57] \u001b[32mTrain: [  8/50] Step 500/520 Loss 3.066 Prec@(1,5) (41.7%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:45:58] \u001b[32mTrain: [  8/50] Step 520/520 Loss 3.066 Prec@(1,5) (41.8%, 73.3%)\u001b[0m\n",
      "[2024-01-15 09:45:59] \u001b[32mTrain: [  8/50] Final Prec@1 41.7760%\u001b[0m\n",
      "[2024-01-15 09:46:04] \u001b[32mValid: [  8/50] Step 000/104 Loss 2.794 Prec@(1,5) (47.9%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:46:04] \u001b[32mValid: [  8/50] Step 020/104 Loss 3.156 Prec@(1,5) (40.2%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:46:04] \u001b[32mValid: [  8/50] Step 040/104 Loss 3.144 Prec@(1,5) (39.8%, 72.0%)\u001b[0m\n",
      "[2024-01-15 09:46:05] \u001b[32mValid: [  8/50] Step 060/104 Loss 3.103 Prec@(1,5) (39.8%, 72.1%)\u001b[0m\n",
      "[2024-01-15 09:46:05] \u001b[32mValid: [  8/50] Step 080/104 Loss 3.128 Prec@(1,5) (39.6%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:46:06] \u001b[32mValid: [  8/50] Step 100/104 Loss 3.119 Prec@(1,5) (39.8%, 71.8%)\u001b[0m\n",
      "[2024-01-15 09:46:06] \u001b[32mValid: [  8/50] Step 104/104 Loss 3.118 Prec@(1,5) (39.7%, 71.9%)\u001b[0m\n",
      "[2024-01-15 09:46:06] \u001b[32mValid: [  8/50] Final Prec@1 39.7200%\u001b[0m\n",
      "[2024-01-15 09:46:06] \u001b[32mEpoch 8 LR 0.023454\u001b[0m\n",
      "[2024-01-15 09:46:14] \u001b[32mTrain: [  9/50] Step 000/520 Loss 3.093 Prec@(1,5) (43.8%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:46:15] \u001b[32mTrain: [  9/50] Step 020/520 Loss 3.040 Prec@(1,5) (42.6%, 72.9%)\u001b[0m\n",
      "[2024-01-15 09:46:16] \u001b[32mTrain: [  9/50] Step 040/520 Loss 3.005 Prec@(1,5) (43.0%, 73.2%)\u001b[0m\n",
      "[2024-01-15 09:46:17] \u001b[32mTrain: [  9/50] Step 060/520 Loss 3.003 Prec@(1,5) (42.6%, 73.5%)\u001b[0m\n",
      "[2024-01-15 09:46:18] \u001b[32mTrain: [  9/50] Step 080/520 Loss 3.011 Prec@(1,5) (42.4%, 73.4%)\u001b[0m\n",
      "[2024-01-15 09:46:19] \u001b[32mTrain: [  9/50] Step 100/520 Loss 3.012 Prec@(1,5) (42.3%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:46:20] \u001b[32mTrain: [  9/50] Step 120/520 Loss 3.009 Prec@(1,5) (42.4%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:46:21] \u001b[32mTrain: [  9/50] Step 140/520 Loss 2.989 Prec@(1,5) (42.7%, 74.1%)\u001b[0m\n",
      "[2024-01-15 09:46:22] \u001b[32mTrain: [  9/50] Step 160/520 Loss 2.978 Prec@(1,5) (42.8%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:46:23] \u001b[32mTrain: [  9/50] Step 180/520 Loss 2.977 Prec@(1,5) (42.8%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:46:24] \u001b[32mTrain: [  9/50] Step 200/520 Loss 2.984 Prec@(1,5) (42.7%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:46:25] \u001b[32mTrain: [  9/50] Step 220/520 Loss 2.976 Prec@(1,5) (42.9%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:25] \u001b[32mTrain: [  9/50] Step 240/520 Loss 2.973 Prec@(1,5) (42.9%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:26] \u001b[32mTrain: [  9/50] Step 260/520 Loss 2.975 Prec@(1,5) (43.0%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:46:27] \u001b[32mTrain: [  9/50] Step 280/520 Loss 2.971 Prec@(1,5) (43.2%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:46:28] \u001b[32mTrain: [  9/50] Step 300/520 Loss 2.970 Prec@(1,5) (43.2%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:46:29] \u001b[32mTrain: [  9/50] Step 320/520 Loss 2.967 Prec@(1,5) (43.3%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:30] \u001b[32mTrain: [  9/50] Step 340/520 Loss 2.965 Prec@(1,5) (43.3%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:31] \u001b[32mTrain: [  9/50] Step 360/520 Loss 2.963 Prec@(1,5) (43.3%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:46:32] \u001b[32mTrain: [  9/50] Step 380/520 Loss 2.961 Prec@(1,5) (43.3%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:33] \u001b[32mTrain: [  9/50] Step 400/520 Loss 2.961 Prec@(1,5) (43.2%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:34] \u001b[32mTrain: [  9/50] Step 420/520 Loss 2.959 Prec@(1,5) (43.3%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:35] \u001b[32mTrain: [  9/50] Step 440/520 Loss 2.958 Prec@(1,5) (43.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:36] \u001b[32mTrain: [  9/50] Step 460/520 Loss 2.961 Prec@(1,5) (43.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:37] \u001b[32mTrain: [  9/50] Step 480/520 Loss 2.962 Prec@(1,5) (43.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:38] \u001b[32mTrain: [  9/50] Step 500/520 Loss 2.961 Prec@(1,5) (43.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 09:46:39] \u001b[32mTrain: [  9/50] Step 520/520 Loss 2.958 Prec@(1,5) (43.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 09:46:39] \u001b[32mTrain: [  9/50] Final Prec@1 43.4300%\u001b[0m\n",
      "[2024-01-15 09:46:44] \u001b[32mValid: [  9/50] Step 000/104 Loss 3.087 Prec@(1,5) (43.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:46:44] \u001b[32mValid: [  9/50] Step 020/104 Loss 3.132 Prec@(1,5) (40.3%, 71.6%)\u001b[0m\n",
      "[2024-01-15 09:46:45] \u001b[32mValid: [  9/50] Step 040/104 Loss 3.106 Prec@(1,5) (40.0%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:46:45] \u001b[32mValid: [  9/50] Step 060/104 Loss 3.049 Prec@(1,5) (40.2%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:46:45] \u001b[32mValid: [  9/50] Step 080/104 Loss 3.064 Prec@(1,5) (39.8%, 71.3%)\u001b[0m\n",
      "[2024-01-15 09:46:46] \u001b[32mValid: [  9/50] Step 100/104 Loss 3.052 Prec@(1,5) (39.7%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:46:46] \u001b[32mValid: [  9/50] Step 104/104 Loss 3.052 Prec@(1,5) (39.7%, 71.7%)\u001b[0m\n",
      "[2024-01-15 09:46:46] \u001b[32mValid: [  9/50] Final Prec@1 39.7100%\u001b[0m\n",
      "[2024-01-15 09:46:46] \u001b[32mEpoch 9 LR 0.023054\u001b[0m\n",
      "[2024-01-15 09:46:54] \u001b[32mTrain: [ 10/50] Step 000/520 Loss 2.490 Prec@(1,5) (52.1%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:46:55] \u001b[32mTrain: [ 10/50] Step 020/520 Loss 2.835 Prec@(1,5) (45.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:46:56] \u001b[32mTrain: [ 10/50] Step 040/520 Loss 2.897 Prec@(1,5) (44.5%, 75.1%)\u001b[0m\n",
      "[2024-01-15 09:46:57] \u001b[32mTrain: [ 10/50] Step 060/520 Loss 2.900 Prec@(1,5) (44.8%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:46:58] \u001b[32mTrain: [ 10/50] Step 080/520 Loss 2.891 Prec@(1,5) (44.7%, 75.3%)\u001b[0m\n",
      "[2024-01-15 09:46:59] \u001b[32mTrain: [ 10/50] Step 100/520 Loss 2.887 Prec@(1,5) (44.7%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:00] \u001b[32mTrain: [ 10/50] Step 120/520 Loss 2.893 Prec@(1,5) (44.6%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:00] \u001b[32mTrain: [ 10/50] Step 140/520 Loss 2.880 Prec@(1,5) (44.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:01] \u001b[32mTrain: [ 10/50] Step 160/520 Loss 2.885 Prec@(1,5) (44.7%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:47:02] \u001b[32mTrain: [ 10/50] Step 180/520 Loss 2.879 Prec@(1,5) (44.8%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:03] \u001b[32mTrain: [ 10/50] Step 200/520 Loss 2.879 Prec@(1,5) (44.7%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:04] \u001b[32mTrain: [ 10/50] Step 220/520 Loss 2.881 Prec@(1,5) (44.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:05] \u001b[32mTrain: [ 10/50] Step 240/520 Loss 2.879 Prec@(1,5) (44.6%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:47:06] \u001b[32mTrain: [ 10/50] Step 260/520 Loss 2.886 Prec@(1,5) (44.5%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:47:07] \u001b[32mTrain: [ 10/50] Step 280/520 Loss 2.883 Prec@(1,5) (44.5%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:08] \u001b[32mTrain: [ 10/50] Step 300/520 Loss 2.886 Prec@(1,5) (44.3%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:09] \u001b[32mTrain: [ 10/50] Step 320/520 Loss 2.886 Prec@(1,5) (44.4%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:47:10] \u001b[32mTrain: [ 10/50] Step 340/520 Loss 2.886 Prec@(1,5) (44.4%, 75.8%)\u001b[0m\n",
      "[2024-01-15 09:47:11] \u001b[32mTrain: [ 10/50] Step 360/520 Loss 2.885 Prec@(1,5) (44.4%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:12] \u001b[32mTrain: [ 10/50] Step 380/520 Loss 2.887 Prec@(1,5) (44.4%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:13] \u001b[32mTrain: [ 10/50] Step 400/520 Loss 2.887 Prec@(1,5) (44.4%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:14] \u001b[32mTrain: [ 10/50] Step 420/520 Loss 2.883 Prec@(1,5) (44.5%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:15] \u001b[32mTrain: [ 10/50] Step 440/520 Loss 2.885 Prec@(1,5) (44.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:47:16] \u001b[32mTrain: [ 10/50] Step 460/520 Loss 2.883 Prec@(1,5) (44.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:16] \u001b[32mTrain: [ 10/50] Step 480/520 Loss 2.881 Prec@(1,5) (44.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:17] \u001b[32mTrain: [ 10/50] Step 500/520 Loss 2.882 Prec@(1,5) (44.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:18] \u001b[32mTrain: [ 10/50] Step 520/520 Loss 2.883 Prec@(1,5) (44.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:47:19] \u001b[32mTrain: [ 10/50] Final Prec@1 44.5880%\u001b[0m\n",
      "[2024-01-15 09:47:24] \u001b[32mValid: [ 10/50] Step 000/104 Loss 3.016 Prec@(1,5) (41.7%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:47:24] \u001b[32mValid: [ 10/50] Step 020/104 Loss 2.834 Prec@(1,5) (43.4%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:47:24] \u001b[32mValid: [ 10/50] Step 040/104 Loss 2.861 Prec@(1,5) (43.4%, 74.1%)\u001b[0m\n",
      "[2024-01-15 09:47:25] \u001b[32mValid: [ 10/50] Step 060/104 Loss 2.849 Prec@(1,5) (43.0%, 74.1%)\u001b[0m\n",
      "[2024-01-15 09:47:25] \u001b[32mValid: [ 10/50] Step 080/104 Loss 2.857 Prec@(1,5) (42.6%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:47:25] \u001b[32mValid: [ 10/50] Step 100/104 Loss 2.844 Prec@(1,5) (42.6%, 74.4%)\u001b[0m\n",
      "[2024-01-15 09:47:25] \u001b[32mValid: [ 10/50] Step 104/104 Loss 2.838 Prec@(1,5) (42.7%, 74.5%)\u001b[0m\n",
      "[2024-01-15 09:47:26] \u001b[32mValid: [ 10/50] Final Prec@1 42.6900%\u001b[0m\n",
      "[2024-01-15 09:47:26] \u001b[32mEpoch 10 LR 0.022613\u001b[0m\n",
      "[2024-01-15 09:47:34] \u001b[32mTrain: [ 11/50] Step 000/520 Loss 3.276 Prec@(1,5) (35.4%, 67.7%)\u001b[0m\n",
      "[2024-01-15 09:47:35] \u001b[32mTrain: [ 11/50] Step 020/520 Loss 2.829 Prec@(1,5) (45.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 09:47:36] \u001b[32mTrain: [ 11/50] Step 040/520 Loss 2.791 Prec@(1,5) (46.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:47:37] \u001b[32mTrain: [ 11/50] Step 060/520 Loss 2.765 Prec@(1,5) (46.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:47:38] \u001b[32mTrain: [ 11/50] Step 080/520 Loss 2.764 Prec@(1,5) (46.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:47:39] \u001b[32mTrain: [ 11/50] Step 100/520 Loss 2.772 Prec@(1,5) (46.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:47:40] \u001b[32mTrain: [ 11/50] Step 120/520 Loss 2.774 Prec@(1,5) (46.5%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:47:40] \u001b[32mTrain: [ 11/50] Step 140/520 Loss 2.779 Prec@(1,5) (46.3%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:47:41] \u001b[32mTrain: [ 11/50] Step 160/520 Loss 2.772 Prec@(1,5) (46.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:47:42] \u001b[32mTrain: [ 11/50] Step 180/520 Loss 2.767 Prec@(1,5) (46.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:47:43] \u001b[32mTrain: [ 11/50] Step 200/520 Loss 2.781 Prec@(1,5) (46.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:47:44] \u001b[32mTrain: [ 11/50] Step 220/520 Loss 2.786 Prec@(1,5) (46.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:47:45] \u001b[32mTrain: [ 11/50] Step 240/520 Loss 2.797 Prec@(1,5) (45.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:46] \u001b[32mTrain: [ 11/50] Step 260/520 Loss 2.793 Prec@(1,5) (45.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:47:47] \u001b[32mTrain: [ 11/50] Step 280/520 Loss 2.794 Prec@(1,5) (45.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:48] \u001b[32mTrain: [ 11/50] Step 300/520 Loss 2.796 Prec@(1,5) (45.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:49] \u001b[32mTrain: [ 11/50] Step 320/520 Loss 2.799 Prec@(1,5) (45.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:47:50] \u001b[32mTrain: [ 11/50] Step 340/520 Loss 2.803 Prec@(1,5) (45.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:47:51] \u001b[32mTrain: [ 11/50] Step 360/520 Loss 2.802 Prec@(1,5) (45.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:47:52] \u001b[32mTrain: [ 11/50] Step 380/520 Loss 2.803 Prec@(1,5) (45.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:47:53] \u001b[32mTrain: [ 11/50] Step 400/520 Loss 2.801 Prec@(1,5) (45.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:47:54] \u001b[32mTrain: [ 11/50] Step 420/520 Loss 2.800 Prec@(1,5) (45.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:47:55] \u001b[32mTrain: [ 11/50] Step 440/520 Loss 2.797 Prec@(1,5) (45.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:56] \u001b[32mTrain: [ 11/50] Step 460/520 Loss 2.798 Prec@(1,5) (45.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:56] \u001b[32mTrain: [ 11/50] Step 480/520 Loss 2.800 Prec@(1,5) (45.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:57] \u001b[32mTrain: [ 11/50] Step 500/520 Loss 2.799 Prec@(1,5) (45.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:58] \u001b[32mTrain: [ 11/50] Step 520/520 Loss 2.796 Prec@(1,5) (46.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:47:59] \u001b[32mTrain: [ 11/50] Final Prec@1 45.9600%\u001b[0m\n",
      "[2024-01-15 09:48:04] \u001b[32mValid: [ 11/50] Step 000/104 Loss 2.604 Prec@(1,5) (51.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:48:04] \u001b[32mValid: [ 11/50] Step 020/104 Loss 2.997 Prec@(1,5) (43.7%, 73.7%)\u001b[0m\n",
      "[2024-01-15 09:48:04] \u001b[32mValid: [ 11/50] Step 040/104 Loss 2.988 Prec@(1,5) (42.7%, 73.6%)\u001b[0m\n",
      "[2024-01-15 09:48:05] \u001b[32mValid: [ 11/50] Step 060/104 Loss 2.942 Prec@(1,5) (43.1%, 74.2%)\u001b[0m\n",
      "[2024-01-15 09:48:05] \u001b[32mValid: [ 11/50] Step 080/104 Loss 2.954 Prec@(1,5) (42.7%, 73.9%)\u001b[0m\n",
      "[2024-01-15 09:48:05] \u001b[32mValid: [ 11/50] Step 100/104 Loss 2.939 Prec@(1,5) (42.7%, 74.3%)\u001b[0m\n",
      "[2024-01-15 09:48:05] \u001b[32mValid: [ 11/50] Step 104/104 Loss 2.940 Prec@(1,5) (42.7%, 74.2%)\u001b[0m\n",
      "[2024-01-15 09:48:06] \u001b[32mValid: [ 11/50] Final Prec@1 42.7100%\u001b[0m\n",
      "[2024-01-15 09:48:06] \u001b[32mEpoch 11 LR 0.022132\u001b[0m\n",
      "[2024-01-15 09:48:14] \u001b[32mTrain: [ 12/50] Step 000/520 Loss 2.592 Prec@(1,5) (50.0%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:48:15] \u001b[32mTrain: [ 12/50] Step 020/520 Loss 2.703 Prec@(1,5) (47.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:48:16] \u001b[32mTrain: [ 12/50] Step 040/520 Loss 2.681 Prec@(1,5) (48.3%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:48:17] \u001b[32mTrain: [ 12/50] Step 060/520 Loss 2.688 Prec@(1,5) (47.9%, 77.8%)\u001b[0m\n",
      "[2024-01-15 09:48:18] \u001b[32mTrain: [ 12/50] Step 080/520 Loss 2.731 Prec@(1,5) (47.4%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:48:19] \u001b[32mTrain: [ 12/50] Step 100/520 Loss 2.738 Prec@(1,5) (47.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:48:20] \u001b[32mTrain: [ 12/50] Step 120/520 Loss 2.737 Prec@(1,5) (47.5%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:48:21] \u001b[32mTrain: [ 12/50] Step 140/520 Loss 2.737 Prec@(1,5) (47.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:48:22] \u001b[32mTrain: [ 12/50] Step 160/520 Loss 2.733 Prec@(1,5) (47.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:48:22] \u001b[32mTrain: [ 12/50] Step 180/520 Loss 2.723 Prec@(1,5) (47.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:48:23] \u001b[32mTrain: [ 12/50] Step 200/520 Loss 2.730 Prec@(1,5) (47.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:48:24] \u001b[32mTrain: [ 12/50] Step 220/520 Loss 2.726 Prec@(1,5) (47.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:48:25] \u001b[32mTrain: [ 12/50] Step 240/520 Loss 2.726 Prec@(1,5) (47.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:48:26] \u001b[32mTrain: [ 12/50] Step 260/520 Loss 2.729 Prec@(1,5) (47.3%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:48:27] \u001b[32mTrain: [ 12/50] Step 280/520 Loss 2.734 Prec@(1,5) (47.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:48:28] \u001b[32mTrain: [ 12/50] Step 300/520 Loss 2.734 Prec@(1,5) (47.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:48:29] \u001b[32mTrain: [ 12/50] Step 320/520 Loss 2.739 Prec@(1,5) (47.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:48:30] \u001b[32mTrain: [ 12/50] Step 340/520 Loss 2.736 Prec@(1,5) (47.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:48:31] \u001b[32mTrain: [ 12/50] Step 360/520 Loss 2.730 Prec@(1,5) (47.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:32] \u001b[32mTrain: [ 12/50] Step 380/520 Loss 2.730 Prec@(1,5) (47.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:33] \u001b[32mTrain: [ 12/50] Step 400/520 Loss 2.731 Prec@(1,5) (47.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:48:34] \u001b[32mTrain: [ 12/50] Step 420/520 Loss 2.735 Prec@(1,5) (47.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:48:35] \u001b[32mTrain: [ 12/50] Step 440/520 Loss 2.736 Prec@(1,5) (47.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:36] \u001b[32mTrain: [ 12/50] Step 460/520 Loss 2.735 Prec@(1,5) (47.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:37] \u001b[32mTrain: [ 12/50] Step 480/520 Loss 2.732 Prec@(1,5) (47.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:38] \u001b[32mTrain: [ 12/50] Step 500/520 Loss 2.733 Prec@(1,5) (47.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:39] \u001b[32mTrain: [ 12/50] Step 520/520 Loss 2.733 Prec@(1,5) (47.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 09:48:39] \u001b[32mTrain: [ 12/50] Final Prec@1 47.1800%\u001b[0m\n",
      "[2024-01-15 09:48:44] \u001b[32mValid: [ 12/50] Step 000/104 Loss 2.865 Prec@(1,5) (51.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:48:45] \u001b[32mValid: [ 12/50] Step 020/104 Loss 2.766 Prec@(1,5) (45.3%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:48:45] \u001b[32mValid: [ 12/50] Step 040/104 Loss 2.739 Prec@(1,5) (45.0%, 75.5%)\u001b[0m\n",
      "[2024-01-15 09:48:45] \u001b[32mValid: [ 12/50] Step 060/104 Loss 2.716 Prec@(1,5) (45.2%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:48:46] \u001b[32mValid: [ 12/50] Step 080/104 Loss 2.735 Prec@(1,5) (44.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 09:48:46] \u001b[32mValid: [ 12/50] Step 100/104 Loss 2.713 Prec@(1,5) (45.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:48:46] \u001b[32mValid: [ 12/50] Step 104/104 Loss 2.714 Prec@(1,5) (45.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:48:46] \u001b[32mValid: [ 12/50] Final Prec@1 45.0000%\u001b[0m\n",
      "[2024-01-15 09:48:46] \u001b[32mEpoch 12 LR 0.021612\u001b[0m\n",
      "[2024-01-15 09:48:55] \u001b[32mTrain: [ 13/50] Step 000/520 Loss 2.909 Prec@(1,5) (41.7%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:48:56] \u001b[32mTrain: [ 13/50] Step 020/520 Loss 2.600 Prec@(1,5) (51.0%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:48:57] \u001b[32mTrain: [ 13/50] Step 040/520 Loss 2.621 Prec@(1,5) (50.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:48:58] \u001b[32mTrain: [ 13/50] Step 060/520 Loss 2.659 Prec@(1,5) (49.2%, 78.6%)\u001b[0m\n",
      "[2024-01-15 09:48:59] \u001b[32mTrain: [ 13/50] Step 080/520 Loss 2.658 Prec@(1,5) (48.9%, 78.4%)\u001b[0m\n",
      "[2024-01-15 09:48:59] \u001b[32mTrain: [ 13/50] Step 100/520 Loss 2.665 Prec@(1,5) (48.8%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:00] \u001b[32mTrain: [ 13/50] Step 120/520 Loss 2.668 Prec@(1,5) (48.7%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:01] \u001b[32mTrain: [ 13/50] Step 140/520 Loss 2.672 Prec@(1,5) (48.6%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:49:02] \u001b[32mTrain: [ 13/50] Step 160/520 Loss 2.679 Prec@(1,5) (48.3%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:03] \u001b[32mTrain: [ 13/50] Step 180/520 Loss 2.678 Prec@(1,5) (48.3%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:04] \u001b[32mTrain: [ 13/50] Step 200/520 Loss 2.672 Prec@(1,5) (48.5%, 78.4%)\u001b[0m\n",
      "[2024-01-15 09:49:05] \u001b[32mTrain: [ 13/50] Step 220/520 Loss 2.676 Prec@(1,5) (48.5%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:49:06] \u001b[32mTrain: [ 13/50] Step 240/520 Loss 2.680 Prec@(1,5) (48.5%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:49:07] \u001b[32mTrain: [ 13/50] Step 260/520 Loss 2.686 Prec@(1,5) (48.5%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:49:08] \u001b[32mTrain: [ 13/50] Step 280/520 Loss 2.687 Prec@(1,5) (48.5%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:49:09] \u001b[32mTrain: [ 13/50] Step 300/520 Loss 2.689 Prec@(1,5) (48.3%, 78.0%)\u001b[0m\n",
      "[2024-01-15 09:49:10] \u001b[32mTrain: [ 13/50] Step 320/520 Loss 2.686 Prec@(1,5) (48.4%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:49:11] \u001b[32mTrain: [ 13/50] Step 340/520 Loss 2.683 Prec@(1,5) (48.4%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:12] \u001b[32mTrain: [ 13/50] Step 360/520 Loss 2.680 Prec@(1,5) (48.3%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:13] \u001b[32mTrain: [ 13/50] Step 380/520 Loss 2.679 Prec@(1,5) (48.4%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:14] \u001b[32mTrain: [ 13/50] Step 400/520 Loss 2.679 Prec@(1,5) (48.4%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:15] \u001b[32mTrain: [ 13/50] Step 420/520 Loss 2.679 Prec@(1,5) (48.4%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:49:16] \u001b[32mTrain: [ 13/50] Step 440/520 Loss 2.680 Prec@(1,5) (48.3%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:17] \u001b[32mTrain: [ 13/50] Step 460/520 Loss 2.683 Prec@(1,5) (48.2%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:18] \u001b[32mTrain: [ 13/50] Step 480/520 Loss 2.681 Prec@(1,5) (48.2%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:49:19] \u001b[32mTrain: [ 13/50] Step 500/520 Loss 2.677 Prec@(1,5) (48.3%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:49:20] \u001b[32mTrain: [ 13/50] Step 520/520 Loss 2.674 Prec@(1,5) (48.3%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:49:20] \u001b[32mTrain: [ 13/50] Final Prec@1 48.2960%\u001b[0m\n",
      "[2024-01-15 09:49:25] \u001b[32mValid: [ 13/50] Step 000/104 Loss 2.663 Prec@(1,5) (46.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:49:25] \u001b[32mValid: [ 13/50] Step 020/104 Loss 2.847 Prec@(1,5) (44.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 09:49:26] \u001b[32mValid: [ 13/50] Step 040/104 Loss 2.849 Prec@(1,5) (44.0%, 75.7%)\u001b[0m\n",
      "[2024-01-15 09:49:26] \u001b[32mValid: [ 13/50] Step 060/104 Loss 2.805 Prec@(1,5) (44.7%, 76.1%)\u001b[0m\n",
      "[2024-01-15 09:49:26] \u001b[32mValid: [ 13/50] Step 080/104 Loss 2.817 Prec@(1,5) (44.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:49:27] \u001b[32mValid: [ 13/50] Step 100/104 Loss 2.823 Prec@(1,5) (44.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:49:27] \u001b[32mValid: [ 13/50] Step 104/104 Loss 2.826 Prec@(1,5) (44.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 09:49:27] \u001b[32mValid: [ 13/50] Final Prec@1 43.9600%\u001b[0m\n",
      "[2024-01-15 09:49:27] \u001b[32mEpoch 13 LR 0.021057\u001b[0m\n",
      "[2024-01-15 09:49:35] \u001b[32mTrain: [ 14/50] Step 000/520 Loss 2.725 Prec@(1,5) (44.8%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:36] \u001b[32mTrain: [ 14/50] Step 020/520 Loss 2.582 Prec@(1,5) (49.0%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:49:37] \u001b[32mTrain: [ 14/50] Step 040/520 Loss 2.568 Prec@(1,5) (49.5%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:49:38] \u001b[32mTrain: [ 14/50] Step 060/520 Loss 2.588 Prec@(1,5) (49.5%, 79.5%)\u001b[0m\n",
      "[2024-01-15 09:49:39] \u001b[32mTrain: [ 14/50] Step 080/520 Loss 2.591 Prec@(1,5) (49.3%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:49:40] \u001b[32mTrain: [ 14/50] Step 100/520 Loss 2.579 Prec@(1,5) (49.5%, 79.6%)\u001b[0m\n",
      "[2024-01-15 09:49:41] \u001b[32mTrain: [ 14/50] Step 120/520 Loss 2.575 Prec@(1,5) (49.6%, 79.6%)\u001b[0m\n",
      "[2024-01-15 09:49:42] \u001b[32mTrain: [ 14/50] Step 140/520 Loss 2.584 Prec@(1,5) (49.4%, 79.6%)\u001b[0m\n",
      "[2024-01-15 09:49:43] \u001b[32mTrain: [ 14/50] Step 160/520 Loss 2.584 Prec@(1,5) (49.5%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:49:44] \u001b[32mTrain: [ 14/50] Step 180/520 Loss 2.588 Prec@(1,5) (49.6%, 79.6%)\u001b[0m\n",
      "[2024-01-15 09:49:45] \u001b[32mTrain: [ 14/50] Step 200/520 Loss 2.599 Prec@(1,5) (49.5%, 79.5%)\u001b[0m\n",
      "[2024-01-15 09:49:46] \u001b[32mTrain: [ 14/50] Step 220/520 Loss 2.604 Prec@(1,5) (49.4%, 79.5%)\u001b[0m\n",
      "[2024-01-15 09:49:47] \u001b[32mTrain: [ 14/50] Step 240/520 Loss 2.609 Prec@(1,5) (49.3%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:49:48] \u001b[32mTrain: [ 14/50] Step 260/520 Loss 2.605 Prec@(1,5) (49.4%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:49:49] \u001b[32mTrain: [ 14/50] Step 280/520 Loss 2.607 Prec@(1,5) (49.3%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:49:50] \u001b[32mTrain: [ 14/50] Step 300/520 Loss 2.614 Prec@(1,5) (49.2%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:49:51] \u001b[32mTrain: [ 14/50] Step 320/520 Loss 2.620 Prec@(1,5) (49.1%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:52] \u001b[32mTrain: [ 14/50] Step 340/520 Loss 2.621 Prec@(1,5) (49.1%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:52] \u001b[32mTrain: [ 14/50] Step 360/520 Loss 2.626 Prec@(1,5) (49.0%, 79.1%)\u001b[0m\n",
      "[2024-01-15 09:49:53] \u001b[32mTrain: [ 14/50] Step 380/520 Loss 2.625 Prec@(1,5) (49.0%, 79.1%)\u001b[0m\n",
      "[2024-01-15 09:49:54] \u001b[32mTrain: [ 14/50] Step 400/520 Loss 2.625 Prec@(1,5) (49.0%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:55] \u001b[32mTrain: [ 14/50] Step 420/520 Loss 2.623 Prec@(1,5) (49.0%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:56] \u001b[32mTrain: [ 14/50] Step 440/520 Loss 2.627 Prec@(1,5) (48.9%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:57] \u001b[32mTrain: [ 14/50] Step 460/520 Loss 2.629 Prec@(1,5) (48.8%, 79.1%)\u001b[0m\n",
      "[2024-01-15 09:49:58] \u001b[32mTrain: [ 14/50] Step 480/520 Loss 2.628 Prec@(1,5) (48.9%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:49:59] \u001b[32mTrain: [ 14/50] Step 500/520 Loss 2.627 Prec@(1,5) (48.9%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:50:00] \u001b[32mTrain: [ 14/50] Step 520/520 Loss 2.625 Prec@(1,5) (48.9%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:50:01] \u001b[32mTrain: [ 14/50] Final Prec@1 48.9380%\u001b[0m\n",
      "[2024-01-15 09:50:05] \u001b[32mValid: [ 14/50] Step 000/104 Loss 2.479 Prec@(1,5) (50.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:50:06] \u001b[32mValid: [ 14/50] Step 020/104 Loss 2.691 Prec@(1,5) (47.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:50:06] \u001b[32mValid: [ 14/50] Step 040/104 Loss 2.682 Prec@(1,5) (46.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:50:06] \u001b[32mValid: [ 14/50] Step 060/104 Loss 2.672 Prec@(1,5) (46.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:50:07] \u001b[32mValid: [ 14/50] Step 080/104 Loss 2.682 Prec@(1,5) (46.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 09:50:07] \u001b[32mValid: [ 14/50] Step 100/104 Loss 2.664 Prec@(1,5) (45.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:50:07] \u001b[32mValid: [ 14/50] Step 104/104 Loss 2.665 Prec@(1,5) (45.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:50:07] \u001b[32mValid: [ 14/50] Final Prec@1 45.8900%\u001b[0m\n",
      "[2024-01-15 09:50:07] \u001b[32mEpoch 14 LR 0.020468\u001b[0m\n",
      "[2024-01-15 09:50:16] \u001b[32mTrain: [ 15/50] Step 000/520 Loss 2.948 Prec@(1,5) (46.9%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:50:17] \u001b[32mTrain: [ 15/50] Step 020/520 Loss 2.555 Prec@(1,5) (50.1%, 80.7%)\u001b[0m\n",
      "[2024-01-15 09:50:18] \u001b[32mTrain: [ 15/50] Step 040/520 Loss 2.578 Prec@(1,5) (50.1%, 79.8%)\u001b[0m\n",
      "[2024-01-15 09:50:19] \u001b[32mTrain: [ 15/50] Step 060/520 Loss 2.607 Prec@(1,5) (49.4%, 79.1%)\u001b[0m\n",
      "[2024-01-15 09:50:19] \u001b[32mTrain: [ 15/50] Step 080/520 Loss 2.610 Prec@(1,5) (49.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:50:20] \u001b[32mTrain: [ 15/50] Step 100/520 Loss 2.584 Prec@(1,5) (49.7%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:50:21] \u001b[32mTrain: [ 15/50] Step 120/520 Loss 2.569 Prec@(1,5) (49.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:22] \u001b[32mTrain: [ 15/50] Step 140/520 Loss 2.567 Prec@(1,5) (49.7%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:23] \u001b[32mTrain: [ 15/50] Step 160/520 Loss 2.566 Prec@(1,5) (49.8%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:24] \u001b[32mTrain: [ 15/50] Step 180/520 Loss 2.561 Prec@(1,5) (49.8%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:50:25] \u001b[32mTrain: [ 15/50] Step 200/520 Loss 2.558 Prec@(1,5) (49.9%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:50:26] \u001b[32mTrain: [ 15/50] Step 220/520 Loss 2.564 Prec@(1,5) (49.7%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:50:27] \u001b[32mTrain: [ 15/50] Step 240/520 Loss 2.567 Prec@(1,5) (49.8%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:50:28] \u001b[32mTrain: [ 15/50] Step 260/520 Loss 2.563 Prec@(1,5) (49.9%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:50:29] \u001b[32mTrain: [ 15/50] Step 280/520 Loss 2.567 Prec@(1,5) (49.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:30] \u001b[32mTrain: [ 15/50] Step 300/520 Loss 2.569 Prec@(1,5) (49.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:31] \u001b[32mTrain: [ 15/50] Step 320/520 Loss 2.565 Prec@(1,5) (49.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:32] \u001b[32mTrain: [ 15/50] Step 340/520 Loss 2.565 Prec@(1,5) (49.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:33] \u001b[32mTrain: [ 15/50] Step 360/520 Loss 2.568 Prec@(1,5) (49.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:34] \u001b[32mTrain: [ 15/50] Step 380/520 Loss 2.570 Prec@(1,5) (49.8%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:35] \u001b[32mTrain: [ 15/50] Step 400/520 Loss 2.568 Prec@(1,5) (49.8%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:36] \u001b[32mTrain: [ 15/50] Step 420/520 Loss 2.572 Prec@(1,5) (49.7%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:36] \u001b[32mTrain: [ 15/50] Step 440/520 Loss 2.575 Prec@(1,5) (49.7%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:50:37] \u001b[32mTrain: [ 15/50] Step 460/520 Loss 2.578 Prec@(1,5) (49.6%, 79.8%)\u001b[0m\n",
      "[2024-01-15 09:50:38] \u001b[32mTrain: [ 15/50] Step 480/520 Loss 2.579 Prec@(1,5) (49.6%, 79.8%)\u001b[0m\n",
      "[2024-01-15 09:50:39] \u001b[32mTrain: [ 15/50] Step 500/520 Loss 2.578 Prec@(1,5) (49.6%, 79.8%)\u001b[0m\n",
      "[2024-01-15 09:50:40] \u001b[32mTrain: [ 15/50] Step 520/520 Loss 2.577 Prec@(1,5) (49.7%, 79.8%)\u001b[0m\n",
      "[2024-01-15 09:50:41] \u001b[32mTrain: [ 15/50] Final Prec@1 49.6620%\u001b[0m\n",
      "[2024-01-15 09:50:46] \u001b[32mValid: [ 15/50] Step 000/104 Loss 2.793 Prec@(1,5) (45.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:50:46] \u001b[32mValid: [ 15/50] Step 020/104 Loss 2.630 Prec@(1,5) (46.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:50:46] \u001b[32mValid: [ 15/50] Step 040/104 Loss 2.624 Prec@(1,5) (46.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 09:50:47] \u001b[32mValid: [ 15/50] Step 060/104 Loss 2.605 Prec@(1,5) (46.6%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:50:47] \u001b[32mValid: [ 15/50] Step 080/104 Loss 2.624 Prec@(1,5) (46.2%, 77.9%)\u001b[0m\n",
      "[2024-01-15 09:50:47] \u001b[32mValid: [ 15/50] Step 100/104 Loss 2.601 Prec@(1,5) (46.4%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:50:47] \u001b[32mValid: [ 15/50] Step 104/104 Loss 2.597 Prec@(1,5) (46.4%, 78.2%)\u001b[0m\n",
      "[2024-01-15 09:50:48] \u001b[32mValid: [ 15/50] Final Prec@1 46.4300%\u001b[0m\n",
      "[2024-01-15 09:50:48] \u001b[32mEpoch 15 LR 0.019848\u001b[0m\n",
      "[2024-01-15 09:50:56] \u001b[32mTrain: [ 16/50] Step 000/520 Loss 2.376 Prec@(1,5) (59.4%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:50:57] \u001b[32mTrain: [ 16/50] Step 020/520 Loss 2.514 Prec@(1,5) (50.9%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:50:58] \u001b[32mTrain: [ 16/50] Step 040/520 Loss 2.557 Prec@(1,5) (50.2%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:50:59] \u001b[32mTrain: [ 16/50] Step 060/520 Loss 2.527 Prec@(1,5) (50.8%, 80.9%)\u001b[0m\n",
      "[2024-01-15 09:51:00] \u001b[32mTrain: [ 16/50] Step 080/520 Loss 2.533 Prec@(1,5) (50.9%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:01] \u001b[32mTrain: [ 16/50] Step 100/520 Loss 2.512 Prec@(1,5) (51.3%, 80.8%)\u001b[0m\n",
      "[2024-01-15 09:51:02] \u001b[32mTrain: [ 16/50] Step 120/520 Loss 2.522 Prec@(1,5) (51.0%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:02] \u001b[32mTrain: [ 16/50] Step 140/520 Loss 2.521 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:03] \u001b[32mTrain: [ 16/50] Step 160/520 Loss 2.520 Prec@(1,5) (51.0%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:04] \u001b[32mTrain: [ 16/50] Step 180/520 Loss 2.526 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:05] \u001b[32mTrain: [ 16/50] Step 200/520 Loss 2.524 Prec@(1,5) (50.8%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:06] \u001b[32mTrain: [ 16/50] Step 220/520 Loss 2.525 Prec@(1,5) (50.7%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:07] \u001b[32mTrain: [ 16/50] Step 240/520 Loss 2.527 Prec@(1,5) (50.7%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:08] \u001b[32mTrain: [ 16/50] Step 260/520 Loss 2.523 Prec@(1,5) (50.8%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:09] \u001b[32mTrain: [ 16/50] Step 280/520 Loss 2.518 Prec@(1,5) (50.9%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:10] \u001b[32mTrain: [ 16/50] Step 300/520 Loss 2.521 Prec@(1,5) (50.8%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:11] \u001b[32mTrain: [ 16/50] Step 320/520 Loss 2.517 Prec@(1,5) (50.9%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:12] \u001b[32mTrain: [ 16/50] Step 340/520 Loss 2.517 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:13] \u001b[32mTrain: [ 16/50] Step 360/520 Loss 2.516 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:14] \u001b[32mTrain: [ 16/50] Step 380/520 Loss 2.518 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:15] \u001b[32mTrain: [ 16/50] Step 400/520 Loss 2.521 Prec@(1,5) (50.8%, 80.3%)\u001b[0m\n",
      "[2024-01-15 09:51:16] \u001b[32mTrain: [ 16/50] Step 420/520 Loss 2.519 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:17] \u001b[32mTrain: [ 16/50] Step 440/520 Loss 2.521 Prec@(1,5) (50.9%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:18] \u001b[32mTrain: [ 16/50] Step 460/520 Loss 2.521 Prec@(1,5) (50.9%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:19] \u001b[32mTrain: [ 16/50] Step 480/520 Loss 2.522 Prec@(1,5) (50.8%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:20] \u001b[32mTrain: [ 16/50] Step 500/520 Loss 2.525 Prec@(1,5) (50.8%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:21] \u001b[32mTrain: [ 16/50] Step 520/520 Loss 2.527 Prec@(1,5) (50.7%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:21] \u001b[32mTrain: [ 16/50] Final Prec@1 50.7120%\u001b[0m\n",
      "[2024-01-15 09:51:26] \u001b[32mValid: [ 16/50] Step 000/104 Loss 2.370 Prec@(1,5) (51.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:51:26] \u001b[32mValid: [ 16/50] Step 020/104 Loss 2.655 Prec@(1,5) (47.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 09:51:26] \u001b[32mValid: [ 16/50] Step 040/104 Loss 2.650 Prec@(1,5) (46.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 09:51:27] \u001b[32mValid: [ 16/50] Step 060/104 Loss 2.607 Prec@(1,5) (47.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:51:27] \u001b[32mValid: [ 16/50] Step 080/104 Loss 2.625 Prec@(1,5) (47.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 09:51:27] \u001b[32mValid: [ 16/50] Step 100/104 Loss 2.607 Prec@(1,5) (47.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 09:51:27] \u001b[32mValid: [ 16/50] Step 104/104 Loss 2.609 Prec@(1,5) (47.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 09:51:28] \u001b[32mValid: [ 16/50] Final Prec@1 47.1200%\u001b[0m\n",
      "[2024-01-15 09:51:28] \u001b[32mEpoch 16 LR 0.019198\u001b[0m\n",
      "[2024-01-15 09:51:36] \u001b[32mTrain: [ 17/50] Step 000/520 Loss 2.079 Prec@(1,5) (56.2%, 86.5%)\u001b[0m\n",
      "[2024-01-15 09:51:37] \u001b[32mTrain: [ 17/50] Step 020/520 Loss 2.533 Prec@(1,5) (50.5%, 80.4%)\u001b[0m\n",
      "[2024-01-15 09:51:38] \u001b[32mTrain: [ 17/50] Step 040/520 Loss 2.489 Prec@(1,5) (51.5%, 81.0%)\u001b[0m\n",
      "[2024-01-15 09:51:39] \u001b[32mTrain: [ 17/50] Step 060/520 Loss 2.487 Prec@(1,5) (51.8%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:51:39] \u001b[32mTrain: [ 17/50] Step 080/520 Loss 2.493 Prec@(1,5) (51.7%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:51:40] \u001b[32mTrain: [ 17/50] Step 100/520 Loss 2.490 Prec@(1,5) (51.4%, 81.0%)\u001b[0m\n",
      "[2024-01-15 09:51:41] \u001b[32mTrain: [ 17/50] Step 120/520 Loss 2.479 Prec@(1,5) (51.5%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:51:42] \u001b[32mTrain: [ 17/50] Step 140/520 Loss 2.477 Prec@(1,5) (51.6%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:51:43] \u001b[32mTrain: [ 17/50] Step 160/520 Loss 2.479 Prec@(1,5) (51.6%, 81.0%)\u001b[0m\n",
      "[2024-01-15 09:51:44] \u001b[32mTrain: [ 17/50] Step 180/520 Loss 2.488 Prec@(1,5) (51.5%, 80.8%)\u001b[0m\n",
      "[2024-01-15 09:51:45] \u001b[32mTrain: [ 17/50] Step 200/520 Loss 2.489 Prec@(1,5) (51.5%, 80.7%)\u001b[0m\n",
      "[2024-01-15 09:51:46] \u001b[32mTrain: [ 17/50] Step 220/520 Loss 2.497 Prec@(1,5) (51.2%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:47] \u001b[32mTrain: [ 17/50] Step 240/520 Loss 2.497 Prec@(1,5) (51.2%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:48] \u001b[32mTrain: [ 17/50] Step 260/520 Loss 2.495 Prec@(1,5) (51.3%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:49] \u001b[32mTrain: [ 17/50] Step 280/520 Loss 2.488 Prec@(1,5) (51.3%, 80.7%)\u001b[0m\n",
      "[2024-01-15 09:51:50] \u001b[32mTrain: [ 17/50] Step 300/520 Loss 2.490 Prec@(1,5) (51.3%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:51] \u001b[32mTrain: [ 17/50] Step 320/520 Loss 2.497 Prec@(1,5) (51.3%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:52] \u001b[32mTrain: [ 17/50] Step 340/520 Loss 2.496 Prec@(1,5) (51.3%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:52] \u001b[32mTrain: [ 17/50] Step 360/520 Loss 2.493 Prec@(1,5) (51.4%, 80.5%)\u001b[0m\n",
      "[2024-01-15 09:51:53] \u001b[32mTrain: [ 17/50] Step 380/520 Loss 2.494 Prec@(1,5) (51.4%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:54] \u001b[32mTrain: [ 17/50] Step 400/520 Loss 2.494 Prec@(1,5) (51.5%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:55] \u001b[32mTrain: [ 17/50] Step 420/520 Loss 2.493 Prec@(1,5) (51.4%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:56] \u001b[32mTrain: [ 17/50] Step 440/520 Loss 2.493 Prec@(1,5) (51.5%, 80.7%)\u001b[0m\n",
      "[2024-01-15 09:51:57] \u001b[32mTrain: [ 17/50] Step 460/520 Loss 2.494 Prec@(1,5) (51.4%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:58] \u001b[32mTrain: [ 17/50] Step 480/520 Loss 2.500 Prec@(1,5) (51.2%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:51:59] \u001b[32mTrain: [ 17/50] Step 500/520 Loss 2.500 Prec@(1,5) (51.3%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:52:00] \u001b[32mTrain: [ 17/50] Step 520/520 Loss 2.499 Prec@(1,5) (51.3%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:52:01] \u001b[32mTrain: [ 17/50] Final Prec@1 51.3100%\u001b[0m\n",
      "[2024-01-15 09:52:06] \u001b[32mValid: [ 17/50] Step 000/104 Loss 2.591 Prec@(1,5) (52.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 09:52:06] \u001b[32mValid: [ 17/50] Step 020/104 Loss 2.519 Prec@(1,5) (48.7%, 78.3%)\u001b[0m\n",
      "[2024-01-15 09:52:06] \u001b[32mValid: [ 17/50] Step 040/104 Loss 2.496 Prec@(1,5) (48.3%, 78.8%)\u001b[0m\n",
      "[2024-01-15 09:52:07] \u001b[32mValid: [ 17/50] Step 060/104 Loss 2.454 Prec@(1,5) (48.5%, 79.0%)\u001b[0m\n",
      "[2024-01-15 09:52:07] \u001b[32mValid: [ 17/50] Step 080/104 Loss 2.469 Prec@(1,5) (48.1%, 78.9%)\u001b[0m\n",
      "[2024-01-15 09:52:07] \u001b[32mValid: [ 17/50] Step 100/104 Loss 2.454 Prec@(1,5) (48.3%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:52:07] \u001b[32mValid: [ 17/50] Step 104/104 Loss 2.456 Prec@(1,5) (48.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:52:08] \u001b[32mValid: [ 17/50] Final Prec@1 48.2300%\u001b[0m\n",
      "[2024-01-15 09:52:08] \u001b[32mEpoch 17 LR 0.018522\u001b[0m\n",
      "[2024-01-15 09:52:16] \u001b[32mTrain: [ 18/50] Step 000/520 Loss 2.510 Prec@(1,5) (47.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:52:17] \u001b[32mTrain: [ 18/50] Step 020/520 Loss 2.515 Prec@(1,5) (51.1%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:52:18] \u001b[32mTrain: [ 18/50] Step 040/520 Loss 2.439 Prec@(1,5) (52.7%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:19] \u001b[32mTrain: [ 18/50] Step 060/520 Loss 2.448 Prec@(1,5) (52.7%, 80.9%)\u001b[0m\n",
      "[2024-01-15 09:52:20] \u001b[32mTrain: [ 18/50] Step 080/520 Loss 2.465 Prec@(1,5) (52.5%, 80.9%)\u001b[0m\n",
      "[2024-01-15 09:52:21] \u001b[32mTrain: [ 18/50] Step 100/520 Loss 2.454 Prec@(1,5) (52.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:21] \u001b[32mTrain: [ 18/50] Step 120/520 Loss 2.442 Prec@(1,5) (52.4%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:22] \u001b[32mTrain: [ 18/50] Step 140/520 Loss 2.439 Prec@(1,5) (52.4%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:23] \u001b[32mTrain: [ 18/50] Step 160/520 Loss 2.432 Prec@(1,5) (52.6%, 81.3%)\u001b[0m\n",
      "[2024-01-15 09:52:24] \u001b[32mTrain: [ 18/50] Step 180/520 Loss 2.434 Prec@(1,5) (52.6%, 81.3%)\u001b[0m\n",
      "[2024-01-15 09:52:25] \u001b[32mTrain: [ 18/50] Step 200/520 Loss 2.441 Prec@(1,5) (52.5%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:26] \u001b[32mTrain: [ 18/50] Step 220/520 Loss 2.444 Prec@(1,5) (52.3%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:27] \u001b[32mTrain: [ 18/50] Step 240/520 Loss 2.441 Prec@(1,5) (52.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:28] \u001b[32mTrain: [ 18/50] Step 260/520 Loss 2.446 Prec@(1,5) (52.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:29] \u001b[32mTrain: [ 18/50] Step 280/520 Loss 2.451 Prec@(1,5) (52.1%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:30] \u001b[32mTrain: [ 18/50] Step 300/520 Loss 2.454 Prec@(1,5) (52.1%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:31] \u001b[32mTrain: [ 18/50] Step 320/520 Loss 2.453 Prec@(1,5) (52.1%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:32] \u001b[32mTrain: [ 18/50] Step 340/520 Loss 2.451 Prec@(1,5) (52.1%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:33] \u001b[32mTrain: [ 18/50] Step 360/520 Loss 2.451 Prec@(1,5) (52.1%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:34] \u001b[32mTrain: [ 18/50] Step 380/520 Loss 2.447 Prec@(1,5) (52.1%, 81.3%)\u001b[0m\n",
      "[2024-01-15 09:52:35] \u001b[32mTrain: [ 18/50] Step 400/520 Loss 2.449 Prec@(1,5) (52.1%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:36] \u001b[32mTrain: [ 18/50] Step 420/520 Loss 2.449 Prec@(1,5) (52.1%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:37] \u001b[32mTrain: [ 18/50] Step 440/520 Loss 2.454 Prec@(1,5) (52.0%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:38] \u001b[32mTrain: [ 18/50] Step 460/520 Loss 2.454 Prec@(1,5) (52.0%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:39] \u001b[32mTrain: [ 18/50] Step 480/520 Loss 2.453 Prec@(1,5) (51.9%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:40] \u001b[32mTrain: [ 18/50] Step 500/520 Loss 2.452 Prec@(1,5) (52.0%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:52:40] \u001b[32mTrain: [ 18/50] Step 520/520 Loss 2.455 Prec@(1,5) (51.9%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:52:41] \u001b[32mTrain: [ 18/50] Final Prec@1 51.9280%\u001b[0m\n",
      "[2024-01-15 09:52:46] \u001b[32mValid: [ 18/50] Step 000/104 Loss 2.195 Prec@(1,5) (56.2%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:52:46] \u001b[32mValid: [ 18/50] Step 020/104 Loss 2.571 Prec@(1,5) (49.0%, 80.3%)\u001b[0m\n",
      "[2024-01-15 09:52:46] \u001b[32mValid: [ 18/50] Step 040/104 Loss 2.530 Prec@(1,5) (48.2%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:52:47] \u001b[32mValid: [ 18/50] Step 060/104 Loss 2.505 Prec@(1,5) (48.3%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:52:47] \u001b[32mValid: [ 18/50] Step 080/104 Loss 2.510 Prec@(1,5) (47.9%, 79.6%)\u001b[0m\n",
      "[2024-01-15 09:52:47] \u001b[32mValid: [ 18/50] Step 100/104 Loss 2.493 Prec@(1,5) (48.3%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:52:47] \u001b[32mValid: [ 18/50] Step 104/104 Loss 2.494 Prec@(1,5) (48.3%, 79.7%)\u001b[0m\n",
      "[2024-01-15 09:52:48] \u001b[32mValid: [ 18/50] Final Prec@1 48.2700%\u001b[0m\n",
      "[2024-01-15 09:52:48] \u001b[32mEpoch 18 LR 0.017823\u001b[0m\n",
      "[2024-01-15 09:52:56] \u001b[32mTrain: [ 19/50] Step 000/520 Loss 2.606 Prec@(1,5) (54.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:52:57] \u001b[32mTrain: [ 19/50] Step 020/520 Loss 2.300 Prec@(1,5) (54.1%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:52:58] \u001b[32mTrain: [ 19/50] Step 040/520 Loss 2.361 Prec@(1,5) (53.2%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:52:59] \u001b[32mTrain: [ 19/50] Step 060/520 Loss 2.374 Prec@(1,5) (53.2%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:53:00] \u001b[32mTrain: [ 19/50] Step 080/520 Loss 2.372 Prec@(1,5) (53.2%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:53:00] \u001b[32mTrain: [ 19/50] Step 100/520 Loss 2.388 Prec@(1,5) (53.2%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:01] \u001b[32mTrain: [ 19/50] Step 120/520 Loss 2.390 Prec@(1,5) (53.2%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:02] \u001b[32mTrain: [ 19/50] Step 140/520 Loss 2.399 Prec@(1,5) (53.1%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:53:03] \u001b[32mTrain: [ 19/50] Step 160/520 Loss 2.402 Prec@(1,5) (53.1%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:53:04] \u001b[32mTrain: [ 19/50] Step 180/520 Loss 2.405 Prec@(1,5) (52.9%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:53:05] \u001b[32mTrain: [ 19/50] Step 200/520 Loss 2.411 Prec@(1,5) (52.8%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:53:06] \u001b[32mTrain: [ 19/50] Step 220/520 Loss 2.413 Prec@(1,5) (52.7%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:53:07] \u001b[32mTrain: [ 19/50] Step 240/520 Loss 2.410 Prec@(1,5) (52.7%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:53:08] \u001b[32mTrain: [ 19/50] Step 260/520 Loss 2.410 Prec@(1,5) (52.8%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:53:09] \u001b[32mTrain: [ 19/50] Step 280/520 Loss 2.411 Prec@(1,5) (52.7%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:53:10] \u001b[32mTrain: [ 19/50] Step 300/520 Loss 2.411 Prec@(1,5) (52.6%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:53:11] \u001b[32mTrain: [ 19/50] Step 320/520 Loss 2.411 Prec@(1,5) (52.7%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:53:12] \u001b[32mTrain: [ 19/50] Step 340/520 Loss 2.413 Prec@(1,5) (52.5%, 81.8%)\u001b[0m\n",
      "[2024-01-15 09:53:13] \u001b[32mTrain: [ 19/50] Step 360/520 Loss 2.415 Prec@(1,5) (52.5%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:53:14] \u001b[32mTrain: [ 19/50] Step 380/520 Loss 2.420 Prec@(1,5) (52.4%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:53:15] \u001b[32mTrain: [ 19/50] Step 400/520 Loss 2.420 Prec@(1,5) (52.4%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:53:16] \u001b[32mTrain: [ 19/50] Step 420/520 Loss 2.422 Prec@(1,5) (52.4%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:53:17] \u001b[32mTrain: [ 19/50] Step 440/520 Loss 2.427 Prec@(1,5) (52.3%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:53:18] \u001b[32mTrain: [ 19/50] Step 460/520 Loss 2.427 Prec@(1,5) (52.3%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:53:18] \u001b[32mTrain: [ 19/50] Step 480/520 Loss 2.428 Prec@(1,5) (52.3%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:53:19] \u001b[32mTrain: [ 19/50] Step 500/520 Loss 2.429 Prec@(1,5) (52.3%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:53:20] \u001b[32mTrain: [ 19/50] Step 520/520 Loss 2.430 Prec@(1,5) (52.3%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:53:21] \u001b[32mTrain: [ 19/50] Final Prec@1 52.2940%\u001b[0m\n",
      "[2024-01-15 09:53:26] \u001b[32mValid: [ 19/50] Step 000/104 Loss 2.440 Prec@(1,5) (55.2%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:53:26] \u001b[32mValid: [ 19/50] Step 020/104 Loss 2.542 Prec@(1,5) (49.8%, 80.3%)\u001b[0m\n",
      "[2024-01-15 09:53:26] \u001b[32mValid: [ 19/50] Step 040/104 Loss 2.508 Prec@(1,5) (49.4%, 80.6%)\u001b[0m\n",
      "[2024-01-15 09:53:27] \u001b[32mValid: [ 19/50] Step 060/104 Loss 2.479 Prec@(1,5) (49.9%, 80.3%)\u001b[0m\n",
      "[2024-01-15 09:53:27] \u001b[32mValid: [ 19/50] Step 080/104 Loss 2.506 Prec@(1,5) (49.5%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:53:27] \u001b[32mValid: [ 19/50] Step 100/104 Loss 2.497 Prec@(1,5) (49.7%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:53:27] \u001b[32mValid: [ 19/50] Step 104/104 Loss 2.498 Prec@(1,5) (49.7%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:53:28] \u001b[32mValid: [ 19/50] Final Prec@1 49.7100%\u001b[0m\n",
      "[2024-01-15 09:53:28] \u001b[32mEpoch 19 LR 0.017102\u001b[0m\n",
      "[2024-01-15 09:53:36] \u001b[32mTrain: [ 20/50] Step 000/520 Loss 2.460 Prec@(1,5) (46.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:53:37] \u001b[32mTrain: [ 20/50] Step 020/520 Loss 2.327 Prec@(1,5) (54.2%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:53:38] \u001b[32mTrain: [ 20/50] Step 040/520 Loss 2.344 Prec@(1,5) (53.9%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:53:39] \u001b[32mTrain: [ 20/50] Step 060/520 Loss 2.349 Prec@(1,5) (54.3%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:53:40] \u001b[32mTrain: [ 20/50] Step 080/520 Loss 2.359 Prec@(1,5) (54.0%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:53:40] \u001b[32mTrain: [ 20/50] Step 100/520 Loss 2.364 Prec@(1,5) (53.7%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:53:41] \u001b[32mTrain: [ 20/50] Step 120/520 Loss 2.355 Prec@(1,5) (54.1%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:53:42] \u001b[32mTrain: [ 20/50] Step 140/520 Loss 2.352 Prec@(1,5) (54.1%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:53:43] \u001b[32mTrain: [ 20/50] Step 160/520 Loss 2.361 Prec@(1,5) (53.9%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:53:44] \u001b[32mTrain: [ 20/50] Step 180/520 Loss 2.367 Prec@(1,5) (53.6%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:53:45] \u001b[32mTrain: [ 20/50] Step 200/520 Loss 2.361 Prec@(1,5) (53.7%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:53:46] \u001b[32mTrain: [ 20/50] Step 220/520 Loss 2.369 Prec@(1,5) (53.4%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:53:47] \u001b[32mTrain: [ 20/50] Step 240/520 Loss 2.374 Prec@(1,5) (53.4%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:53:48] \u001b[32mTrain: [ 20/50] Step 260/520 Loss 2.389 Prec@(1,5) (53.2%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:53:49] \u001b[32mTrain: [ 20/50] Step 280/520 Loss 2.391 Prec@(1,5) (53.3%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:50] \u001b[32mTrain: [ 20/50] Step 300/520 Loss 2.394 Prec@(1,5) (53.3%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:51] \u001b[32mTrain: [ 20/50] Step 320/520 Loss 2.395 Prec@(1,5) (53.3%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:53:52] \u001b[32mTrain: [ 20/50] Step 340/520 Loss 2.397 Prec@(1,5) (53.2%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:53:53] \u001b[32mTrain: [ 20/50] Step 360/520 Loss 2.397 Prec@(1,5) (53.2%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:53:54] \u001b[32mTrain: [ 20/50] Step 380/520 Loss 2.394 Prec@(1,5) (53.3%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:53:55] \u001b[32mTrain: [ 20/50] Step 400/520 Loss 2.392 Prec@(1,5) (53.3%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:56] \u001b[32mTrain: [ 20/50] Step 420/520 Loss 2.390 Prec@(1,5) (53.3%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:56] \u001b[32mTrain: [ 20/50] Step 440/520 Loss 2.395 Prec@(1,5) (53.2%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:53:57] \u001b[32mTrain: [ 20/50] Step 460/520 Loss 2.397 Prec@(1,5) (53.2%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:53:58] \u001b[32mTrain: [ 20/50] Step 480/520 Loss 2.397 Prec@(1,5) (53.2%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:53:59] \u001b[32mTrain: [ 20/50] Step 500/520 Loss 2.400 Prec@(1,5) (53.2%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:54:00] \u001b[32mTrain: [ 20/50] Step 520/520 Loss 2.401 Prec@(1,5) (53.2%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:54:01] \u001b[32mTrain: [ 20/50] Final Prec@1 53.1880%\u001b[0m\n",
      "[2024-01-15 09:54:05] \u001b[32mValid: [ 20/50] Step 000/104 Loss 2.534 Prec@(1,5) (50.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:54:06] \u001b[32mValid: [ 20/50] Step 020/104 Loss 2.486 Prec@(1,5) (49.8%, 79.8%)\u001b[0m\n",
      "[2024-01-15 09:54:06] \u001b[32mValid: [ 20/50] Step 040/104 Loss 2.500 Prec@(1,5) (49.2%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:54:07] \u001b[32mValid: [ 20/50] Step 060/104 Loss 2.468 Prec@(1,5) (49.5%, 79.6%)\u001b[0m\n",
      "[2024-01-15 09:54:07] \u001b[32mValid: [ 20/50] Step 080/104 Loss 2.496 Prec@(1,5) (48.7%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:54:07] \u001b[32mValid: [ 20/50] Step 100/104 Loss 2.479 Prec@(1,5) (49.0%, 79.4%)\u001b[0m\n",
      "[2024-01-15 09:54:07] \u001b[32mValid: [ 20/50] Step 104/104 Loss 2.475 Prec@(1,5) (49.0%, 79.5%)\u001b[0m\n",
      "[2024-01-15 09:54:07] \u001b[32mValid: [ 20/50] Final Prec@1 49.0000%\u001b[0m\n",
      "[2024-01-15 09:54:07] \u001b[32mEpoch 20 LR 0.016363\u001b[0m\n",
      "[2024-01-15 09:54:16] \u001b[32mTrain: [ 21/50] Step 000/520 Loss 2.457 Prec@(1,5) (49.0%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:54:17] \u001b[32mTrain: [ 21/50] Step 020/520 Loss 2.309 Prec@(1,5) (54.3%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:54:18] \u001b[32mTrain: [ 21/50] Step 040/520 Loss 2.336 Prec@(1,5) (54.0%, 82.9%)\u001b[0m\n",
      "[2024-01-15 09:54:19] \u001b[32mTrain: [ 21/50] Step 060/520 Loss 2.342 Prec@(1,5) (54.2%, 82.9%)\u001b[0m\n",
      "[2024-01-15 09:54:20] \u001b[32mTrain: [ 21/50] Step 080/520 Loss 2.337 Prec@(1,5) (54.5%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:54:21] \u001b[32mTrain: [ 21/50] Step 100/520 Loss 2.336 Prec@(1,5) (54.4%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:54:22] \u001b[32mTrain: [ 21/50] Step 120/520 Loss 2.356 Prec@(1,5) (53.8%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:22] \u001b[32mTrain: [ 21/50] Step 140/520 Loss 2.353 Prec@(1,5) (53.7%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:23] \u001b[32mTrain: [ 21/50] Step 160/520 Loss 2.345 Prec@(1,5) (54.0%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:54:24] \u001b[32mTrain: [ 21/50] Step 180/520 Loss 2.340 Prec@(1,5) (54.1%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:54:25] \u001b[32mTrain: [ 21/50] Step 200/520 Loss 2.342 Prec@(1,5) (54.2%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:54:26] \u001b[32mTrain: [ 21/50] Step 220/520 Loss 2.343 Prec@(1,5) (54.2%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:54:27] \u001b[32mTrain: [ 21/50] Step 240/520 Loss 2.343 Prec@(1,5) (54.1%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:54:28] \u001b[32mTrain: [ 21/50] Step 260/520 Loss 2.347 Prec@(1,5) (54.0%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:29] \u001b[32mTrain: [ 21/50] Step 280/520 Loss 2.352 Prec@(1,5) (53.9%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:30] \u001b[32mTrain: [ 21/50] Step 300/520 Loss 2.351 Prec@(1,5) (54.0%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:31] \u001b[32mTrain: [ 21/50] Step 320/520 Loss 2.354 Prec@(1,5) (53.8%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:32] \u001b[32mTrain: [ 21/50] Step 340/520 Loss 2.352 Prec@(1,5) (53.9%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:33] \u001b[32mTrain: [ 21/50] Step 360/520 Loss 2.354 Prec@(1,5) (53.8%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:54:34] \u001b[32mTrain: [ 21/50] Step 380/520 Loss 2.357 Prec@(1,5) (53.8%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:54:36] \u001b[32mTrain: [ 21/50] Step 400/520 Loss 2.362 Prec@(1,5) (53.8%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:54:37] \u001b[32mTrain: [ 21/50] Step 420/520 Loss 2.363 Prec@(1,5) (53.8%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:54:38] \u001b[32mTrain: [ 21/50] Step 440/520 Loss 2.366 Prec@(1,5) (53.7%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:54:39] \u001b[32mTrain: [ 21/50] Step 460/520 Loss 2.365 Prec@(1,5) (53.7%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:54:40] \u001b[32mTrain: [ 21/50] Step 480/520 Loss 2.364 Prec@(1,5) (53.8%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:54:41] \u001b[32mTrain: [ 21/50] Step 500/520 Loss 2.363 Prec@(1,5) (53.8%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:54:42] \u001b[32mTrain: [ 21/50] Step 520/520 Loss 2.367 Prec@(1,5) (53.7%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:54:42] \u001b[32mTrain: [ 21/50] Final Prec@1 53.6820%\u001b[0m\n",
      "[2024-01-15 09:54:47] \u001b[32mValid: [ 21/50] Step 000/104 Loss 2.396 Prec@(1,5) (56.2%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:54:48] \u001b[32mValid: [ 21/50] Step 020/104 Loss 2.398 Prec@(1,5) (51.6%, 79.3%)\u001b[0m\n",
      "[2024-01-15 09:54:48] \u001b[32mValid: [ 21/50] Step 040/104 Loss 2.347 Prec@(1,5) (51.2%, 79.9%)\u001b[0m\n",
      "[2024-01-15 09:54:48] \u001b[32mValid: [ 21/50] Step 060/104 Loss 2.309 Prec@(1,5) (51.8%, 80.0%)\u001b[0m\n",
      "[2024-01-15 09:54:49] \u001b[32mValid: [ 21/50] Step 080/104 Loss 2.310 Prec@(1,5) (51.5%, 80.1%)\u001b[0m\n",
      "[2024-01-15 09:54:49] \u001b[32mValid: [ 21/50] Step 100/104 Loss 2.298 Prec@(1,5) (51.7%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:54:49] \u001b[32mValid: [ 21/50] Step 104/104 Loss 2.300 Prec@(1,5) (51.5%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:54:49] \u001b[32mValid: [ 21/50] Final Prec@1 51.4800%\u001b[0m\n",
      "[2024-01-15 09:54:49] \u001b[32mEpoch 21 LR 0.015609\u001b[0m\n",
      "[2024-01-15 09:54:58] \u001b[32mTrain: [ 22/50] Step 000/520 Loss 2.890 Prec@(1,5) (46.9%, 74.0%)\u001b[0m\n",
      "[2024-01-15 09:54:59] \u001b[32mTrain: [ 22/50] Step 020/520 Loss 2.281 Prec@(1,5) (55.0%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:55:00] \u001b[32mTrain: [ 22/50] Step 040/520 Loss 2.308 Prec@(1,5) (54.6%, 82.9%)\u001b[0m\n",
      "[2024-01-15 09:55:01] \u001b[32mTrain: [ 22/50] Step 060/520 Loss 2.301 Prec@(1,5) (54.6%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:55:01] \u001b[32mTrain: [ 22/50] Step 080/520 Loss 2.308 Prec@(1,5) (54.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:55:02] \u001b[32mTrain: [ 22/50] Step 100/520 Loss 2.305 Prec@(1,5) (54.4%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:55:03] \u001b[32mTrain: [ 22/50] Step 120/520 Loss 2.311 Prec@(1,5) (54.5%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:55:04] \u001b[32mTrain: [ 22/50] Step 140/520 Loss 2.310 Prec@(1,5) (54.5%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:55:05] \u001b[32mTrain: [ 22/50] Step 160/520 Loss 2.309 Prec@(1,5) (54.5%, 82.9%)\u001b[0m\n",
      "[2024-01-15 09:55:06] \u001b[32mTrain: [ 22/50] Step 180/520 Loss 2.324 Prec@(1,5) (54.3%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:55:07] \u001b[32mTrain: [ 22/50] Step 200/520 Loss 2.320 Prec@(1,5) (54.4%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:55:08] \u001b[32mTrain: [ 22/50] Step 220/520 Loss 2.331 Prec@(1,5) (54.2%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:09] \u001b[32mTrain: [ 22/50] Step 240/520 Loss 2.334 Prec@(1,5) (54.2%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:10] \u001b[32mTrain: [ 22/50] Step 260/520 Loss 2.340 Prec@(1,5) (54.2%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:11] \u001b[32mTrain: [ 22/50] Step 280/520 Loss 2.333 Prec@(1,5) (54.2%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:12] \u001b[32mTrain: [ 22/50] Step 300/520 Loss 2.334 Prec@(1,5) (54.3%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:13] \u001b[32mTrain: [ 22/50] Step 320/520 Loss 2.333 Prec@(1,5) (54.3%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:14] \u001b[32mTrain: [ 22/50] Step 340/520 Loss 2.330 Prec@(1,5) (54.3%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:15] \u001b[32mTrain: [ 22/50] Step 360/520 Loss 2.333 Prec@(1,5) (54.1%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:16] \u001b[32mTrain: [ 22/50] Step 380/520 Loss 2.332 Prec@(1,5) (54.2%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:17] \u001b[32mTrain: [ 22/50] Step 400/520 Loss 2.330 Prec@(1,5) (54.2%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:18] \u001b[32mTrain: [ 22/50] Step 420/520 Loss 2.333 Prec@(1,5) (54.2%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:19] \u001b[32mTrain: [ 22/50] Step 440/520 Loss 2.335 Prec@(1,5) (54.2%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:20] \u001b[32mTrain: [ 22/50] Step 460/520 Loss 2.336 Prec@(1,5) (54.1%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:21] \u001b[32mTrain: [ 22/50] Step 480/520 Loss 2.341 Prec@(1,5) (54.0%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:22] \u001b[32mTrain: [ 22/50] Step 500/520 Loss 2.342 Prec@(1,5) (54.0%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:22] \u001b[32mTrain: [ 22/50] Step 520/520 Loss 2.340 Prec@(1,5) (54.0%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:23] \u001b[32mTrain: [ 22/50] Final Prec@1 54.0100%\u001b[0m\n",
      "[2024-01-15 09:55:28] \u001b[32mValid: [ 22/50] Step 000/104 Loss 2.274 Prec@(1,5) (54.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:55:28] \u001b[32mValid: [ 22/50] Step 020/104 Loss 2.320 Prec@(1,5) (52.6%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:55:28] \u001b[32mValid: [ 22/50] Step 040/104 Loss 2.296 Prec@(1,5) (52.7%, 81.1%)\u001b[0m\n",
      "[2024-01-15 09:55:29] \u001b[32mValid: [ 22/50] Step 060/104 Loss 2.279 Prec@(1,5) (52.9%, 81.3%)\u001b[0m\n",
      "[2024-01-15 09:55:29] \u001b[32mValid: [ 22/50] Step 080/104 Loss 2.298 Prec@(1,5) (52.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:55:29] \u001b[32mValid: [ 22/50] Step 100/104 Loss 2.269 Prec@(1,5) (52.6%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:55:29] \u001b[32mValid: [ 22/50] Step 104/104 Loss 2.270 Prec@(1,5) (52.6%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:55:30] \u001b[32mValid: [ 22/50] Final Prec@1 52.5900%\u001b[0m\n",
      "[2024-01-15 09:55:30] \u001b[32mEpoch 22 LR 0.014843\u001b[0m\n",
      "[2024-01-15 09:55:38] \u001b[32mTrain: [ 23/50] Step 000/520 Loss 2.427 Prec@(1,5) (56.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:55:39] \u001b[32mTrain: [ 23/50] Step 020/520 Loss 2.303 Prec@(1,5) (54.2%, 82.4%)\u001b[0m\n",
      "[2024-01-15 09:55:40] \u001b[32mTrain: [ 23/50] Step 040/520 Loss 2.277 Prec@(1,5) (55.0%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:55:41] \u001b[32mTrain: [ 23/50] Step 060/520 Loss 2.271 Prec@(1,5) (55.4%, 82.8%)\u001b[0m\n",
      "[2024-01-15 09:55:42] \u001b[32mTrain: [ 23/50] Step 080/520 Loss 2.288 Prec@(1,5) (54.8%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:43] \u001b[32mTrain: [ 23/50] Step 100/520 Loss 2.299 Prec@(1,5) (54.7%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:55:44] \u001b[32mTrain: [ 23/50] Step 120/520 Loss 2.305 Prec@(1,5) (54.5%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:45] \u001b[32mTrain: [ 23/50] Step 140/520 Loss 2.305 Prec@(1,5) (54.5%, 82.7%)\u001b[0m\n",
      "[2024-01-15 09:55:45] \u001b[32mTrain: [ 23/50] Step 160/520 Loss 2.320 Prec@(1,5) (54.4%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:46] \u001b[32mTrain: [ 23/50] Step 180/520 Loss 2.322 Prec@(1,5) (54.4%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:47] \u001b[32mTrain: [ 23/50] Step 200/520 Loss 2.323 Prec@(1,5) (54.5%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:48] \u001b[32mTrain: [ 23/50] Step 220/520 Loss 2.319 Prec@(1,5) (54.6%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:49] \u001b[32mTrain: [ 23/50] Step 240/520 Loss 2.317 Prec@(1,5) (54.6%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:50] \u001b[32mTrain: [ 23/50] Step 260/520 Loss 2.311 Prec@(1,5) (54.7%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:51] \u001b[32mTrain: [ 23/50] Step 280/520 Loss 2.316 Prec@(1,5) (54.7%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:52] \u001b[32mTrain: [ 23/50] Step 300/520 Loss 2.320 Prec@(1,5) (54.6%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:53] \u001b[32mTrain: [ 23/50] Step 320/520 Loss 2.319 Prec@(1,5) (54.6%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:54] \u001b[32mTrain: [ 23/50] Step 340/520 Loss 2.315 Prec@(1,5) (54.6%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:55] \u001b[32mTrain: [ 23/50] Step 360/520 Loss 2.318 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:56] \u001b[32mTrain: [ 23/50] Step 380/520 Loss 2.318 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:57] \u001b[32mTrain: [ 23/50] Step 400/520 Loss 2.323 Prec@(1,5) (54.4%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:55:58] \u001b[32mTrain: [ 23/50] Step 420/520 Loss 2.320 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:55:59] \u001b[32mTrain: [ 23/50] Step 440/520 Loss 2.316 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:56:00] \u001b[32mTrain: [ 23/50] Step 460/520 Loss 2.316 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:56:01] \u001b[32mTrain: [ 23/50] Step 480/520 Loss 2.317 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:56:02] \u001b[32mTrain: [ 23/50] Step 500/520 Loss 2.320 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:56:03] \u001b[32mTrain: [ 23/50] Step 520/520 Loss 2.318 Prec@(1,5) (54.5%, 82.6%)\u001b[0m\n",
      "[2024-01-15 09:56:03] \u001b[32mTrain: [ 23/50] Final Prec@1 54.4820%\u001b[0m\n",
      "[2024-01-15 09:56:08] \u001b[32mValid: [ 23/50] Step 000/104 Loss 2.456 Prec@(1,5) (54.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:56:08] \u001b[32mValid: [ 23/50] Step 020/104 Loss 2.303 Prec@(1,5) (52.2%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:56:09] \u001b[32mValid: [ 23/50] Step 040/104 Loss 2.311 Prec@(1,5) (52.0%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:56:09] \u001b[32mValid: [ 23/50] Step 060/104 Loss 2.285 Prec@(1,5) (52.2%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:56:09] \u001b[32mValid: [ 23/50] Step 080/104 Loss 2.295 Prec@(1,5) (51.9%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:56:10] \u001b[32mValid: [ 23/50] Step 100/104 Loss 2.280 Prec@(1,5) (52.0%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:56:10] \u001b[32mValid: [ 23/50] Step 104/104 Loss 2.283 Prec@(1,5) (52.0%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:56:10] \u001b[32mValid: [ 23/50] Final Prec@1 51.9700%\u001b[0m\n",
      "[2024-01-15 09:56:10] \u001b[32mEpoch 23 LR 0.014067\u001b[0m\n",
      "[2024-01-15 09:56:19] \u001b[32mTrain: [ 24/50] Step 000/520 Loss 2.132 Prec@(1,5) (57.3%, 87.5%)\u001b[0m\n",
      "[2024-01-15 09:56:20] \u001b[32mTrain: [ 24/50] Step 020/520 Loss 2.197 Prec@(1,5) (56.3%, 85.1%)\u001b[0m\n",
      "[2024-01-15 09:56:21] \u001b[32mTrain: [ 24/50] Step 040/520 Loss 2.259 Prec@(1,5) (55.3%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:56:21] \u001b[32mTrain: [ 24/50] Step 060/520 Loss 2.248 Prec@(1,5) (55.6%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:56:22] \u001b[32mTrain: [ 24/50] Step 080/520 Loss 2.234 Prec@(1,5) (56.0%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:56:23] \u001b[32mTrain: [ 24/50] Step 100/520 Loss 2.246 Prec@(1,5) (55.7%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:56:24] \u001b[32mTrain: [ 24/50] Step 120/520 Loss 2.242 Prec@(1,5) (55.8%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:56:25] \u001b[32mTrain: [ 24/50] Step 140/520 Loss 2.256 Prec@(1,5) (55.6%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:56:26] \u001b[32mTrain: [ 24/50] Step 160/520 Loss 2.264 Prec@(1,5) (55.5%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:56:27] \u001b[32mTrain: [ 24/50] Step 180/520 Loss 2.257 Prec@(1,5) (55.7%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:56:28] \u001b[32mTrain: [ 24/50] Step 200/520 Loss 2.252 Prec@(1,5) (55.4%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:56:29] \u001b[32mTrain: [ 24/50] Step 220/520 Loss 2.259 Prec@(1,5) (55.3%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:56:30] \u001b[32mTrain: [ 24/50] Step 240/520 Loss 2.260 Prec@(1,5) (55.3%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:56:31] \u001b[32mTrain: [ 24/50] Step 260/520 Loss 2.271 Prec@(1,5) (55.1%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:56:32] \u001b[32mTrain: [ 24/50] Step 280/520 Loss 2.271 Prec@(1,5) (55.2%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:56:33] \u001b[32mTrain: [ 24/50] Step 300/520 Loss 2.276 Prec@(1,5) (55.1%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:56:34] \u001b[32mTrain: [ 24/50] Step 320/520 Loss 2.277 Prec@(1,5) (55.1%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:56:35] \u001b[32mTrain: [ 24/50] Step 340/520 Loss 2.278 Prec@(1,5) (55.1%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:56:36] \u001b[32mTrain: [ 24/50] Step 360/520 Loss 2.281 Prec@(1,5) (55.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:56:37] \u001b[32mTrain: [ 24/50] Step 380/520 Loss 2.283 Prec@(1,5) (55.1%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:56:37] \u001b[32mTrain: [ 24/50] Step 400/520 Loss 2.286 Prec@(1,5) (55.0%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:56:38] \u001b[32mTrain: [ 24/50] Step 420/520 Loss 2.286 Prec@(1,5) (55.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:56:39] \u001b[32mTrain: [ 24/50] Step 440/520 Loss 2.288 Prec@(1,5) (55.0%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:56:40] \u001b[32mTrain: [ 24/50] Step 460/520 Loss 2.288 Prec@(1,5) (55.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:56:41] \u001b[32mTrain: [ 24/50] Step 480/520 Loss 2.288 Prec@(1,5) (55.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:56:42] \u001b[32mTrain: [ 24/50] Step 500/520 Loss 2.291 Prec@(1,5) (54.9%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:56:43] \u001b[32mTrain: [ 24/50] Step 520/520 Loss 2.295 Prec@(1,5) (54.9%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:56:43] \u001b[32mTrain: [ 24/50] Final Prec@1 54.8780%\u001b[0m\n",
      "[2024-01-15 09:56:48] \u001b[32mValid: [ 24/50] Step 000/104 Loss 2.306 Prec@(1,5) (52.1%, 81.2%)\u001b[0m\n",
      "[2024-01-15 09:56:48] \u001b[32mValid: [ 24/50] Step 020/104 Loss 2.186 Prec@(1,5) (54.4%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:56:49] \u001b[32mValid: [ 24/50] Step 040/104 Loss 2.186 Prec@(1,5) (53.9%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:56:49] \u001b[32mValid: [ 24/50] Step 060/104 Loss 2.166 Prec@(1,5) (53.6%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:56:49] \u001b[32mValid: [ 24/50] Step 080/104 Loss 2.187 Prec@(1,5) (53.1%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:56:50] \u001b[32mValid: [ 24/50] Step 100/104 Loss 2.167 Prec@(1,5) (53.3%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:56:50] \u001b[32mValid: [ 24/50] Step 104/104 Loss 2.168 Prec@(1,5) (53.2%, 82.5%)\u001b[0m\n",
      "[2024-01-15 09:56:50] \u001b[32mValid: [ 24/50] Final Prec@1 53.1500%\u001b[0m\n",
      "[2024-01-15 09:56:50] \u001b[32mEpoch 24 LR 0.013285\u001b[0m\n",
      "[2024-01-15 09:56:58] \u001b[32mTrain: [ 25/50] Step 000/520 Loss 2.372 Prec@(1,5) (51.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 09:56:59] \u001b[32mTrain: [ 25/50] Step 020/520 Loss 2.285 Prec@(1,5) (55.1%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:00] \u001b[32mTrain: [ 25/50] Step 040/520 Loss 2.246 Prec@(1,5) (56.0%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:57:01] \u001b[32mTrain: [ 25/50] Step 060/520 Loss 2.247 Prec@(1,5) (55.4%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:02] \u001b[32mTrain: [ 25/50] Step 080/520 Loss 2.235 Prec@(1,5) (55.6%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:57:03] \u001b[32mTrain: [ 25/50] Step 100/520 Loss 2.254 Prec@(1,5) (55.5%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:04] \u001b[32mTrain: [ 25/50] Step 120/520 Loss 2.251 Prec@(1,5) (55.6%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:57:05] \u001b[32mTrain: [ 25/50] Step 140/520 Loss 2.259 Prec@(1,5) (55.4%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:57:06] \u001b[32mTrain: [ 25/50] Step 160/520 Loss 2.251 Prec@(1,5) (55.5%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:57:07] \u001b[32mTrain: [ 25/50] Step 180/520 Loss 2.261 Prec@(1,5) (55.3%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:08] \u001b[32mTrain: [ 25/50] Step 200/520 Loss 2.265 Prec@(1,5) (55.2%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:09] \u001b[32mTrain: [ 25/50] Step 220/520 Loss 2.261 Prec@(1,5) (55.3%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:10] \u001b[32mTrain: [ 25/50] Step 240/520 Loss 2.252 Prec@(1,5) (55.4%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:11] \u001b[32mTrain: [ 25/50] Step 260/520 Loss 2.251 Prec@(1,5) (55.5%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:12] \u001b[32mTrain: [ 25/50] Step 280/520 Loss 2.259 Prec@(1,5) (55.3%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:57:13] \u001b[32mTrain: [ 25/50] Step 300/520 Loss 2.261 Prec@(1,5) (55.2%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:57:14] \u001b[32mTrain: [ 25/50] Step 320/520 Loss 2.260 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:14] \u001b[32mTrain: [ 25/50] Step 340/520 Loss 2.262 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:15] \u001b[32mTrain: [ 25/50] Step 360/520 Loss 2.261 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:16] \u001b[32mTrain: [ 25/50] Step 380/520 Loss 2.262 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:17] \u001b[32mTrain: [ 25/50] Step 400/520 Loss 2.263 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:18] \u001b[32mTrain: [ 25/50] Step 420/520 Loss 2.262 Prec@(1,5) (55.3%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:57:19] \u001b[32mTrain: [ 25/50] Step 440/520 Loss 2.258 Prec@(1,5) (55.3%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:57:20] \u001b[32mTrain: [ 25/50] Step 460/520 Loss 2.260 Prec@(1,5) (55.3%, 83.5%)\u001b[0m\n",
      "[2024-01-15 09:57:21] \u001b[32mTrain: [ 25/50] Step 480/520 Loss 2.263 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:22] \u001b[32mTrain: [ 25/50] Step 500/520 Loss 2.264 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 09:57:23] \u001b[32mTrain: [ 25/50] Step 520/520 Loss 2.265 Prec@(1,5) (55.3%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:57:23] \u001b[32mTrain: [ 25/50] Final Prec@1 55.2540%\u001b[0m\n",
      "[2024-01-15 09:57:28] \u001b[32mValid: [ 25/50] Step 000/104 Loss 2.326 Prec@(1,5) (52.1%, 80.2%)\u001b[0m\n",
      "[2024-01-15 09:57:29] \u001b[32mValid: [ 25/50] Step 020/104 Loss 2.336 Prec@(1,5) (51.1%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:57:29] \u001b[32mValid: [ 25/50] Step 040/104 Loss 2.325 Prec@(1,5) (51.5%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:57:30] \u001b[32mValid: [ 25/50] Step 060/104 Loss 2.304 Prec@(1,5) (51.7%, 82.1%)\u001b[0m\n",
      "[2024-01-15 09:57:30] \u001b[32mValid: [ 25/50] Step 080/104 Loss 2.320 Prec@(1,5) (51.3%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:57:30] \u001b[32mValid: [ 25/50] Step 100/104 Loss 2.305 Prec@(1,5) (51.6%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:57:30] \u001b[32mValid: [ 25/50] Step 104/104 Loss 2.306 Prec@(1,5) (51.7%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:57:31] \u001b[32mValid: [ 25/50] Final Prec@1 51.6600%\u001b[0m\n",
      "[2024-01-15 09:57:31] \u001b[32mEpoch 25 LR 0.012500\u001b[0m\n",
      "[2024-01-15 09:57:39] \u001b[32mTrain: [ 26/50] Step 000/520 Loss 2.300 Prec@(1,5) (55.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:57:40] \u001b[32mTrain: [ 26/50] Step 020/520 Loss 2.219 Prec@(1,5) (56.0%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:57:41] \u001b[32mTrain: [ 26/50] Step 040/520 Loss 2.211 Prec@(1,5) (56.4%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:57:42] \u001b[32mTrain: [ 26/50] Step 060/520 Loss 2.227 Prec@(1,5) (55.9%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:57:42] \u001b[32mTrain: [ 26/50] Step 080/520 Loss 2.213 Prec@(1,5) (56.2%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:57:43] \u001b[32mTrain: [ 26/50] Step 100/520 Loss 2.206 Prec@(1,5) (56.4%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:57:44] \u001b[32mTrain: [ 26/50] Step 120/520 Loss 2.209 Prec@(1,5) (56.3%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:57:45] \u001b[32mTrain: [ 26/50] Step 140/520 Loss 2.199 Prec@(1,5) (56.7%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:57:46] \u001b[32mTrain: [ 26/50] Step 160/520 Loss 2.200 Prec@(1,5) (56.6%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:57:47] \u001b[32mTrain: [ 26/50] Step 180/520 Loss 2.202 Prec@(1,5) (56.6%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:57:48] \u001b[32mTrain: [ 26/50] Step 200/520 Loss 2.211 Prec@(1,5) (56.4%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:57:49] \u001b[32mTrain: [ 26/50] Step 220/520 Loss 2.218 Prec@(1,5) (56.3%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:50] \u001b[32mTrain: [ 26/50] Step 240/520 Loss 2.222 Prec@(1,5) (56.2%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:51] \u001b[32mTrain: [ 26/50] Step 260/520 Loss 2.222 Prec@(1,5) (56.1%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:52] \u001b[32mTrain: [ 26/50] Step 280/520 Loss 2.222 Prec@(1,5) (56.1%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:57:53] \u001b[32mTrain: [ 26/50] Step 300/520 Loss 2.217 Prec@(1,5) (56.2%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:54] \u001b[32mTrain: [ 26/50] Step 320/520 Loss 2.216 Prec@(1,5) (56.2%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:57:55] \u001b[32mTrain: [ 26/50] Step 340/520 Loss 2.217 Prec@(1,5) (56.2%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:57:56] \u001b[32mTrain: [ 26/50] Step 360/520 Loss 2.216 Prec@(1,5) (56.2%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:57:57] \u001b[32mTrain: [ 26/50] Step 380/520 Loss 2.222 Prec@(1,5) (56.3%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:58] \u001b[32mTrain: [ 26/50] Step 400/520 Loss 2.223 Prec@(1,5) (56.3%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:57:59] \u001b[32mTrain: [ 26/50] Step 420/520 Loss 2.231 Prec@(1,5) (56.1%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:58:00] \u001b[32mTrain: [ 26/50] Step 440/520 Loss 2.235 Prec@(1,5) (56.0%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:58:00] \u001b[32mTrain: [ 26/50] Step 460/520 Loss 2.236 Prec@(1,5) (56.1%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:58:01] \u001b[32mTrain: [ 26/50] Step 480/520 Loss 2.234 Prec@(1,5) (56.1%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:58:02] \u001b[32mTrain: [ 26/50] Step 500/520 Loss 2.236 Prec@(1,5) (56.0%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:58:03] \u001b[32mTrain: [ 26/50] Step 520/520 Loss 2.234 Prec@(1,5) (56.1%, 83.6%)\u001b[0m\n",
      "[2024-01-15 09:58:04] \u001b[32mTrain: [ 26/50] Final Prec@1 56.0600%\u001b[0m\n",
      "[2024-01-15 09:58:09] \u001b[32mValid: [ 26/50] Step 000/104 Loss 2.147 Prec@(1,5) (62.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:58:09] \u001b[32mValid: [ 26/50] Step 020/104 Loss 2.359 Prec@(1,5) (52.5%, 82.2%)\u001b[0m\n",
      "[2024-01-15 09:58:09] \u001b[32mValid: [ 26/50] Step 040/104 Loss 2.332 Prec@(1,5) (51.9%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:58:10] \u001b[32mValid: [ 26/50] Step 060/104 Loss 2.288 Prec@(1,5) (51.8%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:58:10] \u001b[32mValid: [ 26/50] Step 080/104 Loss 2.293 Prec@(1,5) (51.4%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:58:10] \u001b[32mValid: [ 26/50] Step 100/104 Loss 2.282 Prec@(1,5) (51.9%, 82.0%)\u001b[0m\n",
      "[2024-01-15 09:58:10] \u001b[32mValid: [ 26/50] Step 104/104 Loss 2.282 Prec@(1,5) (51.9%, 81.9%)\u001b[0m\n",
      "[2024-01-15 09:58:11] \u001b[32mValid: [ 26/50] Final Prec@1 51.9100%\u001b[0m\n",
      "[2024-01-15 09:58:11] \u001b[32mEpoch 26 LR 0.011716\u001b[0m\n",
      "[2024-01-15 09:58:19] \u001b[32mTrain: [ 27/50] Step 000/520 Loss 2.301 Prec@(1,5) (55.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:58:20] \u001b[32mTrain: [ 27/50] Step 020/520 Loss 2.188 Prec@(1,5) (56.3%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:58:21] \u001b[32mTrain: [ 27/50] Step 040/520 Loss 2.149 Prec@(1,5) (58.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:58:22] \u001b[32mTrain: [ 27/50] Step 060/520 Loss 2.161 Prec@(1,5) (58.0%, 84.3%)\u001b[0m\n",
      "[2024-01-15 09:58:23] \u001b[32mTrain: [ 27/50] Step 080/520 Loss 2.162 Prec@(1,5) (57.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:58:24] \u001b[32mTrain: [ 27/50] Step 100/520 Loss 2.173 Prec@(1,5) (57.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:58:25] \u001b[32mTrain: [ 27/50] Step 120/520 Loss 2.180 Prec@(1,5) (57.0%, 84.3%)\u001b[0m\n",
      "[2024-01-15 09:58:26] \u001b[32mTrain: [ 27/50] Step 140/520 Loss 2.200 Prec@(1,5) (56.6%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:58:27] \u001b[32mTrain: [ 27/50] Step 160/520 Loss 2.211 Prec@(1,5) (56.2%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:58:28] \u001b[32mTrain: [ 27/50] Step 180/520 Loss 2.217 Prec@(1,5) (56.1%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:29] \u001b[32mTrain: [ 27/50] Step 200/520 Loss 2.224 Prec@(1,5) (56.0%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:30] \u001b[32mTrain: [ 27/50] Step 220/520 Loss 2.229 Prec@(1,5) (55.7%, 83.7%)\u001b[0m\n",
      "[2024-01-15 09:58:31] \u001b[32mTrain: [ 27/50] Step 240/520 Loss 2.226 Prec@(1,5) (55.7%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:32] \u001b[32mTrain: [ 27/50] Step 260/520 Loss 2.227 Prec@(1,5) (55.7%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:33] \u001b[32mTrain: [ 27/50] Step 280/520 Loss 2.225 Prec@(1,5) (55.7%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:34] \u001b[32mTrain: [ 27/50] Step 300/520 Loss 2.225 Prec@(1,5) (55.7%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:35] \u001b[32mTrain: [ 27/50] Step 320/520 Loss 2.227 Prec@(1,5) (55.7%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:35] \u001b[32mTrain: [ 27/50] Step 340/520 Loss 2.224 Prec@(1,5) (55.8%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:36] \u001b[32mTrain: [ 27/50] Step 360/520 Loss 2.220 Prec@(1,5) (55.9%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:37] \u001b[32mTrain: [ 27/50] Step 380/520 Loss 2.221 Prec@(1,5) (56.0%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:38] \u001b[32mTrain: [ 27/50] Step 400/520 Loss 2.221 Prec@(1,5) (56.1%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:39] \u001b[32mTrain: [ 27/50] Step 420/520 Loss 2.222 Prec@(1,5) (56.0%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:40] \u001b[32mTrain: [ 27/50] Step 440/520 Loss 2.225 Prec@(1,5) (56.0%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:41] \u001b[32mTrain: [ 27/50] Step 460/520 Loss 2.225 Prec@(1,5) (56.0%, 83.8%)\u001b[0m\n",
      "[2024-01-15 09:58:42] \u001b[32mTrain: [ 27/50] Step 480/520 Loss 2.224 Prec@(1,5) (56.1%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:43] \u001b[32mTrain: [ 27/50] Step 500/520 Loss 2.223 Prec@(1,5) (56.1%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:44] \u001b[32mTrain: [ 27/50] Step 520/520 Loss 2.223 Prec@(1,5) (56.1%, 83.9%)\u001b[0m\n",
      "[2024-01-15 09:58:44] \u001b[32mTrain: [ 27/50] Final Prec@1 56.0960%\u001b[0m\n",
      "[2024-01-15 09:58:49] \u001b[32mValid: [ 27/50] Step 000/104 Loss 2.378 Prec@(1,5) (50.0%, 79.2%)\u001b[0m\n",
      "[2024-01-15 09:58:50] \u001b[32mValid: [ 27/50] Step 020/104 Loss 2.481 Prec@(1,5) (51.6%, 80.9%)\u001b[0m\n",
      "[2024-01-15 09:58:50] \u001b[32mValid: [ 27/50] Step 040/104 Loss 2.442 Prec@(1,5) (51.6%, 81.4%)\u001b[0m\n",
      "[2024-01-15 09:58:51] \u001b[32mValid: [ 27/50] Step 060/104 Loss 2.391 Prec@(1,5) (51.7%, 81.7%)\u001b[0m\n",
      "[2024-01-15 09:58:51] \u001b[32mValid: [ 27/50] Step 080/104 Loss 2.395 Prec@(1,5) (51.6%, 81.5%)\u001b[0m\n",
      "[2024-01-15 09:58:51] \u001b[32mValid: [ 27/50] Step 100/104 Loss 2.385 Prec@(1,5) (51.8%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:58:51] \u001b[32mValid: [ 27/50] Step 104/104 Loss 2.383 Prec@(1,5) (51.8%, 81.6%)\u001b[0m\n",
      "[2024-01-15 09:58:52] \u001b[32mValid: [ 27/50] Final Prec@1 51.8000%\u001b[0m\n",
      "[2024-01-15 09:58:52] \u001b[32mEpoch 27 LR 0.010934\u001b[0m\n",
      "[2024-01-15 09:59:00] \u001b[32mTrain: [ 28/50] Step 000/520 Loss 2.391 Prec@(1,5) (47.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 09:59:01] \u001b[32mTrain: [ 28/50] Step 020/520 Loss 2.166 Prec@(1,5) (56.7%, 84.3%)\u001b[0m\n",
      "[2024-01-15 09:59:02] \u001b[32mTrain: [ 28/50] Step 040/520 Loss 2.162 Prec@(1,5) (57.0%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:59:03] \u001b[32mTrain: [ 28/50] Step 060/520 Loss 2.163 Prec@(1,5) (57.0%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:04] \u001b[32mTrain: [ 28/50] Step 080/520 Loss 2.162 Prec@(1,5) (57.3%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:05] \u001b[32mTrain: [ 28/50] Step 100/520 Loss 2.152 Prec@(1,5) (57.4%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:06] \u001b[32mTrain: [ 28/50] Step 120/520 Loss 2.150 Prec@(1,5) (57.4%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:07] \u001b[32mTrain: [ 28/50] Step 140/520 Loss 2.162 Prec@(1,5) (57.3%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:08] \u001b[32mTrain: [ 28/50] Step 160/520 Loss 2.157 Prec@(1,5) (57.5%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:09] \u001b[32mTrain: [ 28/50] Step 180/520 Loss 2.164 Prec@(1,5) (57.4%, 84.3%)\u001b[0m\n",
      "[2024-01-15 09:59:10] \u001b[32mTrain: [ 28/50] Step 200/520 Loss 2.166 Prec@(1,5) (57.3%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:11] \u001b[32mTrain: [ 28/50] Step 220/520 Loss 2.175 Prec@(1,5) (57.0%, 84.3%)\u001b[0m\n",
      "[2024-01-15 09:59:12] \u001b[32mTrain: [ 28/50] Step 240/520 Loss 2.177 Prec@(1,5) (56.9%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:59:13] \u001b[32mTrain: [ 28/50] Step 260/520 Loss 2.182 Prec@(1,5) (56.8%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:14] \u001b[32mTrain: [ 28/50] Step 280/520 Loss 2.179 Prec@(1,5) (56.9%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:15] \u001b[32mTrain: [ 28/50] Step 300/520 Loss 2.180 Prec@(1,5) (56.9%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:59:16] \u001b[32mTrain: [ 28/50] Step 320/520 Loss 2.179 Prec@(1,5) (56.9%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:59:17] \u001b[32mTrain: [ 28/50] Step 340/520 Loss 2.181 Prec@(1,5) (56.9%, 84.3%)\u001b[0m\n",
      "[2024-01-15 09:59:18] \u001b[32mTrain: [ 28/50] Step 360/520 Loss 2.183 Prec@(1,5) (56.8%, 84.2%)\u001b[0m\n",
      "[2024-01-15 09:59:19] \u001b[32mTrain: [ 28/50] Step 380/520 Loss 2.188 Prec@(1,5) (56.7%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:20] \u001b[32mTrain: [ 28/50] Step 400/520 Loss 2.191 Prec@(1,5) (56.7%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:21] \u001b[32mTrain: [ 28/50] Step 420/520 Loss 2.189 Prec@(1,5) (56.7%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:22] \u001b[32mTrain: [ 28/50] Step 440/520 Loss 2.188 Prec@(1,5) (56.7%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:23] \u001b[32mTrain: [ 28/50] Step 460/520 Loss 2.188 Prec@(1,5) (56.8%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:24] \u001b[32mTrain: [ 28/50] Step 480/520 Loss 2.189 Prec@(1,5) (56.8%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:25] \u001b[32mTrain: [ 28/50] Step 500/520 Loss 2.190 Prec@(1,5) (56.8%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:26] \u001b[32mTrain: [ 28/50] Step 520/520 Loss 2.192 Prec@(1,5) (56.7%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:26] \u001b[32mTrain: [ 28/50] Final Prec@1 56.7440%\u001b[0m\n",
      "[2024-01-15 09:59:31] \u001b[32mValid: [ 28/50] Step 000/104 Loss 2.047 Prec@(1,5) (58.3%, 82.3%)\u001b[0m\n",
      "[2024-01-15 09:59:32] \u001b[32mValid: [ 28/50] Step 020/104 Loss 2.233 Prec@(1,5) (53.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:59:32] \u001b[32mValid: [ 28/50] Step 040/104 Loss 2.210 Prec@(1,5) (53.0%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:59:33] \u001b[32mValid: [ 28/50] Step 060/104 Loss 2.165 Prec@(1,5) (53.5%, 83.1%)\u001b[0m\n",
      "[2024-01-15 09:59:33] \u001b[32mValid: [ 28/50] Step 080/104 Loss 2.169 Prec@(1,5) (53.4%, 83.0%)\u001b[0m\n",
      "[2024-01-15 09:59:33] \u001b[32mValid: [ 28/50] Step 100/104 Loss 2.148 Prec@(1,5) (53.7%, 83.3%)\u001b[0m\n",
      "[2024-01-15 09:59:33] \u001b[32mValid: [ 28/50] Step 104/104 Loss 2.148 Prec@(1,5) (53.8%, 83.2%)\u001b[0m\n",
      "[2024-01-15 09:59:34] \u001b[32mValid: [ 28/50] Final Prec@1 53.8200%\u001b[0m\n",
      "[2024-01-15 09:59:34] \u001b[32mEpoch 28 LR 0.010158\u001b[0m\n",
      "[2024-01-15 09:59:42] \u001b[32mTrain: [ 29/50] Step 000/520 Loss 2.175 Prec@(1,5) (62.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:43] \u001b[32mTrain: [ 29/50] Step 020/520 Loss 2.159 Prec@(1,5) (57.0%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:44] \u001b[32mTrain: [ 29/50] Step 040/520 Loss 2.179 Prec@(1,5) (56.3%, 84.0%)\u001b[0m\n",
      "[2024-01-15 09:59:45] \u001b[32mTrain: [ 29/50] Step 060/520 Loss 2.176 Prec@(1,5) (56.5%, 84.1%)\u001b[0m\n",
      "[2024-01-15 09:59:46] \u001b[32mTrain: [ 29/50] Step 080/520 Loss 2.156 Prec@(1,5) (57.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:47] \u001b[32mTrain: [ 29/50] Step 100/520 Loss 2.163 Prec@(1,5) (57.1%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:47] \u001b[32mTrain: [ 29/50] Step 120/520 Loss 2.156 Prec@(1,5) (57.4%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:59:48] \u001b[32mTrain: [ 29/50] Step 140/520 Loss 2.158 Prec@(1,5) (57.4%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:49] \u001b[32mTrain: [ 29/50] Step 160/520 Loss 2.159 Prec@(1,5) (57.4%, 84.4%)\u001b[0m\n",
      "[2024-01-15 09:59:50] \u001b[32mTrain: [ 29/50] Step 180/520 Loss 2.156 Prec@(1,5) (57.5%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:51] \u001b[32mTrain: [ 29/50] Step 200/520 Loss 2.150 Prec@(1,5) (57.6%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:59:52] \u001b[32mTrain: [ 29/50] Step 220/520 Loss 2.154 Prec@(1,5) (57.4%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:59:53] \u001b[32mTrain: [ 29/50] Step 240/520 Loss 2.158 Prec@(1,5) (57.2%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:59:54] \u001b[32mTrain: [ 29/50] Step 260/520 Loss 2.161 Prec@(1,5) (57.2%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:59:55] \u001b[32mTrain: [ 29/50] Step 280/520 Loss 2.164 Prec@(1,5) (57.1%, 84.6%)\u001b[0m\n",
      "[2024-01-15 09:59:56] \u001b[32mTrain: [ 29/50] Step 300/520 Loss 2.164 Prec@(1,5) (57.1%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:57] \u001b[32mTrain: [ 29/50] Step 320/520 Loss 2.167 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:58] \u001b[32mTrain: [ 29/50] Step 340/520 Loss 2.172 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 09:59:59] \u001b[32mTrain: [ 29/50] Step 360/520 Loss 2.175 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:00] \u001b[32mTrain: [ 29/50] Step 380/520 Loss 2.174 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:01] \u001b[32mTrain: [ 29/50] Step 400/520 Loss 2.170 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:02] \u001b[32mTrain: [ 29/50] Step 420/520 Loss 2.171 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:02] \u001b[32mTrain: [ 29/50] Step 440/520 Loss 2.175 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:03] \u001b[32mTrain: [ 29/50] Step 460/520 Loss 2.173 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:04] \u001b[32mTrain: [ 29/50] Step 480/520 Loss 2.173 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:05] \u001b[32mTrain: [ 29/50] Step 500/520 Loss 2.172 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:06] \u001b[32mTrain: [ 29/50] Step 520/520 Loss 2.174 Prec@(1,5) (57.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:07] \u001b[32mTrain: [ 29/50] Final Prec@1 56.9860%\u001b[0m\n",
      "[2024-01-15 10:00:11] \u001b[32mValid: [ 29/50] Step 000/104 Loss 2.096 Prec@(1,5) (57.3%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:00:12] \u001b[32mValid: [ 29/50] Step 020/104 Loss 2.158 Prec@(1,5) (55.0%, 83.7%)\u001b[0m\n",
      "[2024-01-15 10:00:12] \u001b[32mValid: [ 29/50] Step 040/104 Loss 2.129 Prec@(1,5) (55.2%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:00:12] \u001b[32mValid: [ 29/50] Step 060/104 Loss 2.096 Prec@(1,5) (55.4%, 83.9%)\u001b[0m\n",
      "[2024-01-15 10:00:13] \u001b[32mValid: [ 29/50] Step 080/104 Loss 2.115 Prec@(1,5) (54.7%, 83.7%)\u001b[0m\n",
      "[2024-01-15 10:00:13] \u001b[32mValid: [ 29/50] Step 100/104 Loss 2.094 Prec@(1,5) (55.2%, 83.8%)\u001b[0m\n",
      "[2024-01-15 10:00:13] \u001b[32mValid: [ 29/50] Step 104/104 Loss 2.097 Prec@(1,5) (55.2%, 83.8%)\u001b[0m\n",
      "[2024-01-15 10:00:13] \u001b[32mValid: [ 29/50] Final Prec@1 55.1800%\u001b[0m\n",
      "[2024-01-15 10:00:13] \u001b[32mEpoch 29 LR 0.009392\u001b[0m\n",
      "[2024-01-15 10:00:22] \u001b[32mTrain: [ 30/50] Step 000/520 Loss 2.365 Prec@(1,5) (55.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:00:22] \u001b[32mTrain: [ 30/50] Step 020/520 Loss 2.143 Prec@(1,5) (59.0%, 84.3%)\u001b[0m\n",
      "[2024-01-15 10:00:23] \u001b[32mTrain: [ 30/50] Step 040/520 Loss 2.142 Prec@(1,5) (58.9%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:24] \u001b[32mTrain: [ 30/50] Step 060/520 Loss 2.175 Prec@(1,5) (58.2%, 84.2%)\u001b[0m\n",
      "[2024-01-15 10:00:25] \u001b[32mTrain: [ 30/50] Step 080/520 Loss 2.161 Prec@(1,5) (58.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:00:26] \u001b[32mTrain: [ 30/50] Step 100/520 Loss 2.152 Prec@(1,5) (58.1%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:27] \u001b[32mTrain: [ 30/50] Step 120/520 Loss 2.153 Prec@(1,5) (58.0%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:28] \u001b[32mTrain: [ 30/50] Step 140/520 Loss 2.148 Prec@(1,5) (58.2%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:29] \u001b[32mTrain: [ 30/50] Step 160/520 Loss 2.150 Prec@(1,5) (58.1%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:30] \u001b[32mTrain: [ 30/50] Step 180/520 Loss 2.141 Prec@(1,5) (58.2%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:31] \u001b[32mTrain: [ 30/50] Step 200/520 Loss 2.148 Prec@(1,5) (58.0%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:32] \u001b[32mTrain: [ 30/50] Step 220/520 Loss 2.138 Prec@(1,5) (58.3%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:33] \u001b[32mTrain: [ 30/50] Step 240/520 Loss 2.141 Prec@(1,5) (58.1%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:00:34] \u001b[32mTrain: [ 30/50] Step 260/520 Loss 2.140 Prec@(1,5) (58.0%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:00:35] \u001b[32mTrain: [ 30/50] Step 280/520 Loss 2.137 Prec@(1,5) (58.1%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:00:36] \u001b[32mTrain: [ 30/50] Step 300/520 Loss 2.141 Prec@(1,5) (58.0%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:36] \u001b[32mTrain: [ 30/50] Step 320/520 Loss 2.140 Prec@(1,5) (58.0%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:37] \u001b[32mTrain: [ 30/50] Step 340/520 Loss 2.144 Prec@(1,5) (57.9%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:38] \u001b[32mTrain: [ 30/50] Step 360/520 Loss 2.144 Prec@(1,5) (57.8%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:39] \u001b[32mTrain: [ 30/50] Step 380/520 Loss 2.145 Prec@(1,5) (57.9%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:40] \u001b[32mTrain: [ 30/50] Step 400/520 Loss 2.145 Prec@(1,5) (57.9%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:41] \u001b[32mTrain: [ 30/50] Step 420/520 Loss 2.148 Prec@(1,5) (57.8%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:42] \u001b[32mTrain: [ 30/50] Step 440/520 Loss 2.147 Prec@(1,5) (57.7%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:43] \u001b[32mTrain: [ 30/50] Step 460/520 Loss 2.147 Prec@(1,5) (57.7%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:44] \u001b[32mTrain: [ 30/50] Step 480/520 Loss 2.149 Prec@(1,5) (57.6%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:45] \u001b[32mTrain: [ 30/50] Step 500/520 Loss 2.154 Prec@(1,5) (57.6%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:00:46] \u001b[32mTrain: [ 30/50] Step 520/520 Loss 2.153 Prec@(1,5) (57.5%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:00:46] \u001b[32mTrain: [ 30/50] Final Prec@1 57.5400%\u001b[0m\n",
      "[2024-01-15 10:00:51] \u001b[32mValid: [ 30/50] Step 000/104 Loss 1.965 Prec@(1,5) (60.4%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:00:52] \u001b[32mValid: [ 30/50] Step 020/104 Loss 2.077 Prec@(1,5) (55.3%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:00:52] \u001b[32mValid: [ 30/50] Step 040/104 Loss 2.083 Prec@(1,5) (55.0%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:00:52] \u001b[32mValid: [ 30/50] Step 060/104 Loss 2.058 Prec@(1,5) (55.2%, 84.3%)\u001b[0m\n",
      "[2024-01-15 10:00:53] \u001b[32mValid: [ 30/50] Step 080/104 Loss 2.070 Prec@(1,5) (54.7%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:00:53] \u001b[32mValid: [ 30/50] Step 100/104 Loss 2.054 Prec@(1,5) (55.2%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:00:53] \u001b[32mValid: [ 30/50] Step 104/104 Loss 2.053 Prec@(1,5) (55.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:00:53] \u001b[32mValid: [ 30/50] Final Prec@1 55.2000%\u001b[0m\n",
      "[2024-01-15 10:00:53] \u001b[32mEpoch 30 LR 0.008638\u001b[0m\n",
      "[2024-01-15 10:01:02] \u001b[32mTrain: [ 31/50] Step 000/520 Loss 2.034 Prec@(1,5) (61.5%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:01:03] \u001b[32mTrain: [ 31/50] Step 020/520 Loss 2.187 Prec@(1,5) (56.9%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:01:03] \u001b[32mTrain: [ 31/50] Step 040/520 Loss 2.141 Prec@(1,5) (58.0%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:04] \u001b[32mTrain: [ 31/50] Step 060/520 Loss 2.135 Prec@(1,5) (58.1%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:05] \u001b[32mTrain: [ 31/50] Step 080/520 Loss 2.130 Prec@(1,5) (57.9%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:06] \u001b[32mTrain: [ 31/50] Step 100/520 Loss 2.115 Prec@(1,5) (58.3%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:07] \u001b[32mTrain: [ 31/50] Step 120/520 Loss 2.120 Prec@(1,5) (58.4%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:01:08] \u001b[32mTrain: [ 31/50] Step 140/520 Loss 2.118 Prec@(1,5) (58.3%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:09] \u001b[32mTrain: [ 31/50] Step 160/520 Loss 2.129 Prec@(1,5) (58.2%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:10] \u001b[32mTrain: [ 31/50] Step 180/520 Loss 2.127 Prec@(1,5) (58.2%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:11] \u001b[32mTrain: [ 31/50] Step 200/520 Loss 2.128 Prec@(1,5) (58.2%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:12] \u001b[32mTrain: [ 31/50] Step 220/520 Loss 2.128 Prec@(1,5) (58.2%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:13] \u001b[32mTrain: [ 31/50] Step 240/520 Loss 2.121 Prec@(1,5) (58.3%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:14] \u001b[32mTrain: [ 31/50] Step 260/520 Loss 2.120 Prec@(1,5) (58.2%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:15] \u001b[32mTrain: [ 31/50] Step 280/520 Loss 2.126 Prec@(1,5) (58.0%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:16] \u001b[32mTrain: [ 31/50] Step 300/520 Loss 2.126 Prec@(1,5) (57.9%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:01:16] \u001b[32mTrain: [ 31/50] Step 320/520 Loss 2.127 Prec@(1,5) (57.9%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:01:17] \u001b[32mTrain: [ 31/50] Step 340/520 Loss 2.131 Prec@(1,5) (57.8%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:18] \u001b[32mTrain: [ 31/50] Step 360/520 Loss 2.134 Prec@(1,5) (57.7%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:19] \u001b[32mTrain: [ 31/50] Step 380/520 Loss 2.136 Prec@(1,5) (57.7%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:20] \u001b[32mTrain: [ 31/50] Step 400/520 Loss 2.136 Prec@(1,5) (57.7%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:21] \u001b[32mTrain: [ 31/50] Step 420/520 Loss 2.140 Prec@(1,5) (57.6%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:01:22] \u001b[32mTrain: [ 31/50] Step 440/520 Loss 2.136 Prec@(1,5) (57.8%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:01:23] \u001b[32mTrain: [ 31/50] Step 460/520 Loss 2.139 Prec@(1,5) (57.7%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:01:24] \u001b[32mTrain: [ 31/50] Step 480/520 Loss 2.144 Prec@(1,5) (57.7%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:01:25] \u001b[32mTrain: [ 31/50] Step 500/520 Loss 2.144 Prec@(1,5) (57.7%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:01:26] \u001b[32mTrain: [ 31/50] Step 520/520 Loss 2.145 Prec@(1,5) (57.6%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:01:26] \u001b[32mTrain: [ 31/50] Final Prec@1 57.6480%\u001b[0m\n",
      "[2024-01-15 10:01:31] \u001b[32mValid: [ 31/50] Step 000/104 Loss 2.255 Prec@(1,5) (59.4%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:01:31] \u001b[32mValid: [ 31/50] Step 020/104 Loss 2.244 Prec@(1,5) (55.1%, 82.8%)\u001b[0m\n",
      "[2024-01-15 10:01:32] \u001b[32mValid: [ 31/50] Step 040/104 Loss 2.198 Prec@(1,5) (54.9%, 82.9%)\u001b[0m\n",
      "[2024-01-15 10:01:32] \u001b[32mValid: [ 31/50] Step 060/104 Loss 2.170 Prec@(1,5) (54.6%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:01:32] \u001b[32mValid: [ 31/50] Step 080/104 Loss 2.179 Prec@(1,5) (54.6%, 83.0%)\u001b[0m\n",
      "[2024-01-15 10:01:33] \u001b[32mValid: [ 31/50] Step 100/104 Loss 2.157 Prec@(1,5) (54.7%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:01:33] \u001b[32mValid: [ 31/50] Step 104/104 Loss 2.159 Prec@(1,5) (54.7%, 83.2%)\u001b[0m\n",
      "[2024-01-15 10:01:33] \u001b[32mValid: [ 31/50] Final Prec@1 54.6500%\u001b[0m\n",
      "[2024-01-15 10:01:33] \u001b[32mEpoch 31 LR 0.007899\u001b[0m\n",
      "[2024-01-15 10:01:41] \u001b[32mTrain: [ 32/50] Step 000/520 Loss 1.887 Prec@(1,5) (61.5%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:01:42] \u001b[32mTrain: [ 32/50] Step 020/520 Loss 2.096 Prec@(1,5) (58.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:43] \u001b[32mTrain: [ 32/50] Step 040/520 Loss 2.085 Prec@(1,5) (58.2%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:44] \u001b[32mTrain: [ 32/50] Step 060/520 Loss 2.095 Prec@(1,5) (58.2%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:01:45] \u001b[32mTrain: [ 32/50] Step 080/520 Loss 2.091 Prec@(1,5) (58.8%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:01:46] \u001b[32mTrain: [ 32/50] Step 100/520 Loss 2.098 Prec@(1,5) (58.8%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:01:47] \u001b[32mTrain: [ 32/50] Step 120/520 Loss 2.089 Prec@(1,5) (58.9%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:01:48] \u001b[32mTrain: [ 32/50] Step 140/520 Loss 2.096 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:49] \u001b[32mTrain: [ 32/50] Step 160/520 Loss 2.084 Prec@(1,5) (59.1%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:01:50] \u001b[32mTrain: [ 32/50] Step 180/520 Loss 2.092 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:51] \u001b[32mTrain: [ 32/50] Step 200/520 Loss 2.095 Prec@(1,5) (58.7%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:01:52] \u001b[32mTrain: [ 32/50] Step 220/520 Loss 2.099 Prec@(1,5) (58.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:53] \u001b[32mTrain: [ 32/50] Step 240/520 Loss 2.101 Prec@(1,5) (58.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:54] \u001b[32mTrain: [ 32/50] Step 260/520 Loss 2.099 Prec@(1,5) (58.6%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:01:55] \u001b[32mTrain: [ 32/50] Step 280/520 Loss 2.100 Prec@(1,5) (58.7%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:01:55] \u001b[32mTrain: [ 32/50] Step 300/520 Loss 2.098 Prec@(1,5) (58.7%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:56] \u001b[32mTrain: [ 32/50] Step 320/520 Loss 2.099 Prec@(1,5) (58.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:57] \u001b[32mTrain: [ 32/50] Step 340/520 Loss 2.100 Prec@(1,5) (58.6%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:01:58] \u001b[32mTrain: [ 32/50] Step 360/520 Loss 2.101 Prec@(1,5) (58.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:01:59] \u001b[32mTrain: [ 32/50] Step 380/520 Loss 2.102 Prec@(1,5) (58.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:00] \u001b[32mTrain: [ 32/50] Step 400/520 Loss 2.106 Prec@(1,5) (58.5%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:01] \u001b[32mTrain: [ 32/50] Step 420/520 Loss 2.108 Prec@(1,5) (58.4%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:02] \u001b[32mTrain: [ 32/50] Step 440/520 Loss 2.110 Prec@(1,5) (58.3%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:03] \u001b[32mTrain: [ 32/50] Step 460/520 Loss 2.111 Prec@(1,5) (58.3%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:04] \u001b[32mTrain: [ 32/50] Step 480/520 Loss 2.110 Prec@(1,5) (58.3%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:05] \u001b[32mTrain: [ 32/50] Step 500/520 Loss 2.110 Prec@(1,5) (58.3%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:06] \u001b[32mTrain: [ 32/50] Step 520/520 Loss 2.110 Prec@(1,5) (58.3%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:06] \u001b[32mTrain: [ 32/50] Final Prec@1 58.3200%\u001b[0m\n",
      "[2024-01-15 10:02:11] \u001b[32mValid: [ 32/50] Step 000/104 Loss 1.924 Prec@(1,5) (60.4%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:02:12] \u001b[32mValid: [ 32/50] Step 020/104 Loss 2.068 Prec@(1,5) (56.7%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:02:12] \u001b[32mValid: [ 32/50] Step 040/104 Loss 2.019 Prec@(1,5) (56.3%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:02:12] \u001b[32mValid: [ 32/50] Step 060/104 Loss 1.986 Prec@(1,5) (56.7%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:13] \u001b[32mValid: [ 32/50] Step 080/104 Loss 1.997 Prec@(1,5) (56.2%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:02:13] \u001b[32mValid: [ 32/50] Step 100/104 Loss 1.976 Prec@(1,5) (56.3%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:13] \u001b[32mValid: [ 32/50] Step 104/104 Loss 1.974 Prec@(1,5) (56.4%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:13] \u001b[32mValid: [ 32/50] Final Prec@1 56.3600%\u001b[0m\n",
      "[2024-01-15 10:02:13] \u001b[32mEpoch 32 LR 0.007178\u001b[0m\n",
      "[2024-01-15 10:02:22] \u001b[32mTrain: [ 33/50] Step 000/520 Loss 2.317 Prec@(1,5) (62.5%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:02:23] \u001b[32mTrain: [ 33/50] Step 020/520 Loss 2.100 Prec@(1,5) (59.4%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:02:24] \u001b[32mTrain: [ 33/50] Step 040/520 Loss 2.071 Prec@(1,5) (59.5%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:02:25] \u001b[32mTrain: [ 33/50] Step 060/520 Loss 2.053 Prec@(1,5) (59.6%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:02:25] \u001b[32mTrain: [ 33/50] Step 080/520 Loss 2.046 Prec@(1,5) (59.6%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:02:26] \u001b[32mTrain: [ 33/50] Step 100/520 Loss 2.072 Prec@(1,5) (59.2%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:27] \u001b[32mTrain: [ 33/50] Step 120/520 Loss 2.071 Prec@(1,5) (59.1%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:28] \u001b[32mTrain: [ 33/50] Step 140/520 Loss 2.085 Prec@(1,5) (58.9%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:02:29] \u001b[32mTrain: [ 33/50] Step 160/520 Loss 2.079 Prec@(1,5) (58.9%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:30] \u001b[32mTrain: [ 33/50] Step 180/520 Loss 2.083 Prec@(1,5) (59.0%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:02:31] \u001b[32mTrain: [ 33/50] Step 200/520 Loss 2.075 Prec@(1,5) (59.2%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:02:32] \u001b[32mTrain: [ 33/50] Step 220/520 Loss 2.072 Prec@(1,5) (59.2%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:33] \u001b[32mTrain: [ 33/50] Step 240/520 Loss 2.076 Prec@(1,5) (59.2%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:02:34] \u001b[32mTrain: [ 33/50] Step 260/520 Loss 2.080 Prec@(1,5) (59.1%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:02:35] \u001b[32mTrain: [ 33/50] Step 280/520 Loss 2.080 Prec@(1,5) (59.1%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:36] \u001b[32mTrain: [ 33/50] Step 300/520 Loss 2.081 Prec@(1,5) (59.1%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:37] \u001b[32mTrain: [ 33/50] Step 320/520 Loss 2.083 Prec@(1,5) (59.0%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:38] \u001b[32mTrain: [ 33/50] Step 340/520 Loss 2.084 Prec@(1,5) (59.0%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:39] \u001b[32mTrain: [ 33/50] Step 360/520 Loss 2.086 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:40] \u001b[32mTrain: [ 33/50] Step 380/520 Loss 2.088 Prec@(1,5) (58.9%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:41] \u001b[32mTrain: [ 33/50] Step 400/520 Loss 2.090 Prec@(1,5) (58.9%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:42] \u001b[32mTrain: [ 33/50] Step 420/520 Loss 2.089 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:43] \u001b[32mTrain: [ 33/50] Step 440/520 Loss 2.090 Prec@(1,5) (58.9%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:43] \u001b[32mTrain: [ 33/50] Step 460/520 Loss 2.089 Prec@(1,5) (59.0%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:02:44] \u001b[32mTrain: [ 33/50] Step 480/520 Loss 2.088 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:45] \u001b[32mTrain: [ 33/50] Step 500/520 Loss 2.088 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:46] \u001b[32mTrain: [ 33/50] Step 520/520 Loss 2.088 Prec@(1,5) (58.9%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:02:47] \u001b[32mTrain: [ 33/50] Final Prec@1 58.8940%\u001b[0m\n",
      "[2024-01-15 10:02:52] \u001b[32mValid: [ 33/50] Step 000/104 Loss 1.998 Prec@(1,5) (57.3%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:02:52] \u001b[32mValid: [ 33/50] Step 020/104 Loss 2.191 Prec@(1,5) (55.3%, 82.7%)\u001b[0m\n",
      "[2024-01-15 10:02:53] \u001b[32mValid: [ 33/50] Step 040/104 Loss 2.157 Prec@(1,5) (55.1%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:02:53] \u001b[32mValid: [ 33/50] Step 060/104 Loss 2.128 Prec@(1,5) (55.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:02:53] \u001b[32mValid: [ 33/50] Step 080/104 Loss 2.139 Prec@(1,5) (55.1%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:02:54] \u001b[32mValid: [ 33/50] Step 100/104 Loss 2.116 Prec@(1,5) (55.3%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:02:54] \u001b[32mValid: [ 33/50] Step 104/104 Loss 2.118 Prec@(1,5) (55.4%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:02:54] \u001b[32mValid: [ 33/50] Final Prec@1 55.3700%\u001b[0m\n",
      "[2024-01-15 10:02:54] \u001b[32mEpoch 33 LR 0.006479\u001b[0m\n",
      "[2024-01-15 10:03:03] \u001b[32mTrain: [ 34/50] Step 000/520 Loss 1.686 Prec@(1,5) (66.7%, 89.6%)\u001b[0m\n",
      "[2024-01-15 10:03:04] \u001b[32mTrain: [ 34/50] Step 020/520 Loss 2.040 Prec@(1,5) (59.0%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:03:04] \u001b[32mTrain: [ 34/50] Step 040/520 Loss 2.058 Prec@(1,5) (59.3%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:03:05] \u001b[32mTrain: [ 34/50] Step 060/520 Loss 2.045 Prec@(1,5) (59.5%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:03:06] \u001b[32mTrain: [ 34/50] Step 080/520 Loss 2.030 Prec@(1,5) (59.6%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:03:07] \u001b[32mTrain: [ 34/50] Step 100/520 Loss 2.025 Prec@(1,5) (59.9%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:03:08] \u001b[32mTrain: [ 34/50] Step 120/520 Loss 2.035 Prec@(1,5) (59.6%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:03:09] \u001b[32mTrain: [ 34/50] Step 140/520 Loss 2.045 Prec@(1,5) (59.6%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:03:10] \u001b[32mTrain: [ 34/50] Step 160/520 Loss 2.041 Prec@(1,5) (59.6%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:03:11] \u001b[32mTrain: [ 34/50] Step 180/520 Loss 2.044 Prec@(1,5) (59.5%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:03:12] \u001b[32mTrain: [ 34/50] Step 200/520 Loss 2.055 Prec@(1,5) (59.2%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:13] \u001b[32mTrain: [ 34/50] Step 220/520 Loss 2.050 Prec@(1,5) (59.4%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:14] \u001b[32mTrain: [ 34/50] Step 240/520 Loss 2.048 Prec@(1,5) (59.4%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:03:15] \u001b[32mTrain: [ 34/50] Step 260/520 Loss 2.055 Prec@(1,5) (59.3%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:16] \u001b[32mTrain: [ 34/50] Step 280/520 Loss 2.054 Prec@(1,5) (59.2%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:17] \u001b[32mTrain: [ 34/50] Step 300/520 Loss 2.054 Prec@(1,5) (59.1%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:03:18] \u001b[32mTrain: [ 34/50] Step 320/520 Loss 2.066 Prec@(1,5) (58.9%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:19] \u001b[32mTrain: [ 34/50] Step 340/520 Loss 2.068 Prec@(1,5) (58.8%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:19] \u001b[32mTrain: [ 34/50] Step 360/520 Loss 2.066 Prec@(1,5) (58.9%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:20] \u001b[32mTrain: [ 34/50] Step 380/520 Loss 2.067 Prec@(1,5) (58.9%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:03:21] \u001b[32mTrain: [ 34/50] Step 400/520 Loss 2.072 Prec@(1,5) (58.7%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:03:22] \u001b[32mTrain: [ 34/50] Step 420/520 Loss 2.077 Prec@(1,5) (58.7%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:03:23] \u001b[32mTrain: [ 34/50] Step 440/520 Loss 2.078 Prec@(1,5) (58.6%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:03:24] \u001b[32mTrain: [ 34/50] Step 460/520 Loss 2.080 Prec@(1,5) (58.6%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:03:25] \u001b[32mTrain: [ 34/50] Step 480/520 Loss 2.082 Prec@(1,5) (58.6%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:03:26] \u001b[32mTrain: [ 34/50] Step 500/520 Loss 2.081 Prec@(1,5) (58.7%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:03:27] \u001b[32mTrain: [ 34/50] Step 520/520 Loss 2.079 Prec@(1,5) (58.7%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:03:27] \u001b[32mTrain: [ 34/50] Final Prec@1 58.7240%\u001b[0m\n",
      "[2024-01-15 10:03:32] \u001b[32mValid: [ 34/50] Step 000/104 Loss 2.018 Prec@(1,5) (61.5%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:03:33] \u001b[32mValid: [ 34/50] Step 020/104 Loss 2.023 Prec@(1,5) (56.6%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:03:33] \u001b[32mValid: [ 34/50] Step 040/104 Loss 2.006 Prec@(1,5) (56.7%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:03:33] \u001b[32mValid: [ 34/50] Step 060/104 Loss 1.981 Prec@(1,5) (56.8%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:03:34] \u001b[32mValid: [ 34/50] Step 080/104 Loss 1.989 Prec@(1,5) (56.6%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:03:34] \u001b[32mValid: [ 34/50] Step 100/104 Loss 1.964 Prec@(1,5) (56.9%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:03:34] \u001b[32mValid: [ 34/50] Step 104/104 Loss 1.962 Prec@(1,5) (56.9%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:03:34] \u001b[32mValid: [ 34/50] Final Prec@1 56.8900%\u001b[0m\n",
      "[2024-01-15 10:03:34] \u001b[32mEpoch 34 LR 0.005803\u001b[0m\n",
      "[2024-01-15 10:03:43] \u001b[32mTrain: [ 35/50] Step 000/520 Loss 2.202 Prec@(1,5) (60.4%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:03:44] \u001b[32mTrain: [ 35/50] Step 020/520 Loss 1.982 Prec@(1,5) (59.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:03:44] \u001b[32mTrain: [ 35/50] Step 040/520 Loss 2.001 Prec@(1,5) (59.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:03:45] \u001b[32mTrain: [ 35/50] Step 060/520 Loss 2.016 Prec@(1,5) (59.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:03:46] \u001b[32mTrain: [ 35/50] Step 080/520 Loss 2.017 Prec@(1,5) (60.1%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:03:47] \u001b[32mTrain: [ 35/50] Step 100/520 Loss 2.027 Prec@(1,5) (59.9%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:03:48] \u001b[32mTrain: [ 35/50] Step 120/520 Loss 2.028 Prec@(1,5) (59.7%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:03:49] \u001b[32mTrain: [ 35/50] Step 140/520 Loss 2.021 Prec@(1,5) (60.0%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:03:50] \u001b[32mTrain: [ 35/50] Step 160/520 Loss 2.028 Prec@(1,5) (59.8%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:03:51] \u001b[32mTrain: [ 35/50] Step 180/520 Loss 2.024 Prec@(1,5) (59.9%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:03:52] \u001b[32mTrain: [ 35/50] Step 200/520 Loss 2.025 Prec@(1,5) (59.9%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:03:53] \u001b[32mTrain: [ 35/50] Step 220/520 Loss 2.026 Prec@(1,5) (59.9%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:03:54] \u001b[32mTrain: [ 35/50] Step 240/520 Loss 2.026 Prec@(1,5) (59.9%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:03:55] \u001b[32mTrain: [ 35/50] Step 260/520 Loss 2.027 Prec@(1,5) (59.7%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:03:56] \u001b[32mTrain: [ 35/50] Step 280/520 Loss 2.029 Prec@(1,5) (59.8%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:03:57] \u001b[32mTrain: [ 35/50] Step 300/520 Loss 2.028 Prec@(1,5) (59.8%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:03:58] \u001b[32mTrain: [ 35/50] Step 320/520 Loss 2.034 Prec@(1,5) (59.8%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:03:59] \u001b[32mTrain: [ 35/50] Step 340/520 Loss 2.038 Prec@(1,5) (59.6%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:03:59] \u001b[32mTrain: [ 35/50] Step 360/520 Loss 2.044 Prec@(1,5) (59.5%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:00] \u001b[32mTrain: [ 35/50] Step 380/520 Loss 2.049 Prec@(1,5) (59.5%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:01] \u001b[32mTrain: [ 35/50] Step 400/520 Loss 2.049 Prec@(1,5) (59.5%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:02] \u001b[32mTrain: [ 35/50] Step 420/520 Loss 2.046 Prec@(1,5) (59.5%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:03] \u001b[32mTrain: [ 35/50] Step 440/520 Loss 2.045 Prec@(1,5) (59.5%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:04] \u001b[32mTrain: [ 35/50] Step 460/520 Loss 2.045 Prec@(1,5) (59.5%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:05] \u001b[32mTrain: [ 35/50] Step 480/520 Loss 2.049 Prec@(1,5) (59.4%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:06] \u001b[32mTrain: [ 35/50] Step 500/520 Loss 2.051 Prec@(1,5) (59.3%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:07] \u001b[32mTrain: [ 35/50] Step 520/520 Loss 2.048 Prec@(1,5) (59.4%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:08] \u001b[32mTrain: [ 35/50] Final Prec@1 59.4020%\u001b[0m\n",
      "[2024-01-15 10:04:13] \u001b[32mValid: [ 35/50] Step 000/104 Loss 2.023 Prec@(1,5) (63.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:04:13] \u001b[32mValid: [ 35/50] Step 020/104 Loss 2.234 Prec@(1,5) (55.7%, 82.7%)\u001b[0m\n",
      "[2024-01-15 10:04:14] \u001b[32mValid: [ 35/50] Step 040/104 Loss 2.200 Prec@(1,5) (55.3%, 82.9%)\u001b[0m\n",
      "[2024-01-15 10:04:14] \u001b[32mValid: [ 35/50] Step 060/104 Loss 2.156 Prec@(1,5) (55.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:04:14] \u001b[32mValid: [ 35/50] Step 080/104 Loss 2.172 Prec@(1,5) (55.0%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:04:15] \u001b[32mValid: [ 35/50] Step 100/104 Loss 2.150 Prec@(1,5) (55.3%, 83.4%)\u001b[0m\n",
      "[2024-01-15 10:04:15] \u001b[32mValid: [ 35/50] Step 104/104 Loss 2.153 Prec@(1,5) (55.3%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:04:15] \u001b[32mValid: [ 35/50] Final Prec@1 55.2800%\u001b[0m\n",
      "[2024-01-15 10:04:15] \u001b[32mEpoch 35 LR 0.005153\u001b[0m\n",
      "[2024-01-15 10:04:24] \u001b[32mTrain: [ 36/50] Step 000/520 Loss 1.902 Prec@(1,5) (66.7%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:04:25] \u001b[32mTrain: [ 36/50] Step 020/520 Loss 1.976 Prec@(1,5) (59.1%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:26] \u001b[32mTrain: [ 36/50] Step 040/520 Loss 2.013 Prec@(1,5) (59.3%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:04:27] \u001b[32mTrain: [ 36/50] Step 060/520 Loss 2.024 Prec@(1,5) (59.4%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:28] \u001b[32mTrain: [ 36/50] Step 080/520 Loss 2.023 Prec@(1,5) (59.6%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:04:29] \u001b[32mTrain: [ 36/50] Step 100/520 Loss 2.024 Prec@(1,5) (59.5%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:30] \u001b[32mTrain: [ 36/50] Step 120/520 Loss 2.037 Prec@(1,5) (59.3%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:31] \u001b[32mTrain: [ 36/50] Step 140/520 Loss 2.041 Prec@(1,5) (59.4%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:32] \u001b[32mTrain: [ 36/50] Step 160/520 Loss 2.039 Prec@(1,5) (59.7%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:33] \u001b[32mTrain: [ 36/50] Step 180/520 Loss 2.046 Prec@(1,5) (59.6%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:34] \u001b[32mTrain: [ 36/50] Step 200/520 Loss 2.050 Prec@(1,5) (59.5%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:35] \u001b[32mTrain: [ 36/50] Step 220/520 Loss 2.051 Prec@(1,5) (59.5%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:04:35] \u001b[32mTrain: [ 36/50] Step 240/520 Loss 2.053 Prec@(1,5) (59.5%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:04:36] \u001b[32mTrain: [ 36/50] Step 260/520 Loss 2.046 Prec@(1,5) (59.6%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:37] \u001b[32mTrain: [ 36/50] Step 280/520 Loss 2.041 Prec@(1,5) (59.7%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:38] \u001b[32mTrain: [ 36/50] Step 300/520 Loss 2.041 Prec@(1,5) (59.7%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:04:39] \u001b[32mTrain: [ 36/50] Step 320/520 Loss 2.039 Prec@(1,5) (59.7%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:40] \u001b[32mTrain: [ 36/50] Step 340/520 Loss 2.040 Prec@(1,5) (59.7%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:41] \u001b[32mTrain: [ 36/50] Step 360/520 Loss 2.038 Prec@(1,5) (59.7%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:42] \u001b[32mTrain: [ 36/50] Step 380/520 Loss 2.038 Prec@(1,5) (59.7%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:04:43] \u001b[32mTrain: [ 36/50] Step 400/520 Loss 2.042 Prec@(1,5) (59.6%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:04:44] \u001b[32mTrain: [ 36/50] Step 420/520 Loss 2.037 Prec@(1,5) (59.7%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:04:45] \u001b[32mTrain: [ 36/50] Step 440/520 Loss 2.036 Prec@(1,5) (59.7%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:46] \u001b[32mTrain: [ 36/50] Step 460/520 Loss 2.039 Prec@(1,5) (59.7%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:04:47] \u001b[32mTrain: [ 36/50] Step 480/520 Loss 2.037 Prec@(1,5) (59.7%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:04:48] \u001b[32mTrain: [ 36/50] Step 500/520 Loss 2.036 Prec@(1,5) (59.8%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:49] \u001b[32mTrain: [ 36/50] Step 520/520 Loss 2.035 Prec@(1,5) (59.7%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:49] \u001b[32mTrain: [ 36/50] Final Prec@1 59.7460%\u001b[0m\n",
      "[2024-01-15 10:04:54] \u001b[32mValid: [ 36/50] Step 000/104 Loss 1.938 Prec@(1,5) (58.3%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:04:54] \u001b[32mValid: [ 36/50] Step 020/104 Loss 1.985 Prec@(1,5) (57.5%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:04:55] \u001b[32mValid: [ 36/50] Step 040/104 Loss 1.973 Prec@(1,5) (57.1%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:04:55] \u001b[32mValid: [ 36/50] Step 060/104 Loss 1.940 Prec@(1,5) (57.4%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:04:55] \u001b[32mValid: [ 36/50] Step 080/104 Loss 1.956 Prec@(1,5) (57.0%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:04:56] \u001b[32mValid: [ 36/50] Step 100/104 Loss 1.940 Prec@(1,5) (57.3%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:56] \u001b[32mValid: [ 36/50] Step 104/104 Loss 1.940 Prec@(1,5) (57.4%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:04:56] \u001b[32mValid: [ 36/50] Final Prec@1 57.3700%\u001b[0m\n",
      "[2024-01-15 10:04:56] \u001b[32mEpoch 36 LR 0.004533\u001b[0m\n",
      "[2024-01-15 10:05:04] \u001b[32mTrain: [ 37/50] Step 000/520 Loss 2.180 Prec@(1,5) (51.0%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:05:05] \u001b[32mTrain: [ 37/50] Step 020/520 Loss 2.027 Prec@(1,5) (60.7%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:05:06] \u001b[32mTrain: [ 37/50] Step 040/520 Loss 2.030 Prec@(1,5) (59.6%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:05:07] \u001b[32mTrain: [ 37/50] Step 060/520 Loss 2.017 Prec@(1,5) (60.3%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:05:08] \u001b[32mTrain: [ 37/50] Step 080/520 Loss 2.019 Prec@(1,5) (60.2%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:05:09] \u001b[32mTrain: [ 37/50] Step 100/520 Loss 2.028 Prec@(1,5) (60.1%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:05:10] \u001b[32mTrain: [ 37/50] Step 120/520 Loss 2.033 Prec@(1,5) (59.9%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:05:11] \u001b[32mTrain: [ 37/50] Step 140/520 Loss 2.031 Prec@(1,5) (60.0%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:05:12] \u001b[32mTrain: [ 37/50] Step 160/520 Loss 2.028 Prec@(1,5) (60.0%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:05:13] \u001b[32mTrain: [ 37/50] Step 180/520 Loss 2.029 Prec@(1,5) (59.8%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:05:14] \u001b[32mTrain: [ 37/50] Step 200/520 Loss 2.025 Prec@(1,5) (60.0%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:05:15] \u001b[32mTrain: [ 37/50] Step 220/520 Loss 2.027 Prec@(1,5) (60.0%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:05:16] \u001b[32mTrain: [ 37/50] Step 240/520 Loss 2.019 Prec@(1,5) (60.2%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:17] \u001b[32mTrain: [ 37/50] Step 260/520 Loss 2.022 Prec@(1,5) (60.1%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:17] \u001b[32mTrain: [ 37/50] Step 280/520 Loss 2.019 Prec@(1,5) (60.1%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:18] \u001b[32mTrain: [ 37/50] Step 300/520 Loss 2.016 Prec@(1,5) (60.2%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:19] \u001b[32mTrain: [ 37/50] Step 320/520 Loss 2.017 Prec@(1,5) (60.1%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:20] \u001b[32mTrain: [ 37/50] Step 340/520 Loss 2.018 Prec@(1,5) (60.0%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:21] \u001b[32mTrain: [ 37/50] Step 360/520 Loss 2.016 Prec@(1,5) (60.1%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:22] \u001b[32mTrain: [ 37/50] Step 380/520 Loss 2.017 Prec@(1,5) (60.1%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:05:23] \u001b[32mTrain: [ 37/50] Step 400/520 Loss 2.017 Prec@(1,5) (60.1%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:24] \u001b[32mTrain: [ 37/50] Step 420/520 Loss 2.016 Prec@(1,5) (60.1%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:25] \u001b[32mTrain: [ 37/50] Step 440/520 Loss 2.016 Prec@(1,5) (60.1%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:26] \u001b[32mTrain: [ 37/50] Step 460/520 Loss 2.016 Prec@(1,5) (60.1%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:27] \u001b[32mTrain: [ 37/50] Step 480/520 Loss 2.014 Prec@(1,5) (60.2%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:28] \u001b[32mTrain: [ 37/50] Step 500/520 Loss 2.015 Prec@(1,5) (60.2%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:29] \u001b[32mTrain: [ 37/50] Step 520/520 Loss 2.015 Prec@(1,5) (60.2%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:29] \u001b[32mTrain: [ 37/50] Final Prec@1 60.1760%\u001b[0m\n",
      "[2024-01-15 10:05:34] \u001b[32mValid: [ 37/50] Step 000/104 Loss 2.034 Prec@(1,5) (55.2%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:05:34] \u001b[32mValid: [ 37/50] Step 020/104 Loss 2.037 Prec@(1,5) (56.7%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:05:35] \u001b[32mValid: [ 37/50] Step 040/104 Loss 2.023 Prec@(1,5) (56.5%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:05:35] \u001b[32mValid: [ 37/50] Step 060/104 Loss 1.983 Prec@(1,5) (57.0%, 85.3%)\u001b[0m\n",
      "[2024-01-15 10:05:35] \u001b[32mValid: [ 37/50] Step 080/104 Loss 1.995 Prec@(1,5) (56.6%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:05:36] \u001b[32mValid: [ 37/50] Step 100/104 Loss 1.982 Prec@(1,5) (57.0%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:05:36] \u001b[32mValid: [ 37/50] Step 104/104 Loss 1.982 Prec@(1,5) (57.1%, 85.1%)\u001b[0m\n",
      "[2024-01-15 10:05:36] \u001b[32mValid: [ 37/50] Final Prec@1 57.0800%\u001b[0m\n",
      "[2024-01-15 10:05:36] \u001b[32mEpoch 37 LR 0.003944\u001b[0m\n",
      "[2024-01-15 10:05:44] \u001b[32mTrain: [ 38/50] Step 000/520 Loss 1.830 Prec@(1,5) (60.4%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:05:45] \u001b[32mTrain: [ 38/50] Step 020/520 Loss 1.991 Prec@(1,5) (59.7%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:05:46] \u001b[32mTrain: [ 38/50] Step 040/520 Loss 2.005 Prec@(1,5) (60.1%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:05:47] \u001b[32mTrain: [ 38/50] Step 060/520 Loss 1.972 Prec@(1,5) (60.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:05:48] \u001b[32mTrain: [ 38/50] Step 080/520 Loss 1.964 Prec@(1,5) (60.4%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:05:49] \u001b[32mTrain: [ 38/50] Step 100/520 Loss 1.973 Prec@(1,5) (60.3%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:05:50] \u001b[32mTrain: [ 38/50] Step 120/520 Loss 1.970 Prec@(1,5) (60.3%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:05:51] \u001b[32mTrain: [ 38/50] Step 140/520 Loss 1.963 Prec@(1,5) (60.6%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:05:52] \u001b[32mTrain: [ 38/50] Step 160/520 Loss 1.976 Prec@(1,5) (60.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:05:53] \u001b[32mTrain: [ 38/50] Step 180/520 Loss 1.974 Prec@(1,5) (60.4%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:05:54] \u001b[32mTrain: [ 38/50] Step 200/520 Loss 1.965 Prec@(1,5) (60.6%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:05:55] \u001b[32mTrain: [ 38/50] Step 220/520 Loss 1.960 Prec@(1,5) (60.6%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:05:56] \u001b[32mTrain: [ 38/50] Step 240/520 Loss 1.962 Prec@(1,5) (60.6%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:05:56] \u001b[32mTrain: [ 38/50] Step 260/520 Loss 1.963 Prec@(1,5) (60.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:05:57] \u001b[32mTrain: [ 38/50] Step 280/520 Loss 1.969 Prec@(1,5) (60.5%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:05:58] \u001b[32mTrain: [ 38/50] Step 300/520 Loss 1.974 Prec@(1,5) (60.6%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:05:59] \u001b[32mTrain: [ 38/50] Step 320/520 Loss 1.972 Prec@(1,5) (60.6%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:06:00] \u001b[32mTrain: [ 38/50] Step 340/520 Loss 1.971 Prec@(1,5) (60.7%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:06:01] \u001b[32mTrain: [ 38/50] Step 360/520 Loss 1.974 Prec@(1,5) (60.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:06:02] \u001b[32mTrain: [ 38/50] Step 380/520 Loss 1.975 Prec@(1,5) (60.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:06:03] \u001b[32mTrain: [ 38/50] Step 400/520 Loss 1.975 Prec@(1,5) (60.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:06:04] \u001b[32mTrain: [ 38/50] Step 420/520 Loss 1.972 Prec@(1,5) (60.7%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:06:05] \u001b[32mTrain: [ 38/50] Step 440/520 Loss 1.974 Prec@(1,5) (60.7%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:06:06] \u001b[32mTrain: [ 38/50] Step 460/520 Loss 1.978 Prec@(1,5) (60.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:06:07] \u001b[32mTrain: [ 38/50] Step 480/520 Loss 1.981 Prec@(1,5) (60.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:06:08] \u001b[32mTrain: [ 38/50] Step 500/520 Loss 1.983 Prec@(1,5) (60.6%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:09] \u001b[32mTrain: [ 38/50] Step 520/520 Loss 1.983 Prec@(1,5) (60.6%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:09] \u001b[32mTrain: [ 38/50] Final Prec@1 60.5940%\u001b[0m\n",
      "[2024-01-15 10:06:14] \u001b[32mValid: [ 38/50] Step 000/104 Loss 1.878 Prec@(1,5) (64.6%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:06:15] \u001b[32mValid: [ 38/50] Step 020/104 Loss 1.963 Prec@(1,5) (58.5%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:15] \u001b[32mValid: [ 38/50] Step 040/104 Loss 1.936 Prec@(1,5) (57.7%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:16] \u001b[32mValid: [ 38/50] Step 060/104 Loss 1.907 Prec@(1,5) (58.2%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:06:16] \u001b[32mValid: [ 38/50] Step 080/104 Loss 1.925 Prec@(1,5) (57.9%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:06:16] \u001b[32mValid: [ 38/50] Step 100/104 Loss 1.903 Prec@(1,5) (58.3%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:16] \u001b[32mValid: [ 38/50] Step 104/104 Loss 1.901 Prec@(1,5) (58.4%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:06:16] \u001b[32mValid: [ 38/50] Final Prec@1 58.3500%\u001b[0m\n",
      "[2024-01-15 10:06:16] \u001b[32mEpoch 38 LR 0.003389\u001b[0m\n",
      "[2024-01-15 10:06:25] \u001b[32mTrain: [ 39/50] Step 000/520 Loss 1.834 Prec@(1,5) (63.5%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:06:26] \u001b[32mTrain: [ 39/50] Step 020/520 Loss 1.923 Prec@(1,5) (61.9%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:06:27] \u001b[32mTrain: [ 39/50] Step 040/520 Loss 1.953 Prec@(1,5) (61.1%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:06:28] \u001b[32mTrain: [ 39/50] Step 060/520 Loss 1.950 Prec@(1,5) (61.4%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:29] \u001b[32mTrain: [ 39/50] Step 080/520 Loss 1.954 Prec@(1,5) (61.3%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:30] \u001b[32mTrain: [ 39/50] Step 100/520 Loss 1.954 Prec@(1,5) (61.4%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:31] \u001b[32mTrain: [ 39/50] Step 120/520 Loss 1.961 Prec@(1,5) (61.4%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:32] \u001b[32mTrain: [ 39/50] Step 140/520 Loss 1.967 Prec@(1,5) (61.4%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:33] \u001b[32mTrain: [ 39/50] Step 160/520 Loss 1.969 Prec@(1,5) (61.2%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:34] \u001b[32mTrain: [ 39/50] Step 180/520 Loss 1.968 Prec@(1,5) (61.2%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:34] \u001b[32mTrain: [ 39/50] Step 200/520 Loss 1.968 Prec@(1,5) (61.0%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:35] \u001b[32mTrain: [ 39/50] Step 220/520 Loss 1.967 Prec@(1,5) (61.1%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:36] \u001b[32mTrain: [ 39/50] Step 240/520 Loss 1.971 Prec@(1,5) (61.0%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:37] \u001b[32mTrain: [ 39/50] Step 260/520 Loss 1.971 Prec@(1,5) (61.0%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:38] \u001b[32mTrain: [ 39/50] Step 280/520 Loss 1.969 Prec@(1,5) (61.1%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:39] \u001b[32mTrain: [ 39/50] Step 300/520 Loss 1.970 Prec@(1,5) (61.1%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:40] \u001b[32mTrain: [ 39/50] Step 320/520 Loss 1.969 Prec@(1,5) (61.1%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:41] \u001b[32mTrain: [ 39/50] Step 340/520 Loss 1.972 Prec@(1,5) (61.1%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:06:42] \u001b[32mTrain: [ 39/50] Step 360/520 Loss 1.968 Prec@(1,5) (61.1%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:43] \u001b[32mTrain: [ 39/50] Step 380/520 Loss 1.969 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:44] \u001b[32mTrain: [ 39/50] Step 400/520 Loss 1.970 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:45] \u001b[32mTrain: [ 39/50] Step 420/520 Loss 1.971 Prec@(1,5) (61.0%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:46] \u001b[32mTrain: [ 39/50] Step 440/520 Loss 1.968 Prec@(1,5) (61.0%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:47] \u001b[32mTrain: [ 39/50] Step 460/520 Loss 1.967 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:48] \u001b[32mTrain: [ 39/50] Step 480/520 Loss 1.969 Prec@(1,5) (61.0%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:49] \u001b[32mTrain: [ 39/50] Step 500/520 Loss 1.970 Prec@(1,5) (61.0%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:06:50] \u001b[32mTrain: [ 39/50] Step 520/520 Loss 1.974 Prec@(1,5) (61.0%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:06:51] \u001b[32mTrain: [ 39/50] Final Prec@1 60.9920%\u001b[0m\n",
      "[2024-01-15 10:06:56] \u001b[32mValid: [ 39/50] Step 000/104 Loss 1.980 Prec@(1,5) (55.2%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:06:56] \u001b[32mValid: [ 39/50] Step 020/104 Loss 1.989 Prec@(1,5) (57.4%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:06:57] \u001b[32mValid: [ 39/50] Step 040/104 Loss 1.963 Prec@(1,5) (57.8%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:06:57] \u001b[32mValid: [ 39/50] Step 060/104 Loss 1.928 Prec@(1,5) (58.1%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:06:57] \u001b[32mValid: [ 39/50] Step 080/104 Loss 1.952 Prec@(1,5) (57.7%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:06:58] \u001b[32mValid: [ 39/50] Step 100/104 Loss 1.930 Prec@(1,5) (57.9%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:06:58] \u001b[32mValid: [ 39/50] Step 104/104 Loss 1.929 Prec@(1,5) (57.9%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:06:58] \u001b[32mValid: [ 39/50] Final Prec@1 57.9400%\u001b[0m\n",
      "[2024-01-15 10:06:58] \u001b[32mEpoch 39 LR 0.002869\u001b[0m\n",
      "[2024-01-15 10:07:06] \u001b[32mTrain: [ 40/50] Step 000/520 Loss 2.145 Prec@(1,5) (57.3%, 79.2%)\u001b[0m\n",
      "[2024-01-15 10:07:07] \u001b[32mTrain: [ 40/50] Step 020/520 Loss 1.958 Prec@(1,5) (61.1%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:07:08] \u001b[32mTrain: [ 40/50] Step 040/520 Loss 1.951 Prec@(1,5) (61.5%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:07:09] \u001b[32mTrain: [ 40/50] Step 060/520 Loss 1.931 Prec@(1,5) (62.1%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:07:10] \u001b[32mTrain: [ 40/50] Step 080/520 Loss 1.919 Prec@(1,5) (62.5%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:07:11] \u001b[32mTrain: [ 40/50] Step 100/520 Loss 1.922 Prec@(1,5) (62.4%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:07:12] \u001b[32mTrain: [ 40/50] Step 120/520 Loss 1.937 Prec@(1,5) (62.2%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:07:13] \u001b[32mTrain: [ 40/50] Step 140/520 Loss 1.940 Prec@(1,5) (62.3%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:07:14] \u001b[32mTrain: [ 40/50] Step 160/520 Loss 1.942 Prec@(1,5) (62.0%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:07:15] \u001b[32mTrain: [ 40/50] Step 180/520 Loss 1.954 Prec@(1,5) (61.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:07:16] \u001b[32mTrain: [ 40/50] Step 200/520 Loss 1.950 Prec@(1,5) (61.7%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:07:17] \u001b[32mTrain: [ 40/50] Step 220/520 Loss 1.946 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:07:18] \u001b[32mTrain: [ 40/50] Step 240/520 Loss 1.951 Prec@(1,5) (61.7%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:07:19] \u001b[32mTrain: [ 40/50] Step 260/520 Loss 1.958 Prec@(1,5) (61.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:07:20] \u001b[32mTrain: [ 40/50] Step 280/520 Loss 1.956 Prec@(1,5) (61.7%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:07:21] \u001b[32mTrain: [ 40/50] Step 300/520 Loss 1.955 Prec@(1,5) (61.6%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:07:22] \u001b[32mTrain: [ 40/50] Step 320/520 Loss 1.962 Prec@(1,5) (61.5%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:07:23] \u001b[32mTrain: [ 40/50] Step 340/520 Loss 1.963 Prec@(1,5) (61.4%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:07:24] \u001b[32mTrain: [ 40/50] Step 360/520 Loss 1.965 Prec@(1,5) (61.3%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:25] \u001b[32mTrain: [ 40/50] Step 380/520 Loss 1.964 Prec@(1,5) (61.3%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:26] \u001b[32mTrain: [ 40/50] Step 400/520 Loss 1.966 Prec@(1,5) (61.2%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:27] \u001b[32mTrain: [ 40/50] Step 420/520 Loss 1.965 Prec@(1,5) (61.2%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:28] \u001b[32mTrain: [ 40/50] Step 440/520 Loss 1.967 Prec@(1,5) (61.2%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:29] \u001b[32mTrain: [ 40/50] Step 460/520 Loss 1.970 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:30] \u001b[32mTrain: [ 40/50] Step 480/520 Loss 1.970 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:31] \u001b[32mTrain: [ 40/50] Step 500/520 Loss 1.969 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:32] \u001b[32mTrain: [ 40/50] Step 520/520 Loss 1.969 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:32] \u001b[32mTrain: [ 40/50] Final Prec@1 61.1100%\u001b[0m\n",
      "[2024-01-15 10:07:37] \u001b[32mValid: [ 40/50] Step 000/104 Loss 1.852 Prec@(1,5) (57.3%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:07:37] \u001b[32mValid: [ 40/50] Step 020/104 Loss 1.953 Prec@(1,5) (58.6%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:07:38] \u001b[32mValid: [ 40/50] Step 040/104 Loss 1.921 Prec@(1,5) (58.6%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:07:38] \u001b[32mValid: [ 40/50] Step 060/104 Loss 1.883 Prec@(1,5) (58.7%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:07:38] \u001b[32mValid: [ 40/50] Step 080/104 Loss 1.903 Prec@(1,5) (58.2%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:07:39] \u001b[32mValid: [ 40/50] Step 100/104 Loss 1.878 Prec@(1,5) (58.3%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:07:39] \u001b[32mValid: [ 40/50] Step 104/104 Loss 1.878 Prec@(1,5) (58.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:07:39] \u001b[32mValid: [ 40/50] Final Prec@1 58.3100%\u001b[0m\n",
      "[2024-01-15 10:07:39] \u001b[32mEpoch 40 LR 0.002388\u001b[0m\n",
      "[2024-01-15 10:07:48] \u001b[32mTrain: [ 41/50] Step 000/520 Loss 1.821 Prec@(1,5) (65.6%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:07:49] \u001b[32mTrain: [ 41/50] Step 020/520 Loss 1.884 Prec@(1,5) (61.8%, 87.9%)\u001b[0m\n",
      "[2024-01-15 10:07:49] \u001b[32mTrain: [ 41/50] Step 040/520 Loss 1.921 Prec@(1,5) (61.4%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:07:50] \u001b[32mTrain: [ 41/50] Step 060/520 Loss 1.929 Prec@(1,5) (61.8%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:07:51] \u001b[32mTrain: [ 41/50] Step 080/520 Loss 1.921 Prec@(1,5) (62.0%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:07:52] \u001b[32mTrain: [ 41/50] Step 100/520 Loss 1.919 Prec@(1,5) (62.0%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:07:53] \u001b[32mTrain: [ 41/50] Step 120/520 Loss 1.936 Prec@(1,5) (61.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:07:54] \u001b[32mTrain: [ 41/50] Step 140/520 Loss 1.934 Prec@(1,5) (61.5%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:07:55] \u001b[32mTrain: [ 41/50] Step 160/520 Loss 1.937 Prec@(1,5) (61.4%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:07:56] \u001b[32mTrain: [ 41/50] Step 180/520 Loss 1.946 Prec@(1,5) (61.2%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:07:57] \u001b[32mTrain: [ 41/50] Step 200/520 Loss 1.951 Prec@(1,5) (61.2%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:07:58] \u001b[32mTrain: [ 41/50] Step 220/520 Loss 1.944 Prec@(1,5) (61.3%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:07:59] \u001b[32mTrain: [ 41/50] Step 240/520 Loss 1.949 Prec@(1,5) (61.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:00] \u001b[32mTrain: [ 41/50] Step 260/520 Loss 1.948 Prec@(1,5) (61.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:01] \u001b[32mTrain: [ 41/50] Step 280/520 Loss 1.950 Prec@(1,5) (61.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:02] \u001b[32mTrain: [ 41/50] Step 300/520 Loss 1.952 Prec@(1,5) (61.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:03] \u001b[32mTrain: [ 41/50] Step 320/520 Loss 1.959 Prec@(1,5) (61.2%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:08:04] \u001b[32mTrain: [ 41/50] Step 340/520 Loss 1.959 Prec@(1,5) (61.2%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:08:05] \u001b[32mTrain: [ 41/50] Step 360/520 Loss 1.956 Prec@(1,5) (61.2%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:06] \u001b[32mTrain: [ 41/50] Step 380/520 Loss 1.952 Prec@(1,5) (61.4%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:07] \u001b[32mTrain: [ 41/50] Step 400/520 Loss 1.955 Prec@(1,5) (61.4%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:08] \u001b[32mTrain: [ 41/50] Step 420/520 Loss 1.955 Prec@(1,5) (61.4%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:09] \u001b[32mTrain: [ 41/50] Step 440/520 Loss 1.958 Prec@(1,5) (61.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:10] \u001b[32mTrain: [ 41/50] Step 460/520 Loss 1.962 Prec@(1,5) (61.3%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:08:11] \u001b[32mTrain: [ 41/50] Step 480/520 Loss 1.962 Prec@(1,5) (61.3%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:08:12] \u001b[32mTrain: [ 41/50] Step 500/520 Loss 1.961 Prec@(1,5) (61.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:13] \u001b[32mTrain: [ 41/50] Step 520/520 Loss 1.964 Prec@(1,5) (61.2%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:08:13] \u001b[32mTrain: [ 41/50] Final Prec@1 61.1540%\u001b[0m\n",
      "[2024-01-15 10:08:18] \u001b[32mValid: [ 41/50] Step 000/104 Loss 1.838 Prec@(1,5) (60.4%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:08:19] \u001b[32mValid: [ 41/50] Step 020/104 Loss 1.951 Prec@(1,5) (58.2%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:19] \u001b[32mValid: [ 41/50] Step 040/104 Loss 1.940 Prec@(1,5) (57.6%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:08:19] \u001b[32mValid: [ 41/50] Step 060/104 Loss 1.901 Prec@(1,5) (57.9%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:08:20] \u001b[32mValid: [ 41/50] Step 080/104 Loss 1.914 Prec@(1,5) (57.7%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:08:20] \u001b[32mValid: [ 41/50] Step 100/104 Loss 1.887 Prec@(1,5) (58.1%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:08:20] \u001b[32mValid: [ 41/50] Step 104/104 Loss 1.887 Prec@(1,5) (58.2%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:08:20] \u001b[32mValid: [ 41/50] Final Prec@1 58.1600%\u001b[0m\n",
      "[2024-01-15 10:08:20] \u001b[32mEpoch 41 LR 0.001947\u001b[0m\n",
      "[2024-01-15 10:08:29] \u001b[32mTrain: [ 42/50] Step 000/520 Loss 2.204 Prec@(1,5) (57.3%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:08:30] \u001b[32mTrain: [ 42/50] Step 020/520 Loss 1.879 Prec@(1,5) (62.8%, 87.2%)\u001b[0m\n",
      "[2024-01-15 10:08:31] \u001b[32mTrain: [ 42/50] Step 040/520 Loss 1.907 Prec@(1,5) (62.7%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:08:31] \u001b[32mTrain: [ 42/50] Step 060/520 Loss 1.886 Prec@(1,5) (62.7%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:08:32] \u001b[32mTrain: [ 42/50] Step 080/520 Loss 1.892 Prec@(1,5) (62.3%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:08:33] \u001b[32mTrain: [ 42/50] Step 100/520 Loss 1.901 Prec@(1,5) (62.2%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:08:34] \u001b[32mTrain: [ 42/50] Step 120/520 Loss 1.912 Prec@(1,5) (62.0%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:08:35] \u001b[32mTrain: [ 42/50] Step 140/520 Loss 1.911 Prec@(1,5) (62.0%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:08:36] \u001b[32mTrain: [ 42/50] Step 160/520 Loss 1.918 Prec@(1,5) (61.8%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:08:37] \u001b[32mTrain: [ 42/50] Step 180/520 Loss 1.927 Prec@(1,5) (61.6%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:39] \u001b[32mTrain: [ 42/50] Step 200/520 Loss 1.930 Prec@(1,5) (61.6%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:08:39] \u001b[32mTrain: [ 42/50] Step 220/520 Loss 1.925 Prec@(1,5) (61.6%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:40] \u001b[32mTrain: [ 42/50] Step 240/520 Loss 1.919 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:41] \u001b[32mTrain: [ 42/50] Step 260/520 Loss 1.922 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:42] \u001b[32mTrain: [ 42/50] Step 280/520 Loss 1.920 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:44] \u001b[32mTrain: [ 42/50] Step 300/520 Loss 1.919 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:44] \u001b[32mTrain: [ 42/50] Step 320/520 Loss 1.922 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:46] \u001b[32mTrain: [ 42/50] Step 340/520 Loss 1.925 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:08:47] \u001b[32mTrain: [ 42/50] Step 360/520 Loss 1.930 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:08:47] \u001b[32mTrain: [ 42/50] Step 380/520 Loss 1.933 Prec@(1,5) (61.8%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:08:48] \u001b[32mTrain: [ 42/50] Step 400/520 Loss 1.934 Prec@(1,5) (61.8%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:08:49] \u001b[32mTrain: [ 42/50] Step 420/520 Loss 1.931 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:08:50] \u001b[32mTrain: [ 42/50] Step 440/520 Loss 1.931 Prec@(1,5) (61.8%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:08:51] \u001b[32mTrain: [ 42/50] Step 460/520 Loss 1.929 Prec@(1,5) (61.9%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:08:52] \u001b[32mTrain: [ 42/50] Step 480/520 Loss 1.926 Prec@(1,5) (61.9%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:08:54] \u001b[32mTrain: [ 42/50] Step 500/520 Loss 1.928 Prec@(1,5) (61.9%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:08:55] \u001b[32mTrain: [ 42/50] Step 520/520 Loss 1.929 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:08:55] \u001b[32mTrain: [ 42/50] Final Prec@1 61.8580%\u001b[0m\n",
      "[2024-01-15 10:08:59] \u001b[32mValid: [ 42/50] Step 000/104 Loss 1.855 Prec@(1,5) (61.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:08:59] \u001b[32mValid: [ 42/50] Step 020/104 Loss 1.927 Prec@(1,5) (58.7%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:08:59] \u001b[32mValid: [ 42/50] Step 040/104 Loss 1.904 Prec@(1,5) (58.5%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:08:59] \u001b[32mValid: [ 42/50] Step 060/104 Loss 1.869 Prec@(1,5) (58.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:00] \u001b[32mValid: [ 42/50] Step 080/104 Loss 1.882 Prec@(1,5) (58.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:00] \u001b[32mValid: [ 42/50] Step 100/104 Loss 1.857 Prec@(1,5) (58.9%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:00] \u001b[32mValid: [ 42/50] Step 104/104 Loss 1.856 Prec@(1,5) (59.0%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:00] \u001b[32mValid: [ 42/50] Final Prec@1 59.0000%\u001b[0m\n",
      "[2024-01-15 10:09:00] \u001b[32mEpoch 42 LR 0.001547\u001b[0m\n",
      "[2024-01-15 10:09:05] \u001b[32mTrain: [ 43/50] Step 000/520 Loss 1.942 Prec@(1,5) (63.5%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:09:05] \u001b[32mTrain: [ 43/50] Step 020/520 Loss 1.931 Prec@(1,5) (60.9%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:09:06] \u001b[32mTrain: [ 43/50] Step 040/520 Loss 1.935 Prec@(1,5) (61.4%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:06] \u001b[32mTrain: [ 43/50] Step 060/520 Loss 1.926 Prec@(1,5) (61.7%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:07] \u001b[32mTrain: [ 43/50] Step 080/520 Loss 1.916 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:07] \u001b[32mTrain: [ 43/50] Step 100/520 Loss 1.910 Prec@(1,5) (62.1%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:08] \u001b[32mTrain: [ 43/50] Step 120/520 Loss 1.912 Prec@(1,5) (62.0%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:08] \u001b[32mTrain: [ 43/50] Step 140/520 Loss 1.914 Prec@(1,5) (61.9%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:09] \u001b[32mTrain: [ 43/50] Step 160/520 Loss 1.914 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:09] \u001b[32mTrain: [ 43/50] Step 180/520 Loss 1.903 Prec@(1,5) (62.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:10] \u001b[32mTrain: [ 43/50] Step 200/520 Loss 1.910 Prec@(1,5) (62.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:10] \u001b[32mTrain: [ 43/50] Step 220/520 Loss 1.905 Prec@(1,5) (62.2%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:09:11] \u001b[32mTrain: [ 43/50] Step 240/520 Loss 1.908 Prec@(1,5) (62.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:11] \u001b[32mTrain: [ 43/50] Step 260/520 Loss 1.914 Prec@(1,5) (62.1%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:12] \u001b[32mTrain: [ 43/50] Step 280/520 Loss 1.911 Prec@(1,5) (62.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:12] \u001b[32mTrain: [ 43/50] Step 300/520 Loss 1.914 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:13] \u001b[32mTrain: [ 43/50] Step 320/520 Loss 1.916 Prec@(1,5) (61.9%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:13] \u001b[32mTrain: [ 43/50] Step 340/520 Loss 1.917 Prec@(1,5) (61.9%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:14] \u001b[32mTrain: [ 43/50] Step 360/520 Loss 1.921 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:14] \u001b[32mTrain: [ 43/50] Step 380/520 Loss 1.925 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:15] \u001b[32mTrain: [ 43/50] Step 400/520 Loss 1.927 Prec@(1,5) (61.8%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:15] \u001b[32mTrain: [ 43/50] Step 420/520 Loss 1.926 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:16] \u001b[32mTrain: [ 43/50] Step 440/520 Loss 1.928 Prec@(1,5) (61.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:16] \u001b[32mTrain: [ 43/50] Step 460/520 Loss 1.930 Prec@(1,5) (61.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:17] \u001b[32mTrain: [ 43/50] Step 480/520 Loss 1.933 Prec@(1,5) (61.6%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:17] \u001b[32mTrain: [ 43/50] Step 500/520 Loss 1.931 Prec@(1,5) (61.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:18] \u001b[32mTrain: [ 43/50] Step 520/520 Loss 1.932 Prec@(1,5) (61.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:18] \u001b[32mTrain: [ 43/50] Final Prec@1 61.6920%\u001b[0m\n",
      "[2024-01-15 10:09:21] \u001b[32mValid: [ 43/50] Step 000/104 Loss 1.888 Prec@(1,5) (60.4%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:22] \u001b[32mValid: [ 43/50] Step 020/104 Loss 1.934 Prec@(1,5) (59.1%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:09:22] \u001b[32mValid: [ 43/50] Step 040/104 Loss 1.927 Prec@(1,5) (58.7%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:22] \u001b[32mValid: [ 43/50] Step 060/104 Loss 1.882 Prec@(1,5) (58.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:22] \u001b[32mValid: [ 43/50] Step 080/104 Loss 1.898 Prec@(1,5) (58.5%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:22] \u001b[32mValid: [ 43/50] Step 100/104 Loss 1.876 Prec@(1,5) (58.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:22] \u001b[32mValid: [ 43/50] Step 104/104 Loss 1.876 Prec@(1,5) (58.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:23] \u001b[32mValid: [ 43/50] Final Prec@1 58.8400%\u001b[0m\n",
      "[2024-01-15 10:09:23] \u001b[32mEpoch 43 LR 0.001191\u001b[0m\n",
      "[2024-01-15 10:09:27] \u001b[32mTrain: [ 44/50] Step 000/520 Loss 2.388 Prec@(1,5) (54.2%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:09:28] \u001b[32mTrain: [ 44/50] Step 020/520 Loss 1.899 Prec@(1,5) (61.4%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:09:28] \u001b[32mTrain: [ 44/50] Step 040/520 Loss 1.904 Prec@(1,5) (61.2%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:09:29] \u001b[32mTrain: [ 44/50] Step 060/520 Loss 1.934 Prec@(1,5) (61.1%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:29] \u001b[32mTrain: [ 44/50] Step 080/520 Loss 1.918 Prec@(1,5) (61.4%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:09:30] \u001b[32mTrain: [ 44/50] Step 100/520 Loss 1.913 Prec@(1,5) (61.6%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:09:30] \u001b[32mTrain: [ 44/50] Step 120/520 Loss 1.913 Prec@(1,5) (61.9%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:09:31] \u001b[32mTrain: [ 44/50] Step 140/520 Loss 1.918 Prec@(1,5) (61.8%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:31] \u001b[32mTrain: [ 44/50] Step 160/520 Loss 1.928 Prec@(1,5) (61.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:32] \u001b[32mTrain: [ 44/50] Step 180/520 Loss 1.931 Prec@(1,5) (61.6%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:32] \u001b[32mTrain: [ 44/50] Step 200/520 Loss 1.925 Prec@(1,5) (61.6%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:33] \u001b[32mTrain: [ 44/50] Step 220/520 Loss 1.929 Prec@(1,5) (61.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:33] \u001b[32mTrain: [ 44/50] Step 240/520 Loss 1.930 Prec@(1,5) (61.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:34] \u001b[32mTrain: [ 44/50] Step 260/520 Loss 1.927 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:34] \u001b[32mTrain: [ 44/50] Step 280/520 Loss 1.922 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:35] \u001b[32mTrain: [ 44/50] Step 300/520 Loss 1.918 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:35] \u001b[32mTrain: [ 44/50] Step 320/520 Loss 1.919 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:36] \u001b[32mTrain: [ 44/50] Step 340/520 Loss 1.921 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:36] \u001b[32mTrain: [ 44/50] Step 360/520 Loss 1.923 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:37] \u001b[32mTrain: [ 44/50] Step 380/520 Loss 1.921 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:37] \u001b[32mTrain: [ 44/50] Step 400/520 Loss 1.918 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:38] \u001b[32mTrain: [ 44/50] Step 420/520 Loss 1.919 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:38] \u001b[32mTrain: [ 44/50] Step 440/520 Loss 1.920 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:39] \u001b[32mTrain: [ 44/50] Step 460/520 Loss 1.920 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:39] \u001b[32mTrain: [ 44/50] Step 480/520 Loss 1.922 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:40] \u001b[32mTrain: [ 44/50] Step 500/520 Loss 1.924 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:09:40] \u001b[32mTrain: [ 44/50] Step 520/520 Loss 1.927 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:40] \u001b[32mTrain: [ 44/50] Final Prec@1 61.7640%\u001b[0m\n",
      "[2024-01-15 10:09:45] \u001b[32mValid: [ 44/50] Step 000/104 Loss 1.806 Prec@(1,5) (61.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:09:45] \u001b[32mValid: [ 44/50] Step 020/104 Loss 1.892 Prec@(1,5) (58.7%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:09:45] \u001b[32mValid: [ 44/50] Step 040/104 Loss 1.878 Prec@(1,5) (58.4%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:45] \u001b[32mValid: [ 44/50] Step 060/104 Loss 1.840 Prec@(1,5) (58.9%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:09:45] \u001b[32mValid: [ 44/50] Step 080/104 Loss 1.860 Prec@(1,5) (58.8%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:09:46] \u001b[32mValid: [ 44/50] Step 100/104 Loss 1.835 Prec@(1,5) (59.2%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:09:46] \u001b[32mValid: [ 44/50] Step 104/104 Loss 1.836 Prec@(1,5) (59.3%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:09:46] \u001b[32mValid: [ 44/50] Final Prec@1 59.2800%\u001b[0m\n",
      "[2024-01-15 10:09:46] \u001b[32mEpoch 44 LR 0.000879\u001b[0m\n",
      "[2024-01-15 10:09:51] \u001b[32mTrain: [ 45/50] Step 000/520 Loss 1.647 Prec@(1,5) (67.7%, 89.6%)\u001b[0m\n",
      "[2024-01-15 10:09:51] \u001b[32mTrain: [ 45/50] Step 020/520 Loss 1.898 Prec@(1,5) (62.3%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:52] \u001b[32mTrain: [ 45/50] Step 040/520 Loss 1.921 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:09:52] \u001b[32mTrain: [ 45/50] Step 060/520 Loss 1.935 Prec@(1,5) (61.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:09:53] \u001b[32mTrain: [ 45/50] Step 080/520 Loss 1.935 Prec@(1,5) (61.5%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:53] \u001b[32mTrain: [ 45/50] Step 100/520 Loss 1.939 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:54] \u001b[32mTrain: [ 45/50] Step 120/520 Loss 1.936 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:54] \u001b[32mTrain: [ 45/50] Step 140/520 Loss 1.937 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:09:55] \u001b[32mTrain: [ 45/50] Step 160/520 Loss 1.945 Prec@(1,5) (61.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:09:55] \u001b[32mTrain: [ 45/50] Step 180/520 Loss 1.948 Prec@(1,5) (61.8%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:09:56] \u001b[32mTrain: [ 45/50] Step 200/520 Loss 1.946 Prec@(1,5) (61.8%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:09:56] \u001b[32mTrain: [ 45/50] Step 220/520 Loss 1.942 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:57] \u001b[32mTrain: [ 45/50] Step 240/520 Loss 1.937 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:57] \u001b[32mTrain: [ 45/50] Step 260/520 Loss 1.943 Prec@(1,5) (61.6%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:09:58] \u001b[32mTrain: [ 45/50] Step 280/520 Loss 1.937 Prec@(1,5) (61.7%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:09:58] \u001b[32mTrain: [ 45/50] Step 300/520 Loss 1.933 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:09:59] \u001b[32mTrain: [ 45/50] Step 320/520 Loss 1.935 Prec@(1,5) (61.8%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:09:59] \u001b[32mTrain: [ 45/50] Step 340/520 Loss 1.934 Prec@(1,5) (61.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:00] \u001b[32mTrain: [ 45/50] Step 360/520 Loss 1.933 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:00] \u001b[32mTrain: [ 45/50] Step 380/520 Loss 1.931 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:01] \u001b[32mTrain: [ 45/50] Step 400/520 Loss 1.928 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:01] \u001b[32mTrain: [ 45/50] Step 420/520 Loss 1.929 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:02] \u001b[32mTrain: [ 45/50] Step 440/520 Loss 1.928 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:02] \u001b[32mTrain: [ 45/50] Step 460/520 Loss 1.930 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:03] \u001b[32mTrain: [ 45/50] Step 480/520 Loss 1.932 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:03] \u001b[32mTrain: [ 45/50] Step 500/520 Loss 1.931 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:04] \u001b[32mTrain: [ 45/50] Step 520/520 Loss 1.931 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:04] \u001b[32mTrain: [ 45/50] Final Prec@1 61.8320%\u001b[0m\n",
      "[2024-01-15 10:10:08] \u001b[32mValid: [ 45/50] Step 000/104 Loss 1.806 Prec@(1,5) (61.5%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:08] \u001b[32mValid: [ 45/50] Step 020/104 Loss 1.860 Prec@(1,5) (59.6%, 87.9%)\u001b[0m\n",
      "[2024-01-15 10:10:08] \u001b[32mValid: [ 45/50] Step 040/104 Loss 1.850 Prec@(1,5) (59.7%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:10:08] \u001b[32mValid: [ 45/50] Step 060/104 Loss 1.816 Prec@(1,5) (60.1%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:10:09] \u001b[32mValid: [ 45/50] Step 080/104 Loss 1.834 Prec@(1,5) (59.8%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:10:09] \u001b[32mValid: [ 45/50] Step 100/104 Loss 1.811 Prec@(1,5) (60.1%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:10:09] \u001b[32mValid: [ 45/50] Step 104/104 Loss 1.810 Prec@(1,5) (60.2%, 87.2%)\u001b[0m\n",
      "[2024-01-15 10:10:09] \u001b[32mValid: [ 45/50] Final Prec@1 60.1700%\u001b[0m\n",
      "[2024-01-15 10:10:09] \u001b[32mEpoch 45 LR 0.000613\u001b[0m\n",
      "[2024-01-15 10:10:14] \u001b[32mTrain: [ 46/50] Step 000/520 Loss 1.738 Prec@(1,5) (59.4%, 88.5%)\u001b[0m\n",
      "[2024-01-15 10:10:15] \u001b[32mTrain: [ 46/50] Step 020/520 Loss 1.963 Prec@(1,5) (60.8%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:10:15] \u001b[32mTrain: [ 46/50] Step 040/520 Loss 1.935 Prec@(1,5) (61.5%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:16] \u001b[32mTrain: [ 46/50] Step 060/520 Loss 1.914 Prec@(1,5) (62.1%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:16] \u001b[32mTrain: [ 46/50] Step 080/520 Loss 1.923 Prec@(1,5) (62.0%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:17] \u001b[32mTrain: [ 46/50] Step 100/520 Loss 1.933 Prec@(1,5) (61.8%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:17] \u001b[32mTrain: [ 46/50] Step 120/520 Loss 1.936 Prec@(1,5) (61.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:10:18] \u001b[32mTrain: [ 46/50] Step 140/520 Loss 1.941 Prec@(1,5) (61.9%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:10:18] \u001b[32mTrain: [ 46/50] Step 160/520 Loss 1.938 Prec@(1,5) (61.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:10:19] \u001b[32mTrain: [ 46/50] Step 180/520 Loss 1.942 Prec@(1,5) (61.8%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:10:19] \u001b[32mTrain: [ 46/50] Step 200/520 Loss 1.936 Prec@(1,5) (61.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:10:20] \u001b[32mTrain: [ 46/50] Step 220/520 Loss 1.930 Prec@(1,5) (61.9%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:20] \u001b[32mTrain: [ 46/50] Step 240/520 Loss 1.931 Prec@(1,5) (61.9%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:21] \u001b[32mTrain: [ 46/50] Step 260/520 Loss 1.929 Prec@(1,5) (62.0%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:21] \u001b[32mTrain: [ 46/50] Step 280/520 Loss 1.937 Prec@(1,5) (61.9%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:22] \u001b[32mTrain: [ 46/50] Step 300/520 Loss 1.937 Prec@(1,5) (61.8%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:22] \u001b[32mTrain: [ 46/50] Step 320/520 Loss 1.937 Prec@(1,5) (61.8%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:22] \u001b[32mTrain: [ 46/50] Step 340/520 Loss 1.935 Prec@(1,5) (61.8%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:10:23] \u001b[32mTrain: [ 46/50] Step 360/520 Loss 1.929 Prec@(1,5) (61.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:24] \u001b[32mTrain: [ 46/50] Step 380/520 Loss 1.931 Prec@(1,5) (61.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:24] \u001b[32mTrain: [ 46/50] Step 400/520 Loss 1.928 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:25] \u001b[32mTrain: [ 46/50] Step 420/520 Loss 1.927 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:25] \u001b[32mTrain: [ 46/50] Step 440/520 Loss 1.927 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:26] \u001b[32mTrain: [ 46/50] Step 460/520 Loss 1.927 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:26] \u001b[32mTrain: [ 46/50] Step 480/520 Loss 1.926 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:27] \u001b[32mTrain: [ 46/50] Step 500/520 Loss 1.924 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:27] \u001b[32mTrain: [ 46/50] Step 520/520 Loss 1.926 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:27] \u001b[32mTrain: [ 46/50] Final Prec@1 61.9380%\u001b[0m\n",
      "[2024-01-15 10:10:31] \u001b[32mValid: [ 46/50] Step 000/104 Loss 1.810 Prec@(1,5) (61.5%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:10:31] \u001b[32mValid: [ 46/50] Step 020/104 Loss 1.902 Prec@(1,5) (59.0%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mValid: [ 46/50] Step 040/104 Loss 1.883 Prec@(1,5) (58.7%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mValid: [ 46/50] Step 060/104 Loss 1.849 Prec@(1,5) (59.0%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mValid: [ 46/50] Step 080/104 Loss 1.866 Prec@(1,5) (59.1%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mValid: [ 46/50] Step 100/104 Loss 1.842 Prec@(1,5) (59.3%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mValid: [ 46/50] Step 104/104 Loss 1.841 Prec@(1,5) (59.3%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mValid: [ 46/50] Final Prec@1 59.3400%\u001b[0m\n",
      "[2024-01-15 10:10:32] \u001b[32mEpoch 46 LR 0.000394\u001b[0m\n",
      "[2024-01-15 10:10:37] \u001b[32mTrain: [ 47/50] Step 000/520 Loss 2.110 Prec@(1,5) (60.4%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:10:38] \u001b[32mTrain: [ 47/50] Step 020/520 Loss 1.995 Prec@(1,5) (60.4%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:10:38] \u001b[32mTrain: [ 47/50] Step 040/520 Loss 1.912 Prec@(1,5) (61.4%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:39] \u001b[32mTrain: [ 47/50] Step 060/520 Loss 1.923 Prec@(1,5) (61.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:10:39] \u001b[32mTrain: [ 47/50] Step 080/520 Loss 1.942 Prec@(1,5) (60.9%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:40] \u001b[32mTrain: [ 47/50] Step 100/520 Loss 1.946 Prec@(1,5) (61.0%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:10:40] \u001b[32mTrain: [ 47/50] Step 120/520 Loss 1.938 Prec@(1,5) (61.2%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:41] \u001b[32mTrain: [ 47/50] Step 140/520 Loss 1.939 Prec@(1,5) (61.2%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:10:41] \u001b[32mTrain: [ 47/50] Step 160/520 Loss 1.932 Prec@(1,5) (61.4%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:42] \u001b[32mTrain: [ 47/50] Step 180/520 Loss 1.934 Prec@(1,5) (61.6%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:42] \u001b[32mTrain: [ 47/50] Step 200/520 Loss 1.922 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:10:43] \u001b[32mTrain: [ 47/50] Step 220/520 Loss 1.920 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:10:43] \u001b[32mTrain: [ 47/50] Step 240/520 Loss 1.925 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:44] \u001b[32mTrain: [ 47/50] Step 260/520 Loss 1.927 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:44] \u001b[32mTrain: [ 47/50] Step 280/520 Loss 1.923 Prec@(1,5) (61.9%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:10:45] \u001b[32mTrain: [ 47/50] Step 300/520 Loss 1.925 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:45] \u001b[32mTrain: [ 47/50] Step 320/520 Loss 1.923 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:46] \u001b[32mTrain: [ 47/50] Step 340/520 Loss 1.926 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:46] \u001b[32mTrain: [ 47/50] Step 360/520 Loss 1.923 Prec@(1,5) (61.8%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:47] \u001b[32mTrain: [ 47/50] Step 380/520 Loss 1.927 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:47] \u001b[32mTrain: [ 47/50] Step 400/520 Loss 1.927 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:48] \u001b[32mTrain: [ 47/50] Step 420/520 Loss 1.928 Prec@(1,5) (61.7%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:48] \u001b[32mTrain: [ 47/50] Step 440/520 Loss 1.927 Prec@(1,5) (61.7%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:49] \u001b[32mTrain: [ 47/50] Step 460/520 Loss 1.928 Prec@(1,5) (61.7%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:49] \u001b[32mTrain: [ 47/50] Step 480/520 Loss 1.925 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:50] \u001b[32mTrain: [ 47/50] Step 500/520 Loss 1.927 Prec@(1,5) (61.8%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:10:50] \u001b[32mTrain: [ 47/50] Step 520/520 Loss 1.926 Prec@(1,5) (61.8%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:51] \u001b[32mTrain: [ 47/50] Final Prec@1 61.8480%\u001b[0m\n",
      "[2024-01-15 10:10:54] \u001b[32mValid: [ 47/50] Step 000/104 Loss 1.805 Prec@(1,5) (61.5%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Step 020/104 Loss 1.892 Prec@(1,5) (60.0%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Step 040/104 Loss 1.874 Prec@(1,5) (59.4%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Step 060/104 Loss 1.836 Prec@(1,5) (59.7%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Step 080/104 Loss 1.855 Prec@(1,5) (59.4%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Step 100/104 Loss 1.833 Prec@(1,5) (59.6%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Step 104/104 Loss 1.832 Prec@(1,5) (59.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mValid: [ 47/50] Final Prec@1 59.6700%\u001b[0m\n",
      "[2024-01-15 10:10:55] \u001b[32mEpoch 47 LR 0.000222\u001b[0m\n",
      "[2024-01-15 10:11:01] \u001b[32mTrain: [ 48/50] Step 000/520 Loss 2.004 Prec@(1,5) (63.5%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:11:01] \u001b[32mTrain: [ 48/50] Step 020/520 Loss 1.965 Prec@(1,5) (61.6%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:11:02] \u001b[32mTrain: [ 48/50] Step 040/520 Loss 1.987 Prec@(1,5) (61.4%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:11:02] \u001b[32mTrain: [ 48/50] Step 060/520 Loss 1.962 Prec@(1,5) (62.0%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:11:02] \u001b[32mTrain: [ 48/50] Step 080/520 Loss 1.954 Prec@(1,5) (61.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:11:03] \u001b[32mTrain: [ 48/50] Step 100/520 Loss 1.939 Prec@(1,5) (62.0%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:03] \u001b[32mTrain: [ 48/50] Step 120/520 Loss 1.929 Prec@(1,5) (62.0%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:04] \u001b[32mTrain: [ 48/50] Step 140/520 Loss 1.935 Prec@(1,5) (62.0%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:04] \u001b[32mTrain: [ 48/50] Step 160/520 Loss 1.940 Prec@(1,5) (61.8%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:05] \u001b[32mTrain: [ 48/50] Step 180/520 Loss 1.935 Prec@(1,5) (62.1%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:05] \u001b[32mTrain: [ 48/50] Step 200/520 Loss 1.936 Prec@(1,5) (62.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:06] \u001b[32mTrain: [ 48/50] Step 220/520 Loss 1.933 Prec@(1,5) (62.3%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:06] \u001b[32mTrain: [ 48/50] Step 240/520 Loss 1.935 Prec@(1,5) (62.2%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:07] \u001b[32mTrain: [ 48/50] Step 260/520 Loss 1.938 Prec@(1,5) (62.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:07] \u001b[32mTrain: [ 48/50] Step 280/520 Loss 1.937 Prec@(1,5) (62.1%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:08] \u001b[32mTrain: [ 48/50] Step 300/520 Loss 1.935 Prec@(1,5) (62.1%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:08] \u001b[32mTrain: [ 48/50] Step 320/520 Loss 1.936 Prec@(1,5) (61.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:09] \u001b[32mTrain: [ 48/50] Step 340/520 Loss 1.930 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:09] \u001b[32mTrain: [ 48/50] Step 360/520 Loss 1.929 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:10] \u001b[32mTrain: [ 48/50] Step 380/520 Loss 1.931 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:10] \u001b[32mTrain: [ 48/50] Step 400/520 Loss 1.930 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:11] \u001b[32mTrain: [ 48/50] Step 420/520 Loss 1.930 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:11] \u001b[32mTrain: [ 48/50] Step 440/520 Loss 1.929 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:12] \u001b[32mTrain: [ 48/50] Step 460/520 Loss 1.928 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:12] \u001b[32mTrain: [ 48/50] Step 480/520 Loss 1.929 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:13] \u001b[32mTrain: [ 48/50] Step 500/520 Loss 1.927 Prec@(1,5) (62.1%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:13] \u001b[32mTrain: [ 48/50] Step 520/520 Loss 1.927 Prec@(1,5) (62.1%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:13] \u001b[32mTrain: [ 48/50] Final Prec@1 62.0760%\u001b[0m\n",
      "[2024-01-15 10:11:17] \u001b[32mValid: [ 48/50] Step 000/104 Loss 1.812 Prec@(1,5) (61.5%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:17] \u001b[32mValid: [ 48/50] Step 020/104 Loss 1.888 Prec@(1,5) (59.7%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mValid: [ 48/50] Step 040/104 Loss 1.870 Prec@(1,5) (59.3%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mValid: [ 48/50] Step 060/104 Loss 1.835 Prec@(1,5) (59.5%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mValid: [ 48/50] Step 080/104 Loss 1.853 Prec@(1,5) (59.3%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mValid: [ 48/50] Step 100/104 Loss 1.831 Prec@(1,5) (59.6%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mValid: [ 48/50] Step 104/104 Loss 1.830 Prec@(1,5) (59.7%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mValid: [ 48/50] Final Prec@1 59.6500%\u001b[0m\n",
      "[2024-01-15 10:11:18] \u001b[32mEpoch 48 LR 0.000100\u001b[0m\n",
      "[2024-01-15 10:11:23] \u001b[32mTrain: [ 49/50] Step 000/520 Loss 1.891 Prec@(1,5) (57.3%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:11:24] \u001b[32mTrain: [ 49/50] Step 020/520 Loss 1.955 Prec@(1,5) (61.0%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:24] \u001b[32mTrain: [ 49/50] Step 040/520 Loss 1.952 Prec@(1,5) (61.0%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:25] \u001b[32mTrain: [ 49/50] Step 060/520 Loss 1.940 Prec@(1,5) (61.3%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:11:25] \u001b[32mTrain: [ 49/50] Step 080/520 Loss 1.927 Prec@(1,5) (61.5%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:26] \u001b[32mTrain: [ 49/50] Step 100/520 Loss 1.918 Prec@(1,5) (61.5%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:26] \u001b[32mTrain: [ 49/50] Step 120/520 Loss 1.914 Prec@(1,5) (61.7%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:27] \u001b[32mTrain: [ 49/50] Step 140/520 Loss 1.914 Prec@(1,5) (61.7%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:11:27] \u001b[32mTrain: [ 49/50] Step 160/520 Loss 1.907 Prec@(1,5) (61.9%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:11:28] \u001b[32mTrain: [ 49/50] Step 180/520 Loss 1.908 Prec@(1,5) (62.1%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:11:28] \u001b[32mTrain: [ 49/50] Step 200/520 Loss 1.902 Prec@(1,5) (62.1%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:11:29] \u001b[32mTrain: [ 49/50] Step 220/520 Loss 1.908 Prec@(1,5) (61.9%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:29] \u001b[32mTrain: [ 49/50] Step 240/520 Loss 1.904 Prec@(1,5) (62.1%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:30] \u001b[32mTrain: [ 49/50] Step 260/520 Loss 1.908 Prec@(1,5) (62.1%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:30] \u001b[32mTrain: [ 49/50] Step 280/520 Loss 1.908 Prec@(1,5) (62.0%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:31] \u001b[32mTrain: [ 49/50] Step 300/520 Loss 1.911 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:31] \u001b[32mTrain: [ 49/50] Step 320/520 Loss 1.907 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:32] \u001b[32mTrain: [ 49/50] Step 340/520 Loss 1.910 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:32] \u001b[32mTrain: [ 49/50] Step 360/520 Loss 1.910 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:33] \u001b[32mTrain: [ 49/50] Step 380/520 Loss 1.907 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:33] \u001b[32mTrain: [ 49/50] Step 400/520 Loss 1.911 Prec@(1,5) (62.0%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:34] \u001b[32mTrain: [ 49/50] Step 420/520 Loss 1.916 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:34] \u001b[32mTrain: [ 49/50] Step 440/520 Loss 1.917 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:35] \u001b[32mTrain: [ 49/50] Step 460/520 Loss 1.917 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:35] \u001b[32mTrain: [ 49/50] Step 480/520 Loss 1.918 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:36] \u001b[32mTrain: [ 49/50] Step 500/520 Loss 1.917 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:36] \u001b[32mTrain: [ 49/50] Step 520/520 Loss 1.920 Prec@(1,5) (61.9%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:36] \u001b[32mTrain: [ 49/50] Final Prec@1 61.8720%\u001b[0m\n",
      "[2024-01-15 10:11:40] \u001b[32mValid: [ 49/50] Step 000/104 Loss 1.807 Prec@(1,5) (60.4%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:40] \u001b[32mValid: [ 49/50] Step 020/104 Loss 1.881 Prec@(1,5) (59.6%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mValid: [ 49/50] Step 040/104 Loss 1.866 Prec@(1,5) (59.3%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mValid: [ 49/50] Step 060/104 Loss 1.831 Prec@(1,5) (59.7%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mValid: [ 49/50] Step 080/104 Loss 1.847 Prec@(1,5) (59.5%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mValid: [ 49/50] Step 100/104 Loss 1.824 Prec@(1,5) (59.8%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mValid: [ 49/50] Step 104/104 Loss 1.823 Prec@(1,5) (59.9%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mValid: [ 49/50] Final Prec@1 59.8600%\u001b[0m\n",
      "[2024-01-15 10:11:41] \u001b[32mEpoch 49 LR 0.000026\u001b[0m\n",
      "[2024-01-15 10:11:46] \u001b[32mTrain: [ 50/50] Step 000/520 Loss 1.930 Prec@(1,5) (60.4%, 89.6%)\u001b[0m\n",
      "[2024-01-15 10:11:47] \u001b[32mTrain: [ 50/50] Step 020/520 Loss 1.999 Prec@(1,5) (60.0%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:11:47] \u001b[32mTrain: [ 50/50] Step 040/520 Loss 1.972 Prec@(1,5) (60.8%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:11:48] \u001b[32mTrain: [ 50/50] Step 060/520 Loss 1.980 Prec@(1,5) (60.7%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:11:48] \u001b[32mTrain: [ 50/50] Step 080/520 Loss 1.981 Prec@(1,5) (60.5%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:11:49] \u001b[32mTrain: [ 50/50] Step 100/520 Loss 1.964 Prec@(1,5) (61.1%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:49] \u001b[32mTrain: [ 50/50] Step 120/520 Loss 1.957 Prec@(1,5) (61.3%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:50] \u001b[32mTrain: [ 50/50] Step 140/520 Loss 1.950 Prec@(1,5) (61.5%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:50] \u001b[32mTrain: [ 50/50] Step 160/520 Loss 1.947 Prec@(1,5) (61.5%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:51] \u001b[32mTrain: [ 50/50] Step 180/520 Loss 1.952 Prec@(1,5) (61.4%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:11:51] \u001b[32mTrain: [ 50/50] Step 200/520 Loss 1.953 Prec@(1,5) (61.6%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:11:52] \u001b[32mTrain: [ 50/50] Step 220/520 Loss 1.950 Prec@(1,5) (61.7%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:11:52] \u001b[32mTrain: [ 50/50] Step 240/520 Loss 1.942 Prec@(1,5) (61.8%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:11:53] \u001b[32mTrain: [ 50/50] Step 260/520 Loss 1.933 Prec@(1,5) (62.0%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:53] \u001b[32mTrain: [ 50/50] Step 280/520 Loss 1.936 Prec@(1,5) (61.9%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:54] \u001b[32mTrain: [ 50/50] Step 300/520 Loss 1.940 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:54] \u001b[32mTrain: [ 50/50] Step 320/520 Loss 1.943 Prec@(1,5) (61.7%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:11:55] \u001b[32mTrain: [ 50/50] Step 340/520 Loss 1.939 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:55] \u001b[32mTrain: [ 50/50] Step 360/520 Loss 1.936 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:56] \u001b[32mTrain: [ 50/50] Step 380/520 Loss 1.940 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:56] \u001b[32mTrain: [ 50/50] Step 400/520 Loss 1.937 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:57] \u001b[32mTrain: [ 50/50] Step 420/520 Loss 1.940 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:57] \u001b[32mTrain: [ 50/50] Step 440/520 Loss 1.940 Prec@(1,5) (61.8%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:58] \u001b[32mTrain: [ 50/50] Step 460/520 Loss 1.939 Prec@(1,5) (61.9%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:11:58] \u001b[32mTrain: [ 50/50] Step 480/520 Loss 1.940 Prec@(1,5) (61.8%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:59] \u001b[32mTrain: [ 50/50] Step 500/520 Loss 1.937 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:59] \u001b[32mTrain: [ 50/50] Step 520/520 Loss 1.936 Prec@(1,5) (61.9%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:11:59] \u001b[32mTrain: [ 50/50] Final Prec@1 61.9480%\u001b[0m\n",
      "[2024-01-15 10:12:03] \u001b[32mValid: [ 50/50] Step 000/104 Loss 1.803 Prec@(1,5) (60.4%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Step 020/104 Loss 1.897 Prec@(1,5) (59.7%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Step 040/104 Loss 1.881 Prec@(1,5) (59.1%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Step 060/104 Loss 1.843 Prec@(1,5) (59.5%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Step 080/104 Loss 1.860 Prec@(1,5) (59.3%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Step 100/104 Loss 1.837 Prec@(1,5) (59.6%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Step 104/104 Loss 1.836 Prec@(1,5) (59.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:12:04] \u001b[32mValid: [ 50/50] Final Prec@1 59.6500%\u001b[0m\n",
      "Final best Prec@1 = 60.1700%\n",
      "./checkpoints/cifar100/random/3/\n",
      "[2024-01-15 10:12:04] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'maxpool', 'reduce_n3_p0': 'sepconv5x5', 'reduce_n3_p1': 'dilconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'sepconv5x5', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'sepconv5x5', 'reduce_n5_p4': 'skipconnect', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\u001b[0m\n",
      "[2024-01-15 10:12:05] \u001b[32mEpoch 0 LR 0.025000\u001b[0m\n",
      "[2024-01-15 10:12:09] \u001b[32mTrain: [  1/50] Step 000/520 Loss 6.487 Prec@(1,5) (1.0%, 6.2%)\u001b[0m\n",
      "[2024-01-15 10:12:10] \u001b[32mTrain: [  1/50] Step 020/520 Loss 6.416 Prec@(1,5) (1.4%, 6.9%)\u001b[0m\n",
      "[2024-01-15 10:12:10] \u001b[32mTrain: [  1/50] Step 040/520 Loss 6.309 Prec@(1,5) (2.3%, 10.1%)\u001b[0m\n",
      "[2024-01-15 10:12:11] \u001b[32mTrain: [  1/50] Step 060/520 Loss 6.204 Prec@(1,5) (3.2%, 12.7%)\u001b[0m\n",
      "[2024-01-15 10:12:11] \u001b[32mTrain: [  1/50] Step 080/520 Loss 6.113 Prec@(1,5) (3.9%, 14.6%)\u001b[0m\n",
      "[2024-01-15 10:12:11] \u001b[32mTrain: [  1/50] Step 100/520 Loss 6.031 Prec@(1,5) (4.3%, 16.4%)\u001b[0m\n",
      "[2024-01-15 10:12:12] \u001b[32mTrain: [  1/50] Step 120/520 Loss 5.956 Prec@(1,5) (4.8%, 18.1%)\u001b[0m\n",
      "[2024-01-15 10:12:12] \u001b[32mTrain: [  1/50] Step 140/520 Loss 5.887 Prec@(1,5) (5.4%, 19.5%)\u001b[0m\n",
      "[2024-01-15 10:12:12] \u001b[32mTrain: [  1/50] Step 160/520 Loss 5.834 Prec@(1,5) (5.8%, 20.6%)\u001b[0m\n",
      "[2024-01-15 10:12:13] \u001b[32mTrain: [  1/50] Step 180/520 Loss 5.792 Prec@(1,5) (6.2%, 21.7%)\u001b[0m\n",
      "[2024-01-15 10:12:13] \u001b[32mTrain: [  1/50] Step 200/520 Loss 5.751 Prec@(1,5) (6.6%, 22.6%)\u001b[0m\n",
      "[2024-01-15 10:12:13] \u001b[32mTrain: [  1/50] Step 220/520 Loss 5.711 Prec@(1,5) (7.0%, 23.6%)\u001b[0m\n",
      "[2024-01-15 10:12:14] \u001b[32mTrain: [  1/50] Step 240/520 Loss 5.675 Prec@(1,5) (7.3%, 24.4%)\u001b[0m\n",
      "[2024-01-15 10:12:14] \u001b[32mTrain: [  1/50] Step 260/520 Loss 5.633 Prec@(1,5) (7.7%, 25.3%)\u001b[0m\n",
      "[2024-01-15 10:12:15] \u001b[32mTrain: [  1/50] Step 280/520 Loss 5.600 Prec@(1,5) (8.0%, 26.2%)\u001b[0m\n",
      "[2024-01-15 10:12:15] \u001b[32mTrain: [  1/50] Step 300/520 Loss 5.572 Prec@(1,5) (8.2%, 26.8%)\u001b[0m\n",
      "[2024-01-15 10:12:15] \u001b[32mTrain: [  1/50] Step 320/520 Loss 5.543 Prec@(1,5) (8.5%, 27.5%)\u001b[0m\n",
      "[2024-01-15 10:12:16] \u001b[32mTrain: [  1/50] Step 340/520 Loss 5.516 Prec@(1,5) (8.8%, 28.0%)\u001b[0m\n",
      "[2024-01-15 10:12:16] \u001b[32mTrain: [  1/50] Step 360/520 Loss 5.487 Prec@(1,5) (9.1%, 28.7%)\u001b[0m\n",
      "[2024-01-15 10:12:16] \u001b[32mTrain: [  1/50] Step 380/520 Loss 5.458 Prec@(1,5) (9.4%, 29.4%)\u001b[0m\n",
      "[2024-01-15 10:12:17] \u001b[32mTrain: [  1/50] Step 400/520 Loss 5.433 Prec@(1,5) (9.6%, 29.9%)\u001b[0m\n",
      "[2024-01-15 10:12:17] \u001b[32mTrain: [  1/50] Step 420/520 Loss 5.407 Prec@(1,5) (9.9%, 30.5%)\u001b[0m\n",
      "[2024-01-15 10:12:18] \u001b[32mTrain: [  1/50] Step 440/520 Loss 5.381 Prec@(1,5) (10.2%, 31.1%)\u001b[0m\n",
      "[2024-01-15 10:12:18] \u001b[32mTrain: [  1/50] Step 460/520 Loss 5.354 Prec@(1,5) (10.5%, 31.7%)\u001b[0m\n",
      "[2024-01-15 10:12:18] \u001b[32mTrain: [  1/50] Step 480/520 Loss 5.331 Prec@(1,5) (10.7%, 32.3%)\u001b[0m\n",
      "[2024-01-15 10:12:19] \u001b[32mTrain: [  1/50] Step 500/520 Loss 5.310 Prec@(1,5) (11.0%, 32.7%)\u001b[0m\n",
      "[2024-01-15 10:12:19] \u001b[32mTrain: [  1/50] Step 520/520 Loss 5.287 Prec@(1,5) (11.3%, 33.2%)\u001b[0m\n",
      "[2024-01-15 10:12:19] \u001b[32mTrain: [  1/50] Final Prec@1 11.2560%\u001b[0m\n",
      "[2024-01-15 10:12:23] \u001b[32mValid: [  1/50] Step 000/104 Loss 4.363 Prec@(1,5) (12.5%, 38.5%)\u001b[0m\n",
      "[2024-01-15 10:12:23] \u001b[32mValid: [  1/50] Step 020/104 Loss 4.302 Prec@(1,5) (15.4%, 43.2%)\u001b[0m\n",
      "[2024-01-15 10:12:23] \u001b[32mValid: [  1/50] Step 040/104 Loss 4.275 Prec@(1,5) (16.0%, 43.4%)\u001b[0m\n",
      "[2024-01-15 10:12:23] \u001b[32mValid: [  1/50] Step 060/104 Loss 4.258 Prec@(1,5) (16.4%, 43.5%)\u001b[0m\n",
      "[2024-01-15 10:12:23] \u001b[32mValid: [  1/50] Step 080/104 Loss 4.255 Prec@(1,5) (16.5%, 43.9%)\u001b[0m\n",
      "[2024-01-15 10:12:24] \u001b[32mValid: [  1/50] Step 100/104 Loss 4.271 Prec@(1,5) (16.4%, 43.7%)\u001b[0m\n",
      "[2024-01-15 10:12:24] \u001b[32mValid: [  1/50] Step 104/104 Loss 4.269 Prec@(1,5) (16.4%, 43.7%)\u001b[0m\n",
      "[2024-01-15 10:12:24] \u001b[32mValid: [  1/50] Final Prec@1 16.4300%\u001b[0m\n",
      "[2024-01-15 10:12:24] \u001b[32mEpoch 1 LR 0.024975\u001b[0m\n",
      "[2024-01-15 10:12:29] \u001b[32mTrain: [  2/50] Step 000/520 Loss 4.808 Prec@(1,5) (16.7%, 46.9%)\u001b[0m\n",
      "[2024-01-15 10:12:29] \u001b[32mTrain: [  2/50] Step 020/520 Loss 4.717 Prec@(1,5) (18.1%, 46.3%)\u001b[0m\n",
      "[2024-01-15 10:12:30] \u001b[32mTrain: [  2/50] Step 040/520 Loss 4.688 Prec@(1,5) (18.0%, 46.1%)\u001b[0m\n",
      "[2024-01-15 10:12:30] \u001b[32mTrain: [  2/50] Step 060/520 Loss 4.673 Prec@(1,5) (18.3%, 46.6%)\u001b[0m\n",
      "[2024-01-15 10:12:30] \u001b[32mTrain: [  2/50] Step 080/520 Loss 4.662 Prec@(1,5) (18.8%, 46.8%)\u001b[0m\n",
      "[2024-01-15 10:12:31] \u001b[32mTrain: [  2/50] Step 100/520 Loss 4.650 Prec@(1,5) (18.6%, 47.1%)\u001b[0m\n",
      "[2024-01-15 10:12:31] \u001b[32mTrain: [  2/50] Step 120/520 Loss 4.625 Prec@(1,5) (18.9%, 47.5%)\u001b[0m\n",
      "[2024-01-15 10:12:32] \u001b[32mTrain: [  2/50] Step 140/520 Loss 4.605 Prec@(1,5) (19.4%, 47.7%)\u001b[0m\n",
      "[2024-01-15 10:12:32] \u001b[32mTrain: [  2/50] Step 160/520 Loss 4.590 Prec@(1,5) (19.6%, 47.9%)\u001b[0m\n",
      "[2024-01-15 10:12:33] \u001b[32mTrain: [  2/50] Step 180/520 Loss 4.571 Prec@(1,5) (19.9%, 48.2%)\u001b[0m\n",
      "[2024-01-15 10:12:33] \u001b[32mTrain: [  2/50] Step 200/520 Loss 4.549 Prec@(1,5) (20.2%, 48.7%)\u001b[0m\n",
      "[2024-01-15 10:12:33] \u001b[32mTrain: [  2/50] Step 220/520 Loss 4.540 Prec@(1,5) (20.4%, 48.9%)\u001b[0m\n",
      "[2024-01-15 10:12:34] \u001b[32mTrain: [  2/50] Step 240/520 Loss 4.521 Prec@(1,5) (20.7%, 49.3%)\u001b[0m\n",
      "[2024-01-15 10:12:34] \u001b[32mTrain: [  2/50] Step 260/520 Loss 4.510 Prec@(1,5) (20.8%, 49.5%)\u001b[0m\n",
      "[2024-01-15 10:12:35] \u001b[32mTrain: [  2/50] Step 280/520 Loss 4.507 Prec@(1,5) (20.8%, 49.6%)\u001b[0m\n",
      "[2024-01-15 10:12:35] \u001b[32mTrain: [  2/50] Step 300/520 Loss 4.489 Prec@(1,5) (21.0%, 49.9%)\u001b[0m\n",
      "[2024-01-15 10:12:36] \u001b[32mTrain: [  2/50] Step 320/520 Loss 4.479 Prec@(1,5) (21.1%, 50.1%)\u001b[0m\n",
      "[2024-01-15 10:12:36] \u001b[32mTrain: [  2/50] Step 340/520 Loss 4.470 Prec@(1,5) (21.3%, 50.3%)\u001b[0m\n",
      "[2024-01-15 10:12:36] \u001b[32mTrain: [  2/50] Step 360/520 Loss 4.461 Prec@(1,5) (21.4%, 50.5%)\u001b[0m\n",
      "[2024-01-15 10:12:37] \u001b[32mTrain: [  2/50] Step 380/520 Loss 4.447 Prec@(1,5) (21.5%, 50.7%)\u001b[0m\n",
      "[2024-01-15 10:12:37] \u001b[32mTrain: [  2/50] Step 400/520 Loss 4.436 Prec@(1,5) (21.6%, 50.9%)\u001b[0m\n",
      "[2024-01-15 10:12:38] \u001b[32mTrain: [  2/50] Step 420/520 Loss 4.427 Prec@(1,5) (21.8%, 51.1%)\u001b[0m\n",
      "[2024-01-15 10:12:38] \u001b[32mTrain: [  2/50] Step 440/520 Loss 4.420 Prec@(1,5) (21.8%, 51.3%)\u001b[0m\n",
      "[2024-01-15 10:12:38] \u001b[32mTrain: [  2/50] Step 460/520 Loss 4.411 Prec@(1,5) (22.0%, 51.4%)\u001b[0m\n",
      "[2024-01-15 10:12:39] \u001b[32mTrain: [  2/50] Step 480/520 Loss 4.401 Prec@(1,5) (22.2%, 51.5%)\u001b[0m\n",
      "[2024-01-15 10:12:39] \u001b[32mTrain: [  2/50] Step 500/520 Loss 4.391 Prec@(1,5) (22.3%, 51.7%)\u001b[0m\n",
      "[2024-01-15 10:12:40] \u001b[32mTrain: [  2/50] Step 520/520 Loss 4.379 Prec@(1,5) (22.5%, 51.9%)\u001b[0m\n",
      "[2024-01-15 10:12:40] \u001b[32mTrain: [  2/50] Final Prec@1 22.4980%\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 000/104 Loss 4.480 Prec@(1,5) (24.0%, 50.0%)\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 020/104 Loss 3.965 Prec@(1,5) (25.1%, 56.5%)\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 040/104 Loss 3.919 Prec@(1,5) (25.0%, 57.0%)\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 060/104 Loss 3.908 Prec@(1,5) (25.6%, 56.8%)\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 080/104 Loss 3.920 Prec@(1,5) (25.4%, 56.6%)\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 100/104 Loss 3.940 Prec@(1,5) (25.2%, 56.5%)\u001b[0m\n",
      "[2024-01-15 10:12:44] \u001b[32mValid: [  2/50] Step 104/104 Loss 3.941 Prec@(1,5) (25.2%, 56.5%)\u001b[0m\n",
      "[2024-01-15 10:12:45] \u001b[32mValid: [  2/50] Final Prec@1 25.1700%\u001b[0m\n",
      "[2024-01-15 10:12:45] \u001b[32mEpoch 2 LR 0.024901\u001b[0m\n",
      "[2024-01-15 10:12:50] \u001b[32mTrain: [  3/50] Step 000/520 Loss 3.874 Prec@(1,5) (24.0%, 61.5%)\u001b[0m\n",
      "[2024-01-15 10:12:50] \u001b[32mTrain: [  3/50] Step 020/520 Loss 4.048 Prec@(1,5) (25.5%, 58.3%)\u001b[0m\n",
      "[2024-01-15 10:12:50] \u001b[32mTrain: [  3/50] Step 040/520 Loss 4.032 Prec@(1,5) (26.6%, 58.4%)\u001b[0m\n",
      "[2024-01-15 10:12:51] \u001b[32mTrain: [  3/50] Step 060/520 Loss 4.030 Prec@(1,5) (27.1%, 58.4%)\u001b[0m\n",
      "[2024-01-15 10:12:51] \u001b[32mTrain: [  3/50] Step 080/520 Loss 4.013 Prec@(1,5) (27.4%, 58.7%)\u001b[0m\n",
      "[2024-01-15 10:12:52] \u001b[32mTrain: [  3/50] Step 100/520 Loss 4.005 Prec@(1,5) (27.5%, 58.7%)\u001b[0m\n",
      "[2024-01-15 10:12:52] \u001b[32mTrain: [  3/50] Step 120/520 Loss 4.013 Prec@(1,5) (27.1%, 58.5%)\u001b[0m\n",
      "[2024-01-15 10:12:52] \u001b[32mTrain: [  3/50] Step 140/520 Loss 3.997 Prec@(1,5) (27.3%, 58.7%)\u001b[0m\n",
      "[2024-01-15 10:12:53] \u001b[32mTrain: [  3/50] Step 160/520 Loss 4.005 Prec@(1,5) (27.3%, 58.5%)\u001b[0m\n",
      "[2024-01-15 10:12:53] \u001b[32mTrain: [  3/50] Step 180/520 Loss 4.005 Prec@(1,5) (27.3%, 58.5%)\u001b[0m\n",
      "[2024-01-15 10:12:54] \u001b[32mTrain: [  3/50] Step 200/520 Loss 4.001 Prec@(1,5) (27.3%, 58.6%)\u001b[0m\n",
      "[2024-01-15 10:12:54] \u001b[32mTrain: [  3/50] Step 220/520 Loss 3.995 Prec@(1,5) (27.4%, 58.7%)\u001b[0m\n",
      "[2024-01-15 10:12:54] \u001b[32mTrain: [  3/50] Step 240/520 Loss 3.984 Prec@(1,5) (27.5%, 59.0%)\u001b[0m\n",
      "[2024-01-15 10:12:55] \u001b[32mTrain: [  3/50] Step 260/520 Loss 3.982 Prec@(1,5) (27.5%, 58.9%)\u001b[0m\n",
      "[2024-01-15 10:12:55] \u001b[32mTrain: [  3/50] Step 280/520 Loss 3.978 Prec@(1,5) (27.6%, 59.1%)\u001b[0m\n",
      "[2024-01-15 10:12:56] \u001b[32mTrain: [  3/50] Step 300/520 Loss 3.971 Prec@(1,5) (27.7%, 59.2%)\u001b[0m\n",
      "[2024-01-15 10:12:56] \u001b[32mTrain: [  3/50] Step 320/520 Loss 3.962 Prec@(1,5) (27.8%, 59.3%)\u001b[0m\n",
      "[2024-01-15 10:12:56] \u001b[32mTrain: [  3/50] Step 340/520 Loss 3.960 Prec@(1,5) (27.8%, 59.3%)\u001b[0m\n",
      "[2024-01-15 10:12:57] \u001b[32mTrain: [  3/50] Step 360/520 Loss 3.952 Prec@(1,5) (27.9%, 59.5%)\u001b[0m\n",
      "[2024-01-15 10:12:57] \u001b[32mTrain: [  3/50] Step 380/520 Loss 3.942 Prec@(1,5) (28.1%, 59.6%)\u001b[0m\n",
      "[2024-01-15 10:12:57] \u001b[32mTrain: [  3/50] Step 400/520 Loss 3.936 Prec@(1,5) (28.3%, 59.7%)\u001b[0m\n",
      "[2024-01-15 10:12:58] \u001b[32mTrain: [  3/50] Step 420/520 Loss 3.933 Prec@(1,5) (28.3%, 59.8%)\u001b[0m\n",
      "[2024-01-15 10:12:58] \u001b[32mTrain: [  3/50] Step 440/520 Loss 3.925 Prec@(1,5) (28.4%, 59.9%)\u001b[0m\n",
      "[2024-01-15 10:12:59] \u001b[32mTrain: [  3/50] Step 460/520 Loss 3.917 Prec@(1,5) (28.6%, 60.1%)\u001b[0m\n",
      "[2024-01-15 10:12:59] \u001b[32mTrain: [  3/50] Step 480/520 Loss 3.913 Prec@(1,5) (28.6%, 60.2%)\u001b[0m\n",
      "[2024-01-15 10:12:59] \u001b[32mTrain: [  3/50] Step 500/520 Loss 3.909 Prec@(1,5) (28.7%, 60.3%)\u001b[0m\n",
      "[2024-01-15 10:13:00] \u001b[32mTrain: [  3/50] Step 520/520 Loss 3.906 Prec@(1,5) (28.7%, 60.3%)\u001b[0m\n",
      "[2024-01-15 10:13:00] \u001b[32mTrain: [  3/50] Final Prec@1 28.6880%\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 000/104 Loss 3.633 Prec@(1,5) (32.3%, 61.5%)\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 020/104 Loss 3.512 Prec@(1,5) (29.5%, 63.6%)\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 040/104 Loss 3.467 Prec@(1,5) (29.6%, 64.1%)\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 060/104 Loss 3.470 Prec@(1,5) (30.4%, 63.6%)\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 080/104 Loss 3.487 Prec@(1,5) (30.0%, 63.4%)\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 100/104 Loss 3.508 Prec@(1,5) (29.7%, 63.2%)\u001b[0m\n",
      "[2024-01-15 10:13:04] \u001b[32mValid: [  3/50] Step 104/104 Loss 3.504 Prec@(1,5) (29.7%, 63.1%)\u001b[0m\n",
      "[2024-01-15 10:13:05] \u001b[32mValid: [  3/50] Final Prec@1 29.7200%\u001b[0m\n",
      "[2024-01-15 10:13:05] \u001b[32mEpoch 3 LR 0.024779\u001b[0m\n",
      "[2024-01-15 10:13:09] \u001b[32mTrain: [  4/50] Step 000/520 Loss 3.525 Prec@(1,5) (32.3%, 64.6%)\u001b[0m\n",
      "[2024-01-15 10:13:10] \u001b[32mTrain: [  4/50] Step 020/520 Loss 3.733 Prec@(1,5) (31.6%, 64.4%)\u001b[0m\n",
      "[2024-01-15 10:13:10] \u001b[32mTrain: [  4/50] Step 040/520 Loss 3.739 Prec@(1,5) (31.4%, 63.9%)\u001b[0m\n",
      "[2024-01-15 10:13:11] \u001b[32mTrain: [  4/50] Step 060/520 Loss 3.731 Prec@(1,5) (31.1%, 63.8%)\u001b[0m\n",
      "[2024-01-15 10:13:11] \u001b[32mTrain: [  4/50] Step 080/520 Loss 3.726 Prec@(1,5) (31.6%, 63.7%)\u001b[0m\n",
      "[2024-01-15 10:13:11] \u001b[32mTrain: [  4/50] Step 100/520 Loss 3.724 Prec@(1,5) (31.6%, 63.6%)\u001b[0m\n",
      "[2024-01-15 10:13:12] \u001b[32mTrain: [  4/50] Step 120/520 Loss 3.704 Prec@(1,5) (31.8%, 64.0%)\u001b[0m\n",
      "[2024-01-15 10:13:12] \u001b[32mTrain: [  4/50] Step 140/520 Loss 3.710 Prec@(1,5) (31.8%, 63.9%)\u001b[0m\n",
      "[2024-01-15 10:13:13] \u001b[32mTrain: [  4/50] Step 160/520 Loss 3.698 Prec@(1,5) (31.9%, 64.1%)\u001b[0m\n",
      "[2024-01-15 10:13:13] \u001b[32mTrain: [  4/50] Step 180/520 Loss 3.694 Prec@(1,5) (32.1%, 64.1%)\u001b[0m\n",
      "[2024-01-15 10:13:13] \u001b[32mTrain: [  4/50] Step 200/520 Loss 3.678 Prec@(1,5) (32.1%, 64.4%)\u001b[0m\n",
      "[2024-01-15 10:13:14] \u001b[32mTrain: [  4/50] Step 220/520 Loss 3.663 Prec@(1,5) (32.4%, 64.6%)\u001b[0m\n",
      "[2024-01-15 10:13:14] \u001b[32mTrain: [  4/50] Step 240/520 Loss 3.656 Prec@(1,5) (32.4%, 64.7%)\u001b[0m\n",
      "[2024-01-15 10:13:15] \u001b[32mTrain: [  4/50] Step 260/520 Loss 3.651 Prec@(1,5) (32.4%, 64.7%)\u001b[0m\n",
      "[2024-01-15 10:13:15] \u001b[32mTrain: [  4/50] Step 280/520 Loss 3.648 Prec@(1,5) (32.4%, 64.8%)\u001b[0m\n",
      "[2024-01-15 10:13:15] \u001b[32mTrain: [  4/50] Step 300/520 Loss 3.646 Prec@(1,5) (32.4%, 64.9%)\u001b[0m\n",
      "[2024-01-15 10:13:16] \u001b[32mTrain: [  4/50] Step 320/520 Loss 3.638 Prec@(1,5) (32.5%, 65.0%)\u001b[0m\n",
      "[2024-01-15 10:13:16] \u001b[32mTrain: [  4/50] Step 340/520 Loss 3.636 Prec@(1,5) (32.6%, 65.0%)\u001b[0m\n",
      "[2024-01-15 10:13:17] \u001b[32mTrain: [  4/50] Step 360/520 Loss 3.632 Prec@(1,5) (32.7%, 65.1%)\u001b[0m\n",
      "[2024-01-15 10:13:17] \u001b[32mTrain: [  4/50] Step 380/520 Loss 3.631 Prec@(1,5) (32.6%, 65.2%)\u001b[0m\n",
      "[2024-01-15 10:13:17] \u001b[32mTrain: [  4/50] Step 400/520 Loss 3.625 Prec@(1,5) (32.7%, 65.3%)\u001b[0m\n",
      "[2024-01-15 10:13:18] \u001b[32mTrain: [  4/50] Step 420/520 Loss 3.622 Prec@(1,5) (32.7%, 65.3%)\u001b[0m\n",
      "[2024-01-15 10:13:18] \u001b[32mTrain: [  4/50] Step 440/520 Loss 3.618 Prec@(1,5) (32.8%, 65.4%)\u001b[0m\n",
      "[2024-01-15 10:13:19] \u001b[32mTrain: [  4/50] Step 460/520 Loss 3.616 Prec@(1,5) (32.9%, 65.3%)\u001b[0m\n",
      "[2024-01-15 10:13:19] \u001b[32mTrain: [  4/50] Step 480/520 Loss 3.616 Prec@(1,5) (32.9%, 65.3%)\u001b[0m\n",
      "[2024-01-15 10:13:19] \u001b[32mTrain: [  4/50] Step 500/520 Loss 3.613 Prec@(1,5) (32.9%, 65.3%)\u001b[0m\n",
      "[2024-01-15 10:13:20] \u001b[32mTrain: [  4/50] Step 520/520 Loss 3.610 Prec@(1,5) (33.0%, 65.3%)\u001b[0m\n",
      "[2024-01-15 10:13:20] \u001b[32mTrain: [  4/50] Final Prec@1 32.9660%\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 000/104 Loss 3.129 Prec@(1,5) (37.5%, 67.7%)\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 020/104 Loss 3.327 Prec@(1,5) (33.2%, 67.2%)\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 040/104 Loss 3.240 Prec@(1,5) (33.6%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 060/104 Loss 3.236 Prec@(1,5) (34.3%, 68.0%)\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 080/104 Loss 3.267 Prec@(1,5) (34.0%, 67.7%)\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 100/104 Loss 3.290 Prec@(1,5) (33.7%, 67.3%)\u001b[0m\n",
      "[2024-01-15 10:13:24] \u001b[32mValid: [  4/50] Step 104/104 Loss 3.290 Prec@(1,5) (33.6%, 67.2%)\u001b[0m\n",
      "[2024-01-15 10:13:25] \u001b[32mValid: [  4/50] Final Prec@1 33.6100%\u001b[0m\n",
      "[2024-01-15 10:13:25] \u001b[32mEpoch 4 LR 0.024607\u001b[0m\n",
      "[2024-01-15 10:13:31] \u001b[32mTrain: [  5/50] Step 000/520 Loss 3.455 Prec@(1,5) (37.5%, 66.7%)\u001b[0m\n",
      "[2024-01-15 10:13:31] \u001b[32mTrain: [  5/50] Step 020/520 Loss 3.478 Prec@(1,5) (34.9%, 67.5%)\u001b[0m\n",
      "[2024-01-15 10:13:31] \u001b[32mTrain: [  5/50] Step 040/520 Loss 3.434 Prec@(1,5) (35.6%, 67.9%)\u001b[0m\n",
      "[2024-01-15 10:13:32] \u001b[32mTrain: [  5/50] Step 060/520 Loss 3.417 Prec@(1,5) (35.9%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:32] \u001b[32mTrain: [  5/50] Step 080/520 Loss 3.418 Prec@(1,5) (36.0%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:33] \u001b[32mTrain: [  5/50] Step 100/520 Loss 3.395 Prec@(1,5) (36.4%, 68.7%)\u001b[0m\n",
      "[2024-01-15 10:13:33] \u001b[32mTrain: [  5/50] Step 120/520 Loss 3.394 Prec@(1,5) (36.5%, 68.6%)\u001b[0m\n",
      "[2024-01-15 10:13:34] \u001b[32mTrain: [  5/50] Step 140/520 Loss 3.404 Prec@(1,5) (36.4%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:34] \u001b[32mTrain: [  5/50] Step 160/520 Loss 3.398 Prec@(1,5) (36.3%, 68.5%)\u001b[0m\n",
      "[2024-01-15 10:13:34] \u001b[32mTrain: [  5/50] Step 180/520 Loss 3.402 Prec@(1,5) (36.4%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:35] \u001b[32mTrain: [  5/50] Step 200/520 Loss 3.408 Prec@(1,5) (36.4%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:35] \u001b[32mTrain: [  5/50] Step 220/520 Loss 3.409 Prec@(1,5) (36.5%, 68.3%)\u001b[0m\n",
      "[2024-01-15 10:13:36] \u001b[32mTrain: [  5/50] Step 240/520 Loss 3.414 Prec@(1,5) (36.3%, 68.3%)\u001b[0m\n",
      "[2024-01-15 10:13:36] \u001b[32mTrain: [  5/50] Step 260/520 Loss 3.409 Prec@(1,5) (36.4%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:36] \u001b[32mTrain: [  5/50] Step 280/520 Loss 3.412 Prec@(1,5) (36.4%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:37] \u001b[32mTrain: [  5/50] Step 300/520 Loss 3.406 Prec@(1,5) (36.5%, 68.3%)\u001b[0m\n",
      "[2024-01-15 10:13:37] \u001b[32mTrain: [  5/50] Step 320/520 Loss 3.403 Prec@(1,5) (36.5%, 68.4%)\u001b[0m\n",
      "[2024-01-15 10:13:38] \u001b[32mTrain: [  5/50] Step 340/520 Loss 3.401 Prec@(1,5) (36.6%, 68.5%)\u001b[0m\n",
      "[2024-01-15 10:13:38] \u001b[32mTrain: [  5/50] Step 360/520 Loss 3.398 Prec@(1,5) (36.7%, 68.5%)\u001b[0m\n",
      "[2024-01-15 10:13:39] \u001b[32mTrain: [  5/50] Step 380/520 Loss 3.395 Prec@(1,5) (36.7%, 68.5%)\u001b[0m\n",
      "[2024-01-15 10:13:39] \u001b[32mTrain: [  5/50] Step 400/520 Loss 3.395 Prec@(1,5) (36.7%, 68.5%)\u001b[0m\n",
      "[2024-01-15 10:13:39] \u001b[32mTrain: [  5/50] Step 420/520 Loss 3.390 Prec@(1,5) (36.8%, 68.7%)\u001b[0m\n",
      "[2024-01-15 10:13:40] \u001b[32mTrain: [  5/50] Step 440/520 Loss 3.390 Prec@(1,5) (36.7%, 68.7%)\u001b[0m\n",
      "[2024-01-15 10:13:40] \u001b[32mTrain: [  5/50] Step 460/520 Loss 3.385 Prec@(1,5) (36.9%, 68.7%)\u001b[0m\n",
      "[2024-01-15 10:13:41] \u001b[32mTrain: [  5/50] Step 480/520 Loss 3.385 Prec@(1,5) (36.8%, 68.6%)\u001b[0m\n",
      "[2024-01-15 10:13:41] \u001b[32mTrain: [  5/50] Step 500/520 Loss 3.386 Prec@(1,5) (36.8%, 68.7%)\u001b[0m\n",
      "[2024-01-15 10:13:42] \u001b[32mTrain: [  5/50] Step 520/520 Loss 3.386 Prec@(1,5) (36.8%, 68.7%)\u001b[0m\n",
      "[2024-01-15 10:13:42] \u001b[32mTrain: [  5/50] Final Prec@1 36.8160%\u001b[0m\n",
      "[2024-01-15 10:13:46] \u001b[32mValid: [  5/50] Step 000/104 Loss 3.010 Prec@(1,5) (38.5%, 71.9%)\u001b[0m\n",
      "[2024-01-15 10:13:46] \u001b[32mValid: [  5/50] Step 020/104 Loss 3.097 Prec@(1,5) (36.8%, 69.9%)\u001b[0m\n",
      "[2024-01-15 10:13:46] \u001b[32mValid: [  5/50] Step 040/104 Loss 3.047 Prec@(1,5) (36.9%, 70.5%)\u001b[0m\n",
      "[2024-01-15 10:13:46] \u001b[32mValid: [  5/50] Step 060/104 Loss 3.018 Prec@(1,5) (37.3%, 70.1%)\u001b[0m\n",
      "[2024-01-15 10:13:46] \u001b[32mValid: [  5/50] Step 080/104 Loss 3.045 Prec@(1,5) (36.9%, 69.8%)\u001b[0m\n",
      "[2024-01-15 10:13:47] \u001b[32mValid: [  5/50] Step 100/104 Loss 3.056 Prec@(1,5) (36.9%, 69.8%)\u001b[0m\n",
      "[2024-01-15 10:13:47] \u001b[32mValid: [  5/50] Step 104/104 Loss 3.055 Prec@(1,5) (36.8%, 69.7%)\u001b[0m\n",
      "[2024-01-15 10:13:47] \u001b[32mValid: [  5/50] Final Prec@1 36.8300%\u001b[0m\n",
      "[2024-01-15 10:13:47] \u001b[32mEpoch 5 LR 0.024388\u001b[0m\n",
      "[2024-01-15 10:13:52] \u001b[32mTrain: [  6/50] Step 000/520 Loss 3.486 Prec@(1,5) (35.4%, 68.8%)\u001b[0m\n",
      "[2024-01-15 10:13:52] \u001b[32mTrain: [  6/50] Step 020/520 Loss 3.282 Prec@(1,5) (38.0%, 70.7%)\u001b[0m\n",
      "[2024-01-15 10:13:53] \u001b[32mTrain: [  6/50] Step 040/520 Loss 3.275 Prec@(1,5) (38.4%, 70.2%)\u001b[0m\n",
      "[2024-01-15 10:13:53] \u001b[32mTrain: [  6/50] Step 060/520 Loss 3.247 Prec@(1,5) (38.6%, 70.9%)\u001b[0m\n",
      "[2024-01-15 10:13:54] \u001b[32mTrain: [  6/50] Step 080/520 Loss 3.250 Prec@(1,5) (39.0%, 70.7%)\u001b[0m\n",
      "[2024-01-15 10:13:54] \u001b[32mTrain: [  6/50] Step 100/520 Loss 3.238 Prec@(1,5) (39.2%, 70.6%)\u001b[0m\n",
      "[2024-01-15 10:13:55] \u001b[32mTrain: [  6/50] Step 120/520 Loss 3.235 Prec@(1,5) (39.1%, 70.7%)\u001b[0m\n",
      "[2024-01-15 10:13:55] \u001b[32mTrain: [  6/50] Step 140/520 Loss 3.243 Prec@(1,5) (39.0%, 70.7%)\u001b[0m\n",
      "[2024-01-15 10:13:55] \u001b[32mTrain: [  6/50] Step 160/520 Loss 3.242 Prec@(1,5) (38.9%, 70.6%)\u001b[0m\n",
      "[2024-01-15 10:13:56] \u001b[32mTrain: [  6/50] Step 180/520 Loss 3.242 Prec@(1,5) (38.9%, 70.5%)\u001b[0m\n",
      "[2024-01-15 10:13:56] \u001b[32mTrain: [  6/50] Step 200/520 Loss 3.242 Prec@(1,5) (39.0%, 70.5%)\u001b[0m\n",
      "[2024-01-15 10:13:57] \u001b[32mTrain: [  6/50] Step 220/520 Loss 3.245 Prec@(1,5) (39.0%, 70.4%)\u001b[0m\n",
      "[2024-01-15 10:13:57] \u001b[32mTrain: [  6/50] Step 240/520 Loss 3.253 Prec@(1,5) (38.9%, 70.2%)\u001b[0m\n",
      "[2024-01-15 10:13:58] \u001b[32mTrain: [  6/50] Step 260/520 Loss 3.253 Prec@(1,5) (39.0%, 70.3%)\u001b[0m\n",
      "[2024-01-15 10:13:58] \u001b[32mTrain: [  6/50] Step 280/520 Loss 3.247 Prec@(1,5) (39.0%, 70.4%)\u001b[0m\n",
      "[2024-01-15 10:13:58] \u001b[32mTrain: [  6/50] Step 300/520 Loss 3.242 Prec@(1,5) (39.1%, 70.6%)\u001b[0m\n",
      "[2024-01-15 10:13:59] \u001b[32mTrain: [  6/50] Step 320/520 Loss 3.240 Prec@(1,5) (39.1%, 70.7%)\u001b[0m\n",
      "[2024-01-15 10:13:59] \u001b[32mTrain: [  6/50] Step 340/520 Loss 3.240 Prec@(1,5) (39.2%, 70.6%)\u001b[0m\n",
      "[2024-01-15 10:14:00] \u001b[32mTrain: [  6/50] Step 360/520 Loss 3.238 Prec@(1,5) (39.2%, 70.7%)\u001b[0m\n",
      "[2024-01-15 10:14:00] \u001b[32mTrain: [  6/50] Step 380/520 Loss 3.234 Prec@(1,5) (39.3%, 70.8%)\u001b[0m\n",
      "[2024-01-15 10:14:00] \u001b[32mTrain: [  6/50] Step 400/520 Loss 3.235 Prec@(1,5) (39.3%, 70.8%)\u001b[0m\n",
      "[2024-01-15 10:14:01] \u001b[32mTrain: [  6/50] Step 420/520 Loss 3.231 Prec@(1,5) (39.4%, 70.9%)\u001b[0m\n",
      "[2024-01-15 10:14:01] \u001b[32mTrain: [  6/50] Step 440/520 Loss 3.231 Prec@(1,5) (39.3%, 70.9%)\u001b[0m\n",
      "[2024-01-15 10:14:02] \u001b[32mTrain: [  6/50] Step 460/520 Loss 3.233 Prec@(1,5) (39.3%, 70.9%)\u001b[0m\n",
      "[2024-01-15 10:14:02] \u001b[32mTrain: [  6/50] Step 480/520 Loss 3.231 Prec@(1,5) (39.2%, 71.0%)\u001b[0m\n",
      "[2024-01-15 10:14:02] \u001b[32mTrain: [  6/50] Step 500/520 Loss 3.228 Prec@(1,5) (39.3%, 71.0%)\u001b[0m\n",
      "[2024-01-15 10:14:03] \u001b[32mTrain: [  6/50] Step 520/520 Loss 3.227 Prec@(1,5) (39.2%, 71.0%)\u001b[0m\n",
      "[2024-01-15 10:14:03] \u001b[32mTrain: [  6/50] Final Prec@1 39.2140%\u001b[0m\n",
      "[2024-01-15 10:14:07] \u001b[32mValid: [  6/50] Step 000/104 Loss 2.878 Prec@(1,5) (41.7%, 71.9%)\u001b[0m\n",
      "[2024-01-15 10:14:07] \u001b[32mValid: [  6/50] Step 020/104 Loss 2.769 Prec@(1,5) (41.3%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:14:07] \u001b[32mValid: [  6/50] Step 040/104 Loss 2.732 Prec@(1,5) (40.8%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:14:07] \u001b[32mValid: [  6/50] Step 060/104 Loss 2.721 Prec@(1,5) (41.2%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:14:08] \u001b[32mValid: [  6/50] Step 080/104 Loss 2.733 Prec@(1,5) (40.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:14:08] \u001b[32mValid: [  6/50] Step 100/104 Loss 2.731 Prec@(1,5) (40.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:14:08] \u001b[32mValid: [  6/50] Step 104/104 Loss 2.735 Prec@(1,5) (40.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:14:08] \u001b[32mValid: [  6/50] Final Prec@1 40.8200%\u001b[0m\n",
      "[2024-01-15 10:14:08] \u001b[32mEpoch 6 LR 0.024122\u001b[0m\n",
      "[2024-01-15 10:14:13] \u001b[32mTrain: [  7/50] Step 000/520 Loss 3.440 Prec@(1,5) (40.6%, 62.5%)\u001b[0m\n",
      "[2024-01-15 10:14:13] \u001b[32mTrain: [  7/50] Step 020/520 Loss 3.043 Prec@(1,5) (41.8%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:14:14] \u001b[32mTrain: [  7/50] Step 040/520 Loss 3.040 Prec@(1,5) (42.3%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:14:14] \u001b[32mTrain: [  7/50] Step 060/520 Loss 3.035 Prec@(1,5) (42.4%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:14:15] \u001b[32mTrain: [  7/50] Step 080/520 Loss 3.058 Prec@(1,5) (41.7%, 72.9%)\u001b[0m\n",
      "[2024-01-15 10:14:15] \u001b[32mTrain: [  7/50] Step 100/520 Loss 3.079 Prec@(1,5) (41.3%, 72.8%)\u001b[0m\n",
      "[2024-01-15 10:14:15] \u001b[32mTrain: [  7/50] Step 120/520 Loss 3.084 Prec@(1,5) (41.4%, 72.7%)\u001b[0m\n",
      "[2024-01-15 10:14:16] \u001b[32mTrain: [  7/50] Step 140/520 Loss 3.094 Prec@(1,5) (41.1%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:16] \u001b[32mTrain: [  7/50] Step 160/520 Loss 3.105 Prec@(1,5) (41.0%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:17] \u001b[32mTrain: [  7/50] Step 180/520 Loss 3.099 Prec@(1,5) (41.0%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:17] \u001b[32mTrain: [  7/50] Step 200/520 Loss 3.106 Prec@(1,5) (40.7%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:18] \u001b[32mTrain: [  7/50] Step 220/520 Loss 3.100 Prec@(1,5) (41.0%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:18] \u001b[32mTrain: [  7/50] Step 240/520 Loss 3.099 Prec@(1,5) (40.9%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:18] \u001b[32mTrain: [  7/50] Step 260/520 Loss 3.100 Prec@(1,5) (40.9%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:19] \u001b[32mTrain: [  7/50] Step 280/520 Loss 3.106 Prec@(1,5) (40.8%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:19] \u001b[32mTrain: [  7/50] Step 300/520 Loss 3.104 Prec@(1,5) (40.9%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:20] \u001b[32mTrain: [  7/50] Step 320/520 Loss 3.103 Prec@(1,5) (41.0%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:20] \u001b[32mTrain: [  7/50] Step 340/520 Loss 3.104 Prec@(1,5) (40.9%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:20] \u001b[32mTrain: [  7/50] Step 360/520 Loss 3.100 Prec@(1,5) (41.0%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:21] \u001b[32mTrain: [  7/50] Step 380/520 Loss 3.103 Prec@(1,5) (41.0%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:21] \u001b[32mTrain: [  7/50] Step 400/520 Loss 3.109 Prec@(1,5) (40.9%, 72.4%)\u001b[0m\n",
      "[2024-01-15 10:14:22] \u001b[32mTrain: [  7/50] Step 420/520 Loss 3.108 Prec@(1,5) (41.0%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:22] \u001b[32mTrain: [  7/50] Step 440/520 Loss 3.105 Prec@(1,5) (41.0%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:22] \u001b[32mTrain: [  7/50] Step 460/520 Loss 3.106 Prec@(1,5) (41.0%, 72.5%)\u001b[0m\n",
      "[2024-01-15 10:14:23] \u001b[32mTrain: [  7/50] Step 480/520 Loss 3.105 Prec@(1,5) (41.0%, 72.6%)\u001b[0m\n",
      "[2024-01-15 10:14:23] \u001b[32mTrain: [  7/50] Step 500/520 Loss 3.104 Prec@(1,5) (41.0%, 72.6%)\u001b[0m\n",
      "[2024-01-15 10:14:24] \u001b[32mTrain: [  7/50] Step 520/520 Loss 3.105 Prec@(1,5) (41.0%, 72.6%)\u001b[0m\n",
      "[2024-01-15 10:14:24] \u001b[32mTrain: [  7/50] Final Prec@1 41.0100%\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 000/104 Loss 2.894 Prec@(1,5) (42.7%, 69.8%)\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 020/104 Loss 2.896 Prec@(1,5) (41.2%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 040/104 Loss 2.868 Prec@(1,5) (41.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 060/104 Loss 2.869 Prec@(1,5) (41.9%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 080/104 Loss 2.884 Prec@(1,5) (41.3%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 100/104 Loss 2.881 Prec@(1,5) (41.2%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:14:28] \u001b[32mValid: [  7/50] Step 104/104 Loss 2.877 Prec@(1,5) (41.2%, 74.3%)\u001b[0m\n",
      "[2024-01-15 10:14:29] \u001b[32mValid: [  7/50] Final Prec@1 41.1600%\u001b[0m\n",
      "[2024-01-15 10:14:29] \u001b[32mEpoch 7 LR 0.023810\u001b[0m\n",
      "[2024-01-15 10:14:34] \u001b[32mTrain: [  8/50] Step 000/520 Loss 2.936 Prec@(1,5) (44.8%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:14:34] \u001b[32mTrain: [  8/50] Step 020/520 Loss 2.993 Prec@(1,5) (43.3%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:14:34] \u001b[32mTrain: [  8/50] Step 040/520 Loss 3.017 Prec@(1,5) (42.5%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:14:35] \u001b[32mTrain: [  8/50] Step 060/520 Loss 3.021 Prec@(1,5) (42.8%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:35] \u001b[32mTrain: [  8/50] Step 080/520 Loss 3.031 Prec@(1,5) (42.6%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:14:36] \u001b[32mTrain: [  8/50] Step 100/520 Loss 3.030 Prec@(1,5) (42.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:14:36] \u001b[32mTrain: [  8/50] Step 120/520 Loss 3.020 Prec@(1,5) (42.6%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:14:36] \u001b[32mTrain: [  8/50] Step 140/520 Loss 3.022 Prec@(1,5) (42.6%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:37] \u001b[32mTrain: [  8/50] Step 160/520 Loss 3.016 Prec@(1,5) (42.8%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:37] \u001b[32mTrain: [  8/50] Step 180/520 Loss 3.017 Prec@(1,5) (42.8%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:38] \u001b[32mTrain: [  8/50] Step 200/520 Loss 3.009 Prec@(1,5) (43.0%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:38] \u001b[32mTrain: [  8/50] Step 220/520 Loss 3.011 Prec@(1,5) (43.0%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:38] \u001b[32mTrain: [  8/50] Step 240/520 Loss 3.015 Prec@(1,5) (42.9%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:14:39] \u001b[32mTrain: [  8/50] Step 260/520 Loss 3.010 Prec@(1,5) (42.9%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:39] \u001b[32mTrain: [  8/50] Step 280/520 Loss 3.012 Prec@(1,5) (42.9%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:40] \u001b[32mTrain: [  8/50] Step 300/520 Loss 3.009 Prec@(1,5) (42.9%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:40] \u001b[32mTrain: [  8/50] Step 320/520 Loss 3.007 Prec@(1,5) (42.9%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:40] \u001b[32mTrain: [  8/50] Step 340/520 Loss 3.013 Prec@(1,5) (42.8%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:41] \u001b[32mTrain: [  8/50] Step 360/520 Loss 3.012 Prec@(1,5) (42.8%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:41] \u001b[32mTrain: [  8/50] Step 380/520 Loss 3.015 Prec@(1,5) (42.8%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:14:42] \u001b[32mTrain: [  8/50] Step 400/520 Loss 3.011 Prec@(1,5) (42.9%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:42] \u001b[32mTrain: [  8/50] Step 420/520 Loss 3.008 Prec@(1,5) (43.0%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:42] \u001b[32mTrain: [  8/50] Step 440/520 Loss 3.006 Prec@(1,5) (43.0%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:43] \u001b[32mTrain: [  8/50] Step 460/520 Loss 3.006 Prec@(1,5) (43.1%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:14:43] \u001b[32mTrain: [  8/50] Step 480/520 Loss 3.011 Prec@(1,5) (43.0%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:44] \u001b[32mTrain: [  8/50] Step 500/520 Loss 3.010 Prec@(1,5) (43.0%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:44] \u001b[32mTrain: [  8/50] Step 520/520 Loss 3.010 Prec@(1,5) (43.0%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:14:44] \u001b[32mTrain: [  8/50] Final Prec@1 42.9600%\u001b[0m\n",
      "[2024-01-15 10:14:48] \u001b[32mValid: [  8/50] Step 000/104 Loss 2.772 Prec@(1,5) (37.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:14:48] \u001b[32mValid: [  8/50] Step 020/104 Loss 2.546 Prec@(1,5) (44.3%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:14:48] \u001b[32mValid: [  8/50] Step 040/104 Loss 2.547 Prec@(1,5) (44.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:14:48] \u001b[32mValid: [  8/50] Step 060/104 Loss 2.536 Prec@(1,5) (44.5%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:14:49] \u001b[32mValid: [  8/50] Step 080/104 Loss 2.555 Prec@(1,5) (44.2%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:14:49] \u001b[32mValid: [  8/50] Step 100/104 Loss 2.544 Prec@(1,5) (44.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:14:49] \u001b[32mValid: [  8/50] Step 104/104 Loss 2.549 Prec@(1,5) (44.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:14:49] \u001b[32mValid: [  8/50] Final Prec@1 44.3200%\u001b[0m\n",
      "[2024-01-15 10:14:49] \u001b[32mEpoch 8 LR 0.023454\u001b[0m\n",
      "[2024-01-15 10:14:54] \u001b[32mTrain: [  9/50] Step 000/520 Loss 3.236 Prec@(1,5) (38.5%, 71.9%)\u001b[0m\n",
      "[2024-01-15 10:14:54] \u001b[32mTrain: [  9/50] Step 020/520 Loss 2.908 Prec@(1,5) (43.8%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:14:55] \u001b[32mTrain: [  9/50] Step 040/520 Loss 2.945 Prec@(1,5) (43.2%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:14:55] \u001b[32mTrain: [  9/50] Step 060/520 Loss 2.953 Prec@(1,5) (43.2%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:14:56] \u001b[32mTrain: [  9/50] Step 080/520 Loss 2.950 Prec@(1,5) (43.5%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:14:56] \u001b[32mTrain: [  9/50] Step 100/520 Loss 2.950 Prec@(1,5) (43.5%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:14:56] \u001b[32mTrain: [  9/50] Step 120/520 Loss 2.950 Prec@(1,5) (43.6%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:14:57] \u001b[32mTrain: [  9/50] Step 140/520 Loss 2.935 Prec@(1,5) (44.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:14:57] \u001b[32mTrain: [  9/50] Step 160/520 Loss 2.948 Prec@(1,5) (43.9%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:14:58] \u001b[32mTrain: [  9/50] Step 180/520 Loss 2.958 Prec@(1,5) (43.7%, 74.5%)\u001b[0m\n",
      "[2024-01-15 10:14:58] \u001b[32mTrain: [  9/50] Step 200/520 Loss 2.959 Prec@(1,5) (43.7%, 74.6%)\u001b[0m\n",
      "[2024-01-15 10:14:58] \u001b[32mTrain: [  9/50] Step 220/520 Loss 2.950 Prec@(1,5) (43.8%, 74.6%)\u001b[0m\n",
      "[2024-01-15 10:14:59] \u001b[32mTrain: [  9/50] Step 240/520 Loss 2.946 Prec@(1,5) (43.8%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:14:59] \u001b[32mTrain: [  9/50] Step 260/520 Loss 2.945 Prec@(1,5) (43.8%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:14:59] \u001b[32mTrain: [  9/50] Step 280/520 Loss 2.946 Prec@(1,5) (43.8%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:15:00] \u001b[32mTrain: [  9/50] Step 300/520 Loss 2.945 Prec@(1,5) (43.9%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:00] \u001b[32mTrain: [  9/50] Step 320/520 Loss 2.942 Prec@(1,5) (44.0%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:01] \u001b[32mTrain: [  9/50] Step 340/520 Loss 2.941 Prec@(1,5) (44.1%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:15:01] \u001b[32mTrain: [  9/50] Step 360/520 Loss 2.934 Prec@(1,5) (44.2%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:01] \u001b[32mTrain: [  9/50] Step 380/520 Loss 2.937 Prec@(1,5) (44.1%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:15:02] \u001b[32mTrain: [  9/50] Step 400/520 Loss 2.938 Prec@(1,5) (44.1%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:15:02] \u001b[32mTrain: [  9/50] Step 420/520 Loss 2.937 Prec@(1,5) (44.1%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:03] \u001b[32mTrain: [  9/50] Step 440/520 Loss 2.938 Prec@(1,5) (44.0%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:03] \u001b[32mTrain: [  9/50] Step 460/520 Loss 2.935 Prec@(1,5) (44.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:15:03] \u001b[32mTrain: [  9/50] Step 480/520 Loss 2.937 Prec@(1,5) (44.1%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:04] \u001b[32mTrain: [  9/50] Step 500/520 Loss 2.941 Prec@(1,5) (43.9%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:04] \u001b[32mTrain: [  9/50] Step 520/520 Loss 2.941 Prec@(1,5) (44.0%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:04] \u001b[32mTrain: [  9/50] Final Prec@1 43.9580%\u001b[0m\n",
      "[2024-01-15 10:15:08] \u001b[32mValid: [  9/50] Step 000/104 Loss 2.572 Prec@(1,5) (49.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:08] \u001b[32mValid: [  9/50] Step 020/104 Loss 2.729 Prec@(1,5) (44.7%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:08] \u001b[32mValid: [  9/50] Step 040/104 Loss 2.709 Prec@(1,5) (44.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:15:09] \u001b[32mValid: [  9/50] Step 060/104 Loss 2.670 Prec@(1,5) (45.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:15:09] \u001b[32mValid: [  9/50] Step 080/104 Loss 2.676 Prec@(1,5) (44.4%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:15:09] \u001b[32mValid: [  9/50] Step 100/104 Loss 2.673 Prec@(1,5) (44.5%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:15:09] \u001b[32mValid: [  9/50] Step 104/104 Loss 2.677 Prec@(1,5) (44.5%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:15:09] \u001b[32mValid: [  9/50] Final Prec@1 44.4700%\u001b[0m\n",
      "[2024-01-15 10:15:09] \u001b[32mEpoch 9 LR 0.023054\u001b[0m\n",
      "[2024-01-15 10:15:14] \u001b[32mTrain: [ 10/50] Step 000/520 Loss 2.965 Prec@(1,5) (45.8%, 74.0%)\u001b[0m\n",
      "[2024-01-15 10:15:14] \u001b[32mTrain: [ 10/50] Step 020/520 Loss 2.969 Prec@(1,5) (43.4%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:15:15] \u001b[32mTrain: [ 10/50] Step 040/520 Loss 2.973 Prec@(1,5) (43.6%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:15:15] \u001b[32mTrain: [ 10/50] Step 060/520 Loss 2.921 Prec@(1,5) (44.3%, 74.5%)\u001b[0m\n",
      "[2024-01-15 10:15:16] \u001b[32mTrain: [ 10/50] Step 080/520 Loss 2.910 Prec@(1,5) (44.7%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:16] \u001b[32mTrain: [ 10/50] Step 100/520 Loss 2.889 Prec@(1,5) (44.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:16] \u001b[32mTrain: [ 10/50] Step 120/520 Loss 2.905 Prec@(1,5) (44.6%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:17] \u001b[32mTrain: [ 10/50] Step 140/520 Loss 2.897 Prec@(1,5) (44.8%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:15:17] \u001b[32mTrain: [ 10/50] Step 160/520 Loss 2.901 Prec@(1,5) (44.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:18] \u001b[32mTrain: [ 10/50] Step 180/520 Loss 2.899 Prec@(1,5) (44.8%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:18] \u001b[32mTrain: [ 10/50] Step 200/520 Loss 2.897 Prec@(1,5) (44.9%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:18] \u001b[32mTrain: [ 10/50] Step 220/520 Loss 2.901 Prec@(1,5) (44.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:15:19] \u001b[32mTrain: [ 10/50] Step 240/520 Loss 2.898 Prec@(1,5) (44.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:15:19] \u001b[32mTrain: [ 10/50] Step 260/520 Loss 2.904 Prec@(1,5) (44.9%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:20] \u001b[32mTrain: [ 10/50] Step 280/520 Loss 2.901 Prec@(1,5) (44.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:15:20] \u001b[32mTrain: [ 10/50] Step 300/520 Loss 2.903 Prec@(1,5) (44.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:15:20] \u001b[32mTrain: [ 10/50] Step 320/520 Loss 2.894 Prec@(1,5) (45.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:21] \u001b[32mTrain: [ 10/50] Step 340/520 Loss 2.890 Prec@(1,5) (45.1%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:21] \u001b[32mTrain: [ 10/50] Step 360/520 Loss 2.882 Prec@(1,5) (45.3%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:15:22] \u001b[32mTrain: [ 10/50] Step 380/520 Loss 2.883 Prec@(1,5) (45.2%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:15:22] \u001b[32mTrain: [ 10/50] Step 400/520 Loss 2.879 Prec@(1,5) (45.3%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:22] \u001b[32mTrain: [ 10/50] Step 420/520 Loss 2.881 Prec@(1,5) (45.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:23] \u001b[32mTrain: [ 10/50] Step 440/520 Loss 2.882 Prec@(1,5) (45.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:23] \u001b[32mTrain: [ 10/50] Step 460/520 Loss 2.884 Prec@(1,5) (45.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:24] \u001b[32mTrain: [ 10/50] Step 480/520 Loss 2.884 Prec@(1,5) (45.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:24] \u001b[32mTrain: [ 10/50] Step 500/520 Loss 2.884 Prec@(1,5) (45.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:24] \u001b[32mTrain: [ 10/50] Step 520/520 Loss 2.885 Prec@(1,5) (45.1%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:25] \u001b[32mTrain: [ 10/50] Final Prec@1 45.1440%\u001b[0m\n",
      "[2024-01-15 10:15:28] \u001b[32mValid: [ 10/50] Step 000/104 Loss 2.944 Prec@(1,5) (45.8%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Step 020/104 Loss 3.021 Prec@(1,5) (42.2%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Step 040/104 Loss 2.999 Prec@(1,5) (42.1%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Step 060/104 Loss 2.974 Prec@(1,5) (42.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Step 080/104 Loss 2.973 Prec@(1,5) (42.6%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Step 100/104 Loss 2.967 Prec@(1,5) (42.6%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Step 104/104 Loss 2.970 Prec@(1,5) (42.6%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mValid: [ 10/50] Final Prec@1 42.5500%\u001b[0m\n",
      "[2024-01-15 10:15:29] \u001b[32mEpoch 10 LR 0.022613\u001b[0m\n",
      "[2024-01-15 10:15:34] \u001b[32mTrain: [ 11/50] Step 000/520 Loss 2.490 Prec@(1,5) (51.0%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:15:35] \u001b[32mTrain: [ 11/50] Step 020/520 Loss 2.773 Prec@(1,5) (47.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:35] \u001b[32mTrain: [ 11/50] Step 040/520 Loss 2.830 Prec@(1,5) (46.6%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:36] \u001b[32mTrain: [ 11/50] Step 060/520 Loss 2.832 Prec@(1,5) (46.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:15:36] \u001b[32mTrain: [ 11/50] Step 080/520 Loss 2.833 Prec@(1,5) (46.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:15:36] \u001b[32mTrain: [ 11/50] Step 100/520 Loss 2.844 Prec@(1,5) (46.1%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:15:37] \u001b[32mTrain: [ 11/50] Step 120/520 Loss 2.843 Prec@(1,5) (45.8%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:15:37] \u001b[32mTrain: [ 11/50] Step 140/520 Loss 2.842 Prec@(1,5) (45.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:15:38] \u001b[32mTrain: [ 11/50] Step 160/520 Loss 2.848 Prec@(1,5) (45.8%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:15:38] \u001b[32mTrain: [ 11/50] Step 180/520 Loss 2.834 Prec@(1,5) (46.0%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:15:38] \u001b[32mTrain: [ 11/50] Step 200/520 Loss 2.838 Prec@(1,5) (46.1%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:15:39] \u001b[32mTrain: [ 11/50] Step 220/520 Loss 2.837 Prec@(1,5) (46.1%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:15:39] \u001b[32mTrain: [ 11/50] Step 240/520 Loss 2.829 Prec@(1,5) (46.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:40] \u001b[32mTrain: [ 11/50] Step 260/520 Loss 2.827 Prec@(1,5) (46.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:40] \u001b[32mTrain: [ 11/50] Step 280/520 Loss 2.826 Prec@(1,5) (46.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:40] \u001b[32mTrain: [ 11/50] Step 300/520 Loss 2.824 Prec@(1,5) (46.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:15:41] \u001b[32mTrain: [ 11/50] Step 320/520 Loss 2.828 Prec@(1,5) (46.2%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:41] \u001b[32mTrain: [ 11/50] Step 340/520 Loss 2.824 Prec@(1,5) (46.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:42] \u001b[32mTrain: [ 11/50] Step 360/520 Loss 2.826 Prec@(1,5) (46.3%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:42] \u001b[32mTrain: [ 11/50] Step 380/520 Loss 2.825 Prec@(1,5) (46.2%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:42] \u001b[32mTrain: [ 11/50] Step 400/520 Loss 2.831 Prec@(1,5) (46.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:43] \u001b[32mTrain: [ 11/50] Step 420/520 Loss 2.829 Prec@(1,5) (46.1%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:15:43] \u001b[32mTrain: [ 11/50] Step 440/520 Loss 2.830 Prec@(1,5) (46.1%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:15:44] \u001b[32mTrain: [ 11/50] Step 460/520 Loss 2.828 Prec@(1,5) (46.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:44] \u001b[32mTrain: [ 11/50] Step 480/520 Loss 2.828 Prec@(1,5) (46.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:15:44] \u001b[32mTrain: [ 11/50] Step 500/520 Loss 2.831 Prec@(1,5) (45.9%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:15:45] \u001b[32mTrain: [ 11/50] Step 520/520 Loss 2.835 Prec@(1,5) (45.9%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:15:45] \u001b[32mTrain: [ 11/50] Final Prec@1 45.8840%\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 000/104 Loss 2.890 Prec@(1,5) (49.0%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 020/104 Loss 2.675 Prec@(1,5) (45.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 040/104 Loss 2.663 Prec@(1,5) (45.7%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 060/104 Loss 2.604 Prec@(1,5) (46.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 080/104 Loss 2.615 Prec@(1,5) (46.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 100/104 Loss 2.602 Prec@(1,5) (46.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:15:49] \u001b[32mValid: [ 11/50] Step 104/104 Loss 2.605 Prec@(1,5) (46.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:15:50] \u001b[32mValid: [ 11/50] Final Prec@1 45.9700%\u001b[0m\n",
      "[2024-01-15 10:15:50] \u001b[32mEpoch 11 LR 0.022132\u001b[0m\n",
      "[2024-01-15 10:15:55] \u001b[32mTrain: [ 12/50] Step 000/520 Loss 2.727 Prec@(1,5) (43.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:15:55] \u001b[32mTrain: [ 12/50] Step 020/520 Loss 2.808 Prec@(1,5) (46.5%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:15:55] \u001b[32mTrain: [ 12/50] Step 040/520 Loss 2.774 Prec@(1,5) (47.8%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:15:56] \u001b[32mTrain: [ 12/50] Step 060/520 Loss 2.777 Prec@(1,5) (47.6%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:15:56] \u001b[32mTrain: [ 12/50] Step 080/520 Loss 2.791 Prec@(1,5) (47.1%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:15:57] \u001b[32mTrain: [ 12/50] Step 100/520 Loss 2.782 Prec@(1,5) (47.2%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:15:57] \u001b[32mTrain: [ 12/50] Step 120/520 Loss 2.783 Prec@(1,5) (47.0%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:15:57] \u001b[32mTrain: [ 12/50] Step 140/520 Loss 2.782 Prec@(1,5) (46.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:15:58] \u001b[32mTrain: [ 12/50] Step 160/520 Loss 2.771 Prec@(1,5) (46.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:15:58] \u001b[32mTrain: [ 12/50] Step 180/520 Loss 2.766 Prec@(1,5) (46.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:15:59] \u001b[32mTrain: [ 12/50] Step 200/520 Loss 2.766 Prec@(1,5) (47.1%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:15:59] \u001b[32mTrain: [ 12/50] Step 220/520 Loss 2.762 Prec@(1,5) (47.1%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:15:59] \u001b[32mTrain: [ 12/50] Step 240/520 Loss 2.768 Prec@(1,5) (47.0%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:00] \u001b[32mTrain: [ 12/50] Step 260/520 Loss 2.769 Prec@(1,5) (47.0%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:00] \u001b[32mTrain: [ 12/50] Step 280/520 Loss 2.770 Prec@(1,5) (47.0%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:01] \u001b[32mTrain: [ 12/50] Step 300/520 Loss 2.767 Prec@(1,5) (47.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:16:01] \u001b[32mTrain: [ 12/50] Step 320/520 Loss 2.765 Prec@(1,5) (46.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:16:01] \u001b[32mTrain: [ 12/50] Step 340/520 Loss 2.768 Prec@(1,5) (46.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:16:02] \u001b[32mTrain: [ 12/50] Step 360/520 Loss 2.770 Prec@(1,5) (46.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:02] \u001b[32mTrain: [ 12/50] Step 380/520 Loss 2.767 Prec@(1,5) (47.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:16:03] \u001b[32mTrain: [ 12/50] Step 400/520 Loss 2.767 Prec@(1,5) (46.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:16:03] \u001b[32mTrain: [ 12/50] Step 420/520 Loss 2.770 Prec@(1,5) (46.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:03] \u001b[32mTrain: [ 12/50] Step 440/520 Loss 2.773 Prec@(1,5) (46.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:04] \u001b[32mTrain: [ 12/50] Step 460/520 Loss 2.775 Prec@(1,5) (46.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:04] \u001b[32mTrain: [ 12/50] Step 480/520 Loss 2.773 Prec@(1,5) (46.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:05] \u001b[32mTrain: [ 12/50] Step 500/520 Loss 2.774 Prec@(1,5) (46.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:05] \u001b[32mTrain: [ 12/50] Step 520/520 Loss 2.771 Prec@(1,5) (46.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:05] \u001b[32mTrain: [ 12/50] Final Prec@1 46.9480%\u001b[0m\n",
      "[2024-01-15 10:16:09] \u001b[32mValid: [ 12/50] Step 000/104 Loss 2.609 Prec@(1,5) (55.2%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:16:09] \u001b[32mValid: [ 12/50] Step 020/104 Loss 2.493 Prec@(1,5) (48.9%, 78.4%)\u001b[0m\n",
      "[2024-01-15 10:16:09] \u001b[32mValid: [ 12/50] Step 040/104 Loss 2.473 Prec@(1,5) (47.9%, 78.5%)\u001b[0m\n",
      "[2024-01-15 10:16:09] \u001b[32mValid: [ 12/50] Step 060/104 Loss 2.451 Prec@(1,5) (48.7%, 78.7%)\u001b[0m\n",
      "[2024-01-15 10:16:10] \u001b[32mValid: [ 12/50] Step 080/104 Loss 2.455 Prec@(1,5) (48.4%, 78.7%)\u001b[0m\n",
      "[2024-01-15 10:16:10] \u001b[32mValid: [ 12/50] Step 100/104 Loss 2.448 Prec@(1,5) (48.6%, 78.7%)\u001b[0m\n",
      "[2024-01-15 10:16:10] \u001b[32mValid: [ 12/50] Step 104/104 Loss 2.448 Prec@(1,5) (48.5%, 78.8%)\u001b[0m\n",
      "[2024-01-15 10:16:10] \u001b[32mValid: [ 12/50] Final Prec@1 48.5000%\u001b[0m\n",
      "[2024-01-15 10:16:10] \u001b[32mEpoch 12 LR 0.021612\u001b[0m\n",
      "[2024-01-15 10:16:15] \u001b[32mTrain: [ 13/50] Step 000/520 Loss 2.846 Prec@(1,5) (44.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:16:15] \u001b[32mTrain: [ 13/50] Step 020/520 Loss 2.706 Prec@(1,5) (48.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:16:16] \u001b[32mTrain: [ 13/50] Step 040/520 Loss 2.731 Prec@(1,5) (47.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:16:16] \u001b[32mTrain: [ 13/50] Step 060/520 Loss 2.727 Prec@(1,5) (47.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:16:17] \u001b[32mTrain: [ 13/50] Step 080/520 Loss 2.737 Prec@(1,5) (47.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:16:17] \u001b[32mTrain: [ 13/50] Step 100/520 Loss 2.723 Prec@(1,5) (47.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:17] \u001b[32mTrain: [ 13/50] Step 120/520 Loss 2.753 Prec@(1,5) (47.2%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:18] \u001b[32mTrain: [ 13/50] Step 140/520 Loss 2.755 Prec@(1,5) (47.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:18] \u001b[32mTrain: [ 13/50] Step 160/520 Loss 2.762 Prec@(1,5) (47.3%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:16:19] \u001b[32mTrain: [ 13/50] Step 180/520 Loss 2.759 Prec@(1,5) (47.3%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:16:19] \u001b[32mTrain: [ 13/50] Step 200/520 Loss 2.755 Prec@(1,5) (47.5%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:19] \u001b[32mTrain: [ 13/50] Step 220/520 Loss 2.749 Prec@(1,5) (47.5%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:16:20] \u001b[32mTrain: [ 13/50] Step 240/520 Loss 2.745 Prec@(1,5) (47.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:16:20] \u001b[32mTrain: [ 13/50] Step 260/520 Loss 2.747 Prec@(1,5) (47.6%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:21] \u001b[32mTrain: [ 13/50] Step 280/520 Loss 2.752 Prec@(1,5) (47.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:16:21] \u001b[32mTrain: [ 13/50] Step 300/520 Loss 2.755 Prec@(1,5) (47.6%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:16:21] \u001b[32mTrain: [ 13/50] Step 320/520 Loss 2.754 Prec@(1,5) (47.7%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:22] \u001b[32mTrain: [ 13/50] Step 340/520 Loss 2.754 Prec@(1,5) (47.7%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:16:22] \u001b[32mTrain: [ 13/50] Step 360/520 Loss 2.757 Prec@(1,5) (47.6%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:23] \u001b[32mTrain: [ 13/50] Step 380/520 Loss 2.760 Prec@(1,5) (47.5%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:23] \u001b[32mTrain: [ 13/50] Step 400/520 Loss 2.762 Prec@(1,5) (47.4%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:23] \u001b[32mTrain: [ 13/50] Step 420/520 Loss 2.762 Prec@(1,5) (47.5%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:24] \u001b[32mTrain: [ 13/50] Step 440/520 Loss 2.764 Prec@(1,5) (47.4%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:24] \u001b[32mTrain: [ 13/50] Step 460/520 Loss 2.766 Prec@(1,5) (47.4%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:25] \u001b[32mTrain: [ 13/50] Step 480/520 Loss 2.769 Prec@(1,5) (47.3%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:16:25] \u001b[32mTrain: [ 13/50] Step 500/520 Loss 2.769 Prec@(1,5) (47.3%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:16:25] \u001b[32mTrain: [ 13/50] Step 520/520 Loss 2.765 Prec@(1,5) (47.4%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:16:26] \u001b[32mTrain: [ 13/50] Final Prec@1 47.4060%\u001b[0m\n",
      "[2024-01-15 10:16:29] \u001b[32mValid: [ 13/50] Step 000/104 Loss 2.473 Prec@(1,5) (52.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Step 020/104 Loss 2.393 Prec@(1,5) (49.8%, 80.0%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Step 040/104 Loss 2.399 Prec@(1,5) (48.5%, 79.8%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Step 060/104 Loss 2.374 Prec@(1,5) (48.9%, 79.9%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Step 080/104 Loss 2.374 Prec@(1,5) (48.6%, 80.1%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Step 100/104 Loss 2.371 Prec@(1,5) (48.5%, 80.1%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Step 104/104 Loss 2.371 Prec@(1,5) (48.5%, 80.1%)\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mValid: [ 13/50] Final Prec@1 48.5100%\u001b[0m\n",
      "[2024-01-15 10:16:30] \u001b[32mEpoch 13 LR 0.021057\u001b[0m\n",
      "[2024-01-15 10:16:35] \u001b[32mTrain: [ 14/50] Step 000/520 Loss 2.950 Prec@(1,5) (42.7%, 70.8%)\u001b[0m\n",
      "[2024-01-15 10:16:36] \u001b[32mTrain: [ 14/50] Step 020/520 Loss 2.727 Prec@(1,5) (47.7%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:16:36] \u001b[32mTrain: [ 14/50] Step 040/520 Loss 2.683 Prec@(1,5) (48.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:37] \u001b[32mTrain: [ 14/50] Step 060/520 Loss 2.692 Prec@(1,5) (48.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:37] \u001b[32mTrain: [ 14/50] Step 080/520 Loss 2.682 Prec@(1,5) (48.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:16:37] \u001b[32mTrain: [ 14/50] Step 100/520 Loss 2.705 Prec@(1,5) (48.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:16:38] \u001b[32mTrain: [ 14/50] Step 120/520 Loss 2.686 Prec@(1,5) (48.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:38] \u001b[32mTrain: [ 14/50] Step 140/520 Loss 2.685 Prec@(1,5) (48.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:39] \u001b[32mTrain: [ 14/50] Step 160/520 Loss 2.683 Prec@(1,5) (48.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:39] \u001b[32mTrain: [ 14/50] Step 180/520 Loss 2.687 Prec@(1,5) (48.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:39] \u001b[32mTrain: [ 14/50] Step 200/520 Loss 2.704 Prec@(1,5) (48.6%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:16:40] \u001b[32mTrain: [ 14/50] Step 220/520 Loss 2.710 Prec@(1,5) (48.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:16:40] \u001b[32mTrain: [ 14/50] Step 240/520 Loss 2.710 Prec@(1,5) (48.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:16:41] \u001b[32mTrain: [ 14/50] Step 260/520 Loss 2.707 Prec@(1,5) (48.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:41] \u001b[32mTrain: [ 14/50] Step 280/520 Loss 2.709 Prec@(1,5) (48.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:16:41] \u001b[32mTrain: [ 14/50] Step 300/520 Loss 2.711 Prec@(1,5) (48.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:42] \u001b[32mTrain: [ 14/50] Step 320/520 Loss 2.706 Prec@(1,5) (48.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:42] \u001b[32mTrain: [ 14/50] Step 340/520 Loss 2.705 Prec@(1,5) (48.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:43] \u001b[32mTrain: [ 14/50] Step 360/520 Loss 2.706 Prec@(1,5) (48.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:43] \u001b[32mTrain: [ 14/50] Step 380/520 Loss 2.704 Prec@(1,5) (48.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:16:43] \u001b[32mTrain: [ 14/50] Step 400/520 Loss 2.707 Prec@(1,5) (48.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:44] \u001b[32mTrain: [ 14/50] Step 420/520 Loss 2.712 Prec@(1,5) (48.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:16:44] \u001b[32mTrain: [ 14/50] Step 440/520 Loss 2.714 Prec@(1,5) (48.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:16:45] \u001b[32mTrain: [ 14/50] Step 460/520 Loss 2.719 Prec@(1,5) (48.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:16:45] \u001b[32mTrain: [ 14/50] Step 480/520 Loss 2.725 Prec@(1,5) (48.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:16:45] \u001b[32mTrain: [ 14/50] Step 500/520 Loss 2.729 Prec@(1,5) (48.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:16:46] \u001b[32mTrain: [ 14/50] Step 520/520 Loss 2.727 Prec@(1,5) (48.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:16:46] \u001b[32mTrain: [ 14/50] Final Prec@1 48.0400%\u001b[0m\n",
      "[2024-01-15 10:16:50] \u001b[32mValid: [ 14/50] Step 000/104 Loss 2.331 Prec@(1,5) (54.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:16:50] \u001b[32mValid: [ 14/50] Step 020/104 Loss 2.335 Prec@(1,5) (49.6%, 80.9%)\u001b[0m\n",
      "[2024-01-15 10:16:50] \u001b[32mValid: [ 14/50] Step 040/104 Loss 2.295 Prec@(1,5) (49.4%, 81.3%)\u001b[0m\n",
      "[2024-01-15 10:16:50] \u001b[32mValid: [ 14/50] Step 060/104 Loss 2.255 Prec@(1,5) (49.9%, 81.4%)\u001b[0m\n",
      "[2024-01-15 10:16:51] \u001b[32mValid: [ 14/50] Step 080/104 Loss 2.257 Prec@(1,5) (49.5%, 81.5%)\u001b[0m\n",
      "[2024-01-15 10:16:51] \u001b[32mValid: [ 14/50] Step 100/104 Loss 2.254 Prec@(1,5) (49.4%, 81.4%)\u001b[0m\n",
      "[2024-01-15 10:16:51] \u001b[32mValid: [ 14/50] Step 104/104 Loss 2.257 Prec@(1,5) (49.4%, 81.3%)\u001b[0m\n",
      "[2024-01-15 10:16:51] \u001b[32mValid: [ 14/50] Final Prec@1 49.3800%\u001b[0m\n",
      "[2024-01-15 10:16:51] \u001b[32mEpoch 14 LR 0.020468\u001b[0m\n",
      "[2024-01-15 10:16:56] \u001b[32mTrain: [ 15/50] Step 000/520 Loss 2.917 Prec@(1,5) (51.0%, 67.7%)\u001b[0m\n",
      "[2024-01-15 10:16:56] \u001b[32mTrain: [ 15/50] Step 020/520 Loss 2.657 Prec@(1,5) (49.8%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:16:57] \u001b[32mTrain: [ 15/50] Step 040/520 Loss 2.671 Prec@(1,5) (49.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:16:57] \u001b[32mTrain: [ 15/50] Step 060/520 Loss 2.662 Prec@(1,5) (49.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:16:58] \u001b[32mTrain: [ 15/50] Step 080/520 Loss 2.666 Prec@(1,5) (49.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:16:58] \u001b[32mTrain: [ 15/50] Step 100/520 Loss 2.659 Prec@(1,5) (49.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:16:58] \u001b[32mTrain: [ 15/50] Step 120/520 Loss 2.654 Prec@(1,5) (49.3%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:16:59] \u001b[32mTrain: [ 15/50] Step 140/520 Loss 2.660 Prec@(1,5) (49.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:16:59] \u001b[32mTrain: [ 15/50] Step 160/520 Loss 2.659 Prec@(1,5) (49.3%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:00] \u001b[32mTrain: [ 15/50] Step 180/520 Loss 2.664 Prec@(1,5) (49.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:00] \u001b[32mTrain: [ 15/50] Step 200/520 Loss 2.663 Prec@(1,5) (49.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:00] \u001b[32mTrain: [ 15/50] Step 220/520 Loss 2.671 Prec@(1,5) (49.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:01] \u001b[32mTrain: [ 15/50] Step 240/520 Loss 2.675 Prec@(1,5) (49.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:01] \u001b[32mTrain: [ 15/50] Step 260/520 Loss 2.674 Prec@(1,5) (49.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:02] \u001b[32mTrain: [ 15/50] Step 280/520 Loss 2.669 Prec@(1,5) (49.2%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:02] \u001b[32mTrain: [ 15/50] Step 300/520 Loss 2.676 Prec@(1,5) (49.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:02] \u001b[32mTrain: [ 15/50] Step 320/520 Loss 2.682 Prec@(1,5) (49.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:17:03] \u001b[32mTrain: [ 15/50] Step 340/520 Loss 2.688 Prec@(1,5) (48.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:17:03] \u001b[32mTrain: [ 15/50] Step 360/520 Loss 2.695 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:04] \u001b[32mTrain: [ 15/50] Step 380/520 Loss 2.699 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:04] \u001b[32mTrain: [ 15/50] Step 400/520 Loss 2.700 Prec@(1,5) (48.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:17:04] \u001b[32mTrain: [ 15/50] Step 420/520 Loss 2.698 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:05] \u001b[32mTrain: [ 15/50] Step 440/520 Loss 2.695 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:05] \u001b[32mTrain: [ 15/50] Step 460/520 Loss 2.692 Prec@(1,5) (48.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:17:06] \u001b[32mTrain: [ 15/50] Step 480/520 Loss 2.697 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:06] \u001b[32mTrain: [ 15/50] Step 500/520 Loss 2.696 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:06] \u001b[32mTrain: [ 15/50] Step 520/520 Loss 2.696 Prec@(1,5) (48.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:07] \u001b[32mTrain: [ 15/50] Final Prec@1 48.7080%\u001b[0m\n",
      "[2024-01-15 10:17:10] \u001b[32mValid: [ 15/50] Step 000/104 Loss 2.474 Prec@(1,5) (51.0%, 79.2%)\u001b[0m\n",
      "[2024-01-15 10:17:10] \u001b[32mValid: [ 15/50] Step 020/104 Loss 2.303 Prec@(1,5) (49.7%, 81.6%)\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mValid: [ 15/50] Step 040/104 Loss 2.283 Prec@(1,5) (49.6%, 81.0%)\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mValid: [ 15/50] Step 060/104 Loss 2.256 Prec@(1,5) (50.0%, 81.0%)\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mValid: [ 15/50] Step 080/104 Loss 2.258 Prec@(1,5) (50.0%, 80.9%)\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mValid: [ 15/50] Step 100/104 Loss 2.260 Prec@(1,5) (49.9%, 80.9%)\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mValid: [ 15/50] Step 104/104 Loss 2.258 Prec@(1,5) (50.0%, 80.8%)\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mValid: [ 15/50] Final Prec@1 49.9800%\u001b[0m\n",
      "[2024-01-15 10:17:11] \u001b[32mEpoch 15 LR 0.019848\u001b[0m\n",
      "[2024-01-15 10:17:16] \u001b[32mTrain: [ 16/50] Step 000/520 Loss 2.838 Prec@(1,5) (43.8%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:17:17] \u001b[32mTrain: [ 16/50] Step 020/520 Loss 2.583 Prec@(1,5) (49.9%, 79.4%)\u001b[0m\n",
      "[2024-01-15 10:17:17] \u001b[32mTrain: [ 16/50] Step 040/520 Loss 2.593 Prec@(1,5) (50.3%, 79.4%)\u001b[0m\n",
      "[2024-01-15 10:17:17] \u001b[32mTrain: [ 16/50] Step 060/520 Loss 2.617 Prec@(1,5) (50.7%, 78.4%)\u001b[0m\n",
      "[2024-01-15 10:17:18] \u001b[32mTrain: [ 16/50] Step 080/520 Loss 2.634 Prec@(1,5) (50.2%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:17:18] \u001b[32mTrain: [ 16/50] Step 100/520 Loss 2.645 Prec@(1,5) (50.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:17:19] \u001b[32mTrain: [ 16/50] Step 120/520 Loss 2.655 Prec@(1,5) (49.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:17:19] \u001b[32mTrain: [ 16/50] Step 140/520 Loss 2.644 Prec@(1,5) (49.8%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:17:19] \u001b[32mTrain: [ 16/50] Step 160/520 Loss 2.650 Prec@(1,5) (49.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:17:20] \u001b[32mTrain: [ 16/50] Step 180/520 Loss 2.645 Prec@(1,5) (50.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:17:20] \u001b[32mTrain: [ 16/50] Step 200/520 Loss 2.650 Prec@(1,5) (49.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:21] \u001b[32mTrain: [ 16/50] Step 220/520 Loss 2.649 Prec@(1,5) (49.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:17:21] \u001b[32mTrain: [ 16/50] Step 240/520 Loss 2.649 Prec@(1,5) (49.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:21] \u001b[32mTrain: [ 16/50] Step 260/520 Loss 2.647 Prec@(1,5) (49.6%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:17:22] \u001b[32mTrain: [ 16/50] Step 280/520 Loss 2.654 Prec@(1,5) (49.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:22] \u001b[32mTrain: [ 16/50] Step 300/520 Loss 2.657 Prec@(1,5) (49.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:23] \u001b[32mTrain: [ 16/50] Step 320/520 Loss 2.664 Prec@(1,5) (49.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:23] \u001b[32mTrain: [ 16/50] Step 340/520 Loss 2.669 Prec@(1,5) (49.2%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:23] \u001b[32mTrain: [ 16/50] Step 360/520 Loss 2.669 Prec@(1,5) (49.1%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:24] \u001b[32mTrain: [ 16/50] Step 380/520 Loss 2.672 Prec@(1,5) (49.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:24] \u001b[32mTrain: [ 16/50] Step 400/520 Loss 2.670 Prec@(1,5) (49.1%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:25] \u001b[32mTrain: [ 16/50] Step 420/520 Loss 2.669 Prec@(1,5) (49.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:25] \u001b[32mTrain: [ 16/50] Step 440/520 Loss 2.673 Prec@(1,5) (49.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:25] \u001b[32mTrain: [ 16/50] Step 460/520 Loss 2.674 Prec@(1,5) (49.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:26] \u001b[32mTrain: [ 16/50] Step 480/520 Loss 2.676 Prec@(1,5) (49.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:26] \u001b[32mTrain: [ 16/50] Step 500/520 Loss 2.679 Prec@(1,5) (48.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:27] \u001b[32mTrain: [ 16/50] Step 520/520 Loss 2.680 Prec@(1,5) (48.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:27] \u001b[32mTrain: [ 16/50] Final Prec@1 48.9140%\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 000/104 Loss 2.415 Prec@(1,5) (56.2%, 79.2%)\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 020/104 Loss 2.302 Prec@(1,5) (51.6%, 81.3%)\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 040/104 Loss 2.287 Prec@(1,5) (50.6%, 80.8%)\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 060/104 Loss 2.265 Prec@(1,5) (50.8%, 80.8%)\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 080/104 Loss 2.258 Prec@(1,5) (50.6%, 80.9%)\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 100/104 Loss 2.243 Prec@(1,5) (50.8%, 81.0%)\u001b[0m\n",
      "[2024-01-15 10:17:31] \u001b[32mValid: [ 16/50] Step 104/104 Loss 2.240 Prec@(1,5) (50.8%, 81.0%)\u001b[0m\n",
      "[2024-01-15 10:17:32] \u001b[32mValid: [ 16/50] Final Prec@1 50.8400%\u001b[0m\n",
      "[2024-01-15 10:17:32] \u001b[32mEpoch 16 LR 0.019198\u001b[0m\n",
      "[2024-01-15 10:17:37] \u001b[32mTrain: [ 17/50] Step 000/520 Loss 2.428 Prec@(1,5) (55.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:17:37] \u001b[32mTrain: [ 17/50] Step 020/520 Loss 2.694 Prec@(1,5) (49.0%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:17:37] \u001b[32mTrain: [ 17/50] Step 040/520 Loss 2.710 Prec@(1,5) (48.5%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:17:38] \u001b[32mTrain: [ 17/50] Step 060/520 Loss 2.693 Prec@(1,5) (49.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:17:38] \u001b[32mTrain: [ 17/50] Step 080/520 Loss 2.674 Prec@(1,5) (49.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:17:39] \u001b[32mTrain: [ 17/50] Step 100/520 Loss 2.650 Prec@(1,5) (50.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:39] \u001b[32mTrain: [ 17/50] Step 120/520 Loss 2.648 Prec@(1,5) (50.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:39] \u001b[32mTrain: [ 17/50] Step 140/520 Loss 2.637 Prec@(1,5) (50.3%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:40] \u001b[32mTrain: [ 17/50] Step 160/520 Loss 2.635 Prec@(1,5) (50.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:40] \u001b[32mTrain: [ 17/50] Step 180/520 Loss 2.639 Prec@(1,5) (50.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:41] \u001b[32mTrain: [ 17/50] Step 200/520 Loss 2.649 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:41] \u001b[32mTrain: [ 17/50] Step 220/520 Loss 2.657 Prec@(1,5) (49.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:41] \u001b[32mTrain: [ 17/50] Step 240/520 Loss 2.649 Prec@(1,5) (49.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:42] \u001b[32mTrain: [ 17/50] Step 260/520 Loss 2.652 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:42] \u001b[32mTrain: [ 17/50] Step 280/520 Loss 2.660 Prec@(1,5) (49.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:42] \u001b[32mTrain: [ 17/50] Step 300/520 Loss 2.654 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:43] \u001b[32mTrain: [ 17/50] Step 320/520 Loss 2.653 Prec@(1,5) (49.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:17:43] \u001b[32mTrain: [ 17/50] Step 340/520 Loss 2.654 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:17:44] \u001b[32mTrain: [ 17/50] Step 360/520 Loss 2.661 Prec@(1,5) (49.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:44] \u001b[32mTrain: [ 17/50] Step 380/520 Loss 2.667 Prec@(1,5) (49.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:17:44] \u001b[32mTrain: [ 17/50] Step 400/520 Loss 2.675 Prec@(1,5) (49.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:45] \u001b[32mTrain: [ 17/50] Step 420/520 Loss 2.673 Prec@(1,5) (49.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:17:45] \u001b[32mTrain: [ 17/50] Step 440/520 Loss 2.672 Prec@(1,5) (49.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:17:46] \u001b[32mTrain: [ 17/50] Step 460/520 Loss 2.672 Prec@(1,5) (49.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:46] \u001b[32mTrain: [ 17/50] Step 480/520 Loss 2.673 Prec@(1,5) (49.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:17:46] \u001b[32mTrain: [ 17/50] Step 500/520 Loss 2.672 Prec@(1,5) (49.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:47] \u001b[32mTrain: [ 17/50] Step 520/520 Loss 2.673 Prec@(1,5) (49.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:47] \u001b[32mTrain: [ 17/50] Final Prec@1 49.5140%\u001b[0m\n",
      "[2024-01-15 10:17:51] \u001b[32mValid: [ 17/50] Step 000/104 Loss 2.369 Prec@(1,5) (57.3%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:51] \u001b[32mValid: [ 17/50] Step 020/104 Loss 2.277 Prec@(1,5) (51.6%, 80.3%)\u001b[0m\n",
      "[2024-01-15 10:17:51] \u001b[32mValid: [ 17/50] Step 040/104 Loss 2.262 Prec@(1,5) (50.5%, 80.4%)\u001b[0m\n",
      "[2024-01-15 10:17:51] \u001b[32mValid: [ 17/50] Step 060/104 Loss 2.228 Prec@(1,5) (50.9%, 80.6%)\u001b[0m\n",
      "[2024-01-15 10:17:51] \u001b[32mValid: [ 17/50] Step 080/104 Loss 2.220 Prec@(1,5) (50.8%, 80.7%)\u001b[0m\n",
      "[2024-01-15 10:17:51] \u001b[32mValid: [ 17/50] Step 100/104 Loss 2.215 Prec@(1,5) (50.8%, 80.8%)\u001b[0m\n",
      "[2024-01-15 10:17:52] \u001b[32mValid: [ 17/50] Step 104/104 Loss 2.214 Prec@(1,5) (50.9%, 80.8%)\u001b[0m\n",
      "[2024-01-15 10:17:52] \u001b[32mValid: [ 17/50] Final Prec@1 50.9100%\u001b[0m\n",
      "[2024-01-15 10:17:52] \u001b[32mEpoch 17 LR 0.018522\u001b[0m\n",
      "[2024-01-15 10:17:57] \u001b[32mTrain: [ 18/50] Step 000/520 Loss 2.523 Prec@(1,5) (45.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:17:57] \u001b[32mTrain: [ 18/50] Step 020/520 Loss 2.670 Prec@(1,5) (48.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:17:57] \u001b[32mTrain: [ 18/50] Step 040/520 Loss 2.605 Prec@(1,5) (49.9%, 78.2%)\u001b[0m\n",
      "[2024-01-15 10:17:58] \u001b[32mTrain: [ 18/50] Step 060/520 Loss 2.592 Prec@(1,5) (49.9%, 78.2%)\u001b[0m\n",
      "[2024-01-15 10:17:58] \u001b[32mTrain: [ 18/50] Step 080/520 Loss 2.600 Prec@(1,5) (50.1%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:17:59] \u001b[32mTrain: [ 18/50] Step 100/520 Loss 2.607 Prec@(1,5) (50.1%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:17:59] \u001b[32mTrain: [ 18/50] Step 120/520 Loss 2.604 Prec@(1,5) (50.3%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:17:59] \u001b[32mTrain: [ 18/50] Step 140/520 Loss 2.607 Prec@(1,5) (50.2%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:18:00] \u001b[32mTrain: [ 18/50] Step 160/520 Loss 2.607 Prec@(1,5) (50.2%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:18:00] \u001b[32mTrain: [ 18/50] Step 180/520 Loss 2.617 Prec@(1,5) (50.1%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:18:01] \u001b[32mTrain: [ 18/50] Step 200/520 Loss 2.614 Prec@(1,5) (50.1%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:18:01] \u001b[32mTrain: [ 18/50] Step 220/520 Loss 2.613 Prec@(1,5) (50.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:18:01] \u001b[32mTrain: [ 18/50] Step 240/520 Loss 2.622 Prec@(1,5) (50.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:02] \u001b[32mTrain: [ 18/50] Step 260/520 Loss 2.628 Prec@(1,5) (49.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:02] \u001b[32mTrain: [ 18/50] Step 280/520 Loss 2.629 Prec@(1,5) (49.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:03] \u001b[32mTrain: [ 18/50] Step 300/520 Loss 2.631 Prec@(1,5) (49.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:03] \u001b[32mTrain: [ 18/50] Step 320/520 Loss 2.632 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:03] \u001b[32mTrain: [ 18/50] Step 340/520 Loss 2.633 Prec@(1,5) (49.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:04] \u001b[32mTrain: [ 18/50] Step 360/520 Loss 2.635 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:04] \u001b[32mTrain: [ 18/50] Step 380/520 Loss 2.639 Prec@(1,5) (49.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:05] \u001b[32mTrain: [ 18/50] Step 400/520 Loss 2.641 Prec@(1,5) (49.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:05] \u001b[32mTrain: [ 18/50] Step 420/520 Loss 2.643 Prec@(1,5) (49.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:05] \u001b[32mTrain: [ 18/50] Step 440/520 Loss 2.645 Prec@(1,5) (49.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:06] \u001b[32mTrain: [ 18/50] Step 460/520 Loss 2.646 Prec@(1,5) (49.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:06] \u001b[32mTrain: [ 18/50] Step 480/520 Loss 2.646 Prec@(1,5) (49.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:07] \u001b[32mTrain: [ 18/50] Step 500/520 Loss 2.651 Prec@(1,5) (49.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:18:07] \u001b[32mTrain: [ 18/50] Step 520/520 Loss 2.650 Prec@(1,5) (49.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:18:07] \u001b[32mTrain: [ 18/50] Final Prec@1 49.6860%\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 000/104 Loss 2.495 Prec@(1,5) (55.2%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 020/104 Loss 2.270 Prec@(1,5) (51.7%, 81.6%)\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 040/104 Loss 2.257 Prec@(1,5) (50.7%, 81.0%)\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 060/104 Loss 2.234 Prec@(1,5) (50.9%, 81.4%)\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 080/104 Loss 2.242 Prec@(1,5) (51.0%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 100/104 Loss 2.235 Prec@(1,5) (51.3%, 81.3%)\u001b[0m\n",
      "[2024-01-15 10:18:11] \u001b[32mValid: [ 18/50] Step 104/104 Loss 2.239 Prec@(1,5) (51.3%, 81.3%)\u001b[0m\n",
      "[2024-01-15 10:18:12] \u001b[32mValid: [ 18/50] Final Prec@1 51.2600%\u001b[0m\n",
      "[2024-01-15 10:18:12] \u001b[32mEpoch 18 LR 0.017823\u001b[0m\n",
      "[2024-01-15 10:18:16] \u001b[32mTrain: [ 19/50] Step 000/520 Loss 2.965 Prec@(1,5) (41.7%, 71.9%)\u001b[0m\n",
      "[2024-01-15 10:18:17] \u001b[32mTrain: [ 19/50] Step 020/520 Loss 2.584 Prec@(1,5) (51.1%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:18:17] \u001b[32mTrain: [ 19/50] Step 040/520 Loss 2.617 Prec@(1,5) (50.8%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:18:18] \u001b[32mTrain: [ 19/50] Step 060/520 Loss 2.598 Prec@(1,5) (50.9%, 78.3%)\u001b[0m\n",
      "[2024-01-15 10:18:18] \u001b[32mTrain: [ 19/50] Step 080/520 Loss 2.590 Prec@(1,5) (51.1%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:18:18] \u001b[32mTrain: [ 19/50] Step 100/520 Loss 2.572 Prec@(1,5) (51.6%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:18:19] \u001b[32mTrain: [ 19/50] Step 120/520 Loss 2.582 Prec@(1,5) (51.3%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:18:19] \u001b[32mTrain: [ 19/50] Step 140/520 Loss 2.591 Prec@(1,5) (51.1%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:18:20] \u001b[32mTrain: [ 19/50] Step 160/520 Loss 2.594 Prec@(1,5) (51.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:18:20] \u001b[32mTrain: [ 19/50] Step 180/520 Loss 2.607 Prec@(1,5) (50.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:20] \u001b[32mTrain: [ 19/50] Step 200/520 Loss 2.609 Prec@(1,5) (50.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:21] \u001b[32mTrain: [ 19/50] Step 220/520 Loss 2.616 Prec@(1,5) (50.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:21] \u001b[32mTrain: [ 19/50] Step 240/520 Loss 2.614 Prec@(1,5) (50.5%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:21] \u001b[32mTrain: [ 19/50] Step 260/520 Loss 2.613 Prec@(1,5) (50.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:18:22] \u001b[32mTrain: [ 19/50] Step 280/520 Loss 2.610 Prec@(1,5) (50.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:18:22] \u001b[32mTrain: [ 19/50] Step 300/520 Loss 2.616 Prec@(1,5) (50.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:23] \u001b[32mTrain: [ 19/50] Step 320/520 Loss 2.625 Prec@(1,5) (50.3%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:23] \u001b[32mTrain: [ 19/50] Step 340/520 Loss 2.630 Prec@(1,5) (50.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:24] \u001b[32mTrain: [ 19/50] Step 360/520 Loss 2.636 Prec@(1,5) (50.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:24] \u001b[32mTrain: [ 19/50] Step 380/520 Loss 2.639 Prec@(1,5) (50.1%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:18:24] \u001b[32mTrain: [ 19/50] Step 400/520 Loss 2.639 Prec@(1,5) (50.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:18:25] \u001b[32mTrain: [ 19/50] Step 420/520 Loss 2.637 Prec@(1,5) (50.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:25] \u001b[32mTrain: [ 19/50] Step 440/520 Loss 2.638 Prec@(1,5) (50.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:26] \u001b[32mTrain: [ 19/50] Step 460/520 Loss 2.640 Prec@(1,5) (50.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:26] \u001b[32mTrain: [ 19/50] Step 480/520 Loss 2.638 Prec@(1,5) (50.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:26] \u001b[32mTrain: [ 19/50] Step 500/520 Loss 2.640 Prec@(1,5) (50.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:27] \u001b[32mTrain: [ 19/50] Step 520/520 Loss 2.642 Prec@(1,5) (50.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:27] \u001b[32mTrain: [ 19/50] Final Prec@1 49.9800%\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 000/104 Loss 2.130 Prec@(1,5) (59.4%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 020/104 Loss 2.109 Prec@(1,5) (53.2%, 81.4%)\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 040/104 Loss 2.112 Prec@(1,5) (52.5%, 82.0%)\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 060/104 Loss 2.061 Prec@(1,5) (53.9%, 82.2%)\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 080/104 Loss 2.062 Prec@(1,5) (53.7%, 82.2%)\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 100/104 Loss 2.059 Prec@(1,5) (53.6%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:18:31] \u001b[32mValid: [ 19/50] Step 104/104 Loss 2.059 Prec@(1,5) (53.5%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:18:32] \u001b[32mValid: [ 19/50] Final Prec@1 53.5300%\u001b[0m\n",
      "[2024-01-15 10:18:32] \u001b[32mEpoch 19 LR 0.017102\u001b[0m\n",
      "[2024-01-15 10:18:36] \u001b[32mTrain: [ 20/50] Step 000/520 Loss 2.309 Prec@(1,5) (52.1%, 79.2%)\u001b[0m\n",
      "[2024-01-15 10:18:37] \u001b[32mTrain: [ 20/50] Step 020/520 Loss 2.628 Prec@(1,5) (49.9%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:18:37] \u001b[32mTrain: [ 20/50] Step 040/520 Loss 2.628 Prec@(1,5) (49.6%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:18:38] \u001b[32mTrain: [ 20/50] Step 060/520 Loss 2.590 Prec@(1,5) (50.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:18:38] \u001b[32mTrain: [ 20/50] Step 080/520 Loss 2.597 Prec@(1,5) (50.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:18:38] \u001b[32mTrain: [ 20/50] Step 100/520 Loss 2.584 Prec@(1,5) (51.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:39] \u001b[32mTrain: [ 20/50] Step 120/520 Loss 2.586 Prec@(1,5) (51.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:39] \u001b[32mTrain: [ 20/50] Step 140/520 Loss 2.595 Prec@(1,5) (51.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:40] \u001b[32mTrain: [ 20/50] Step 160/520 Loss 2.591 Prec@(1,5) (51.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:40] \u001b[32mTrain: [ 20/50] Step 180/520 Loss 2.597 Prec@(1,5) (51.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:40] \u001b[32mTrain: [ 20/50] Step 200/520 Loss 2.587 Prec@(1,5) (51.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:41] \u001b[32mTrain: [ 20/50] Step 220/520 Loss 2.589 Prec@(1,5) (51.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:41] \u001b[32mTrain: [ 20/50] Step 240/520 Loss 2.585 Prec@(1,5) (51.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:42] \u001b[32mTrain: [ 20/50] Step 260/520 Loss 2.587 Prec@(1,5) (51.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:42] \u001b[32mTrain: [ 20/50] Step 280/520 Loss 2.584 Prec@(1,5) (51.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:43] \u001b[32mTrain: [ 20/50] Step 300/520 Loss 2.587 Prec@(1,5) (51.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:18:43] \u001b[32mTrain: [ 20/50] Step 320/520 Loss 2.593 Prec@(1,5) (51.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:43] \u001b[32mTrain: [ 20/50] Step 340/520 Loss 2.596 Prec@(1,5) (51.1%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:44] \u001b[32mTrain: [ 20/50] Step 360/520 Loss 2.602 Prec@(1,5) (51.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:44] \u001b[32mTrain: [ 20/50] Step 380/520 Loss 2.608 Prec@(1,5) (51.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:45] \u001b[32mTrain: [ 20/50] Step 400/520 Loss 2.607 Prec@(1,5) (51.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:45] \u001b[32mTrain: [ 20/50] Step 420/520 Loss 2.607 Prec@(1,5) (51.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:45] \u001b[32mTrain: [ 20/50] Step 440/520 Loss 2.610 Prec@(1,5) (50.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:46] \u001b[32mTrain: [ 20/50] Step 460/520 Loss 2.609 Prec@(1,5) (50.8%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:46] \u001b[32mTrain: [ 20/50] Step 480/520 Loss 2.609 Prec@(1,5) (50.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:18:47] \u001b[32mTrain: [ 20/50] Step 500/520 Loss 2.612 Prec@(1,5) (50.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:47] \u001b[32mTrain: [ 20/50] Step 520/520 Loss 2.616 Prec@(1,5) (50.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:47] \u001b[32mTrain: [ 20/50] Final Prec@1 50.7480%\u001b[0m\n",
      "[2024-01-15 10:18:51] \u001b[32mValid: [ 20/50] Step 000/104 Loss 2.288 Prec@(1,5) (56.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:18:51] \u001b[32mValid: [ 20/50] Step 020/104 Loss 2.149 Prec@(1,5) (53.1%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:18:51] \u001b[32mValid: [ 20/50] Step 040/104 Loss 2.111 Prec@(1,5) (52.6%, 83.4%)\u001b[0m\n",
      "[2024-01-15 10:18:51] \u001b[32mValid: [ 20/50] Step 060/104 Loss 2.098 Prec@(1,5) (53.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:18:52] \u001b[32mValid: [ 20/50] Step 080/104 Loss 2.105 Prec@(1,5) (53.1%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:18:52] \u001b[32mValid: [ 20/50] Step 100/104 Loss 2.091 Prec@(1,5) (53.5%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:18:52] \u001b[32mValid: [ 20/50] Step 104/104 Loss 2.089 Prec@(1,5) (53.5%, 83.2%)\u001b[0m\n",
      "[2024-01-15 10:18:52] \u001b[32mValid: [ 20/50] Final Prec@1 53.4500%\u001b[0m\n",
      "[2024-01-15 10:18:52] \u001b[32mEpoch 20 LR 0.016363\u001b[0m\n",
      "[2024-01-15 10:18:58] \u001b[32mTrain: [ 21/50] Step 000/520 Loss 2.819 Prec@(1,5) (51.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 10:18:58] \u001b[32mTrain: [ 21/50] Step 020/520 Loss 2.556 Prec@(1,5) (51.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:18:59] \u001b[32mTrain: [ 21/50] Step 040/520 Loss 2.572 Prec@(1,5) (50.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:18:59] \u001b[32mTrain: [ 21/50] Step 060/520 Loss 2.584 Prec@(1,5) (51.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:00] \u001b[32mTrain: [ 21/50] Step 080/520 Loss 2.585 Prec@(1,5) (51.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:00] \u001b[32mTrain: [ 21/50] Step 100/520 Loss 2.584 Prec@(1,5) (51.0%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:19:00] \u001b[32mTrain: [ 21/50] Step 120/520 Loss 2.582 Prec@(1,5) (51.1%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:19:01] \u001b[32mTrain: [ 21/50] Step 140/520 Loss 2.585 Prec@(1,5) (50.9%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:19:01] \u001b[32mTrain: [ 21/50] Step 160/520 Loss 2.578 Prec@(1,5) (51.2%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:19:01] \u001b[32mTrain: [ 21/50] Step 180/520 Loss 2.588 Prec@(1,5) (51.2%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:19:02] \u001b[32mTrain: [ 21/50] Step 200/520 Loss 2.592 Prec@(1,5) (51.1%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:19:02] \u001b[32mTrain: [ 21/50] Step 220/520 Loss 2.594 Prec@(1,5) (51.1%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:03] \u001b[32mTrain: [ 21/50] Step 240/520 Loss 2.599 Prec@(1,5) (50.9%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:03] \u001b[32mTrain: [ 21/50] Step 260/520 Loss 2.592 Prec@(1,5) (51.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:03] \u001b[32mTrain: [ 21/50] Step 280/520 Loss 2.595 Prec@(1,5) (51.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:04] \u001b[32mTrain: [ 21/50] Step 300/520 Loss 2.592 Prec@(1,5) (51.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:04] \u001b[32mTrain: [ 21/50] Step 320/520 Loss 2.595 Prec@(1,5) (51.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:05] \u001b[32mTrain: [ 21/50] Step 340/520 Loss 2.602 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:05] \u001b[32mTrain: [ 21/50] Step 360/520 Loss 2.599 Prec@(1,5) (50.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:05] \u001b[32mTrain: [ 21/50] Step 380/520 Loss 2.596 Prec@(1,5) (50.9%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:06] \u001b[32mTrain: [ 21/50] Step 400/520 Loss 2.595 Prec@(1,5) (51.0%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:06] \u001b[32mTrain: [ 21/50] Step 420/520 Loss 2.597 Prec@(1,5) (51.0%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:07] \u001b[32mTrain: [ 21/50] Step 440/520 Loss 2.595 Prec@(1,5) (51.0%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:07] \u001b[32mTrain: [ 21/50] Step 460/520 Loss 2.597 Prec@(1,5) (50.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:07] \u001b[32mTrain: [ 21/50] Step 480/520 Loss 2.597 Prec@(1,5) (50.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:08] \u001b[32mTrain: [ 21/50] Step 500/520 Loss 2.596 Prec@(1,5) (51.0%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:08] \u001b[32mTrain: [ 21/50] Step 520/520 Loss 2.599 Prec@(1,5) (50.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:08] \u001b[32mTrain: [ 21/50] Final Prec@1 50.9120%\u001b[0m\n",
      "[2024-01-15 10:19:13] \u001b[32mValid: [ 21/50] Step 000/104 Loss 2.037 Prec@(1,5) (63.5%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Step 020/104 Loss 2.249 Prec@(1,5) (50.8%, 81.8%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Step 040/104 Loss 2.222 Prec@(1,5) (51.0%, 81.9%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Step 060/104 Loss 2.207 Prec@(1,5) (51.3%, 81.6%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Step 080/104 Loss 2.210 Prec@(1,5) (51.0%, 81.5%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Step 100/104 Loss 2.211 Prec@(1,5) (51.3%, 81.3%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Step 104/104 Loss 2.215 Prec@(1,5) (51.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mValid: [ 21/50] Final Prec@1 51.3000%\u001b[0m\n",
      "[2024-01-15 10:19:14] \u001b[32mEpoch 21 LR 0.015609\u001b[0m\n",
      "[2024-01-15 10:19:19] \u001b[32mTrain: [ 22/50] Step 000/520 Loss 3.001 Prec@(1,5) (47.9%, 67.7%)\u001b[0m\n",
      "[2024-01-15 10:19:20] \u001b[32mTrain: [ 22/50] Step 020/520 Loss 2.529 Prec@(1,5) (52.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:20] \u001b[32mTrain: [ 22/50] Step 040/520 Loss 2.534 Prec@(1,5) (51.5%, 78.2%)\u001b[0m\n",
      "[2024-01-15 10:19:20] \u001b[32mTrain: [ 22/50] Step 060/520 Loss 2.563 Prec@(1,5) (51.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:19:21] \u001b[32mTrain: [ 22/50] Step 080/520 Loss 2.541 Prec@(1,5) (51.8%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:19:21] \u001b[32mTrain: [ 22/50] Step 100/520 Loss 2.544 Prec@(1,5) (51.7%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:21] \u001b[32mTrain: [ 22/50] Step 120/520 Loss 2.566 Prec@(1,5) (51.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:22] \u001b[32mTrain: [ 22/50] Step 140/520 Loss 2.574 Prec@(1,5) (50.9%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:22] \u001b[32mTrain: [ 22/50] Step 160/520 Loss 2.576 Prec@(1,5) (50.9%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:23] \u001b[32mTrain: [ 22/50] Step 180/520 Loss 2.579 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:23] \u001b[32mTrain: [ 22/50] Step 200/520 Loss 2.578 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:23] \u001b[32mTrain: [ 22/50] Step 220/520 Loss 2.569 Prec@(1,5) (51.0%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:24] \u001b[32mTrain: [ 22/50] Step 240/520 Loss 2.567 Prec@(1,5) (51.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:24] \u001b[32mTrain: [ 22/50] Step 260/520 Loss 2.575 Prec@(1,5) (50.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:25] \u001b[32mTrain: [ 22/50] Step 280/520 Loss 2.581 Prec@(1,5) (50.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:25] \u001b[32mTrain: [ 22/50] Step 300/520 Loss 2.587 Prec@(1,5) (50.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:19:25] \u001b[32mTrain: [ 22/50] Step 320/520 Loss 2.583 Prec@(1,5) (51.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:19:26] \u001b[32mTrain: [ 22/50] Step 340/520 Loss 2.585 Prec@(1,5) (51.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:19:26] \u001b[32mTrain: [ 22/50] Step 360/520 Loss 2.588 Prec@(1,5) (50.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:19:27] \u001b[32mTrain: [ 22/50] Step 380/520 Loss 2.587 Prec@(1,5) (51.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:27] \u001b[32mTrain: [ 22/50] Step 400/520 Loss 2.588 Prec@(1,5) (51.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:27] \u001b[32mTrain: [ 22/50] Step 420/520 Loss 2.587 Prec@(1,5) (51.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:28] \u001b[32mTrain: [ 22/50] Step 440/520 Loss 2.586 Prec@(1,5) (51.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:28] \u001b[32mTrain: [ 22/50] Step 460/520 Loss 2.588 Prec@(1,5) (51.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:29] \u001b[32mTrain: [ 22/50] Step 480/520 Loss 2.588 Prec@(1,5) (51.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:29] \u001b[32mTrain: [ 22/50] Step 500/520 Loss 2.591 Prec@(1,5) (51.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:30] \u001b[32mTrain: [ 22/50] Step 520/520 Loss 2.588 Prec@(1,5) (51.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:30] \u001b[32mTrain: [ 22/50] Final Prec@1 51.0480%\u001b[0m\n",
      "[2024-01-15 10:19:33] \u001b[32mValid: [ 22/50] Step 000/104 Loss 2.236 Prec@(1,5) (57.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:19:33] \u001b[32mValid: [ 22/50] Step 020/104 Loss 2.151 Prec@(1,5) (53.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mValid: [ 22/50] Step 040/104 Loss 2.112 Prec@(1,5) (53.6%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mValid: [ 22/50] Step 060/104 Loss 2.074 Prec@(1,5) (54.5%, 82.9%)\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mValid: [ 22/50] Step 080/104 Loss 2.079 Prec@(1,5) (54.4%, 83.0%)\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mValid: [ 22/50] Step 100/104 Loss 2.067 Prec@(1,5) (54.2%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mValid: [ 22/50] Step 104/104 Loss 2.064 Prec@(1,5) (54.3%, 83.0%)\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mValid: [ 22/50] Final Prec@1 54.3300%\u001b[0m\n",
      "[2024-01-15 10:19:34] \u001b[32mEpoch 22 LR 0.014843\u001b[0m\n",
      "[2024-01-15 10:19:39] \u001b[32mTrain: [ 23/50] Step 000/520 Loss 2.870 Prec@(1,5) (45.8%, 72.9%)\u001b[0m\n",
      "[2024-01-15 10:19:39] \u001b[32mTrain: [ 23/50] Step 020/520 Loss 2.572 Prec@(1,5) (50.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:19:40] \u001b[32mTrain: [ 23/50] Step 040/520 Loss 2.584 Prec@(1,5) (51.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:40] \u001b[32mTrain: [ 23/50] Step 060/520 Loss 2.586 Prec@(1,5) (51.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:19:41] \u001b[32mTrain: [ 23/50] Step 080/520 Loss 2.569 Prec@(1,5) (51.4%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:41] \u001b[32mTrain: [ 23/50] Step 100/520 Loss 2.573 Prec@(1,5) (51.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:41] \u001b[32mTrain: [ 23/50] Step 120/520 Loss 2.570 Prec@(1,5) (51.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:42] \u001b[32mTrain: [ 23/50] Step 140/520 Loss 2.570 Prec@(1,5) (51.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:42] \u001b[32mTrain: [ 23/50] Step 160/520 Loss 2.576 Prec@(1,5) (51.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:43] \u001b[32mTrain: [ 23/50] Step 180/520 Loss 2.570 Prec@(1,5) (51.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:43] \u001b[32mTrain: [ 23/50] Step 200/520 Loss 2.577 Prec@(1,5) (51.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:43] \u001b[32mTrain: [ 23/50] Step 220/520 Loss 2.582 Prec@(1,5) (51.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:44] \u001b[32mTrain: [ 23/50] Step 240/520 Loss 2.580 Prec@(1,5) (51.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:44] \u001b[32mTrain: [ 23/50] Step 260/520 Loss 2.579 Prec@(1,5) (51.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:45] \u001b[32mTrain: [ 23/50] Step 280/520 Loss 2.580 Prec@(1,5) (51.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:45] \u001b[32mTrain: [ 23/50] Step 300/520 Loss 2.580 Prec@(1,5) (50.9%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:45] \u001b[32mTrain: [ 23/50] Step 320/520 Loss 2.576 Prec@(1,5) (51.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:46] \u001b[32mTrain: [ 23/50] Step 340/520 Loss 2.580 Prec@(1,5) (51.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:19:46] \u001b[32mTrain: [ 23/50] Step 360/520 Loss 2.586 Prec@(1,5) (50.9%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:19:47] \u001b[32mTrain: [ 23/50] Step 380/520 Loss 2.591 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:47] \u001b[32mTrain: [ 23/50] Step 400/520 Loss 2.589 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:48] \u001b[32mTrain: [ 23/50] Step 420/520 Loss 2.591 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:48] \u001b[32mTrain: [ 23/50] Step 440/520 Loss 2.592 Prec@(1,5) (50.8%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:48] \u001b[32mTrain: [ 23/50] Step 460/520 Loss 2.594 Prec@(1,5) (50.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:49] \u001b[32mTrain: [ 23/50] Step 480/520 Loss 2.594 Prec@(1,5) (50.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:49] \u001b[32mTrain: [ 23/50] Step 500/520 Loss 2.594 Prec@(1,5) (50.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:19:50] \u001b[32mTrain: [ 23/50] Step 520/520 Loss 2.592 Prec@(1,5) (50.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:19:50] \u001b[32mTrain: [ 23/50] Final Prec@1 50.9360%\u001b[0m\n",
      "[2024-01-15 10:19:53] \u001b[32mValid: [ 23/50] Step 000/104 Loss 2.179 Prec@(1,5) (58.3%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Step 020/104 Loss 2.125 Prec@(1,5) (55.2%, 82.9%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Step 040/104 Loss 2.098 Prec@(1,5) (54.4%, 83.2%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Step 060/104 Loss 2.057 Prec@(1,5) (54.6%, 83.2%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Step 080/104 Loss 2.055 Prec@(1,5) (54.6%, 83.5%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Step 100/104 Loss 2.051 Prec@(1,5) (54.5%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Step 104/104 Loss 2.047 Prec@(1,5) (54.6%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mValid: [ 23/50] Final Prec@1 54.5700%\u001b[0m\n",
      "[2024-01-15 10:19:54] \u001b[32mEpoch 23 LR 0.014067\u001b[0m\n",
      "[2024-01-15 10:19:59] \u001b[32mTrain: [ 24/50] Step 000/520 Loss 2.486 Prec@(1,5) (61.5%, 72.9%)\u001b[0m\n",
      "[2024-01-15 10:19:59] \u001b[32mTrain: [ 24/50] Step 020/520 Loss 2.458 Prec@(1,5) (54.7%, 78.8%)\u001b[0m\n",
      "[2024-01-15 10:20:00] \u001b[32mTrain: [ 24/50] Step 040/520 Loss 2.489 Prec@(1,5) (53.2%, 78.9%)\u001b[0m\n",
      "[2024-01-15 10:20:00] \u001b[32mTrain: [ 24/50] Step 060/520 Loss 2.502 Prec@(1,5) (53.0%, 78.7%)\u001b[0m\n",
      "[2024-01-15 10:20:01] \u001b[32mTrain: [ 24/50] Step 080/520 Loss 2.508 Prec@(1,5) (53.2%, 78.4%)\u001b[0m\n",
      "[2024-01-15 10:20:01] \u001b[32mTrain: [ 24/50] Step 100/520 Loss 2.527 Prec@(1,5) (53.0%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:20:01] \u001b[32mTrain: [ 24/50] Step 120/520 Loss 2.540 Prec@(1,5) (52.6%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:02] \u001b[32mTrain: [ 24/50] Step 140/520 Loss 2.544 Prec@(1,5) (52.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:02] \u001b[32mTrain: [ 24/50] Step 160/520 Loss 2.550 Prec@(1,5) (52.3%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:03] \u001b[32mTrain: [ 24/50] Step 180/520 Loss 2.544 Prec@(1,5) (52.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:03] \u001b[32mTrain: [ 24/50] Step 200/520 Loss 2.543 Prec@(1,5) (52.2%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:20:03] \u001b[32mTrain: [ 24/50] Step 220/520 Loss 2.550 Prec@(1,5) (52.1%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:04] \u001b[32mTrain: [ 24/50] Step 240/520 Loss 2.554 Prec@(1,5) (52.0%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:04] \u001b[32mTrain: [ 24/50] Step 260/520 Loss 2.560 Prec@(1,5) (51.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:05] \u001b[32mTrain: [ 24/50] Step 280/520 Loss 2.558 Prec@(1,5) (51.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:05] \u001b[32mTrain: [ 24/50] Step 300/520 Loss 2.563 Prec@(1,5) (51.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:05] \u001b[32mTrain: [ 24/50] Step 320/520 Loss 2.559 Prec@(1,5) (51.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:06] \u001b[32mTrain: [ 24/50] Step 340/520 Loss 2.561 Prec@(1,5) (51.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:06] \u001b[32mTrain: [ 24/50] Step 360/520 Loss 2.564 Prec@(1,5) (51.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:07] \u001b[32mTrain: [ 24/50] Step 380/520 Loss 2.568 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:07] \u001b[32mTrain: [ 24/50] Step 400/520 Loss 2.571 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:20:07] \u001b[32mTrain: [ 24/50] Step 420/520 Loss 2.565 Prec@(1,5) (51.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:08] \u001b[32mTrain: [ 24/50] Step 440/520 Loss 2.568 Prec@(1,5) (51.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:08] \u001b[32mTrain: [ 24/50] Step 460/520 Loss 2.568 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:09] \u001b[32mTrain: [ 24/50] Step 480/520 Loss 2.571 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:09] \u001b[32mTrain: [ 24/50] Step 500/520 Loss 2.575 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:20:09] \u001b[32mTrain: [ 24/50] Step 520/520 Loss 2.578 Prec@(1,5) (51.5%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:20:10] \u001b[32mTrain: [ 24/50] Final Prec@1 51.4660%\u001b[0m\n",
      "[2024-01-15 10:20:13] \u001b[32mValid: [ 24/50] Step 000/104 Loss 2.288 Prec@(1,5) (57.3%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:20:13] \u001b[32mValid: [ 24/50] Step 020/104 Loss 2.140 Prec@(1,5) (53.7%, 81.7%)\u001b[0m\n",
      "[2024-01-15 10:20:13] \u001b[32mValid: [ 24/50] Step 040/104 Loss 2.120 Prec@(1,5) (53.4%, 82.4%)\u001b[0m\n",
      "[2024-01-15 10:20:14] \u001b[32mValid: [ 24/50] Step 060/104 Loss 2.092 Prec@(1,5) (53.9%, 82.5%)\u001b[0m\n",
      "[2024-01-15 10:20:14] \u001b[32mValid: [ 24/50] Step 080/104 Loss 2.098 Prec@(1,5) (53.7%, 82.8%)\u001b[0m\n",
      "[2024-01-15 10:20:14] \u001b[32mValid: [ 24/50] Step 100/104 Loss 2.092 Prec@(1,5) (54.0%, 82.7%)\u001b[0m\n",
      "[2024-01-15 10:20:14] \u001b[32mValid: [ 24/50] Step 104/104 Loss 2.095 Prec@(1,5) (54.0%, 82.6%)\u001b[0m\n",
      "[2024-01-15 10:20:14] \u001b[32mValid: [ 24/50] Final Prec@1 53.9800%\u001b[0m\n",
      "[2024-01-15 10:20:14] \u001b[32mEpoch 24 LR 0.013285\u001b[0m\n",
      "[2024-01-15 10:20:19] \u001b[32mTrain: [ 25/50] Step 000/520 Loss 2.512 Prec@(1,5) (52.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:19] \u001b[32mTrain: [ 25/50] Step 020/520 Loss 2.606 Prec@(1,5) (51.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:20:20] \u001b[32mTrain: [ 25/50] Step 040/520 Loss 2.528 Prec@(1,5) (52.7%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:20:20] \u001b[32mTrain: [ 25/50] Step 060/520 Loss 2.538 Prec@(1,5) (52.3%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:20:20] \u001b[32mTrain: [ 25/50] Step 080/520 Loss 2.534 Prec@(1,5) (52.5%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:20:21] \u001b[32mTrain: [ 25/50] Step 100/520 Loss 2.542 Prec@(1,5) (52.2%, 78.0%)\u001b[0m\n",
      "[2024-01-15 10:20:21] \u001b[32mTrain: [ 25/50] Step 120/520 Loss 2.554 Prec@(1,5) (52.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:22] \u001b[32mTrain: [ 25/50] Step 140/520 Loss 2.551 Prec@(1,5) (51.9%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:22] \u001b[32mTrain: [ 25/50] Step 160/520 Loss 2.567 Prec@(1,5) (51.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:22] \u001b[32mTrain: [ 25/50] Step 180/520 Loss 2.570 Prec@(1,5) (51.7%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:23] \u001b[32mTrain: [ 25/50] Step 200/520 Loss 2.573 Prec@(1,5) (51.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:23] \u001b[32mTrain: [ 25/50] Step 220/520 Loss 2.584 Prec@(1,5) (51.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:20:24] \u001b[32mTrain: [ 25/50] Step 240/520 Loss 2.575 Prec@(1,5) (51.5%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:24] \u001b[32mTrain: [ 25/50] Step 260/520 Loss 2.580 Prec@(1,5) (51.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:24] \u001b[32mTrain: [ 25/50] Step 280/520 Loss 2.574 Prec@(1,5) (51.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:20:25] \u001b[32mTrain: [ 25/50] Step 300/520 Loss 2.579 Prec@(1,5) (51.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:25] \u001b[32mTrain: [ 25/50] Step 320/520 Loss 2.583 Prec@(1,5) (51.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:26] \u001b[32mTrain: [ 25/50] Step 340/520 Loss 2.579 Prec@(1,5) (51.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:26] \u001b[32mTrain: [ 25/50] Step 360/520 Loss 2.576 Prec@(1,5) (51.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:26] \u001b[32mTrain: [ 25/50] Step 380/520 Loss 2.578 Prec@(1,5) (51.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:27] \u001b[32mTrain: [ 25/50] Step 400/520 Loss 2.579 Prec@(1,5) (51.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:27] \u001b[32mTrain: [ 25/50] Step 420/520 Loss 2.581 Prec@(1,5) (51.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:28] \u001b[32mTrain: [ 25/50] Step 440/520 Loss 2.583 Prec@(1,5) (51.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:28] \u001b[32mTrain: [ 25/50] Step 460/520 Loss 2.584 Prec@(1,5) (51.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:20:28] \u001b[32mTrain: [ 25/50] Step 480/520 Loss 2.584 Prec@(1,5) (51.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:29] \u001b[32mTrain: [ 25/50] Step 500/520 Loss 2.587 Prec@(1,5) (51.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:29] \u001b[32mTrain: [ 25/50] Step 520/520 Loss 2.585 Prec@(1,5) (51.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:20:29] \u001b[32mTrain: [ 25/50] Final Prec@1 51.6420%\u001b[0m\n",
      "[2024-01-15 10:20:33] \u001b[32mValid: [ 25/50] Step 000/104 Loss 2.112 Prec@(1,5) (61.5%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:20:33] \u001b[32mValid: [ 25/50] Step 020/104 Loss 2.028 Prec@(1,5) (55.4%, 83.1%)\u001b[0m\n",
      "[2024-01-15 10:20:33] \u001b[32mValid: [ 25/50] Step 040/104 Loss 2.019 Prec@(1,5) (55.0%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:20:33] \u001b[32mValid: [ 25/50] Step 060/104 Loss 2.000 Prec@(1,5) (55.2%, 83.4%)\u001b[0m\n",
      "[2024-01-15 10:20:33] \u001b[32mValid: [ 25/50] Step 080/104 Loss 2.000 Prec@(1,5) (54.8%, 83.5%)\u001b[0m\n",
      "[2024-01-15 10:20:34] \u001b[32mValid: [ 25/50] Step 100/104 Loss 1.991 Prec@(1,5) (55.0%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:20:34] \u001b[32mValid: [ 25/50] Step 104/104 Loss 1.989 Prec@(1,5) (55.0%, 83.6%)\u001b[0m\n",
      "[2024-01-15 10:20:34] \u001b[32mValid: [ 25/50] Final Prec@1 55.0400%\u001b[0m\n",
      "[2024-01-15 10:20:34] \u001b[32mEpoch 25 LR 0.012500\u001b[0m\n",
      "[2024-01-15 10:20:39] \u001b[32mTrain: [ 26/50] Step 000/520 Loss 2.820 Prec@(1,5) (44.8%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:20:39] \u001b[32mTrain: [ 26/50] Step 020/520 Loss 2.536 Prec@(1,5) (52.3%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:20:39] \u001b[32mTrain: [ 26/50] Step 040/520 Loss 2.540 Prec@(1,5) (52.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:40] \u001b[32mTrain: [ 26/50] Step 060/520 Loss 2.545 Prec@(1,5) (52.3%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:20:40] \u001b[32mTrain: [ 26/50] Step 080/520 Loss 2.553 Prec@(1,5) (52.2%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:20:41] \u001b[32mTrain: [ 26/50] Step 100/520 Loss 2.557 Prec@(1,5) (52.1%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:20:41] \u001b[32mTrain: [ 26/50] Step 120/520 Loss 2.553 Prec@(1,5) (52.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:41] \u001b[32mTrain: [ 26/50] Step 140/520 Loss 2.556 Prec@(1,5) (52.2%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:20:42] \u001b[32mTrain: [ 26/50] Step 160/520 Loss 2.561 Prec@(1,5) (52.3%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:42] \u001b[32mTrain: [ 26/50] Step 180/520 Loss 2.565 Prec@(1,5) (52.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:43] \u001b[32mTrain: [ 26/50] Step 200/520 Loss 2.565 Prec@(1,5) (52.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:43] \u001b[32mTrain: [ 26/50] Step 220/520 Loss 2.568 Prec@(1,5) (52.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:43] \u001b[32mTrain: [ 26/50] Step 240/520 Loss 2.565 Prec@(1,5) (52.3%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:44] \u001b[32mTrain: [ 26/50] Step 260/520 Loss 2.552 Prec@(1,5) (52.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:44] \u001b[32mTrain: [ 26/50] Step 280/520 Loss 2.549 Prec@(1,5) (52.6%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:44] \u001b[32mTrain: [ 26/50] Step 300/520 Loss 2.558 Prec@(1,5) (52.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:45] \u001b[32mTrain: [ 26/50] Step 320/520 Loss 2.560 Prec@(1,5) (52.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:45] \u001b[32mTrain: [ 26/50] Step 340/520 Loss 2.555 Prec@(1,5) (52.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:20:46] \u001b[32mTrain: [ 26/50] Step 360/520 Loss 2.560 Prec@(1,5) (52.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:46] \u001b[32mTrain: [ 26/50] Step 380/520 Loss 2.562 Prec@(1,5) (52.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:46] \u001b[32mTrain: [ 26/50] Step 400/520 Loss 2.562 Prec@(1,5) (52.3%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:47] \u001b[32mTrain: [ 26/50] Step 420/520 Loss 2.561 Prec@(1,5) (52.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:20:47] \u001b[32mTrain: [ 26/50] Step 440/520 Loss 2.568 Prec@(1,5) (52.1%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:48] \u001b[32mTrain: [ 26/50] Step 460/520 Loss 2.571 Prec@(1,5) (52.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:48] \u001b[32mTrain: [ 26/50] Step 480/520 Loss 2.570 Prec@(1,5) (52.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:20:48] \u001b[32mTrain: [ 26/50] Step 500/520 Loss 2.573 Prec@(1,5) (52.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:49] \u001b[32mTrain: [ 26/50] Step 520/520 Loss 2.574 Prec@(1,5) (52.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:20:49] \u001b[32mTrain: [ 26/50] Final Prec@1 51.9800%\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 000/104 Loss 1.919 Prec@(1,5) (61.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 020/104 Loss 1.979 Prec@(1,5) (55.6%, 83.8%)\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 040/104 Loss 1.963 Prec@(1,5) (54.9%, 84.3%)\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 060/104 Loss 1.933 Prec@(1,5) (55.7%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 080/104 Loss 1.932 Prec@(1,5) (55.8%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 100/104 Loss 1.925 Prec@(1,5) (56.0%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:20:53] \u001b[32mValid: [ 26/50] Step 104/104 Loss 1.924 Prec@(1,5) (56.0%, 84.5%)\u001b[0m\n",
      "[2024-01-15 10:20:54] \u001b[32mValid: [ 26/50] Final Prec@1 56.0000%\u001b[0m\n",
      "[2024-01-15 10:20:54] \u001b[32mEpoch 26 LR 0.011716\u001b[0m\n",
      "[2024-01-15 10:20:58] \u001b[32mTrain: [ 27/50] Step 000/520 Loss 2.326 Prec@(1,5) (56.2%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:20:59] \u001b[32mTrain: [ 27/50] Step 020/520 Loss 2.566 Prec@(1,5) (52.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:59] \u001b[32mTrain: [ 27/50] Step 040/520 Loss 2.540 Prec@(1,5) (52.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:20:59] \u001b[32mTrain: [ 27/50] Step 060/520 Loss 2.531 Prec@(1,5) (53.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:00] \u001b[32mTrain: [ 27/50] Step 080/520 Loss 2.535 Prec@(1,5) (52.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:00] \u001b[32mTrain: [ 27/50] Step 100/520 Loss 2.533 Prec@(1,5) (52.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:01] \u001b[32mTrain: [ 27/50] Step 120/520 Loss 2.523 Prec@(1,5) (52.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:01] \u001b[32mTrain: [ 27/50] Step 140/520 Loss 2.531 Prec@(1,5) (52.5%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:01] \u001b[32mTrain: [ 27/50] Step 160/520 Loss 2.530 Prec@(1,5) (52.4%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:02] \u001b[32mTrain: [ 27/50] Step 180/520 Loss 2.531 Prec@(1,5) (52.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:02] \u001b[32mTrain: [ 27/50] Step 200/520 Loss 2.537 Prec@(1,5) (52.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:03] \u001b[32mTrain: [ 27/50] Step 220/520 Loss 2.541 Prec@(1,5) (52.3%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:03] \u001b[32mTrain: [ 27/50] Step 240/520 Loss 2.535 Prec@(1,5) (52.4%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:21:03] \u001b[32mTrain: [ 27/50] Step 260/520 Loss 2.527 Prec@(1,5) (52.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:21:04] \u001b[32mTrain: [ 27/50] Step 280/520 Loss 2.517 Prec@(1,5) (52.7%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:21:04] \u001b[32mTrain: [ 27/50] Step 300/520 Loss 2.516 Prec@(1,5) (52.6%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:21:05] \u001b[32mTrain: [ 27/50] Step 320/520 Loss 2.521 Prec@(1,5) (52.6%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:21:05] \u001b[32mTrain: [ 27/50] Step 340/520 Loss 2.525 Prec@(1,5) (52.5%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:21:05] \u001b[32mTrain: [ 27/50] Step 360/520 Loss 2.528 Prec@(1,5) (52.4%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:21:06] \u001b[32mTrain: [ 27/50] Step 380/520 Loss 2.535 Prec@(1,5) (52.3%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:06] \u001b[32mTrain: [ 27/50] Step 400/520 Loss 2.540 Prec@(1,5) (52.2%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:07] \u001b[32mTrain: [ 27/50] Step 420/520 Loss 2.546 Prec@(1,5) (52.0%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:07] \u001b[32mTrain: [ 27/50] Step 440/520 Loss 2.549 Prec@(1,5) (52.0%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:07] \u001b[32mTrain: [ 27/50] Step 460/520 Loss 2.555 Prec@(1,5) (51.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:08] \u001b[32mTrain: [ 27/50] Step 480/520 Loss 2.554 Prec@(1,5) (51.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:08] \u001b[32mTrain: [ 27/50] Step 500/520 Loss 2.553 Prec@(1,5) (51.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:09] \u001b[32mTrain: [ 27/50] Step 520/520 Loss 2.555 Prec@(1,5) (51.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:09] \u001b[32mTrain: [ 27/50] Final Prec@1 51.8780%\u001b[0m\n",
      "[2024-01-15 10:21:12] \u001b[32mValid: [ 27/50] Step 000/104 Loss 2.164 Prec@(1,5) (57.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Step 020/104 Loss 2.084 Prec@(1,5) (55.9%, 83.4%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Step 040/104 Loss 2.034 Prec@(1,5) (55.1%, 84.3%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Step 060/104 Loss 2.014 Prec@(1,5) (55.3%, 84.3%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Step 080/104 Loss 2.012 Prec@(1,5) (55.2%, 84.3%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Step 100/104 Loss 2.003 Prec@(1,5) (55.3%, 84.1%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Step 104/104 Loss 2.004 Prec@(1,5) (55.3%, 84.0%)\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mValid: [ 27/50] Final Prec@1 55.3200%\u001b[0m\n",
      "[2024-01-15 10:21:13] \u001b[32mEpoch 27 LR 0.010934\u001b[0m\n",
      "[2024-01-15 10:21:18] \u001b[32mTrain: [ 28/50] Step 000/520 Loss 2.058 Prec@(1,5) (62.5%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:21:18] \u001b[32mTrain: [ 28/50] Step 020/520 Loss 2.466 Prec@(1,5) (53.4%, 78.6%)\u001b[0m\n",
      "[2024-01-15 10:21:19] \u001b[32mTrain: [ 28/50] Step 040/520 Loss 2.511 Prec@(1,5) (53.4%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:21:19] \u001b[32mTrain: [ 28/50] Step 060/520 Loss 2.529 Prec@(1,5) (53.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:20] \u001b[32mTrain: [ 28/50] Step 080/520 Loss 2.503 Prec@(1,5) (53.2%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:21:20] \u001b[32mTrain: [ 28/50] Step 100/520 Loss 2.512 Prec@(1,5) (53.0%, 77.7%)\u001b[0m\n",
      "[2024-01-15 10:21:20] \u001b[32mTrain: [ 28/50] Step 120/520 Loss 2.526 Prec@(1,5) (52.9%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:21] \u001b[32mTrain: [ 28/50] Step 140/520 Loss 2.537 Prec@(1,5) (52.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:21] \u001b[32mTrain: [ 28/50] Step 160/520 Loss 2.538 Prec@(1,5) (52.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:22] \u001b[32mTrain: [ 28/50] Step 180/520 Loss 2.544 Prec@(1,5) (52.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:22] \u001b[32mTrain: [ 28/50] Step 200/520 Loss 2.536 Prec@(1,5) (52.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:22] \u001b[32mTrain: [ 28/50] Step 220/520 Loss 2.530 Prec@(1,5) (52.6%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:21:23] \u001b[32mTrain: [ 28/50] Step 240/520 Loss 2.536 Prec@(1,5) (52.6%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:23] \u001b[32mTrain: [ 28/50] Step 260/520 Loss 2.544 Prec@(1,5) (52.4%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:24] \u001b[32mTrain: [ 28/50] Step 280/520 Loss 2.544 Prec@(1,5) (52.3%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:24] \u001b[32mTrain: [ 28/50] Step 300/520 Loss 2.549 Prec@(1,5) (52.4%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:24] \u001b[32mTrain: [ 28/50] Step 320/520 Loss 2.551 Prec@(1,5) (52.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:25] \u001b[32mTrain: [ 28/50] Step 340/520 Loss 2.552 Prec@(1,5) (52.4%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:25] \u001b[32mTrain: [ 28/50] Step 360/520 Loss 2.551 Prec@(1,5) (52.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:26] \u001b[32mTrain: [ 28/50] Step 380/520 Loss 2.556 Prec@(1,5) (52.3%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:26] \u001b[32mTrain: [ 28/50] Step 400/520 Loss 2.554 Prec@(1,5) (52.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:26] \u001b[32mTrain: [ 28/50] Step 420/520 Loss 2.560 Prec@(1,5) (52.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:27] \u001b[32mTrain: [ 28/50] Step 440/520 Loss 2.560 Prec@(1,5) (52.3%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:27] \u001b[32mTrain: [ 28/50] Step 460/520 Loss 2.565 Prec@(1,5) (52.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:28] \u001b[32mTrain: [ 28/50] Step 480/520 Loss 2.563 Prec@(1,5) (52.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:21:28] \u001b[32mTrain: [ 28/50] Step 500/520 Loss 2.566 Prec@(1,5) (52.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:21:28] \u001b[32mTrain: [ 28/50] Step 520/520 Loss 2.567 Prec@(1,5) (52.2%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:29] \u001b[32mTrain: [ 28/50] Final Prec@1 52.1980%\u001b[0m\n",
      "[2024-01-15 10:21:32] \u001b[32mValid: [ 28/50] Step 000/104 Loss 2.024 Prec@(1,5) (60.4%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:21:32] \u001b[32mValid: [ 28/50] Step 020/104 Loss 1.949 Prec@(1,5) (56.9%, 84.0%)\u001b[0m\n",
      "[2024-01-15 10:21:32] \u001b[32mValid: [ 28/50] Step 040/104 Loss 1.944 Prec@(1,5) (56.5%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:21:33] \u001b[32mValid: [ 28/50] Step 060/104 Loss 1.914 Prec@(1,5) (57.0%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:21:33] \u001b[32mValid: [ 28/50] Step 080/104 Loss 1.907 Prec@(1,5) (57.0%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:21:33] \u001b[32mValid: [ 28/50] Step 100/104 Loss 1.895 Prec@(1,5) (57.2%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:21:33] \u001b[32mValid: [ 28/50] Step 104/104 Loss 1.897 Prec@(1,5) (57.2%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:21:33] \u001b[32mValid: [ 28/50] Final Prec@1 57.2300%\u001b[0m\n",
      "[2024-01-15 10:21:33] \u001b[32mEpoch 28 LR 0.010158\u001b[0m\n",
      "[2024-01-15 10:21:38] \u001b[32mTrain: [ 29/50] Step 000/520 Loss 2.304 Prec@(1,5) (55.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:21:38] \u001b[32mTrain: [ 29/50] Step 020/520 Loss 2.502 Prec@(1,5) (52.3%, 78.3%)\u001b[0m\n",
      "[2024-01-15 10:21:39] \u001b[32mTrain: [ 29/50] Step 040/520 Loss 2.501 Prec@(1,5) (52.9%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:21:39] \u001b[32mTrain: [ 29/50] Step 060/520 Loss 2.502 Prec@(1,5) (53.0%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:21:39] \u001b[32mTrain: [ 29/50] Step 080/520 Loss 2.507 Prec@(1,5) (52.8%, 78.2%)\u001b[0m\n",
      "[2024-01-15 10:21:40] \u001b[32mTrain: [ 29/50] Step 100/520 Loss 2.518 Prec@(1,5) (53.0%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:21:40] \u001b[32mTrain: [ 29/50] Step 120/520 Loss 2.528 Prec@(1,5) (52.9%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:21:41] \u001b[32mTrain: [ 29/50] Step 140/520 Loss 2.530 Prec@(1,5) (53.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:21:41] \u001b[32mTrain: [ 29/50] Step 160/520 Loss 2.531 Prec@(1,5) (53.1%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:21:41] \u001b[32mTrain: [ 29/50] Step 180/520 Loss 2.543 Prec@(1,5) (52.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:42] \u001b[32mTrain: [ 29/50] Step 200/520 Loss 2.543 Prec@(1,5) (52.9%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:21:42] \u001b[32mTrain: [ 29/50] Step 220/520 Loss 2.549 Prec@(1,5) (52.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:43] \u001b[32mTrain: [ 29/50] Step 240/520 Loss 2.547 Prec@(1,5) (52.6%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:43] \u001b[32mTrain: [ 29/50] Step 260/520 Loss 2.550 Prec@(1,5) (52.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:43] \u001b[32mTrain: [ 29/50] Step 280/520 Loss 2.557 Prec@(1,5) (52.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:44] \u001b[32mTrain: [ 29/50] Step 300/520 Loss 2.556 Prec@(1,5) (52.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:44] \u001b[32mTrain: [ 29/50] Step 320/520 Loss 2.560 Prec@(1,5) (52.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:45] \u001b[32mTrain: [ 29/50] Step 340/520 Loss 2.556 Prec@(1,5) (52.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:45] \u001b[32mTrain: [ 29/50] Step 360/520 Loss 2.557 Prec@(1,5) (52.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:21:45] \u001b[32mTrain: [ 29/50] Step 380/520 Loss 2.555 Prec@(1,5) (52.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:46] \u001b[32mTrain: [ 29/50] Step 400/520 Loss 2.551 Prec@(1,5) (52.6%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:21:46] \u001b[32mTrain: [ 29/50] Step 420/520 Loss 2.553 Prec@(1,5) (52.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:46] \u001b[32mTrain: [ 29/50] Step 440/520 Loss 2.553 Prec@(1,5) (52.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:47] \u001b[32mTrain: [ 29/50] Step 460/520 Loss 2.554 Prec@(1,5) (52.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:47] \u001b[32mTrain: [ 29/50] Step 480/520 Loss 2.555 Prec@(1,5) (52.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:21:48] \u001b[32mTrain: [ 29/50] Step 500/520 Loss 2.558 Prec@(1,5) (52.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:48] \u001b[32mTrain: [ 29/50] Step 520/520 Loss 2.557 Prec@(1,5) (52.5%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:21:48] \u001b[32mTrain: [ 29/50] Final Prec@1 52.5060%\u001b[0m\n",
      "[2024-01-15 10:21:52] \u001b[32mValid: [ 29/50] Step 000/104 Loss 2.016 Prec@(1,5) (59.4%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:21:52] \u001b[32mValid: [ 29/50] Step 020/104 Loss 1.950 Prec@(1,5) (56.5%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:21:52] \u001b[32mValid: [ 29/50] Step 040/104 Loss 1.954 Prec@(1,5) (55.5%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:21:52] \u001b[32mValid: [ 29/50] Step 060/104 Loss 1.926 Prec@(1,5) (56.0%, 84.8%)\u001b[0m\n",
      "[2024-01-15 10:21:52] \u001b[32mValid: [ 29/50] Step 080/104 Loss 1.923 Prec@(1,5) (55.5%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:21:53] \u001b[32mValid: [ 29/50] Step 100/104 Loss 1.915 Prec@(1,5) (55.6%, 84.7%)\u001b[0m\n",
      "[2024-01-15 10:21:53] \u001b[32mValid: [ 29/50] Step 104/104 Loss 1.918 Prec@(1,5) (55.6%, 84.6%)\u001b[0m\n",
      "[2024-01-15 10:21:53] \u001b[32mValid: [ 29/50] Final Prec@1 55.6200%\u001b[0m\n",
      "[2024-01-15 10:21:53] \u001b[32mEpoch 29 LR 0.009392\u001b[0m\n",
      "[2024-01-15 10:21:57] \u001b[32mTrain: [ 30/50] Step 000/520 Loss 2.754 Prec@(1,5) (52.1%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:21:58] \u001b[32mTrain: [ 30/50] Step 020/520 Loss 2.511 Prec@(1,5) (52.2%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:21:58] \u001b[32mTrain: [ 30/50] Step 040/520 Loss 2.507 Prec@(1,5) (53.3%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:21:59] \u001b[32mTrain: [ 30/50] Step 060/520 Loss 2.508 Prec@(1,5) (53.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:21:59] \u001b[32mTrain: [ 30/50] Step 080/520 Loss 2.543 Prec@(1,5) (52.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:21:59] \u001b[32mTrain: [ 30/50] Step 100/520 Loss 2.526 Prec@(1,5) (53.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:00] \u001b[32mTrain: [ 30/50] Step 120/520 Loss 2.523 Prec@(1,5) (53.0%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:22:00] \u001b[32mTrain: [ 30/50] Step 140/520 Loss 2.525 Prec@(1,5) (52.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:22:01] \u001b[32mTrain: [ 30/50] Step 160/520 Loss 2.533 Prec@(1,5) (52.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:01] \u001b[32mTrain: [ 30/50] Step 180/520 Loss 2.530 Prec@(1,5) (52.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:01] \u001b[32mTrain: [ 30/50] Step 200/520 Loss 2.528 Prec@(1,5) (52.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:02] \u001b[32mTrain: [ 30/50] Step 220/520 Loss 2.533 Prec@(1,5) (52.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:02] \u001b[32mTrain: [ 30/50] Step 240/520 Loss 2.532 Prec@(1,5) (52.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:03] \u001b[32mTrain: [ 30/50] Step 260/520 Loss 2.532 Prec@(1,5) (52.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:03] \u001b[32mTrain: [ 30/50] Step 280/520 Loss 2.531 Prec@(1,5) (52.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:03] \u001b[32mTrain: [ 30/50] Step 300/520 Loss 2.529 Prec@(1,5) (53.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:04] \u001b[32mTrain: [ 30/50] Step 320/520 Loss 2.532 Prec@(1,5) (53.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:04] \u001b[32mTrain: [ 30/50] Step 340/520 Loss 2.536 Prec@(1,5) (52.9%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:22:05] \u001b[32mTrain: [ 30/50] Step 360/520 Loss 2.541 Prec@(1,5) (52.8%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:05] \u001b[32mTrain: [ 30/50] Step 380/520 Loss 2.546 Prec@(1,5) (52.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:05] \u001b[32mTrain: [ 30/50] Step 400/520 Loss 2.551 Prec@(1,5) (52.6%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:06] \u001b[32mTrain: [ 30/50] Step 420/520 Loss 2.551 Prec@(1,5) (52.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:06] \u001b[32mTrain: [ 30/50] Step 440/520 Loss 2.553 Prec@(1,5) (52.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:06] \u001b[32mTrain: [ 30/50] Step 460/520 Loss 2.551 Prec@(1,5) (52.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:07] \u001b[32mTrain: [ 30/50] Step 480/520 Loss 2.550 Prec@(1,5) (52.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:07] \u001b[32mTrain: [ 30/50] Step 500/520 Loss 2.546 Prec@(1,5) (52.8%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:08] \u001b[32mTrain: [ 30/50] Step 520/520 Loss 2.546 Prec@(1,5) (52.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:08] \u001b[32mTrain: [ 30/50] Final Prec@1 52.7180%\u001b[0m\n",
      "[2024-01-15 10:22:11] \u001b[32mValid: [ 30/50] Step 000/104 Loss 2.065 Prec@(1,5) (59.4%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Step 020/104 Loss 2.021 Prec@(1,5) (56.1%, 84.0%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Step 040/104 Loss 1.958 Prec@(1,5) (55.8%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Step 060/104 Loss 1.916 Prec@(1,5) (56.5%, 85.0%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Step 080/104 Loss 1.909 Prec@(1,5) (56.6%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Step 100/104 Loss 1.900 Prec@(1,5) (57.0%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Step 104/104 Loss 1.900 Prec@(1,5) (57.0%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mValid: [ 30/50] Final Prec@1 57.0200%\u001b[0m\n",
      "[2024-01-15 10:22:12] \u001b[32mEpoch 30 LR 0.008638\u001b[0m\n",
      "[2024-01-15 10:22:17] \u001b[32mTrain: [ 31/50] Step 000/520 Loss 2.754 Prec@(1,5) (49.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 10:22:17] \u001b[32mTrain: [ 31/50] Step 020/520 Loss 2.547 Prec@(1,5) (52.7%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:22:18] \u001b[32mTrain: [ 31/50] Step 040/520 Loss 2.505 Prec@(1,5) (53.7%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:22:18] \u001b[32mTrain: [ 31/50] Step 060/520 Loss 2.555 Prec@(1,5) (52.7%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:22:19] \u001b[32mTrain: [ 31/50] Step 080/520 Loss 2.546 Prec@(1,5) (52.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:19] \u001b[32mTrain: [ 31/50] Step 100/520 Loss 2.535 Prec@(1,5) (52.8%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:22:19] \u001b[32mTrain: [ 31/50] Step 120/520 Loss 2.518 Prec@(1,5) (53.2%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:22:20] \u001b[32mTrain: [ 31/50] Step 140/520 Loss 2.523 Prec@(1,5) (53.0%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:20] \u001b[32mTrain: [ 31/50] Step 160/520 Loss 2.527 Prec@(1,5) (53.0%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:21] \u001b[32mTrain: [ 31/50] Step 180/520 Loss 2.528 Prec@(1,5) (53.0%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:22:21] \u001b[32mTrain: [ 31/50] Step 200/520 Loss 2.523 Prec@(1,5) (53.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:22:21] \u001b[32mTrain: [ 31/50] Step 220/520 Loss 2.533 Prec@(1,5) (53.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:22] \u001b[32mTrain: [ 31/50] Step 240/520 Loss 2.540 Prec@(1,5) (53.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:22] \u001b[32mTrain: [ 31/50] Step 260/520 Loss 2.544 Prec@(1,5) (52.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:23] \u001b[32mTrain: [ 31/50] Step 280/520 Loss 2.541 Prec@(1,5) (53.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:23] \u001b[32mTrain: [ 31/50] Step 300/520 Loss 2.543 Prec@(1,5) (53.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:22:23] \u001b[32mTrain: [ 31/50] Step 320/520 Loss 2.543 Prec@(1,5) (53.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:22:24] \u001b[32mTrain: [ 31/50] Step 340/520 Loss 2.544 Prec@(1,5) (53.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:22:24] \u001b[32mTrain: [ 31/50] Step 360/520 Loss 2.542 Prec@(1,5) (53.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:25] \u001b[32mTrain: [ 31/50] Step 380/520 Loss 2.546 Prec@(1,5) (53.0%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:22:25] \u001b[32mTrain: [ 31/50] Step 400/520 Loss 2.548 Prec@(1,5) (52.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:22:25] \u001b[32mTrain: [ 31/50] Step 420/520 Loss 2.547 Prec@(1,5) (52.9%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:22:26] \u001b[32mTrain: [ 31/50] Step 440/520 Loss 2.546 Prec@(1,5) (52.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:26] \u001b[32mTrain: [ 31/50] Step 460/520 Loss 2.544 Prec@(1,5) (53.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:27] \u001b[32mTrain: [ 31/50] Step 480/520 Loss 2.547 Prec@(1,5) (53.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:27] \u001b[32mTrain: [ 31/50] Step 500/520 Loss 2.546 Prec@(1,5) (52.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:27] \u001b[32mTrain: [ 31/50] Step 520/520 Loss 2.545 Prec@(1,5) (53.0%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:28] \u001b[32mTrain: [ 31/50] Final Prec@1 52.9640%\u001b[0m\n",
      "[2024-01-15 10:22:31] \u001b[32mValid: [ 31/50] Step 000/104 Loss 2.303 Prec@(1,5) (60.4%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:22:31] \u001b[32mValid: [ 31/50] Step 020/104 Loss 1.935 Prec@(1,5) (57.8%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:22:31] \u001b[32mValid: [ 31/50] Step 040/104 Loss 1.905 Prec@(1,5) (57.2%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:22:32] \u001b[32mValid: [ 31/50] Step 060/104 Loss 1.874 Prec@(1,5) (57.8%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:22:32] \u001b[32mValid: [ 31/50] Step 080/104 Loss 1.870 Prec@(1,5) (57.8%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:22:32] \u001b[32mValid: [ 31/50] Step 100/104 Loss 1.860 Prec@(1,5) (57.9%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:22:32] \u001b[32mValid: [ 31/50] Step 104/104 Loss 1.862 Prec@(1,5) (58.0%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:22:32] \u001b[32mValid: [ 31/50] Final Prec@1 58.0000%\u001b[0m\n",
      "[2024-01-15 10:22:32] \u001b[32mEpoch 31 LR 0.007899\u001b[0m\n",
      "[2024-01-15 10:22:37] \u001b[32mTrain: [ 32/50] Step 000/520 Loss 2.428 Prec@(1,5) (61.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:22:37] \u001b[32mTrain: [ 32/50] Step 020/520 Loss 2.508 Prec@(1,5) (54.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:38] \u001b[32mTrain: [ 32/50] Step 040/520 Loss 2.461 Prec@(1,5) (54.3%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:22:38] \u001b[32mTrain: [ 32/50] Step 060/520 Loss 2.465 Prec@(1,5) (54.0%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:22:38] \u001b[32mTrain: [ 32/50] Step 080/520 Loss 2.473 Prec@(1,5) (53.7%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:22:39] \u001b[32mTrain: [ 32/50] Step 100/520 Loss 2.469 Prec@(1,5) (53.9%, 77.9%)\u001b[0m\n",
      "[2024-01-15 10:22:39] \u001b[32mTrain: [ 32/50] Step 120/520 Loss 2.483 Prec@(1,5) (53.9%, 77.6%)\u001b[0m\n",
      "[2024-01-15 10:22:39] \u001b[32mTrain: [ 32/50] Step 140/520 Loss 2.503 Prec@(1,5) (53.5%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:22:40] \u001b[32mTrain: [ 32/50] Step 160/520 Loss 2.514 Prec@(1,5) (53.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:40] \u001b[32mTrain: [ 32/50] Step 180/520 Loss 2.517 Prec@(1,5) (53.5%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:41] \u001b[32mTrain: [ 32/50] Step 200/520 Loss 2.510 Prec@(1,5) (53.7%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:22:41] \u001b[32mTrain: [ 32/50] Step 220/520 Loss 2.511 Prec@(1,5) (53.7%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:22:41] \u001b[32mTrain: [ 32/50] Step 240/520 Loss 2.518 Prec@(1,5) (53.6%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:22:42] \u001b[32mTrain: [ 32/50] Step 260/520 Loss 2.523 Prec@(1,5) (53.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:22:42] \u001b[32mTrain: [ 32/50] Step 280/520 Loss 2.523 Prec@(1,5) (53.5%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:43] \u001b[32mTrain: [ 32/50] Step 300/520 Loss 2.529 Prec@(1,5) (53.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:43] \u001b[32mTrain: [ 32/50] Step 320/520 Loss 2.527 Prec@(1,5) (53.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:43] \u001b[32mTrain: [ 32/50] Step 340/520 Loss 2.526 Prec@(1,5) (53.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:22:44] \u001b[32mTrain: [ 32/50] Step 360/520 Loss 2.530 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:44] \u001b[32mTrain: [ 32/50] Step 380/520 Loss 2.530 Prec@(1,5) (53.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:45] \u001b[32mTrain: [ 32/50] Step 400/520 Loss 2.531 Prec@(1,5) (53.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:45] \u001b[32mTrain: [ 32/50] Step 420/520 Loss 2.535 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:45] \u001b[32mTrain: [ 32/50] Step 440/520 Loss 2.537 Prec@(1,5) (53.3%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:46] \u001b[32mTrain: [ 32/50] Step 460/520 Loss 2.537 Prec@(1,5) (53.3%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:22:46] \u001b[32mTrain: [ 32/50] Step 480/520 Loss 2.536 Prec@(1,5) (53.2%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:46] \u001b[32mTrain: [ 32/50] Step 500/520 Loss 2.534 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:47] \u001b[32mTrain: [ 32/50] Step 520/520 Loss 2.534 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:22:47] \u001b[32mTrain: [ 32/50] Final Prec@1 53.3360%\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 000/104 Loss 2.154 Prec@(1,5) (57.3%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 020/104 Loss 1.892 Prec@(1,5) (57.5%, 84.9%)\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 040/104 Loss 1.877 Prec@(1,5) (56.8%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 060/104 Loss 1.845 Prec@(1,5) (57.6%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 080/104 Loss 1.841 Prec@(1,5) (57.6%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 100/104 Loss 1.833 Prec@(1,5) (57.9%, 85.6%)\u001b[0m\n",
      "[2024-01-15 10:22:51] \u001b[32mValid: [ 32/50] Step 104/104 Loss 1.836 Prec@(1,5) (57.9%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:22:52] \u001b[32mValid: [ 32/50] Final Prec@1 57.9000%\u001b[0m\n",
      "[2024-01-15 10:22:52] \u001b[32mEpoch 32 LR 0.007178\u001b[0m\n",
      "[2024-01-15 10:22:56] \u001b[32mTrain: [ 33/50] Step 000/520 Loss 2.298 Prec@(1,5) (56.2%, 80.2%)\u001b[0m\n",
      "[2024-01-15 10:22:57] \u001b[32mTrain: [ 33/50] Step 020/520 Loss 2.432 Prec@(1,5) (54.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:22:57] \u001b[32mTrain: [ 33/50] Step 040/520 Loss 2.464 Prec@(1,5) (54.8%, 77.8%)\u001b[0m\n",
      "[2024-01-15 10:22:57] \u001b[32mTrain: [ 33/50] Step 060/520 Loss 2.462 Prec@(1,5) (54.8%, 77.5%)\u001b[0m\n",
      "[2024-01-15 10:22:58] \u001b[32mTrain: [ 33/50] Step 080/520 Loss 2.473 Prec@(1,5) (54.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:22:58] \u001b[32mTrain: [ 33/50] Step 100/520 Loss 2.470 Prec@(1,5) (54.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:22:59] \u001b[32mTrain: [ 33/50] Step 120/520 Loss 2.467 Prec@(1,5) (54.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:22:59] \u001b[32mTrain: [ 33/50] Step 140/520 Loss 2.472 Prec@(1,5) (54.2%, 77.4%)\u001b[0m\n",
      "[2024-01-15 10:22:59] \u001b[32mTrain: [ 33/50] Step 160/520 Loss 2.481 Prec@(1,5) (54.1%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:23:00] \u001b[32mTrain: [ 33/50] Step 180/520 Loss 2.488 Prec@(1,5) (53.9%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:23:00] \u001b[32mTrain: [ 33/50] Step 200/520 Loss 2.482 Prec@(1,5) (53.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:23:01] \u001b[32mTrain: [ 33/50] Step 220/520 Loss 2.483 Prec@(1,5) (53.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:23:01] \u001b[32mTrain: [ 33/50] Step 240/520 Loss 2.484 Prec@(1,5) (53.9%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:23:01] \u001b[32mTrain: [ 33/50] Step 260/520 Loss 2.489 Prec@(1,5) (53.8%, 77.3%)\u001b[0m\n",
      "[2024-01-15 10:23:02] \u001b[32mTrain: [ 33/50] Step 280/520 Loss 2.491 Prec@(1,5) (53.8%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:23:02] \u001b[32mTrain: [ 33/50] Step 300/520 Loss 2.494 Prec@(1,5) (53.8%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:23:03] \u001b[32mTrain: [ 33/50] Step 320/520 Loss 2.499 Prec@(1,5) (53.7%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:23:03] \u001b[32mTrain: [ 33/50] Step 340/520 Loss 2.504 Prec@(1,5) (53.6%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:23:03] \u001b[32mTrain: [ 33/50] Step 360/520 Loss 2.505 Prec@(1,5) (53.5%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:23:04] \u001b[32mTrain: [ 33/50] Step 380/520 Loss 2.511 Prec@(1,5) (53.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:04] \u001b[32mTrain: [ 33/50] Step 400/520 Loss 2.511 Prec@(1,5) (53.4%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:05] \u001b[32mTrain: [ 33/50] Step 420/520 Loss 2.512 Prec@(1,5) (53.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:05] \u001b[32mTrain: [ 33/50] Step 440/520 Loss 2.518 Prec@(1,5) (53.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:05] \u001b[32mTrain: [ 33/50] Step 460/520 Loss 2.521 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:06] \u001b[32mTrain: [ 33/50] Step 480/520 Loss 2.522 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:06] \u001b[32mTrain: [ 33/50] Step 500/520 Loss 2.520 Prec@(1,5) (53.3%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:06] \u001b[32mTrain: [ 33/50] Step 520/520 Loss 2.520 Prec@(1,5) (53.2%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:07] \u001b[32mTrain: [ 33/50] Final Prec@1 53.2300%\u001b[0m\n",
      "[2024-01-15 10:23:10] \u001b[32mValid: [ 33/50] Step 000/104 Loss 1.986 Prec@(1,5) (58.3%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:23:10] \u001b[32mValid: [ 33/50] Step 020/104 Loss 1.806 Prec@(1,5) (58.4%, 85.2%)\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mValid: [ 33/50] Step 040/104 Loss 1.790 Prec@(1,5) (58.4%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mValid: [ 33/50] Step 060/104 Loss 1.754 Prec@(1,5) (58.8%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mValid: [ 33/50] Step 080/104 Loss 1.746 Prec@(1,5) (58.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mValid: [ 33/50] Step 100/104 Loss 1.733 Prec@(1,5) (59.3%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mValid: [ 33/50] Step 104/104 Loss 1.734 Prec@(1,5) (59.3%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mValid: [ 33/50] Final Prec@1 59.3300%\u001b[0m\n",
      "[2024-01-15 10:23:11] \u001b[32mEpoch 33 LR 0.006479\u001b[0m\n",
      "[2024-01-15 10:23:16] \u001b[32mTrain: [ 34/50] Step 000/520 Loss 2.849 Prec@(1,5) (49.0%, 71.9%)\u001b[0m\n",
      "[2024-01-15 10:23:16] \u001b[32mTrain: [ 34/50] Step 020/520 Loss 2.541 Prec@(1,5) (53.3%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:23:17] \u001b[32mTrain: [ 34/50] Step 040/520 Loss 2.495 Prec@(1,5) (53.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:23:17] \u001b[32mTrain: [ 34/50] Step 060/520 Loss 2.491 Prec@(1,5) (53.9%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:23:17] \u001b[32mTrain: [ 34/50] Step 080/520 Loss 2.476 Prec@(1,5) (54.3%, 77.2%)\u001b[0m\n",
      "[2024-01-15 10:23:18] \u001b[32mTrain: [ 34/50] Step 100/520 Loss 2.494 Prec@(1,5) (53.9%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:18] \u001b[32mTrain: [ 34/50] Step 120/520 Loss 2.495 Prec@(1,5) (54.2%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:19] \u001b[32mTrain: [ 34/50] Step 140/520 Loss 2.489 Prec@(1,5) (54.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:23:19] \u001b[32mTrain: [ 34/50] Step 160/520 Loss 2.488 Prec@(1,5) (54.1%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:23:19] \u001b[32mTrain: [ 34/50] Step 180/520 Loss 2.487 Prec@(1,5) (54.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:23:20] \u001b[32mTrain: [ 34/50] Step 200/520 Loss 2.485 Prec@(1,5) (54.1%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:23:20] \u001b[32mTrain: [ 34/50] Step 220/520 Loss 2.484 Prec@(1,5) (54.2%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:23:21] \u001b[32mTrain: [ 34/50] Step 240/520 Loss 2.493 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:21] \u001b[32mTrain: [ 34/50] Step 260/520 Loss 2.493 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:21] \u001b[32mTrain: [ 34/50] Step 280/520 Loss 2.491 Prec@(1,5) (54.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:22] \u001b[32mTrain: [ 34/50] Step 300/520 Loss 2.496 Prec@(1,5) (54.1%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:23:22] \u001b[32mTrain: [ 34/50] Step 320/520 Loss 2.491 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:23] \u001b[32mTrain: [ 34/50] Step 340/520 Loss 2.492 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:23] \u001b[32mTrain: [ 34/50] Step 360/520 Loss 2.491 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:23] \u001b[32mTrain: [ 34/50] Step 380/520 Loss 2.488 Prec@(1,5) (54.2%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:24] \u001b[32mTrain: [ 34/50] Step 400/520 Loss 2.491 Prec@(1,5) (54.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:24] \u001b[32mTrain: [ 34/50] Step 420/520 Loss 2.492 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:25] \u001b[32mTrain: [ 34/50] Step 440/520 Loss 2.496 Prec@(1,5) (54.1%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:25] \u001b[32mTrain: [ 34/50] Step 460/520 Loss 2.496 Prec@(1,5) (54.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:25] \u001b[32mTrain: [ 34/50] Step 480/520 Loss 2.498 Prec@(1,5) (54.0%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:26] \u001b[32mTrain: [ 34/50] Step 500/520 Loss 2.498 Prec@(1,5) (54.0%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:26] \u001b[32mTrain: [ 34/50] Step 520/520 Loss 2.500 Prec@(1,5) (53.9%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:23:26] \u001b[32mTrain: [ 34/50] Final Prec@1 53.9180%\u001b[0m\n",
      "[2024-01-15 10:23:30] \u001b[32mValid: [ 34/50] Step 000/104 Loss 2.066 Prec@(1,5) (56.2%, 81.2%)\u001b[0m\n",
      "[2024-01-15 10:23:30] \u001b[32mValid: [ 34/50] Step 020/104 Loss 1.871 Prec@(1,5) (57.9%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:23:30] \u001b[32mValid: [ 34/50] Step 040/104 Loss 1.859 Prec@(1,5) (56.9%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:23:30] \u001b[32mValid: [ 34/50] Step 060/104 Loss 1.821 Prec@(1,5) (57.7%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:23:30] \u001b[32mValid: [ 34/50] Step 080/104 Loss 1.824 Prec@(1,5) (57.6%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:23:31] \u001b[32mValid: [ 34/50] Step 100/104 Loss 1.813 Prec@(1,5) (57.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:23:31] \u001b[32mValid: [ 34/50] Step 104/104 Loss 1.816 Prec@(1,5) (58.0%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:23:31] \u001b[32mValid: [ 34/50] Final Prec@1 57.9700%\u001b[0m\n",
      "[2024-01-15 10:23:31] \u001b[32mEpoch 34 LR 0.005803\u001b[0m\n",
      "[2024-01-15 10:23:35] \u001b[32mTrain: [ 35/50] Step 000/520 Loss 2.653 Prec@(1,5) (46.9%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:23:36] \u001b[32mTrain: [ 35/50] Step 020/520 Loss 2.556 Prec@(1,5) (53.2%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:23:36] \u001b[32mTrain: [ 35/50] Step 040/520 Loss 2.544 Prec@(1,5) (53.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:37] \u001b[32mTrain: [ 35/50] Step 060/520 Loss 2.560 Prec@(1,5) (52.9%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:23:37] \u001b[32mTrain: [ 35/50] Step 080/520 Loss 2.582 Prec@(1,5) (52.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:23:37] \u001b[32mTrain: [ 35/50] Step 100/520 Loss 2.582 Prec@(1,5) (52.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:23:38] \u001b[32mTrain: [ 35/50] Step 120/520 Loss 2.561 Prec@(1,5) (53.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:23:38] \u001b[32mTrain: [ 35/50] Step 140/520 Loss 2.555 Prec@(1,5) (53.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:39] \u001b[32mTrain: [ 35/50] Step 160/520 Loss 2.552 Prec@(1,5) (53.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:39] \u001b[32mTrain: [ 35/50] Step 180/520 Loss 2.555 Prec@(1,5) (53.3%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:39] \u001b[32mTrain: [ 35/50] Step 200/520 Loss 2.565 Prec@(1,5) (53.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:23:40] \u001b[32mTrain: [ 35/50] Step 220/520 Loss 2.560 Prec@(1,5) (53.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:40] \u001b[32mTrain: [ 35/50] Step 240/520 Loss 2.558 Prec@(1,5) (53.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:41] \u001b[32mTrain: [ 35/50] Step 260/520 Loss 2.557 Prec@(1,5) (53.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:41] \u001b[32mTrain: [ 35/50] Step 280/520 Loss 2.556 Prec@(1,5) (53.2%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:41] \u001b[32mTrain: [ 35/50] Step 300/520 Loss 2.557 Prec@(1,5) (53.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:42] \u001b[32mTrain: [ 35/50] Step 320/520 Loss 2.557 Prec@(1,5) (53.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:42] \u001b[32mTrain: [ 35/50] Step 340/520 Loss 2.557 Prec@(1,5) (53.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:43] \u001b[32mTrain: [ 35/50] Step 360/520 Loss 2.560 Prec@(1,5) (53.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:23:43] \u001b[32mTrain: [ 35/50] Step 380/520 Loss 2.556 Prec@(1,5) (53.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:23:43] \u001b[32mTrain: [ 35/50] Step 400/520 Loss 2.554 Prec@(1,5) (53.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:44] \u001b[32mTrain: [ 35/50] Step 420/520 Loss 2.556 Prec@(1,5) (53.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:44] \u001b[32mTrain: [ 35/50] Step 440/520 Loss 2.559 Prec@(1,5) (53.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:45] \u001b[32mTrain: [ 35/50] Step 460/520 Loss 2.560 Prec@(1,5) (53.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:23:45] \u001b[32mTrain: [ 35/50] Step 480/520 Loss 2.558 Prec@(1,5) (53.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:23:45] \u001b[32mTrain: [ 35/50] Step 500/520 Loss 2.555 Prec@(1,5) (53.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:23:46] \u001b[32mTrain: [ 35/50] Step 520/520 Loss 2.556 Prec@(1,5) (53.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:23:46] \u001b[32mTrain: [ 35/50] Final Prec@1 52.9980%\u001b[0m\n",
      "[2024-01-15 10:23:49] \u001b[32mValid: [ 35/50] Step 000/104 Loss 2.014 Prec@(1,5) (58.3%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Step 020/104 Loss 1.889 Prec@(1,5) (58.7%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Step 040/104 Loss 1.851 Prec@(1,5) (57.6%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Step 060/104 Loss 1.818 Prec@(1,5) (58.4%, 85.7%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Step 080/104 Loss 1.812 Prec@(1,5) (58.1%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Step 100/104 Loss 1.802 Prec@(1,5) (58.3%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Step 104/104 Loss 1.804 Prec@(1,5) (58.4%, 85.9%)\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mValid: [ 35/50] Final Prec@1 58.3700%\u001b[0m\n",
      "[2024-01-15 10:23:50] \u001b[32mEpoch 35 LR 0.005153\u001b[0m\n",
      "[2024-01-15 10:23:55] \u001b[32mTrain: [ 36/50] Step 000/520 Loss 2.911 Prec@(1,5) (51.0%, 71.9%)\u001b[0m\n",
      "[2024-01-15 10:23:56] \u001b[32mTrain: [ 36/50] Step 020/520 Loss 2.493 Prec@(1,5) (54.0%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:23:56] \u001b[32mTrain: [ 36/50] Step 040/520 Loss 2.483 Prec@(1,5) (54.8%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:23:56] \u001b[32mTrain: [ 36/50] Step 060/520 Loss 2.520 Prec@(1,5) (53.8%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:23:57] \u001b[32mTrain: [ 36/50] Step 080/520 Loss 2.517 Prec@(1,5) (54.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:23:57] \u001b[32mTrain: [ 36/50] Step 100/520 Loss 2.512 Prec@(1,5) (54.2%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:23:57] \u001b[32mTrain: [ 36/50] Step 120/520 Loss 2.524 Prec@(1,5) (54.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:23:58] \u001b[32mTrain: [ 36/50] Step 140/520 Loss 2.517 Prec@(1,5) (54.3%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:23:58] \u001b[32mTrain: [ 36/50] Step 160/520 Loss 2.508 Prec@(1,5) (54.4%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:23:59] \u001b[32mTrain: [ 36/50] Step 180/520 Loss 2.497 Prec@(1,5) (54.5%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:23:59] \u001b[32mTrain: [ 36/50] Step 200/520 Loss 2.493 Prec@(1,5) (54.4%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:23:59] \u001b[32mTrain: [ 36/50] Step 220/520 Loss 2.498 Prec@(1,5) (54.4%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:00] \u001b[32mTrain: [ 36/50] Step 240/520 Loss 2.504 Prec@(1,5) (54.2%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:00] \u001b[32mTrain: [ 36/50] Step 260/520 Loss 2.507 Prec@(1,5) (54.3%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:01] \u001b[32mTrain: [ 36/50] Step 280/520 Loss 2.509 Prec@(1,5) (54.3%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:01] \u001b[32mTrain: [ 36/50] Step 300/520 Loss 2.508 Prec@(1,5) (54.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:01] \u001b[32mTrain: [ 36/50] Step 320/520 Loss 2.513 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:02] \u001b[32mTrain: [ 36/50] Step 340/520 Loss 2.518 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:02] \u001b[32mTrain: [ 36/50] Step 360/520 Loss 2.518 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:03] \u001b[32mTrain: [ 36/50] Step 380/520 Loss 2.520 Prec@(1,5) (54.1%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:03] \u001b[32mTrain: [ 36/50] Step 400/520 Loss 2.516 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:03] \u001b[32mTrain: [ 36/50] Step 420/520 Loss 2.519 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:04] \u001b[32mTrain: [ 36/50] Step 440/520 Loss 2.519 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:04] \u001b[32mTrain: [ 36/50] Step 460/520 Loss 2.518 Prec@(1,5) (54.2%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:05] \u001b[32mTrain: [ 36/50] Step 480/520 Loss 2.516 Prec@(1,5) (54.1%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:05] \u001b[32mTrain: [ 36/50] Step 500/520 Loss 2.513 Prec@(1,5) (54.2%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:05] \u001b[32mTrain: [ 36/50] Step 520/520 Loss 2.512 Prec@(1,5) (54.2%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:06] \u001b[32mTrain: [ 36/50] Final Prec@1 54.1840%\u001b[0m\n",
      "[2024-01-15 10:24:09] \u001b[32mValid: [ 36/50] Step 000/104 Loss 1.839 Prec@(1,5) (62.5%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:24:09] \u001b[32mValid: [ 36/50] Step 020/104 Loss 1.736 Prec@(1,5) (59.5%, 85.5%)\u001b[0m\n",
      "[2024-01-15 10:24:09] \u001b[32mValid: [ 36/50] Step 040/104 Loss 1.727 Prec@(1,5) (59.0%, 86.0%)\u001b[0m\n",
      "[2024-01-15 10:24:10] \u001b[32mValid: [ 36/50] Step 060/104 Loss 1.702 Prec@(1,5) (59.4%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:24:10] \u001b[32mValid: [ 36/50] Step 080/104 Loss 1.699 Prec@(1,5) (59.4%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:24:10] \u001b[32mValid: [ 36/50] Step 100/104 Loss 1.697 Prec@(1,5) (59.6%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:24:10] \u001b[32mValid: [ 36/50] Step 104/104 Loss 1.700 Prec@(1,5) (59.5%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:24:10] \u001b[32mValid: [ 36/50] Final Prec@1 59.5400%\u001b[0m\n",
      "[2024-01-15 10:24:10] \u001b[32mEpoch 36 LR 0.004533\u001b[0m\n",
      "[2024-01-15 10:24:15] \u001b[32mTrain: [ 37/50] Step 000/520 Loss 2.234 Prec@(1,5) (57.3%, 79.2%)\u001b[0m\n",
      "[2024-01-15 10:24:15] \u001b[32mTrain: [ 37/50] Step 020/520 Loss 2.448 Prec@(1,5) (55.3%, 77.0%)\u001b[0m\n",
      "[2024-01-15 10:24:16] \u001b[32mTrain: [ 37/50] Step 040/520 Loss 2.477 Prec@(1,5) (54.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:16] \u001b[32mTrain: [ 37/50] Step 060/520 Loss 2.476 Prec@(1,5) (54.7%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:24:16] \u001b[32mTrain: [ 37/50] Step 080/520 Loss 2.483 Prec@(1,5) (54.7%, 76.8%)\u001b[0m\n",
      "[2024-01-15 10:24:17] \u001b[32mTrain: [ 37/50] Step 100/520 Loss 2.488 Prec@(1,5) (54.9%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:24:17] \u001b[32mTrain: [ 37/50] Step 120/520 Loss 2.490 Prec@(1,5) (54.8%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:18] \u001b[32mTrain: [ 37/50] Step 140/520 Loss 2.486 Prec@(1,5) (54.8%, 76.7%)\u001b[0m\n",
      "[2024-01-15 10:24:18] \u001b[32mTrain: [ 37/50] Step 160/520 Loss 2.494 Prec@(1,5) (54.6%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:18] \u001b[32mTrain: [ 37/50] Step 180/520 Loss 2.498 Prec@(1,5) (54.3%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:19] \u001b[32mTrain: [ 37/50] Step 200/520 Loss 2.502 Prec@(1,5) (54.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:19] \u001b[32mTrain: [ 37/50] Step 220/520 Loss 2.502 Prec@(1,5) (54.3%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:19] \u001b[32mTrain: [ 37/50] Step 240/520 Loss 2.500 Prec@(1,5) (54.4%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:20] \u001b[32mTrain: [ 37/50] Step 260/520 Loss 2.498 Prec@(1,5) (54.4%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:20] \u001b[32mTrain: [ 37/50] Step 280/520 Loss 2.504 Prec@(1,5) (54.2%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:21] \u001b[32mTrain: [ 37/50] Step 300/520 Loss 2.505 Prec@(1,5) (54.2%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:21] \u001b[32mTrain: [ 37/50] Step 320/520 Loss 2.502 Prec@(1,5) (54.3%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:21] \u001b[32mTrain: [ 37/50] Step 340/520 Loss 2.500 Prec@(1,5) (54.3%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:22] \u001b[32mTrain: [ 37/50] Step 360/520 Loss 2.504 Prec@(1,5) (54.2%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:22] \u001b[32mTrain: [ 37/50] Step 380/520 Loss 2.508 Prec@(1,5) (54.2%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:23] \u001b[32mTrain: [ 37/50] Step 400/520 Loss 2.510 Prec@(1,5) (54.1%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:23] \u001b[32mTrain: [ 37/50] Step 420/520 Loss 2.508 Prec@(1,5) (54.2%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:24:24] \u001b[32mTrain: [ 37/50] Step 440/520 Loss 2.511 Prec@(1,5) (54.1%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:24] \u001b[32mTrain: [ 37/50] Step 460/520 Loss 2.520 Prec@(1,5) (53.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:24] \u001b[32mTrain: [ 37/50] Step 480/520 Loss 2.517 Prec@(1,5) (53.9%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:25] \u001b[32mTrain: [ 37/50] Step 500/520 Loss 2.519 Prec@(1,5) (53.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:25] \u001b[32mTrain: [ 37/50] Step 520/520 Loss 2.520 Prec@(1,5) (53.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:25] \u001b[32mTrain: [ 37/50] Final Prec@1 53.9020%\u001b[0m\n",
      "[2024-01-15 10:24:29] \u001b[32mValid: [ 37/50] Step 000/104 Loss 1.919 Prec@(1,5) (59.4%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:24:29] \u001b[32mValid: [ 37/50] Step 020/104 Loss 1.769 Prec@(1,5) (60.1%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:24:29] \u001b[32mValid: [ 37/50] Step 040/104 Loss 1.765 Prec@(1,5) (58.9%, 86.1%)\u001b[0m\n",
      "[2024-01-15 10:24:30] \u001b[32mValid: [ 37/50] Step 060/104 Loss 1.733 Prec@(1,5) (59.9%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:24:30] \u001b[32mValid: [ 37/50] Step 080/104 Loss 1.730 Prec@(1,5) (59.4%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:24:30] \u001b[32mValid: [ 37/50] Step 100/104 Loss 1.729 Prec@(1,5) (59.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:24:30] \u001b[32mValid: [ 37/50] Step 104/104 Loss 1.732 Prec@(1,5) (59.7%, 86.2%)\u001b[0m\n",
      "[2024-01-15 10:24:30] \u001b[32mValid: [ 37/50] Final Prec@1 59.6900%\u001b[0m\n",
      "[2024-01-15 10:24:30] \u001b[32mEpoch 37 LR 0.003944\u001b[0m\n",
      "[2024-01-15 10:24:35] \u001b[32mTrain: [ 38/50] Step 000/520 Loss 3.085 Prec@(1,5) (46.9%, 62.5%)\u001b[0m\n",
      "[2024-01-15 10:24:35] \u001b[32mTrain: [ 38/50] Step 020/520 Loss 2.527 Prec@(1,5) (53.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:24:36] \u001b[32mTrain: [ 38/50] Step 040/520 Loss 2.579 Prec@(1,5) (53.0%, 74.5%)\u001b[0m\n",
      "[2024-01-15 10:24:36] \u001b[32mTrain: [ 38/50] Step 060/520 Loss 2.560 Prec@(1,5) (53.3%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:24:36] \u001b[32mTrain: [ 38/50] Step 080/520 Loss 2.540 Prec@(1,5) (53.5%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:24:37] \u001b[32mTrain: [ 38/50] Step 100/520 Loss 2.552 Prec@(1,5) (53.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:24:37] \u001b[32mTrain: [ 38/50] Step 120/520 Loss 2.560 Prec@(1,5) (53.5%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:24:38] \u001b[32mTrain: [ 38/50] Step 140/520 Loss 2.548 Prec@(1,5) (53.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:24:38] \u001b[32mTrain: [ 38/50] Step 160/520 Loss 2.545 Prec@(1,5) (53.7%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:24:39] \u001b[32mTrain: [ 38/50] Step 180/520 Loss 2.526 Prec@(1,5) (54.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:24:39] \u001b[32mTrain: [ 38/50] Step 200/520 Loss 2.515 Prec@(1,5) (54.2%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:39] \u001b[32mTrain: [ 38/50] Step 220/520 Loss 2.510 Prec@(1,5) (54.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:24:40] \u001b[32mTrain: [ 38/50] Step 240/520 Loss 2.511 Prec@(1,5) (54.2%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:24:40] \u001b[32mTrain: [ 38/50] Step 260/520 Loss 2.515 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:41] \u001b[32mTrain: [ 38/50] Step 280/520 Loss 2.512 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:41] \u001b[32mTrain: [ 38/50] Step 300/520 Loss 2.514 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:42] \u001b[32mTrain: [ 38/50] Step 320/520 Loss 2.516 Prec@(1,5) (54.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:24:42] \u001b[32mTrain: [ 38/50] Step 340/520 Loss 2.512 Prec@(1,5) (54.2%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:42] \u001b[32mTrain: [ 38/50] Step 360/520 Loss 2.516 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:43] \u001b[32mTrain: [ 38/50] Step 380/520 Loss 2.515 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:43] \u001b[32mTrain: [ 38/50] Step 400/520 Loss 2.517 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:44] \u001b[32mTrain: [ 38/50] Step 420/520 Loss 2.517 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:44] \u001b[32mTrain: [ 38/50] Step 440/520 Loss 2.515 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:45] \u001b[32mTrain: [ 38/50] Step 460/520 Loss 2.516 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:45] \u001b[32mTrain: [ 38/50] Step 480/520 Loss 2.515 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:45] \u001b[32mTrain: [ 38/50] Step 500/520 Loss 2.514 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:46] \u001b[32mTrain: [ 38/50] Step 520/520 Loss 2.514 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:46] \u001b[32mTrain: [ 38/50] Final Prec@1 54.0720%\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 000/104 Loss 1.934 Prec@(1,5) (61.5%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 020/104 Loss 1.793 Prec@(1,5) (60.1%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 040/104 Loss 1.783 Prec@(1,5) (58.8%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 060/104 Loss 1.757 Prec@(1,5) (59.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 080/104 Loss 1.746 Prec@(1,5) (59.3%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 100/104 Loss 1.738 Prec@(1,5) (59.6%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:24:50] \u001b[32mValid: [ 38/50] Step 104/104 Loss 1.739 Prec@(1,5) (59.7%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:24:51] \u001b[32mValid: [ 38/50] Final Prec@1 59.7000%\u001b[0m\n",
      "[2024-01-15 10:24:51] \u001b[32mEpoch 38 LR 0.003389\u001b[0m\n",
      "[2024-01-15 10:24:55] \u001b[32mTrain: [ 39/50] Step 000/520 Loss 2.546 Prec@(1,5) (50.0%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:24:56] \u001b[32mTrain: [ 39/50] Step 020/520 Loss 2.467 Prec@(1,5) (55.1%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:24:56] \u001b[32mTrain: [ 39/50] Step 040/520 Loss 2.492 Prec@(1,5) (54.9%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:24:57] \u001b[32mTrain: [ 39/50] Step 060/520 Loss 2.476 Prec@(1,5) (55.1%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:57] \u001b[32mTrain: [ 39/50] Step 080/520 Loss 2.475 Prec@(1,5) (55.1%, 76.5%)\u001b[0m\n",
      "[2024-01-15 10:24:57] \u001b[32mTrain: [ 39/50] Step 100/520 Loss 2.496 Prec@(1,5) (54.7%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:58] \u001b[32mTrain: [ 39/50] Step 120/520 Loss 2.488 Prec@(1,5) (54.6%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:24:58] \u001b[32mTrain: [ 39/50] Step 140/520 Loss 2.497 Prec@(1,5) (54.4%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:24:58] \u001b[32mTrain: [ 39/50] Step 160/520 Loss 2.494 Prec@(1,5) (54.3%, 76.3%)\u001b[0m\n",
      "[2024-01-15 10:24:59] \u001b[32mTrain: [ 39/50] Step 180/520 Loss 2.501 Prec@(1,5) (54.1%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:24:59] \u001b[32mTrain: [ 39/50] Step 200/520 Loss 2.500 Prec@(1,5) (54.3%, 76.2%)\u001b[0m\n",
      "[2024-01-15 10:25:00] \u001b[32mTrain: [ 39/50] Step 220/520 Loss 2.511 Prec@(1,5) (54.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:00] \u001b[32mTrain: [ 39/50] Step 240/520 Loss 2.513 Prec@(1,5) (54.1%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:00] \u001b[32mTrain: [ 39/50] Step 260/520 Loss 2.519 Prec@(1,5) (54.0%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:01] \u001b[32mTrain: [ 39/50] Step 280/520 Loss 2.515 Prec@(1,5) (54.1%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:01] \u001b[32mTrain: [ 39/50] Step 300/520 Loss 2.515 Prec@(1,5) (54.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:02] \u001b[32mTrain: [ 39/50] Step 320/520 Loss 2.510 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:02] \u001b[32mTrain: [ 39/50] Step 340/520 Loss 2.511 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:02] \u001b[32mTrain: [ 39/50] Step 360/520 Loss 2.509 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:03] \u001b[32mTrain: [ 39/50] Step 380/520 Loss 2.510 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:03] \u001b[32mTrain: [ 39/50] Step 400/520 Loss 2.510 Prec@(1,5) (54.1%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:04] \u001b[32mTrain: [ 39/50] Step 420/520 Loss 2.515 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:04] \u001b[32mTrain: [ 39/50] Step 440/520 Loss 2.518 Prec@(1,5) (54.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:04] \u001b[32mTrain: [ 39/50] Step 460/520 Loss 2.520 Prec@(1,5) (54.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:05] \u001b[32mTrain: [ 39/50] Step 480/520 Loss 2.522 Prec@(1,5) (54.0%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:05] \u001b[32mTrain: [ 39/50] Step 500/520 Loss 2.518 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:05] \u001b[32mTrain: [ 39/50] Step 520/520 Loss 2.519 Prec@(1,5) (54.0%, 76.1%)\u001b[0m\n",
      "[2024-01-15 10:25:06] \u001b[32mTrain: [ 39/50] Final Prec@1 54.0300%\u001b[0m\n",
      "[2024-01-15 10:25:09] \u001b[32mValid: [ 39/50] Step 000/104 Loss 1.988 Prec@(1,5) (59.4%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:25:09] \u001b[32mValid: [ 39/50] Step 020/104 Loss 1.735 Prec@(1,5) (60.9%, 85.8%)\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mValid: [ 39/50] Step 040/104 Loss 1.720 Prec@(1,5) (59.6%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mValid: [ 39/50] Step 060/104 Loss 1.688 Prec@(1,5) (60.1%, 86.3%)\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mValid: [ 39/50] Step 080/104 Loss 1.676 Prec@(1,5) (60.1%, 86.6%)\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mValid: [ 39/50] Step 100/104 Loss 1.665 Prec@(1,5) (60.4%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mValid: [ 39/50] Step 104/104 Loss 1.666 Prec@(1,5) (60.4%, 86.7%)\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mValid: [ 39/50] Final Prec@1 60.4000%\u001b[0m\n",
      "[2024-01-15 10:25:10] \u001b[32mEpoch 39 LR 0.002869\u001b[0m\n",
      "[2024-01-15 10:25:15] \u001b[32mTrain: [ 40/50] Step 000/520 Loss 2.416 Prec@(1,5) (57.3%, 72.9%)\u001b[0m\n",
      "[2024-01-15 10:25:15] \u001b[32mTrain: [ 40/50] Step 020/520 Loss 2.503 Prec@(1,5) (54.6%, 76.4%)\u001b[0m\n",
      "[2024-01-15 10:25:16] \u001b[32mTrain: [ 40/50] Step 040/520 Loss 2.576 Prec@(1,5) (53.2%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:25:16] \u001b[32mTrain: [ 40/50] Step 060/520 Loss 2.559 Prec@(1,5) (53.4%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:25:16] \u001b[32mTrain: [ 40/50] Step 080/520 Loss 2.560 Prec@(1,5) (53.7%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:25:17] \u001b[32mTrain: [ 40/50] Step 100/520 Loss 2.555 Prec@(1,5) (53.7%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:25:17] \u001b[32mTrain: [ 40/50] Step 120/520 Loss 2.540 Prec@(1,5) (54.0%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:18] \u001b[32mTrain: [ 40/50] Step 140/520 Loss 2.532 Prec@(1,5) (54.1%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:25:18] \u001b[32mTrain: [ 40/50] Step 160/520 Loss 2.532 Prec@(1,5) (54.2%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:18] \u001b[32mTrain: [ 40/50] Step 180/520 Loss 2.523 Prec@(1,5) (54.4%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:19] \u001b[32mTrain: [ 40/50] Step 200/520 Loss 2.525 Prec@(1,5) (54.3%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:19] \u001b[32mTrain: [ 40/50] Step 220/520 Loss 2.525 Prec@(1,5) (54.3%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:20] \u001b[32mTrain: [ 40/50] Step 240/520 Loss 2.519 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:20] \u001b[32mTrain: [ 40/50] Step 260/520 Loss 2.526 Prec@(1,5) (54.2%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:20] \u001b[32mTrain: [ 40/50] Step 280/520 Loss 2.528 Prec@(1,5) (54.3%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:21] \u001b[32mTrain: [ 40/50] Step 300/520 Loss 2.531 Prec@(1,5) (54.3%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:25:21] \u001b[32mTrain: [ 40/50] Step 320/520 Loss 2.521 Prec@(1,5) (54.5%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:22] \u001b[32mTrain: [ 40/50] Step 340/520 Loss 2.523 Prec@(1,5) (54.4%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:22] \u001b[32mTrain: [ 40/50] Step 360/520 Loss 2.522 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:22] \u001b[32mTrain: [ 40/50] Step 380/520 Loss 2.519 Prec@(1,5) (54.5%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:23] \u001b[32mTrain: [ 40/50] Step 400/520 Loss 2.519 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:23] \u001b[32mTrain: [ 40/50] Step 420/520 Loss 2.519 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:24] \u001b[32mTrain: [ 40/50] Step 440/520 Loss 2.517 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:24] \u001b[32mTrain: [ 40/50] Step 460/520 Loss 2.515 Prec@(1,5) (54.5%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:24] \u001b[32mTrain: [ 40/50] Step 480/520 Loss 2.517 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:25] \u001b[32mTrain: [ 40/50] Step 500/520 Loss 2.517 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:25] \u001b[32mTrain: [ 40/50] Step 520/520 Loss 2.521 Prec@(1,5) (54.4%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:25] \u001b[32mTrain: [ 40/50] Final Prec@1 54.3500%\u001b[0m\n",
      "[2024-01-15 10:25:29] \u001b[32mValid: [ 40/50] Step 000/104 Loss 2.025 Prec@(1,5) (61.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:25:29] \u001b[32mValid: [ 40/50] Step 020/104 Loss 1.772 Prec@(1,5) (60.1%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:25:29] \u001b[32mValid: [ 40/50] Step 040/104 Loss 1.752 Prec@(1,5) (59.4%, 86.8%)\u001b[0m\n",
      "[2024-01-15 10:25:29] \u001b[32mValid: [ 40/50] Step 060/104 Loss 1.722 Prec@(1,5) (59.9%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:25:29] \u001b[32mValid: [ 40/50] Step 080/104 Loss 1.711 Prec@(1,5) (59.9%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:25:30] \u001b[32mValid: [ 40/50] Step 100/104 Loss 1.701 Prec@(1,5) (60.3%, 87.2%)\u001b[0m\n",
      "[2024-01-15 10:25:30] \u001b[32mValid: [ 40/50] Step 104/104 Loss 1.703 Prec@(1,5) (60.3%, 87.2%)\u001b[0m\n",
      "[2024-01-15 10:25:30] \u001b[32mValid: [ 40/50] Final Prec@1 60.3100%\u001b[0m\n",
      "[2024-01-15 10:25:30] \u001b[32mEpoch 40 LR 0.002388\u001b[0m\n",
      "[2024-01-15 10:25:35] \u001b[32mTrain: [ 41/50] Step 000/520 Loss 2.134 Prec@(1,5) (63.5%, 76.0%)\u001b[0m\n",
      "[2024-01-15 10:25:35] \u001b[32mTrain: [ 41/50] Step 020/520 Loss 2.597 Prec@(1,5) (53.8%, 74.3%)\u001b[0m\n",
      "[2024-01-15 10:25:35] \u001b[32mTrain: [ 41/50] Step 040/520 Loss 2.577 Prec@(1,5) (53.9%, 74.6%)\u001b[0m\n",
      "[2024-01-15 10:25:36] \u001b[32mTrain: [ 41/50] Step 060/520 Loss 2.540 Prec@(1,5) (54.3%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:25:36] \u001b[32mTrain: [ 41/50] Step 080/520 Loss 2.531 Prec@(1,5) (54.1%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:25:37] \u001b[32mTrain: [ 41/50] Step 100/520 Loss 2.534 Prec@(1,5) (54.2%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:25:37] \u001b[32mTrain: [ 41/50] Step 120/520 Loss 2.528 Prec@(1,5) (54.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:25:38] \u001b[32mTrain: [ 41/50] Step 140/520 Loss 2.511 Prec@(1,5) (54.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:25:38] \u001b[32mTrain: [ 41/50] Step 160/520 Loss 2.503 Prec@(1,5) (54.7%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:38] \u001b[32mTrain: [ 41/50] Step 180/520 Loss 2.502 Prec@(1,5) (54.7%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:39] \u001b[32mTrain: [ 41/50] Step 200/520 Loss 2.511 Prec@(1,5) (54.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:39] \u001b[32mTrain: [ 41/50] Step 220/520 Loss 2.519 Prec@(1,5) (54.3%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:25:40] \u001b[32mTrain: [ 41/50] Step 240/520 Loss 2.522 Prec@(1,5) (54.2%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:40] \u001b[32mTrain: [ 41/50] Step 260/520 Loss 2.518 Prec@(1,5) (54.3%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:40] \u001b[32mTrain: [ 41/50] Step 280/520 Loss 2.520 Prec@(1,5) (54.3%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:41] \u001b[32mTrain: [ 41/50] Step 300/520 Loss 2.519 Prec@(1,5) (54.3%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:41] \u001b[32mTrain: [ 41/50] Step 320/520 Loss 2.526 Prec@(1,5) (54.2%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:42] \u001b[32mTrain: [ 41/50] Step 340/520 Loss 2.527 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:42] \u001b[32mTrain: [ 41/50] Step 360/520 Loss 2.528 Prec@(1,5) (54.1%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:25:42] \u001b[32mTrain: [ 41/50] Step 380/520 Loss 2.532 Prec@(1,5) (54.0%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:25:43] \u001b[32mTrain: [ 41/50] Step 400/520 Loss 2.530 Prec@(1,5) (54.0%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:25:43] \u001b[32mTrain: [ 41/50] Step 420/520 Loss 2.522 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:44] \u001b[32mTrain: [ 41/50] Step 440/520 Loss 2.523 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:44] \u001b[32mTrain: [ 41/50] Step 460/520 Loss 2.524 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:44] \u001b[32mTrain: [ 41/50] Step 480/520 Loss 2.523 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:45] \u001b[32mTrain: [ 41/50] Step 500/520 Loss 2.522 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:45] \u001b[32mTrain: [ 41/50] Step 520/520 Loss 2.524 Prec@(1,5) (54.1%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:45] \u001b[32mTrain: [ 41/50] Final Prec@1 54.1100%\u001b[0m\n",
      "[2024-01-15 10:25:49] \u001b[32mValid: [ 41/50] Step 000/104 Loss 1.931 Prec@(1,5) (59.4%, 82.3%)\u001b[0m\n",
      "[2024-01-15 10:25:49] \u001b[32mValid: [ 41/50] Step 020/104 Loss 1.701 Prec@(1,5) (60.7%, 86.4%)\u001b[0m\n",
      "[2024-01-15 10:25:49] \u001b[32mValid: [ 41/50] Step 040/104 Loss 1.695 Prec@(1,5) (59.7%, 86.9%)\u001b[0m\n",
      "[2024-01-15 10:25:49] \u001b[32mValid: [ 41/50] Step 060/104 Loss 1.672 Prec@(1,5) (60.4%, 87.0%)\u001b[0m\n",
      "[2024-01-15 10:25:50] \u001b[32mValid: [ 41/50] Step 080/104 Loss 1.658 Prec@(1,5) (60.6%, 87.2%)\u001b[0m\n",
      "[2024-01-15 10:25:50] \u001b[32mValid: [ 41/50] Step 100/104 Loss 1.644 Prec@(1,5) (60.7%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:25:50] \u001b[32mValid: [ 41/50] Step 104/104 Loss 1.644 Prec@(1,5) (60.8%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:25:50] \u001b[32mValid: [ 41/50] Final Prec@1 60.7600%\u001b[0m\n",
      "[2024-01-15 10:25:50] \u001b[32mEpoch 41 LR 0.001947\u001b[0m\n",
      "[2024-01-15 10:25:55] \u001b[32mTrain: [ 42/50] Step 000/520 Loss 2.897 Prec@(1,5) (45.8%, 69.8%)\u001b[0m\n",
      "[2024-01-15 10:25:55] \u001b[32mTrain: [ 42/50] Step 020/520 Loss 2.536 Prec@(1,5) (53.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:25:56] \u001b[32mTrain: [ 42/50] Step 040/520 Loss 2.563 Prec@(1,5) (53.0%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:25:56] \u001b[32mTrain: [ 42/50] Step 060/520 Loss 2.558 Prec@(1,5) (53.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:25:56] \u001b[32mTrain: [ 42/50] Step 080/520 Loss 2.535 Prec@(1,5) (54.1%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:25:57] \u001b[32mTrain: [ 42/50] Step 100/520 Loss 2.524 Prec@(1,5) (54.6%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:57] \u001b[32mTrain: [ 42/50] Step 120/520 Loss 2.535 Prec@(1,5) (54.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:25:58] \u001b[32mTrain: [ 42/50] Step 140/520 Loss 2.517 Prec@(1,5) (54.9%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:58] \u001b[32mTrain: [ 42/50] Step 160/520 Loss 2.517 Prec@(1,5) (54.9%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:25:58] \u001b[32mTrain: [ 42/50] Step 180/520 Loss 2.525 Prec@(1,5) (54.6%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:59] \u001b[32mTrain: [ 42/50] Step 200/520 Loss 2.526 Prec@(1,5) (54.7%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:25:59] \u001b[32mTrain: [ 42/50] Step 220/520 Loss 2.525 Prec@(1,5) (54.7%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:00] \u001b[32mTrain: [ 42/50] Step 240/520 Loss 2.520 Prec@(1,5) (54.8%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:00] \u001b[32mTrain: [ 42/50] Step 260/520 Loss 2.515 Prec@(1,5) (54.8%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:00] \u001b[32mTrain: [ 42/50] Step 280/520 Loss 2.519 Prec@(1,5) (54.7%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:01] \u001b[32mTrain: [ 42/50] Step 300/520 Loss 2.521 Prec@(1,5) (54.7%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:01] \u001b[32mTrain: [ 42/50] Step 320/520 Loss 2.523 Prec@(1,5) (54.7%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:26:01] \u001b[32mTrain: [ 42/50] Step 340/520 Loss 2.519 Prec@(1,5) (54.7%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:26:02] \u001b[32mTrain: [ 42/50] Step 360/520 Loss 2.522 Prec@(1,5) (54.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:02] \u001b[32mTrain: [ 42/50] Step 380/520 Loss 2.524 Prec@(1,5) (54.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:03] \u001b[32mTrain: [ 42/50] Step 400/520 Loss 2.523 Prec@(1,5) (54.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:03] \u001b[32mTrain: [ 42/50] Step 420/520 Loss 2.524 Prec@(1,5) (54.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:26:03] \u001b[32mTrain: [ 42/50] Step 440/520 Loss 2.528 Prec@(1,5) (54.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:26:04] \u001b[32mTrain: [ 42/50] Step 460/520 Loss 2.530 Prec@(1,5) (54.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:26:04] \u001b[32mTrain: [ 42/50] Step 480/520 Loss 2.530 Prec@(1,5) (54.5%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:26:05] \u001b[32mTrain: [ 42/50] Step 500/520 Loss 2.536 Prec@(1,5) (54.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:05] \u001b[32mTrain: [ 42/50] Step 520/520 Loss 2.536 Prec@(1,5) (54.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:05] \u001b[32mTrain: [ 42/50] Final Prec@1 54.3740%\u001b[0m\n",
      "[2024-01-15 10:26:09] \u001b[32mValid: [ 42/50] Step 000/104 Loss 1.886 Prec@(1,5) (65.6%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:26:09] \u001b[32mValid: [ 42/50] Step 020/104 Loss 1.723 Prec@(1,5) (60.3%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:26:09] \u001b[32mValid: [ 42/50] Step 040/104 Loss 1.727 Prec@(1,5) (59.2%, 87.2%)\u001b[0m\n",
      "[2024-01-15 10:26:09] \u001b[32mValid: [ 42/50] Step 060/104 Loss 1.692 Prec@(1,5) (59.9%, 87.1%)\u001b[0m\n",
      "[2024-01-15 10:26:09] \u001b[32mValid: [ 42/50] Step 080/104 Loss 1.681 Prec@(1,5) (60.1%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:26:10] \u001b[32mValid: [ 42/50] Step 100/104 Loss 1.673 Prec@(1,5) (60.3%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:26:10] \u001b[32mValid: [ 42/50] Step 104/104 Loss 1.674 Prec@(1,5) (60.3%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:26:10] \u001b[32mValid: [ 42/50] Final Prec@1 60.3400%\u001b[0m\n",
      "[2024-01-15 10:26:10] \u001b[32mEpoch 42 LR 0.001547\u001b[0m\n",
      "[2024-01-15 10:26:14] \u001b[32mTrain: [ 43/50] Step 000/520 Loss 2.264 Prec@(1,5) (59.4%, 77.1%)\u001b[0m\n",
      "[2024-01-15 10:26:15] \u001b[32mTrain: [ 43/50] Step 020/520 Loss 2.491 Prec@(1,5) (54.9%, 75.9%)\u001b[0m\n",
      "[2024-01-15 10:26:15] \u001b[32mTrain: [ 43/50] Step 040/520 Loss 2.510 Prec@(1,5) (54.6%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:26:16] \u001b[32mTrain: [ 43/50] Step 060/520 Loss 2.503 Prec@(1,5) (54.9%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:16] \u001b[32mTrain: [ 43/50] Step 080/520 Loss 2.496 Prec@(1,5) (54.8%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:16] \u001b[32mTrain: [ 43/50] Step 100/520 Loss 2.511 Prec@(1,5) (54.6%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:17] \u001b[32mTrain: [ 43/50] Step 120/520 Loss 2.497 Prec@(1,5) (54.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:26:17] \u001b[32mTrain: [ 43/50] Step 140/520 Loss 2.512 Prec@(1,5) (54.5%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:18] \u001b[32mTrain: [ 43/50] Step 160/520 Loss 2.515 Prec@(1,5) (54.5%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:18] \u001b[32mTrain: [ 43/50] Step 180/520 Loss 2.520 Prec@(1,5) (54.4%, 75.3%)\u001b[0m\n",
      "[2024-01-15 10:26:18] \u001b[32mTrain: [ 43/50] Step 200/520 Loss 2.525 Prec@(1,5) (54.3%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:26:19] \u001b[32mTrain: [ 43/50] Step 220/520 Loss 2.532 Prec@(1,5) (54.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:26:19] \u001b[32mTrain: [ 43/50] Step 240/520 Loss 2.533 Prec@(1,5) (54.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:26:20] \u001b[32mTrain: [ 43/50] Step 260/520 Loss 2.537 Prec@(1,5) (54.3%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:20] \u001b[32mTrain: [ 43/50] Step 280/520 Loss 2.542 Prec@(1,5) (54.3%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:20] \u001b[32mTrain: [ 43/50] Step 300/520 Loss 2.549 Prec@(1,5) (54.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:21] \u001b[32mTrain: [ 43/50] Step 320/520 Loss 2.545 Prec@(1,5) (54.2%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:21] \u001b[32mTrain: [ 43/50] Step 340/520 Loss 2.542 Prec@(1,5) (54.1%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:22] \u001b[32mTrain: [ 43/50] Step 360/520 Loss 2.547 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:22] \u001b[32mTrain: [ 43/50] Step 380/520 Loss 2.547 Prec@(1,5) (54.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:22] \u001b[32mTrain: [ 43/50] Step 400/520 Loss 2.549 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:23] \u001b[32mTrain: [ 43/50] Step 420/520 Loss 2.549 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:23] \u001b[32mTrain: [ 43/50] Step 440/520 Loss 2.546 Prec@(1,5) (54.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:23] \u001b[32mTrain: [ 43/50] Step 460/520 Loss 2.547 Prec@(1,5) (54.1%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:24] \u001b[32mTrain: [ 43/50] Step 480/520 Loss 2.547 Prec@(1,5) (54.1%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:24] \u001b[32mTrain: [ 43/50] Step 500/520 Loss 2.549 Prec@(1,5) (54.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:25] \u001b[32mTrain: [ 43/50] Step 520/520 Loss 2.548 Prec@(1,5) (54.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:25] \u001b[32mTrain: [ 43/50] Final Prec@1 54.0780%\u001b[0m\n",
      "[2024-01-15 10:26:28] \u001b[32mValid: [ 43/50] Step 000/104 Loss 1.814 Prec@(1,5) (64.6%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Step 020/104 Loss 1.708 Prec@(1,5) (60.6%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Step 040/104 Loss 1.691 Prec@(1,5) (60.4%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Step 060/104 Loss 1.662 Prec@(1,5) (61.0%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Step 080/104 Loss 1.650 Prec@(1,5) (61.1%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Step 100/104 Loss 1.644 Prec@(1,5) (61.3%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Step 104/104 Loss 1.647 Prec@(1,5) (61.3%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mValid: [ 43/50] Final Prec@1 61.3100%\u001b[0m\n",
      "[2024-01-15 10:26:29] \u001b[32mEpoch 43 LR 0.001191\u001b[0m\n",
      "[2024-01-15 10:26:34] \u001b[32mTrain: [ 44/50] Step 000/520 Loss 2.561 Prec@(1,5) (54.2%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:34] \u001b[32mTrain: [ 44/50] Step 020/520 Loss 2.467 Prec@(1,5) (55.3%, 76.9%)\u001b[0m\n",
      "[2024-01-15 10:26:35] \u001b[32mTrain: [ 44/50] Step 040/520 Loss 2.458 Prec@(1,5) (55.4%, 76.6%)\u001b[0m\n",
      "[2024-01-15 10:26:35] \u001b[32mTrain: [ 44/50] Step 060/520 Loss 2.480 Prec@(1,5) (55.0%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:36] \u001b[32mTrain: [ 44/50] Step 080/520 Loss 2.487 Prec@(1,5) (54.9%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:26:36] \u001b[32mTrain: [ 44/50] Step 100/520 Loss 2.496 Prec@(1,5) (54.7%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:36] \u001b[32mTrain: [ 44/50] Step 120/520 Loss 2.497 Prec@(1,5) (54.6%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:37] \u001b[32mTrain: [ 44/50] Step 140/520 Loss 2.493 Prec@(1,5) (54.6%, 75.8%)\u001b[0m\n",
      "[2024-01-15 10:26:37] \u001b[32mTrain: [ 44/50] Step 160/520 Loss 2.499 Prec@(1,5) (54.7%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:38] \u001b[32mTrain: [ 44/50] Step 180/520 Loss 2.512 Prec@(1,5) (54.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:38] \u001b[32mTrain: [ 44/50] Step 200/520 Loss 2.508 Prec@(1,5) (54.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:38] \u001b[32mTrain: [ 44/50] Step 220/520 Loss 2.513 Prec@(1,5) (54.5%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:39] \u001b[32mTrain: [ 44/50] Step 240/520 Loss 2.514 Prec@(1,5) (54.6%, 75.5%)\u001b[0m\n",
      "[2024-01-15 10:26:39] \u001b[32mTrain: [ 44/50] Step 260/520 Loss 2.515 Prec@(1,5) (54.5%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:26:40] \u001b[32mTrain: [ 44/50] Step 280/520 Loss 2.523 Prec@(1,5) (54.4%, 75.4%)\u001b[0m\n",
      "[2024-01-15 10:26:40] \u001b[32mTrain: [ 44/50] Step 300/520 Loss 2.528 Prec@(1,5) (54.5%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:26:40] \u001b[32mTrain: [ 44/50] Step 320/520 Loss 2.533 Prec@(1,5) (54.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:26:41] \u001b[32mTrain: [ 44/50] Step 340/520 Loss 2.530 Prec@(1,5) (54.4%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:26:41] \u001b[32mTrain: [ 44/50] Step 360/520 Loss 2.529 Prec@(1,5) (54.4%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:26:42] \u001b[32mTrain: [ 44/50] Step 380/520 Loss 2.529 Prec@(1,5) (54.4%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:26:42] \u001b[32mTrain: [ 44/50] Step 400/520 Loss 2.533 Prec@(1,5) (54.3%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:26:42] \u001b[32mTrain: [ 44/50] Step 420/520 Loss 2.536 Prec@(1,5) (54.3%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:43] \u001b[32mTrain: [ 44/50] Step 440/520 Loss 2.540 Prec@(1,5) (54.2%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:43] \u001b[32mTrain: [ 44/50] Step 460/520 Loss 2.543 Prec@(1,5) (54.2%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:44] \u001b[32mTrain: [ 44/50] Step 480/520 Loss 2.544 Prec@(1,5) (54.2%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:44] \u001b[32mTrain: [ 44/50] Step 500/520 Loss 2.541 Prec@(1,5) (54.2%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:44] \u001b[32mTrain: [ 44/50] Step 520/520 Loss 2.544 Prec@(1,5) (54.2%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:45] \u001b[32mTrain: [ 44/50] Final Prec@1 54.1620%\u001b[0m\n",
      "[2024-01-15 10:26:48] \u001b[32mValid: [ 44/50] Step 000/104 Loss 1.862 Prec@(1,5) (63.5%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:26:48] \u001b[32mValid: [ 44/50] Step 020/104 Loss 1.671 Prec@(1,5) (61.6%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:26:48] \u001b[32mValid: [ 44/50] Step 040/104 Loss 1.660 Prec@(1,5) (60.7%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:26:49] \u001b[32mValid: [ 44/50] Step 060/104 Loss 1.630 Prec@(1,5) (61.1%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:26:49] \u001b[32mValid: [ 44/50] Step 080/104 Loss 1.622 Prec@(1,5) (61.0%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:26:49] \u001b[32mValid: [ 44/50] Step 100/104 Loss 1.614 Prec@(1,5) (61.2%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:26:49] \u001b[32mValid: [ 44/50] Step 104/104 Loss 1.615 Prec@(1,5) (61.3%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:26:49] \u001b[32mValid: [ 44/50] Final Prec@1 61.2900%\u001b[0m\n",
      "[2024-01-15 10:26:49] \u001b[32mEpoch 44 LR 0.000879\u001b[0m\n",
      "[2024-01-15 10:26:54] \u001b[32mTrain: [ 45/50] Step 000/520 Loss 2.659 Prec@(1,5) (50.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 10:26:54] \u001b[32mTrain: [ 45/50] Step 020/520 Loss 2.519 Prec@(1,5) (54.3%, 75.6%)\u001b[0m\n",
      "[2024-01-15 10:26:55] \u001b[32mTrain: [ 45/50] Step 040/520 Loss 2.494 Prec@(1,5) (54.5%, 75.7%)\u001b[0m\n",
      "[2024-01-15 10:26:55] \u001b[32mTrain: [ 45/50] Step 060/520 Loss 2.544 Prec@(1,5) (54.0%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:26:55] \u001b[32mTrain: [ 45/50] Step 080/520 Loss 2.531 Prec@(1,5) (54.4%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:56] \u001b[32mTrain: [ 45/50] Step 100/520 Loss 2.548 Prec@(1,5) (54.3%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:56] \u001b[32mTrain: [ 45/50] Step 120/520 Loss 2.539 Prec@(1,5) (54.2%, 75.2%)\u001b[0m\n",
      "[2024-01-15 10:26:57] \u001b[32mTrain: [ 45/50] Step 140/520 Loss 2.542 Prec@(1,5) (54.1%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:26:57] \u001b[32mTrain: [ 45/50] Step 160/520 Loss 2.556 Prec@(1,5) (53.9%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:26:57] \u001b[32mTrain: [ 45/50] Step 180/520 Loss 2.550 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:58] \u001b[32mTrain: [ 45/50] Step 200/520 Loss 2.552 Prec@(1,5) (53.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:58] \u001b[32mTrain: [ 45/50] Step 220/520 Loss 2.556 Prec@(1,5) (53.8%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:26:59] \u001b[32mTrain: [ 45/50] Step 240/520 Loss 2.555 Prec@(1,5) (53.8%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:26:59] \u001b[32mTrain: [ 45/50] Step 260/520 Loss 2.552 Prec@(1,5) (53.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:26:59] \u001b[32mTrain: [ 45/50] Step 280/520 Loss 2.558 Prec@(1,5) (53.8%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:00] \u001b[32mTrain: [ 45/50] Step 300/520 Loss 2.556 Prec@(1,5) (53.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:00] \u001b[32mTrain: [ 45/50] Step 320/520 Loss 2.555 Prec@(1,5) (53.9%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:01] \u001b[32mTrain: [ 45/50] Step 340/520 Loss 2.554 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:01] \u001b[32mTrain: [ 45/50] Step 360/520 Loss 2.549 Prec@(1,5) (54.1%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:01] \u001b[32mTrain: [ 45/50] Step 380/520 Loss 2.554 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:02] \u001b[32mTrain: [ 45/50] Step 400/520 Loss 2.554 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:02] \u001b[32mTrain: [ 45/50] Step 420/520 Loss 2.554 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:03] \u001b[32mTrain: [ 45/50] Step 440/520 Loss 2.553 Prec@(1,5) (54.0%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:03] \u001b[32mTrain: [ 45/50] Step 460/520 Loss 2.558 Prec@(1,5) (53.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:03] \u001b[32mTrain: [ 45/50] Step 480/520 Loss 2.556 Prec@(1,5) (53.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:04] \u001b[32mTrain: [ 45/50] Step 500/520 Loss 2.555 Prec@(1,5) (53.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:04] \u001b[32mTrain: [ 45/50] Step 520/520 Loss 2.557 Prec@(1,5) (53.9%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:04] \u001b[32mTrain: [ 45/50] Final Prec@1 53.9180%\u001b[0m\n",
      "[2024-01-15 10:27:08] \u001b[32mValid: [ 45/50] Step 000/104 Loss 1.917 Prec@(1,5) (65.6%, 83.3%)\u001b[0m\n",
      "[2024-01-15 10:27:08] \u001b[32mValid: [ 45/50] Step 020/104 Loss 1.696 Prec@(1,5) (61.2%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:27:08] \u001b[32mValid: [ 45/50] Step 040/104 Loss 1.682 Prec@(1,5) (60.4%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:27:08] \u001b[32mValid: [ 45/50] Step 060/104 Loss 1.651 Prec@(1,5) (61.2%, 87.4%)\u001b[0m\n",
      "[2024-01-15 10:27:09] \u001b[32mValid: [ 45/50] Step 080/104 Loss 1.639 Prec@(1,5) (61.3%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:27:09] \u001b[32mValid: [ 45/50] Step 100/104 Loss 1.631 Prec@(1,5) (61.4%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:27:09] \u001b[32mValid: [ 45/50] Step 104/104 Loss 1.633 Prec@(1,5) (61.5%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:27:09] \u001b[32mValid: [ 45/50] Final Prec@1 61.4500%\u001b[0m\n",
      "[2024-01-15 10:27:09] \u001b[32mEpoch 45 LR 0.000613\u001b[0m\n",
      "[2024-01-15 10:27:14] \u001b[32mTrain: [ 46/50] Step 000/520 Loss 2.653 Prec@(1,5) (55.2%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:27:14] \u001b[32mTrain: [ 46/50] Step 020/520 Loss 2.576 Prec@(1,5) (53.8%, 74.6%)\u001b[0m\n",
      "[2024-01-15 10:27:14] \u001b[32mTrain: [ 46/50] Step 040/520 Loss 2.540 Prec@(1,5) (54.7%, 75.1%)\u001b[0m\n",
      "[2024-01-15 10:27:15] \u001b[32mTrain: [ 46/50] Step 060/520 Loss 2.550 Prec@(1,5) (54.6%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:15] \u001b[32mTrain: [ 46/50] Step 080/520 Loss 2.541 Prec@(1,5) (54.7%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:16] \u001b[32mTrain: [ 46/50] Step 100/520 Loss 2.534 Prec@(1,5) (54.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:16] \u001b[32mTrain: [ 46/50] Step 120/520 Loss 2.530 Prec@(1,5) (54.8%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:16] \u001b[32mTrain: [ 46/50] Step 140/520 Loss 2.523 Prec@(1,5) (54.9%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:27:17] \u001b[32mTrain: [ 46/50] Step 160/520 Loss 2.527 Prec@(1,5) (54.7%, 75.0%)\u001b[0m\n",
      "[2024-01-15 10:27:17] \u001b[32mTrain: [ 46/50] Step 180/520 Loss 2.531 Prec@(1,5) (54.6%, 74.9%)\u001b[0m\n",
      "[2024-01-15 10:27:18] \u001b[32mTrain: [ 46/50] Step 200/520 Loss 2.536 Prec@(1,5) (54.6%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:18] \u001b[32mTrain: [ 46/50] Step 220/520 Loss 2.541 Prec@(1,5) (54.5%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:18] \u001b[32mTrain: [ 46/50] Step 240/520 Loss 2.539 Prec@(1,5) (54.6%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:19] \u001b[32mTrain: [ 46/50] Step 260/520 Loss 2.541 Prec@(1,5) (54.6%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:19] \u001b[32mTrain: [ 46/50] Step 280/520 Loss 2.549 Prec@(1,5) (54.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 10:27:20] \u001b[32mTrain: [ 46/50] Step 300/520 Loss 2.552 Prec@(1,5) (54.4%, 74.6%)\u001b[0m\n",
      "[2024-01-15 10:27:20] \u001b[32mTrain: [ 46/50] Step 320/520 Loss 2.549 Prec@(1,5) (54.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:20] \u001b[32mTrain: [ 46/50] Step 340/520 Loss 2.544 Prec@(1,5) (54.5%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:21] \u001b[32mTrain: [ 46/50] Step 360/520 Loss 2.549 Prec@(1,5) (54.4%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:21] \u001b[32mTrain: [ 46/50] Step 380/520 Loss 2.546 Prec@(1,5) (54.5%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:22] \u001b[32mTrain: [ 46/50] Step 400/520 Loss 2.551 Prec@(1,5) (54.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:22] \u001b[32mTrain: [ 46/50] Step 420/520 Loss 2.554 Prec@(1,5) (54.4%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:22] \u001b[32mTrain: [ 46/50] Step 440/520 Loss 2.554 Prec@(1,5) (54.3%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:23] \u001b[32mTrain: [ 46/50] Step 460/520 Loss 2.554 Prec@(1,5) (54.3%, 74.7%)\u001b[0m\n",
      "[2024-01-15 10:27:23] \u001b[32mTrain: [ 46/50] Step 480/520 Loss 2.551 Prec@(1,5) (54.3%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:23] \u001b[32mTrain: [ 46/50] Step 500/520 Loss 2.551 Prec@(1,5) (54.2%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:24] \u001b[32mTrain: [ 46/50] Step 520/520 Loss 2.553 Prec@(1,5) (54.2%, 74.8%)\u001b[0m\n",
      "[2024-01-15 10:27:24] \u001b[32mTrain: [ 46/50] Final Prec@1 54.2280%\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 000/104 Loss 1.942 Prec@(1,5) (62.5%, 84.4%)\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 020/104 Loss 1.690 Prec@(1,5) (61.6%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 040/104 Loss 1.683 Prec@(1,5) (60.9%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 060/104 Loss 1.655 Prec@(1,5) (61.4%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 080/104 Loss 1.645 Prec@(1,5) (61.6%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 100/104 Loss 1.638 Prec@(1,5) (61.8%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:27:28] \u001b[32mValid: [ 46/50] Step 104/104 Loss 1.640 Prec@(1,5) (61.8%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:27:29] \u001b[32mValid: [ 46/50] Final Prec@1 61.8200%\u001b[0m\n",
      "[2024-01-15 10:27:29] \u001b[32mEpoch 46 LR 0.000394\u001b[0m\n",
      "[2024-01-15 10:27:33] \u001b[32mTrain: [ 47/50] Step 000/520 Loss 2.821 Prec@(1,5) (47.9%, 72.9%)\u001b[0m\n",
      "[2024-01-15 10:27:34] \u001b[32mTrain: [ 47/50] Step 020/520 Loss 2.579 Prec@(1,5) (54.5%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:27:34] \u001b[32mTrain: [ 47/50] Step 040/520 Loss 2.573 Prec@(1,5) (54.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:27:35] \u001b[32mTrain: [ 47/50] Step 060/520 Loss 2.560 Prec@(1,5) (54.6%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:27:35] \u001b[32mTrain: [ 47/50] Step 080/520 Loss 2.592 Prec@(1,5) (54.1%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:27:35] \u001b[32mTrain: [ 47/50] Step 100/520 Loss 2.591 Prec@(1,5) (54.0%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:27:36] \u001b[32mTrain: [ 47/50] Step 120/520 Loss 2.590 Prec@(1,5) (54.1%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:27:36] \u001b[32mTrain: [ 47/50] Step 140/520 Loss 2.597 Prec@(1,5) (53.8%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:27:36] \u001b[32mTrain: [ 47/50] Step 160/520 Loss 2.597 Prec@(1,5) (53.9%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:27:37] \u001b[32mTrain: [ 47/50] Step 180/520 Loss 2.595 Prec@(1,5) (54.0%, 74.0%)\u001b[0m\n",
      "[2024-01-15 10:27:37] \u001b[32mTrain: [ 47/50] Step 200/520 Loss 2.586 Prec@(1,5) (54.3%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:38] \u001b[32mTrain: [ 47/50] Step 220/520 Loss 2.582 Prec@(1,5) (54.4%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:38] \u001b[32mTrain: [ 47/50] Step 240/520 Loss 2.578 Prec@(1,5) (54.5%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:38] \u001b[32mTrain: [ 47/50] Step 260/520 Loss 2.584 Prec@(1,5) (54.4%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:39] \u001b[32mTrain: [ 47/50] Step 280/520 Loss 2.584 Prec@(1,5) (54.4%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:39] \u001b[32mTrain: [ 47/50] Step 300/520 Loss 2.581 Prec@(1,5) (54.4%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:40] \u001b[32mTrain: [ 47/50] Step 320/520 Loss 2.581 Prec@(1,5) (54.4%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:40] \u001b[32mTrain: [ 47/50] Step 340/520 Loss 2.582 Prec@(1,5) (54.4%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:40] \u001b[32mTrain: [ 47/50] Step 360/520 Loss 2.585 Prec@(1,5) (54.3%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:41] \u001b[32mTrain: [ 47/50] Step 380/520 Loss 2.583 Prec@(1,5) (54.3%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:41] \u001b[32mTrain: [ 47/50] Step 400/520 Loss 2.588 Prec@(1,5) (54.2%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:41] \u001b[32mTrain: [ 47/50] Step 420/520 Loss 2.588 Prec@(1,5) (54.1%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:42] \u001b[32mTrain: [ 47/50] Step 440/520 Loss 2.587 Prec@(1,5) (54.1%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:42] \u001b[32mTrain: [ 47/50] Step 460/520 Loss 2.588 Prec@(1,5) (54.1%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:43] \u001b[32mTrain: [ 47/50] Step 480/520 Loss 2.586 Prec@(1,5) (54.1%, 74.1%)\u001b[0m\n",
      "[2024-01-15 10:27:43] \u001b[32mTrain: [ 47/50] Step 500/520 Loss 2.583 Prec@(1,5) (54.2%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:43] \u001b[32mTrain: [ 47/50] Step 520/520 Loss 2.583 Prec@(1,5) (54.2%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:44] \u001b[32mTrain: [ 47/50] Final Prec@1 54.2080%\u001b[0m\n",
      "[2024-01-15 10:27:47] \u001b[32mValid: [ 47/50] Step 000/104 Loss 1.916 Prec@(1,5) (65.6%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:27:47] \u001b[32mValid: [ 47/50] Step 020/104 Loss 1.689 Prec@(1,5) (61.1%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:27:47] \u001b[32mValid: [ 47/50] Step 040/104 Loss 1.680 Prec@(1,5) (60.4%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:27:48] \u001b[32mValid: [ 47/50] Step 060/104 Loss 1.649 Prec@(1,5) (61.1%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:27:48] \u001b[32mValid: [ 47/50] Step 080/104 Loss 1.638 Prec@(1,5) (61.2%, 87.8%)\u001b[0m\n",
      "[2024-01-15 10:27:48] \u001b[32mValid: [ 47/50] Step 100/104 Loss 1.630 Prec@(1,5) (61.5%, 88.0%)\u001b[0m\n",
      "[2024-01-15 10:27:48] \u001b[32mValid: [ 47/50] Step 104/104 Loss 1.632 Prec@(1,5) (61.5%, 87.9%)\u001b[0m\n",
      "[2024-01-15 10:27:48] \u001b[32mValid: [ 47/50] Final Prec@1 61.4900%\u001b[0m\n",
      "[2024-01-15 10:27:48] \u001b[32mEpoch 47 LR 0.000222\u001b[0m\n",
      "[2024-01-15 10:27:53] \u001b[32mTrain: [ 48/50] Step 000/520 Loss 2.932 Prec@(1,5) (49.0%, 66.7%)\u001b[0m\n",
      "[2024-01-15 10:27:53] \u001b[32mTrain: [ 48/50] Step 020/520 Loss 2.579 Prec@(1,5) (54.3%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:54] \u001b[32mTrain: [ 48/50] Step 040/520 Loss 2.577 Prec@(1,5) (54.4%, 74.3%)\u001b[0m\n",
      "[2024-01-15 10:27:54] \u001b[32mTrain: [ 48/50] Step 060/520 Loss 2.556 Prec@(1,5) (54.8%, 74.2%)\u001b[0m\n",
      "[2024-01-15 10:27:54] \u001b[32mTrain: [ 48/50] Step 080/520 Loss 2.595 Prec@(1,5) (53.9%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:27:55] \u001b[32mTrain: [ 48/50] Step 100/520 Loss 2.609 Prec@(1,5) (53.6%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:27:55] \u001b[32mTrain: [ 48/50] Step 120/520 Loss 2.623 Prec@(1,5) (53.3%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:27:56] \u001b[32mTrain: [ 48/50] Step 140/520 Loss 2.614 Prec@(1,5) (53.5%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:27:56] \u001b[32mTrain: [ 48/50] Step 160/520 Loss 2.602 Prec@(1,5) (53.5%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:27:56] \u001b[32mTrain: [ 48/50] Step 180/520 Loss 2.605 Prec@(1,5) (53.5%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:27:57] \u001b[32mTrain: [ 48/50] Step 200/520 Loss 2.600 Prec@(1,5) (53.5%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:27:57] \u001b[32mTrain: [ 48/50] Step 220/520 Loss 2.600 Prec@(1,5) (53.6%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:27:57] \u001b[32mTrain: [ 48/50] Step 240/520 Loss 2.606 Prec@(1,5) (53.4%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:27:58] \u001b[32mTrain: [ 48/50] Step 260/520 Loss 2.607 Prec@(1,5) (53.4%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:27:58] \u001b[32mTrain: [ 48/50] Step 280/520 Loss 2.604 Prec@(1,5) (53.5%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:27:59] \u001b[32mTrain: [ 48/50] Step 300/520 Loss 2.609 Prec@(1,5) (53.5%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:27:59] \u001b[32mTrain: [ 48/50] Step 320/520 Loss 2.609 Prec@(1,5) (53.5%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:27:59] \u001b[32mTrain: [ 48/50] Step 340/520 Loss 2.603 Prec@(1,5) (53.6%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:28:00] \u001b[32mTrain: [ 48/50] Step 360/520 Loss 2.604 Prec@(1,5) (53.6%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:28:00] \u001b[32mTrain: [ 48/50] Step 380/520 Loss 2.599 Prec@(1,5) (53.7%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:28:01] \u001b[32mTrain: [ 48/50] Step 400/520 Loss 2.603 Prec@(1,5) (53.6%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:28:01] \u001b[32mTrain: [ 48/50] Step 420/520 Loss 2.603 Prec@(1,5) (53.5%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:28:01] \u001b[32mTrain: [ 48/50] Step 440/520 Loss 2.599 Prec@(1,5) (53.6%, 73.8%)\u001b[0m\n",
      "[2024-01-15 10:28:02] \u001b[32mTrain: [ 48/50] Step 460/520 Loss 2.596 Prec@(1,5) (53.6%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:28:02] \u001b[32mTrain: [ 48/50] Step 480/520 Loss 2.597 Prec@(1,5) (53.6%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:28:03] \u001b[32mTrain: [ 48/50] Step 500/520 Loss 2.596 Prec@(1,5) (53.6%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:28:03] \u001b[32mTrain: [ 48/50] Step 520/520 Loss 2.595 Prec@(1,5) (53.6%, 73.9%)\u001b[0m\n",
      "[2024-01-15 10:28:03] \u001b[32mTrain: [ 48/50] Final Prec@1 53.6180%\u001b[0m\n",
      "[2024-01-15 10:28:07] \u001b[32mValid: [ 48/50] Step 000/104 Loss 1.940 Prec@(1,5) (64.6%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:28:07] \u001b[32mValid: [ 48/50] Step 020/104 Loss 1.696 Prec@(1,5) (61.6%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:28:07] \u001b[32mValid: [ 48/50] Step 040/104 Loss 1.688 Prec@(1,5) (60.8%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:28:07] \u001b[32mValid: [ 48/50] Step 060/104 Loss 1.658 Prec@(1,5) (61.4%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:28:07] \u001b[32mValid: [ 48/50] Step 080/104 Loss 1.648 Prec@(1,5) (61.3%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:28:08] \u001b[32mValid: [ 48/50] Step 100/104 Loss 1.640 Prec@(1,5) (61.5%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:28:08] \u001b[32mValid: [ 48/50] Step 104/104 Loss 1.643 Prec@(1,5) (61.5%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:28:08] \u001b[32mValid: [ 48/50] Final Prec@1 61.5300%\u001b[0m\n",
      "[2024-01-15 10:28:08] \u001b[32mEpoch 48 LR 0.000100\u001b[0m\n",
      "[2024-01-15 10:28:12] \u001b[32mTrain: [ 49/50] Step 000/520 Loss 2.703 Prec@(1,5) (53.1%, 68.8%)\u001b[0m\n",
      "[2024-01-15 10:28:13] \u001b[32mTrain: [ 49/50] Step 020/520 Loss 2.598 Prec@(1,5) (53.3%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:13] \u001b[32mTrain: [ 49/50] Step 040/520 Loss 2.634 Prec@(1,5) (52.8%, 72.7%)\u001b[0m\n",
      "[2024-01-15 10:28:14] \u001b[32mTrain: [ 49/50] Step 060/520 Loss 2.639 Prec@(1,5) (52.9%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:14] \u001b[32mTrain: [ 49/50] Step 080/520 Loss 2.631 Prec@(1,5) (53.0%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:28:14] \u001b[32mTrain: [ 49/50] Step 100/520 Loss 2.620 Prec@(1,5) (53.1%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:15] \u001b[32mTrain: [ 49/50] Step 120/520 Loss 2.612 Prec@(1,5) (53.3%, 73.7%)\u001b[0m\n",
      "[2024-01-15 10:28:15] \u001b[32mTrain: [ 49/50] Step 140/520 Loss 2.622 Prec@(1,5) (53.1%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:16] \u001b[32mTrain: [ 49/50] Step 160/520 Loss 2.625 Prec@(1,5) (53.0%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:28:16] \u001b[32mTrain: [ 49/50] Step 180/520 Loss 2.632 Prec@(1,5) (53.0%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:28:16] \u001b[32mTrain: [ 49/50] Step 200/520 Loss 2.629 Prec@(1,5) (53.1%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:28:17] \u001b[32mTrain: [ 49/50] Step 220/520 Loss 2.633 Prec@(1,5) (53.0%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:17] \u001b[32mTrain: [ 49/50] Step 240/520 Loss 2.630 Prec@(1,5) (53.1%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:18] \u001b[32mTrain: [ 49/50] Step 260/520 Loss 2.626 Prec@(1,5) (53.2%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:18] \u001b[32mTrain: [ 49/50] Step 280/520 Loss 2.625 Prec@(1,5) (53.3%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:28:18] \u001b[32mTrain: [ 49/50] Step 300/520 Loss 2.621 Prec@(1,5) (53.3%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:28:19] \u001b[32mTrain: [ 49/50] Step 320/520 Loss 2.616 Prec@(1,5) (53.4%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:19] \u001b[32mTrain: [ 49/50] Step 340/520 Loss 2.618 Prec@(1,5) (53.4%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:20] \u001b[32mTrain: [ 49/50] Step 360/520 Loss 2.614 Prec@(1,5) (53.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:20] \u001b[32mTrain: [ 49/50] Step 380/520 Loss 2.615 Prec@(1,5) (53.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:20] \u001b[32mTrain: [ 49/50] Step 400/520 Loss 2.619 Prec@(1,5) (53.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:28:21] \u001b[32mTrain: [ 49/50] Step 420/520 Loss 2.621 Prec@(1,5) (53.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:28:21] \u001b[32mTrain: [ 49/50] Step 440/520 Loss 2.619 Prec@(1,5) (53.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:28:21] \u001b[32mTrain: [ 49/50] Step 460/520 Loss 2.613 Prec@(1,5) (53.6%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:22] \u001b[32mTrain: [ 49/50] Step 480/520 Loss 2.614 Prec@(1,5) (53.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:22] \u001b[32mTrain: [ 49/50] Step 500/520 Loss 2.614 Prec@(1,5) (53.5%, 73.6%)\u001b[0m\n",
      "[2024-01-15 10:28:23] \u001b[32mTrain: [ 49/50] Step 520/520 Loss 2.615 Prec@(1,5) (53.5%, 73.5%)\u001b[0m\n",
      "[2024-01-15 10:28:23] \u001b[32mTrain: [ 49/50] Final Prec@1 53.4700%\u001b[0m\n",
      "[2024-01-15 10:28:26] \u001b[32mValid: [ 49/50] Step 000/104 Loss 1.880 Prec@(1,5) (65.6%, 85.4%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Step 020/104 Loss 1.685 Prec@(1,5) (61.5%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Step 040/104 Loss 1.675 Prec@(1,5) (60.7%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Step 060/104 Loss 1.643 Prec@(1,5) (61.3%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Step 080/104 Loss 1.631 Prec@(1,5) (61.3%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Step 100/104 Loss 1.625 Prec@(1,5) (61.5%, 87.9%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Step 104/104 Loss 1.627 Prec@(1,5) (61.5%, 87.7%)\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mValid: [ 49/50] Final Prec@1 61.5300%\u001b[0m\n",
      "[2024-01-15 10:28:27] \u001b[32mEpoch 49 LR 0.000026\u001b[0m\n",
      "[2024-01-15 10:28:32] \u001b[32mTrain: [ 50/50] Step 000/520 Loss 2.601 Prec@(1,5) (54.2%, 78.1%)\u001b[0m\n",
      "[2024-01-15 10:28:32] \u001b[32mTrain: [ 50/50] Step 020/520 Loss 2.636 Prec@(1,5) (53.2%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:28:33] \u001b[32mTrain: [ 50/50] Step 040/520 Loss 2.632 Prec@(1,5) (53.3%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:33] \u001b[32mTrain: [ 50/50] Step 060/520 Loss 2.637 Prec@(1,5) (53.0%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:28:34] \u001b[32mTrain: [ 50/50] Step 080/520 Loss 2.639 Prec@(1,5) (52.8%, 72.9%)\u001b[0m\n",
      "[2024-01-15 10:28:34] \u001b[32mTrain: [ 50/50] Step 100/520 Loss 2.641 Prec@(1,5) (52.9%, 72.8%)\u001b[0m\n",
      "[2024-01-15 10:28:34] \u001b[32mTrain: [ 50/50] Step 120/520 Loss 2.653 Prec@(1,5) (52.7%, 72.7%)\u001b[0m\n",
      "[2024-01-15 10:28:35] \u001b[32mTrain: [ 50/50] Step 140/520 Loss 2.657 Prec@(1,5) (52.6%, 72.8%)\u001b[0m\n",
      "[2024-01-15 10:28:35] \u001b[32mTrain: [ 50/50] Step 160/520 Loss 2.641 Prec@(1,5) (52.9%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:28:36] \u001b[32mTrain: [ 50/50] Step 180/520 Loss 2.641 Prec@(1,5) (52.9%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:28:36] \u001b[32mTrain: [ 50/50] Step 200/520 Loss 2.634 Prec@(1,5) (53.0%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:28:36] \u001b[32mTrain: [ 50/50] Step 220/520 Loss 2.627 Prec@(1,5) (53.0%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:37] \u001b[32mTrain: [ 50/50] Step 240/520 Loss 2.620 Prec@(1,5) (53.3%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:28:37] \u001b[32mTrain: [ 50/50] Step 260/520 Loss 2.622 Prec@(1,5) (53.1%, 73.4%)\u001b[0m\n",
      "[2024-01-15 10:28:38] \u001b[32mTrain: [ 50/50] Step 280/520 Loss 2.624 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:38] \u001b[32mTrain: [ 50/50] Step 300/520 Loss 2.621 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:38] \u001b[32mTrain: [ 50/50] Step 320/520 Loss 2.627 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:39] \u001b[32mTrain: [ 50/50] Step 340/520 Loss 2.626 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:39] \u001b[32mTrain: [ 50/50] Step 360/520 Loss 2.628 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:40] \u001b[32mTrain: [ 50/50] Step 380/520 Loss 2.631 Prec@(1,5) (53.1%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:40] \u001b[32mTrain: [ 50/50] Step 400/520 Loss 2.627 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:40] \u001b[32mTrain: [ 50/50] Step 420/520 Loss 2.629 Prec@(1,5) (53.1%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:41] \u001b[32mTrain: [ 50/50] Step 440/520 Loss 2.631 Prec@(1,5) (53.1%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:41] \u001b[32mTrain: [ 50/50] Step 460/520 Loss 2.632 Prec@(1,5) (53.1%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:41] \u001b[32mTrain: [ 50/50] Step 480/520 Loss 2.634 Prec@(1,5) (53.0%, 73.1%)\u001b[0m\n",
      "[2024-01-15 10:28:42] \u001b[32mTrain: [ 50/50] Step 500/520 Loss 2.627 Prec@(1,5) (53.1%, 73.3%)\u001b[0m\n",
      "[2024-01-15 10:28:42] \u001b[32mTrain: [ 50/50] Step 520/520 Loss 2.629 Prec@(1,5) (53.1%, 73.2%)\u001b[0m\n",
      "[2024-01-15 10:28:42] \u001b[32mTrain: [ 50/50] Final Prec@1 53.1060%\u001b[0m\n",
      "[2024-01-15 10:28:46] \u001b[32mValid: [ 50/50] Step 000/104 Loss 1.913 Prec@(1,5) (64.6%, 86.5%)\u001b[0m\n",
      "[2024-01-15 10:28:46] \u001b[32mValid: [ 50/50] Step 020/104 Loss 1.693 Prec@(1,5) (61.4%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:28:46] \u001b[32mValid: [ 50/50] Step 040/104 Loss 1.680 Prec@(1,5) (60.8%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:28:46] \u001b[32mValid: [ 50/50] Step 060/104 Loss 1.649 Prec@(1,5) (61.4%, 87.3%)\u001b[0m\n",
      "[2024-01-15 10:28:47] \u001b[32mValid: [ 50/50] Step 080/104 Loss 1.638 Prec@(1,5) (61.4%, 87.5%)\u001b[0m\n",
      "[2024-01-15 10:28:47] \u001b[32mValid: [ 50/50] Step 100/104 Loss 1.632 Prec@(1,5) (61.5%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:28:47] \u001b[32mValid: [ 50/50] Step 104/104 Loss 1.634 Prec@(1,5) (61.5%, 87.6%)\u001b[0m\n",
      "[2024-01-15 10:28:47] \u001b[32mValid: [ 50/50] Final Prec@1 61.5400%\u001b[0m\n",
      "Final best Prec@1 = 61.8200%\n",
      "{'number=1': 0.5413000190734863, 'number=2': 0.6017000215530396, 'number=3': 0.6182000165939331}\n"
     ]
    }
   ],
   "source": [
    "from retrain import train, validate, fixed_arch\n",
    "# reload(train)\n",
    "\n",
    "config = {\n",
    "'layers' : layers,\n",
    "'batch_size' : batch_size,\n",
    "'log_frequency' : log_frequency,\n",
    "'epochs' : 50,\n",
    "'aux_weight' : 0.4,\n",
    "'drop_path_prob' : 0.1,\n",
    "'workers' : 4,\n",
    "'grad_clip' : 5.,\n",
    "'save_folder' : f\"./checkpoints/{dataset}/\",\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_train, dataset_valid = datasets.get_dataset(dataset, cutout_length=16)\n",
    "\n",
    "best_top1s = {}\n",
    "for number in [1, 2, 3]:\n",
    "    folder = config['save_folder'] + f\"random/{number}/\"\n",
    "    print(folder)\n",
    "    with fixed_arch(folder + 'arc.json'):\n",
    "        if dataset == 'fashionMNIST':\n",
    "            model = CNN(32, 1, 36, 10, config['layers'], auxiliary=True)\n",
    "        if dataset == 'cifar100':\n",
    "            model = CNN(32, 3, 36, 100, config['layers'], auxiliary=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'], eta_min=1E-6)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=config['workers'],\n",
    "                                            pin_memory=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset_valid,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config['workers'],\n",
    "                                            pin_memory=True)\n",
    "\n",
    "    best_top1 = 0.\n",
    "    for epoch in range(config['epochs']):\n",
    "        drop_prob = config['drop_path_prob'] * epoch / config['epochs']\n",
    "        model.drop_path_prob(drop_prob)\n",
    "\n",
    "        # training\n",
    "        train(config, train_loader, model, optimizer, criterion, epoch)\n",
    "\n",
    "        # validation\n",
    "        cur_step = (epoch + 1) * len(train_loader)\n",
    "        top1 = validate(config, valid_loader, model, criterion, epoch, cur_step)\n",
    "        best_top1 = max(best_top1, top1)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(), folder + \"mod.json\")\n",
    "    # torch.save(model.state_dict(), args.save_folder + \"/mod.json\")\n",
    "    print(\"Final best Prec@1 = {:.4%}\".format(best_top1))\n",
    "    best_top1s.update({f'number={number}' : best_top1})\n",
    "print(best_top1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[3]\n",
      "./checkpoints/cifar100/random\\3/arc.json\n",
      "[2024-01-15 20:24:24] \u001b[32mFixed architecture: {'reduce_n2_p0': 'sepconv5x5', 'reduce_n2_p1': 'maxpool', 'reduce_n3_p0': 'sepconv5x5', 'reduce_n3_p1': 'dilconv3x3', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'avgpool', 'reduce_n4_p1': 'dilconv3x3', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'sepconv5x5', 'reduce_n5_p0': 'dilconv5x5', 'reduce_n5_p1': 'sepconv5x5', 'reduce_n5_p2': 'sepconv3x3', 'reduce_n5_p3': 'sepconv5x5', 'reduce_n5_p4': 'skipconnect', 'reduce_n2_switch': [1], 'reduce_n3_switch': [2], 'reduce_n4_switch': [3], 'reduce_n5_switch': [3]}\u001b[0m\n",
      "Models in ensemble: 1\n",
      "[2024-01-15 20:24:28] \u001b[32mValid: Step 000/104 Loss 4.025 Prec@(1,5) (64.6%, 86.5%)\u001b[0m\n",
      "[2024-01-15 20:24:28] \u001b[32mValid: Step 030/104 Loss 4.049 Prec@(1,5) (60.8%, 87.1%)\u001b[0m\n",
      "[2024-01-15 20:24:29] \u001b[32mValid: Step 060/104 Loss 4.047 Prec@(1,5) (61.4%, 87.3%)\u001b[0m\n",
      "[2024-01-15 20:24:29] \u001b[32mValid: Step 090/104 Loss 4.047 Prec@(1,5) (61.4%, 87.6%)\u001b[0m\n",
      "[2024-01-15 20:24:29] \u001b[32mValid: Step 104/104 Loss 4.046 Prec@(1,5) (61.5%, 87.6%)\u001b[0m\n",
      "[2024-01-15 20:24:29] \u001b[32mFinal best Prec@1 = 61.5400%\u001b[0m\n",
      "0.6154000195503235\n"
     ]
    }
   ],
   "source": [
    "from retrain import train, validate, fixed_arch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from nni.retiarii.oneshot.pytorch.utils import AverageMeter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger('nni')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()\n",
    "n_chosen = 1\n",
    "\n",
    "config = {\n",
    "'layers' : 2,\n",
    "'batch_size' : 96,\n",
    "'log_frequency' : 30,\n",
    "'epochs' : 10,\n",
    "'aux_weight' : 0.4,\n",
    "'drop_path_prob' : 0.1,\n",
    "'workers' : 4,\n",
    "'grad_clip' : 5.,\n",
    "'save_folder' : \"./checkpoints/cifar100/random/\",\n",
    "}\n",
    "\n",
    "dataset_train, dataset_valid = datasets.get_dataset(\"cifar100\", cutout_length=16)\n",
    "\n",
    "res_dict_accur = {}\n",
    "models = []\n",
    "\n",
    "chosen_numbers = [3]\n",
    "\n",
    "print(chosen_numbers)\n",
    "\n",
    "for dir in glob(config['save_folder'] + \"*\"):\n",
    "    if int(dir.split('\\\\')[-1]) in chosen_numbers:\n",
    "        print(dir + \"/arc.json\")\n",
    "        with fixed_arch(dir + \"/arc.json\"):\n",
    "            model = CNN(32, 3, 36, 100, config['layers'], auxiliary=True, n_chosen=n_chosen)\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(dir + \"/mod.json\"))\n",
    "        model.eval()\n",
    "        \n",
    "        models.append(model)\n",
    "         \n",
    "\n",
    "print(f\"Models in ensemble: {len(models)}\")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid,\n",
    "                                            batch_size=config['batch_size'],\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=config['workers'],\n",
    "                                            pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "top1 = AverageMeter(\"top1\")\n",
    "top5 = AverageMeter(\"top5\")\n",
    "losses = AverageMeter(\"losses\")\n",
    "\n",
    "# validation\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for step, (X, y) in enumerate(valid_loader):\n",
    "        X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        bs = X.size(0)\n",
    "\n",
    "        probabilities = softmax(models[0](X))\n",
    "        for i in range(1, len(models)):\n",
    "            probabilities += softmax(models[i](X))\n",
    "        probabilities = probabilities / len(models)\n",
    "        loss = criterion(probabilities, y)\n",
    "\n",
    "        accuracy = utils.accuracy(probabilities, y, topk=(1, 5))\n",
    "        losses.update(loss.item(), bs)\n",
    "        top1.update(accuracy[\"acc1\"], bs)\n",
    "        top5.update(accuracy[\"acc5\"], bs)\n",
    "\n",
    "        if step % config['log_frequency'] == 0 or step == len(valid_loader) - 1:\n",
    "            logger.info(\n",
    "                \"Valid: Step {:03d}/{:03d} Loss {losses.avg:.3f} \"\n",
    "                \"Prec@(1,5) ({top1.avg:.1%}, {top5.avg:.1%})\".format(\n",
    "                    step, len(valid_loader) - 1, losses=losses,\n",
    "                    top1=top1, top5=top5))\n",
    "\n",
    "logger.info(\"Final best Prec@1 = {:.4%}\".format(top1.avg))\n",
    "\n",
    "# res_dict_accur[chosen_lambdas] = top1.avg\n",
    "print(top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
