\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{biblatex}
\addbibresource{references.bib}


\title{Differentiable algorithm for searching ensembles of deep learning models with diversity control}

\author{K. Yakovlev, O.Bakhteev, K. Petrushina, P. Babkin
	%% David S.~Hippocampus\thanks{Use footnote for providing further
	%%	information about author (webpage, alternative
	%%	address)---\emph{not} for acknowledging funding agencies.} \\
	%%Department of Computer Science\\
	%%Cranberry-Lemon University\\
	%%Pittsburgh, PA 15213 \\
	%%\texttt{hippo@cs.cranberry-lemon.edu} \\
	%% examples of more authors
	%%\And
	%%Elias D.~Striatum \\
	%%Department of Electrical Engineering\\
	%%Mount-Sheikh University\\
	%%Santa Narimana, Levand \\
	%%\texttt{stariate@ee.mount-sheikh.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{differentiable ensembles search}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Differentiable algorithm for searching ensembles of deep learning models with diversity control},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={K. Yakovlev, O.Bakhteev, K. Petrushina, P. Babkin},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	
This paper is developed to introduce a new method of creating ensembles of deep learning models. 
Many modern researches were focused on creating effective and efficient algorithms of differentiable architecture search,
missing oppotunity to create ensembles of deep learning models. This approach gives impressive results
as it was shown in few modern papers. In our research we investigate an algorithm of sampling deep learning models using
hypernetwork, which controls diversity of the models. This method allows us to sample deep learning models in one-shot,
withiout any additional calculational losses, this independence means that algorithm is able to sample as many models
as we want. To evaluate the performance of the proposed algorithm, we conducted experiments on the Fashion-MNIST and CIFAR-10
datasets and compare the resulting ensembles with ones sampled by other searching algorithms.

\end{abstract}


\keywords{ differential search \and neural emsembles \and hypernetwork \and diversity control }

\section{Introduction}

Nowadays algorithms of neural architecture search (NAS) are well-explored and prooved to be an effective way of creating
more effective and efficient neural networks. Such algorithms use different ways to make problem diffirentiable so it can
be solved by wide range of methods for smooth optimization problems. On the other hand, neural ensemble search (NES) is
modern and not as well investigated problem as NAS, although it is known that ensembles of deep learning models show better
results in different applied problems.

Our paper investigates an algorithm of sampling deep learning models in a new way. Despite the fact that similar algorithms
were discribed and investigated, our scheme has uniqeue set of techniques that gives compatible results and has its own sphere
of implimentations.

First of all, we use DARTS as our basic NAS algorithm. It is prooved to be an effective architecture search algorithm. Some
modern investigations have shown that the algorithm can be upgrated, but the modernisations do not make tangible difference
and are mainly made for some specific cases. So in our paper we are focusing on ensemble sampling, so slightly different base
model does not change anything in resulting ensemble.

Second of all, we use hypernetwork in our sampling algorithm. Hypernetwork is small network so it does not
consume too much computational capacity. This network contains information about another network, which is called target network.
This approach is not new: hypernetworks and supernetworks were intended to control different characteristics such as
complexity of acrhitecture or parameters of the target model in several modern investigations. In our algorithm it controls 
diversity of the target models, so every sampled model differs from previously sampled ones.

The hypernetwork uses Jensen-Shennon divergence (JSd) to measure difference between two architectures which is simmetric and finite
in contrast to more popular Kullbackâ€“Leibler divergence. Our main idea of sampling different model is to use a reguralizer,
based on JSd as a source of diversity.

This way we are able to sample deep learning models in one-shot, without and additional computational losses.
To sum up the scheme of our method:
\begin{enumerate} 
    \item find a base architectire using DARTS
    \item sample architectures in one-shot via differentiable algorithm
    \item inferencing answer is ensemble of the sampled deep learning models
\end{enumerate}

\printbibliography

\end{document}