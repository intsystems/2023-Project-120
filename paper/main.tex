\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{{../figures}}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\usepackage{amsmath}

\usepackage{doi}


\title{Differential neural ensemble search with diversity control}

\author{K. Yakovlev, O.Bakhteev, K. Petrushina, P. Babkin
	%% David S.~Hippocampus\thanks{Use footnote for providing further
	%%	information about author (webpage, alternative
	%%	address)---\emph{not} for acknowledging funding agencies.} \\
	%%Department of Computer Science\\
	%%Cranberry-Lemon University\\
	%%Pittsburgh, PA 15213 \\
	%%\texttt{hippo@cs.cranberry-lemon.edu} \\
	%% examples of more authors
	%%\And
	%%Elias D.~Striatum \\
	%%Department of Electrical Engineering\\
	%%Mount-Sheikh University\\
	%%Santa Narimana, Levand \\
	%%\texttt{stariate@ee.mount-sheikh.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{differentiable ensembles search}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Differentiable algorithm for searching ensembles of deep learning models with diversity control},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={P.Babkin, K.Petrushina, K.Yakovlev, O.Bakhteev},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	
This paper introduces a new method of deep learning ensemble contsruction.
In our research we investigate an algorithm of sampling the deep learning models using
hypernetwork. Hypernetwork is a neural network which controls diversity of the models. It translates a real number into an architecture of a sampled neural network. This method samples deep learning models in one-shot regime, without any additional calculations losses.
To evaluate the performance of the proposed algorithm, we conduct experiments on the Fashion-MNIST and CIFAR-10
datasets and compare the resulting ensembles with ones sampled by other searching algorithms.

\end{abstract}


\keywords{ differential search \and neural ensembles \and hypernetwork \and diversity control }

\section{Introduction}

Nowadays methods of neural architecture search (NAS) are well-explored and proved to be an effective way of creating
more effective and efficient neural networks \citep{darts, robustify, xnas}. Some of these methods use different ways to smooth out the architecture so optimum for it can
be found by wide range of methods for smooth optimization problems. On the other hand, neural ensemble search (NES) is
modern and not as well investigated problem as NAS, although it is known that ensembles of deep learning models show better
results in different applied problems \citep{multi-head}.
Our paper investigates an method of sampling deep learning models in a new way that gives compatible results and has its own sphere
of implementations.

Our method takes the result of NAS as a base architecture than it samples architectures which are close to the optimal one in terms of Jensen-Shennon divergence (JSd). Method can control whether sampled architectures are close enough to the optimal one so they give good results on the original dataset and are diverse enough so every architecture makes its own contribution to the final answer.

To sample architectures we use hypernetwork. This network generates parameters for another network, which is called target network.
Previously hypernetwoks were intended to control different characteristics such as
complexity of architecture \citep{darts-cc} or parameters of the target model \citep{cont-learn} in several modern investigations. In our paper it controls 
diversity of the target models, so every sampled model differs from previously sampled ones in terms of JSd.

The hypernetwork uses JSd to measure difference between two architectures which is symmetric and finite
in contrast to more popular Kullbackâ€“Leibler divergence. Our main idea of sampling different model is to use a regularizer,
based on JSd as a source of diversity.

This way we are able to sample deep learning models in one-shot, without and additional computational losses.
To sum up the scheme of our method: find a base architecture using DARTS, sample architectures in one-shot via differentiable algorithm. Inference answer is ensemble of the sampled deep learning models.

We conducted experiments on CIFAR and MNIST datasets to evaluate performance of the proposed method in terms of accuracy and time. Also we compare the performance with state-of-art NES and NAS algorithms \citep{darts, ???}.

\section{Method}

\subsection{Problem statement}

Contrary to the selection of one single architecture in conventional NAS algorithm, this paper focuses on the problem of selecting a well-performing neural network ensemble with diverse architectures from the NAS search space, i.e., neural ensemble search (NES). We use following terms
\begin{itemize}
    \item $\alpha$ -- an architecture of a model, i.e. a set of operations between nodes
    \item $\alpha^*$ -- a NAS resulting architecture, i.e. an optimal architecture
    \item $w_\alpha^*$ -- optimal parameters for architecture $\alpha$
    \item $\lambda$ -- a measure of diversity, a real number from $0$ to $\Lambda$
    \item $\alpha(\lambda)$ -- an architecture that corresponds $\lambda$ 
    \item $f(w_\alpha, \alpha)$ -- the output of an architecture $\alpha$ and model parameter $w_\alpha$
    \item $S$ -- a set of architectures included into ensemble
    \item $\mathcal{L}_{train}$, $\mathcal{L}_{val}$ -- the training and validation losses, respectively. We imply their dependence on preset dataset and do not denote it explicitly, because we dont deal with it
\end{itemize}

Given the ensemble scheme, NES can be formally framed as

\begin{gather*}
	\min_S \mathcal{L}_{val}\left(\frac{1}{|S|}\sum_{\alpha \in S}f(w_\alpha^*, \alpha)\right) \\
s.t. \text{ }\forall \alpha \in S \text{ } w_\alpha^* = \arg \min_w \mathcal{L}_{train}(f(w_\alpha^*, \alpha))
\end{gather*}

We rearranged the problem:  general for all architectures. Also architectures differ in terms of $\lambda$ so resulting functions can be calculated in terms of expected values.

\begin{gather*}
    \min_{\alpha} \mathbb{E}_\lambda [\mathcal{L}_{val}(w^*, \alpha(\lambda)) - \lambda JS(\alpha^*, \alpha(\lambda))] \\
    s.t. \text{ } w^* = \arg \min_w \mathbb{E}_\lambda[\mathcal{L}_{train}(w, \alpha(\lambda))]
\end{gather*}

\subsection{Computational experiment}

In our model $\lambda$ is a random value, distributed according to uniform distribution from 0 to $\Lambda$ ($\sim U(0, \Lambda)$). Main goal of basic experiment was to estimate $\Lambda$ and also to investigate dependence of resulting architecture's performance on $\lambda$.

\subsubsection{Experiment planning}

In our basic experiment we run the algorithm on fashionMNIST dataset for several different $\lambda$ and looked at resulting architecture and accuracy. Further we will use $\lambda$ distributed randomly, but in basic experiment it is fixed.

\subsubsection{Preliminary report}

Obtained results are depicted in the table below. Maximal accuracy and amount of matching edges between resulting and optimal architecture are written down.

This information allows us to choose $\Lambda = 32$. However amount of matching edges does not really coincide with our expectations. Architectures almost do not intersect. It can be explained by the fact that the optimized function is not convex, so algorithm fall into different minimums, however we can get more matching edges by using negative $\lambda$. 

\begin{table}[h   ]
	\caption{preliminary results}
	\centering
	\begin{tabular}{lll}

		\midrule
		$\lambda$     & accuracy, \% & matched edges \\
		\midrule
        0 (optimum) & 91.24 & - \\ 
        1/2  & 91.24 &  4   \\
		2  & 91.26   &  3   \\
		32 & 89.21   &  2   \\
		64 & 87.09   &  1   \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}







\bibliography{references}

\end{document}